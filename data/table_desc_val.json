[
    {
        "table_id_paper": "D16-1019table_4",
        "description": "Results. Table 4 show the results in the raw setting. On each dataset we report the metrics on three sets: test-I, test-II, and the whole test set (denoted by test-all). Test-I contains test triples that cannot be directly inferred by performing pure logical inference on the training set, and hence might be intrinsically more difficult for the rules. The remaining test triples (i.e., the directly inferable ones) are included in Test-II. These triples have either been used directly as training instances in KALE-Pre, or encoded explicitly in training ground rules in KALE-Joint, making this set trivial for the rules to some extent. From the results, we can see that in both settings: (i) KALE-Pre and KALE-Joint outperform (or at least perform as well as) the other methods which use triples alone on almost all the test sets, demonstrating the superiority of incorporating logical rules. (ii) On the test-I sets which contain triples beyond the scope of pure logical inference, KALE-Joint performs significantly better than KALE-Pre. On these sets KALE-Joint can still beat all the baselines by a significant margin in most cases, while KALE-Pre can hardly outperform KALE-Trip. It demonstrates the capability of the joint embedding scenario to learn more predictive embeddings, through which we can make better predictions even beyond the scope of pure logical inference. (iii) On the test-II sets which contain directly inferable triples, KALE-Pre can easily beat all the baselines (even KALE-Joint). That means, for triples covered by pure logical inference, it is trivial to improve the performance by directly incorporating them as training instances.",
        "sentences": [
            "Results.",
            "Table 4 show the results in the raw setting.",
            "On each dataset we report the metrics on three sets: test-I, test-II, and the whole test set (denoted by test-all).",
            "Test-I contains test triples that cannot be directly inferred by performing pure logical inference on the training set, and hence might be intrinsically more difficult for the rules.",
            "The remaining test triples (i.e., the directly inferable ones) are included in Test-II.",
            "These triples have either been used directly as training instances in KALE-Pre, or encoded explicitly in training ground rules in KALE-Joint, making this set trivial for the rules to some extent.",
            "From the results, we can see that in both settings: (i) KALE-Pre and KALE-Joint outperform (or at least perform as well as) the other methods which use triples alone on almost all the test sets, demonstrating the superiority of incorporating logical rules.",
            "(ii) On the test-I sets which contain triples beyond the scope of pure logical inference, KALE-Joint performs significantly better than KALE-Pre.",
            "On these sets KALE-Joint can still beat all the baselines by a significant margin in most cases, while KALE-Pre can hardly outperform KALE-Trip.",
            "It demonstrates the capability of the joint embedding scenario to learn more predictive embeddings, through which we can make better predictions even beyond the scope of pure logical inference.",
            "(iii) On the test-II sets which contain directly inferable triples, KALE-Pre can easily beat all the baselines (even KALE-Joint).",
            "That means, for triples covered by pure logical inference, it is trivial to improve the performance by directly incorporating them as training instances."
        ],
        "class_sentence": [
            0,
            1,
            2,
            2,
            2,
            2,
            1,
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "Test-I",
                "Test-II",
                "Test-ALL"
            ],
            [
                "Test-I"
            ],
            [
                "Test-II"
            ],
            [
                "Test-I",
                "Test-II",
                "Test-ALL",
                "KALE-Pre",
                "KALE-Joint"
            ],
            [
                "KALE-Pre",
                "KALE-Joint"
            ],
            [
                "Test-I",
                "KALE-Joint",
                "KALE-Pre"
            ],
            [
                "KALE-Joint",
                "KALE-Pre",
                "KALE-Trip"
            ],
            null,
            [
                "Test-II",
                "KALE-Pre",
                "KALE-Joint",
                "TransE",
                "TransH",
                "TransR"
            ],
            null
        ],
        "n_sentence": 12.0,
        "table_id": "table_4",
        "paper_id": "D16-1019",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1038table_6",
        "description": "4.6 End-to-End Event Co-reference Results. Table 6 shows the performance comparison for endto-end event co-reference. We use both SSED and MSEP-EMD as event detection modules and we evaluate on standard co-reference metrics. Results on TAC-KBP show that \u201cSSED+SupervisedExtend\u201d achieves similar performance to the TAC top ranking system while the proposed MSEP event co-reference module helps to outperform supervised methods on B 3 and CEAFe metrics.",
        "sentences": [
            "4.6 End-to-End Event Co-reference Results.",
            "Table 6 shows the performance comparison for end-to-end event co-reference.",
            "We use both SSED and MSEP-EMD as event detection modules and we evaluate on standard co-reference metrics.",
            "Results on TAC-KBP show that \u201cSSED + SupervisedExtend\u201d achieves similar performance to the TAC top ranking system while the proposed MSEP event co-reference module helps to outperform supervised methods on B 3 and CEAFe metrics."
        ],
        "class_sentence": [
            0,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "TAC-KBP (Test Data)",
                "SSED + SupervisedExtend",
                "TAC-TOP",
                "CEAFe",
                "B3",
                "MSEP-EMD + MSEP-CorefESA+AUG+KNOW"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_6",
        "paper_id": "D16-1038",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1042table_3",
        "description": "We use the expectation maximization algorithm (Dempster et al., 1977) for the maximum likelihood estimate of \u00ce\u00bb parameter on the development part of the corpus. The influence of the number of roles on the perplexity is shown in Table 3 and the final results are shown in Table 2. Note that these perplexities are not comparable with those on Figure 3 (PPX(P(w)) vs. PPX(P(w, G))). Weights of LTLM and STLM when interpolated with MKN LM are shown on Figure 4. From the tables we can see several important findings. Standalone LTLM performs worse than MKN on both languages, however their combination leads to dramatic improvements compared with other LMs. Best results are achieved by 4- gram MKN interpolated with 1000 roles LTLM and the deterministic inference. The perplexity was improved by approximately 46% on English and 49% on Czech compared with standalone MKN. The deterministic inference outperformed the nondeterministic one in all cases. LTLM also significantly outperformed STLM where the syntactic dependency trees were provided as a prior knowledge. The joint learning of syntax and semantics of a sentence proved to be more suitable for predicting the words.",
        "sentences": [
            "We use the expectation maximization algorithm (Dempster et al., 1977) for the maximum likelihood estimate of \u00ce\u00bb parameter on the development part of the corpus.",
            "The influence of the number of roles on the perplexity is shown in Table 3 and the final results are shown in Table 2.",
            "Note that these perplexities are not comparable with those on Figure 3 (PPX(P(w)) vs. PPX(P(w, G))).",
            "Weights of LTLM and STLM when interpolated with MKN LM are shown on Figure 4.",
            "From the tables we can see several important findings.",
            "Standalone LTLM performs worse than MKN on both languages, however their combination leads to dramatic improvements compared with other LMs.",
            "Best results are achieved by 4- gram MKN interpolated with 1000 roles LTLM and the deterministic inference.",
            "The perplexity was improved by approximately 46% on English and 49% on Czech compared with standalone MKN.",
            "The deterministic inference outperformed the nondeterministic one in all cases.",
            "LTLM also significantly outperformed STLM where the syntactic dependency trees were provided as a prior knowledge.",
            "The joint learning of syntax and semantics of a sentence proved to be more suitable for predicting the words."
        ],
        "class_sentence": [
            2,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null,
            [
                "non-det. LTLM",
                "det. LTLM"
            ],
            [
                "4-gram MKN + det. LTLM"
            ],
            null,
            null,
            null,
            null
        ],
        "n_sentence": 11.0,
        "table_id": "table_3",
        "paper_id": "D16-1042",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1084table_5",
        "description": "Target in Tweet vs Not in Tweet. Table 5 shows results on the development set for BiCond, compared to the best unidirectional encoding model, TweetCondTar and the baseline model Concat, split by tweets that contain the target and those that do not. All three models perform well when the target is mentioned in the tweet, but less so when the targets are not mentioned explicitly. In the case where the target is mentioned in the tweet, biconditional encoding outperforms unidirectional encoding and unidirectional encoding outperforms Concat. This shows that conditional encoding is able to learn useful dependencies between the tweets and the targets.",
        "sentences": [
            "Target in Tweet vs Not in Tweet.",
            "Table 5 shows results on the development set for BiCond, compared to the best unidirectional encoding model, TweetCondTar and the baseline model Concat, split by tweets that contain the target and those that do not.",
            "All three models perform well when the target is mentioned in the tweet, but less so when the targets are not mentioned explicitly.",
            "In the case where the target is mentioned in the tweet, biconditional encoding outperforms unidirectional encoding and unidirectional encoding outperforms Concat.",
            "This shows that conditional encoding is able to learn useful dependencies between the tweets and the targets."
        ],
        "class_sentence": [
            0,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Method"
            ],
            [
                "Method",
                "inTwe"
            ],
            [
                "inTwe",
                "Yes",
                "Method"
            ],
            [
                "TweetCondTar",
                "BiCond"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_5",
        "paper_id": "D16-1084",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1116table_2",
        "description": "4.1 GeoQuery. The evaluation metric for GEOQUERY is the accuracy of exactly predicting the machine readable query. As results in Table 2 show, our supervised S2S baseline model performs slightly better than the comparable model by Dong and Lapata (2016). The semi-supervised SEQ4 model with the additional generated queries improves on it further.",
        "sentences": [
            "4.1 GeoQuery.",
            "The evaluation metric for GEOQUERY is the accuracy of exactly predicting the machine readable query.",
            "As results in Table 2 show, our supervised S2S baseline model performs slightly better than the comparable model by Dong and Lapata (2016).",
            "The semi-supervised SEQ4 model with the additional generated queries improves on it further."
        ],
        "class_sentence": [
            0,
            0,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "S2S",
                "Dong and Lapata (2016)"
            ],
            [
                "SEQ4"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "D16-1116",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1129table_4",
        "description": "Results in Table 4 suggest that the SVM-RBF baseline system performs poorly and its results are on par with a majority class baseline (not reported in detail). Both deep learning models significantly outperform the baseline, yielding Macro-F1 score about 0.35. The attention-based model performs better than simple BLSTM in two classes (C5 and C6), but the overall Macro-F1 score is not significantly better.",
        "sentences": [
            "Results in Table 4 suggest that the SVM-RBF baseline system performs poorly and its results are on par with a majority class baseline (not reported in detail).",
            "Both deep learning models significantly outperform the baseline, yielding Macro-F1 score about 0.35.",
            "The attention-based model performs better than simple BLSTM in two classes (C5 and C6), but the overall Macro-F1 score is not significantly better."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "SVM-RBF"
            ],
            [
                "Model",
                "M-F1"
            ],
            [
                "M-F1",
                "Class C5",
                "Class C6",
                "BLSTM",
                "BLSTM/ATT/CNN"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_4",
        "paper_id": "D16-1129",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1154table_2",
        "description": "Table 2 shows the perplexity evaluation on the PTB test set. As a first observation, we can clearly see that the proposed approach outperforms all other models for all configurations, in particular, RNN and LSTM. This observation includes other models that were reported in the literature, such as random forest LM (Xu and Jelinek, 2007), structured LM (Filimonov and Harper, 2009) and syntactic neural network LM (Emami and Jelinek, 2004). More particularly, we can conclude that LSRC, with an embedding size of 100, achieves a better performance than all other models while reducing the number of parameters by \u2248 29% and \u2248 17% compared to RNN and LSTM, respectively. Increasing the embedding size to 200, which is used by the other models, improves significantly the performance with a resulting NoP comparable to LSTM. The significance of the improvements obtained here over LSTM were confirmed through a statistical significance t-test, which led to p-values  \u2264 10^\u221210 for a significance level of 5% and 0.01%, respectively. The results of the deep models in Table 2 also show that adding a single non-recurrent hidden layer to LSRC can significantly improve the performance. In fact, the additional layer bridges the gap between the LSRC models with an embedding size of 100 and 200, respectively. The resulting architectures outperform the other deep recurrent models with a significant reduction of the number of parameters(for the embedding size 100), and without usage of dropout regularization, Lp and maxout units or gradient control techniques compared to the deep RNN4 (D-RNN).",
        "sentences": [
            "Table 2 shows the perplexity evaluation on the PTB test set.",
            "As a first observation, we can clearly see that the proposed approach outperforms all other models for all configurations, in particular, RNN and LSTM.",
            "This observation includes other models that were reported in the literature, such as random forest LM (Xu and Jelinek, 2007), structured LM (Filimonov and Harper, 2009) and syntactic neural network LM (Emami and Jelinek, 2004).",
            "More particularly, we can conclude that LSRC, with an embedding size of 100, achieves a better performance than all other models while reducing the number of parameters by \u2248 29% and \u2248 17% compared to RNN and LSTM, respectively.",
            "Increasing the embedding size to 200, which is used by the other models, improves significantly the performance with a resulting NoP comparable to LSTM.",
            "The significance of the improvements obtained here over LSTM were confirmed through a statistical significance t-test, which led to p-values  \u2264 10^\u221210 for a significance level of 5% and 0.01%, respectively.",
            "The results of the deep models in Table 2 also show that adding a single non-recurrent hidden layer to LSRC can significantly improve the performance.",
            "In fact, the additional layer bridges the gap between the LSRC models with an embedding size of 100 and 200, respectively.",
            "The resulting architectures outperform the other deep recurrent models with a significant reduction of the number of parameters(for the embedding size 100), and without usage of dropout regularization, Lp and maxout units or gradient control techniques compared to the deep RNN4 (D-RNN)."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "LSRC (100)",
                "LSRC (200)"
            ],
            null,
            [
                "Recurrent Models (1 Layer)"
            ],
            [
                "NoP",
                "LSTM (1L)"
            ],
            [
                "LSTM (1L)"
            ],
            [
                "D-LSRC (200)",
                "D-LSRC (100)"
            ],
            [
                "D-LSRC (200)",
                "D-LSRC (100)"
            ],
            [
                "Deep Recurrent Models"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_2",
        "paper_id": "D16-1154",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1159table_5",
        "description": "Table 5 reports perplexity on training and development sets, the labeled F1-score on WSJ Section 23, and the Tree Edit Distance (TED) of various systems. Tree Edit Distance (TED) calculates the minimum-cost sequence of node edit operations (delete, insert, rename) between a gold tree and a test tree. When decoding with beam size 10, the four new neural parsers can generate wellformed trees for almost all the 2416 sentences in the WSJ section 23. This makes TED a robust metric to evaluate the overall performance of each parser. Table 5 reports the average TED per sentence. Trees extracted from E2E and PE2PE encoding vectors (via models E2E2P and PE2PE2P, respectively) get TED above 30, whereas the NMT systems get approximately 17 TED. Among the well-formed trees, around half have a mismatch between number of leaves and number of tokens in the source sentence. The labeled F1-score is reported over the rest of the sentences only. Though biased, this still reflects the overall performance: we achieve around 80 F1 with NMT encoding vectors, much higher than with the E2E and PE2PE encoding vectors (below 60).",
        "sentences": [
            "Table 5 reports perplexity on training and development sets, the labeled F1-score on WSJ Section 23, and the Tree Edit Distance (TED) of various systems.",
            "Tree Edit Distance (TED) calculates the minimum-cost sequence of node edit operations (delete, insert, rename) between a gold tree and a test tree.",
            "When decoding with beam size 10, the four new neural parsers can generate wellformed trees for almost all the 2416 sentences in the WSJ section 23.",
            "This makes TED a robust metric to evaluate the overall performance of each parser.",
            "Table 5 reports the average TED per sentence.",
            "Trees extracted from E2E and PE2PE encoding vectors (via models E2E2P and PE2PE2P, respectively) get TED above 30, whereas the NMT systems get approximately 17 TED.",
            "Among the well-formed trees, around half have a mismatch between number of leaves and number of tokens in the source sentence.",
            "The labeled F1-score is reported over the rest of the sentences only.",
            "Though biased, this still reflects the overall performance: we achieve around 80 F1 with NMT encoding vectors, much higher than with the E2E and PE2PE encoding vectors (below 60)."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            1,
            2,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "E2E2P",
                "E2G2P",
                "E2F2P",
                "E2P"
            ],
            [
                "Average TED per sentence",
                "Model"
            ],
            [
                "Average TED per sentence"
            ],
            [
                "E2E2P",
                "PE2PE2P",
                "Average TED per sentence"
            ],
            null,
            null,
            [
                "E2E2P",
                "PE2PE2P"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_5",
        "paper_id": "D16-1159",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1165table_1",
        "description": "Table 1 shows the evaluation results on the TEST dataset for several variants of our pairwise neural network architecture. Adding both appropriateness and relatedness interactions (full network) yields an improvement of another two MAP points absolute (to 54.51), which shows that appropriateness features encode information that is complementary to the information modeled by relevance and relatedness.",
        "sentences": [
            "Table 1 shows the evaluation results on the TEST dataset for several variants of our pairwise neural network architecture.",
            "Adding both appropriateness and relatedness interactions (full network) yields an improvement of another two MAP points absolute (to 54.51), which shows that appropriateness features encode information that is complementary to the information modeled by relevance and relatedness."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Full Network",
                "MAP"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_1",
        "paper_id": "D16-1165",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1173table_3",
        "description": "To further evaluate the proposed mutual distillation framework for learning knowledge, we compare to an extensive set of other possible knowledge optimization approaches. Table 3 shows the results. In row 2, the \u201copt-joint\u201d method optimizes the regularized joint model of Eq.(2) drectly in terms of both the neural network and knowledge parameters. Row 3, \u201copt-knwl-pipeline\u201d, is an approach that first optimizes the standalone knowledge component and then inserts it into the previous framework of (Hu et al., 2016) as a fixed constraint. Without interaction between the knowledge and neural network learning, the pipelined method yields inferior results. Finally, rows 4-5 display a method that adapts the knowledge component at each iteration by optimizing the joint model q in terms of the knowledge parameters. We report the accuracy of both the student network p (row 4) and the joint teacher network q (row 5), and compare with our method in row 6 and 7, respectively. We can see that both models performs poorly, achieving the accuracy of only 68.6% for the knowledge component, similar to the accuracy achieved by the \u201copt-joint\u201d method.",
        "sentences": [
            "To further evaluate the proposed mutual distillation framework for learning knowledge, we compare to an extensive set of other possible knowledge optimization approaches.",
            "Table 3 shows the results.",
            "In row 2, the \u201copt-joint\u201d method optimizes the regularized joint model of Eq.(2) drectly in terms of both the neural network and knowledge parameters.",
            "Row 3, \u201copt-knwl-pipeline\u201d, is an approach that first optimizes the standalone knowledge component and then inserts it into the previous framework of (Hu et al., 2016) as a fixed constraint.",
            "Without interaction between the knowledge and neural network learning, the pipelined method yields inferior results.",
            "Finally, rows 4-5 display a method that adapts the knowledge component at each iteration by optimizing the joint model q in terms of the knowledge parameters.",
            "We report the accuracy of both the student network p (row 4) and the joint teacher network q (row 5), and compare with our method in row 6 and 7, respectively.",
            "We can see that both models performs poorly, achieving the accuracy of only 68.6% for the knowledge component, similar to the accuracy achieved by the \u201copt-joint\u201d method."
        ],
        "class_sentence": [
            0,
            1,
            2,
            2,
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "opt-joint"
            ],
            [
                "opt-knwl-pipeline"
            ],
            [
                "opt-knwl-pipeline"
            ],
            [
                "opt-joint-iterative-p",
                "opt-joint-iterative-q"
            ],
            [
                "opt-joint-iterative-p",
                "opt-joint-iterative-q",
                "mutual-p",
                "mutual-q"
            ],
            [
                "opt-joint-iterative-q",
                "opt-joint"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_3",
        "paper_id": "D16-1173",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1174table_3",
        "description": "As seen in Table 3 our approach shows competitive performance on the other four datasets. The YP-130 dataset focuses on verb similarity, whereas SimLex-999 contains verbs and adjectives and MEN-3K has word pairs with different parts of speech (e.g., a noun compared to a verb). The results we obtain on these datasets exhibit the reliability of our approach in modeling non-nominal word senses.",
        "sentences": [
            "As seen in Table 3 our approach shows competitive performance on the other four datasets.",
            "The YP-130 dataset focuses on verb similarity, whereas SimLex-999 contains verbs and adjectives and MEN-3K has word pairs with different parts of speech (e.g., a noun compared to a verb).",
            "The results we obtain on these datasets exhibit the reliability of our approach in modeling non-nominal word senses."
        ],
        "class_sentence": [
            1,
            2,
            1
        ],
        "header_mention": [
            [
                "DECONF"
            ],
            [
                "YP-130",
                "SimLex-999",
                "MEN-3K"
            ],
            [
                "DECONF"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_3",
        "paper_id": "D16-1174",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1175table_2",
        "description": "Table 2 shows the resulting feature spaces of the composed vectors. It is worth noting that any arithmetic operation can be used to combine the counts of the aligned features, however for this paper we use pointwise addition for both composition functions. One of the advantages of this approach to composition is that the inherent interpretability of count-based models naturally expands beyond the word level, allowing us to study the distributional semantics of phrases in the same space as words. Due to offsetting one of the constituents, the composition operation is not commutative and hence avoids identical representations for house boat and boat house. However, the typed nature of our vector space results in extreme sparsity, for example while the untyped VSM has 130k dimensions, our APT model can have more than 3m dimensions. We therefore need to enrich the elementary vector representations with the distributional information of their nearest neighbours to ease the sparsity effect and infer missing information. Due to the syntactic nature of our composition operation it is not straightforward to apply common dimensionality reduction techniques such as SVD, as the type information needs to be preserved.",
        "sentences": [
            "Table 2 shows the resulting feature spaces of the composed vectors.",
            "It is worth noting that any arithmetic operation can be used to combine the counts of the aligned features, however for this paper we use pointwise addition for both composition functions.",
            "One of the advantages of this approach to composition is that the inherent interpretability of count-based models naturally expands beyond the word level, allowing us to study the distributional semantics of phrases in the same space as words.",
            "Due to offsetting one of the constituents, the composition operation is not commutative and hence avoids identical representations for house boat and boat house.",
            "However, the typed nature of our vector space results in extreme sparsity, for example while the untyped VSM has 130k dimensions, our APT model can have more than 3m dimensions.",
            "We therefore need to enrich the elementary vector representations with the distributional information of their nearest neighbours to ease the sparsity effect and infer missing information.",
            "Due to the syntactic nature of our composition operation it is not straightforward to apply common dimensionality reduction techniques such as SVD, as the type information needs to be preserved."
        ],
        "class_sentence": [
            1,
            2,
            2,
            2,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null,
            null,
            null
        ],
        "n_sentence": 7.0,
        "table_id": "table_2",
        "paper_id": "D16-1175",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1186table_8",
        "description": "Table 8 shows the values for precision, recall, and F1 of the ExtraTreeClassifier. In brief, the introduced approach detects requirements in usergenerated content with an average F1-score of 91%.",
        "sentences": [
            "Table 8 shows the values for precision, recall, and F1 of the ExtraTreeClassifier.",
            "In brief, the introduced approach detects requirements in usergenerated content with an average F1-score of 91%."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "Precision",
                "Recall",
                "F1",
                "Class"
            ],
            [
                "Avg.",
                "F1"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_8",
        "paper_id": "D16-1186",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1189table_2",
        "description": "Our data consists of 5 subreddits (askscience, askmen, todayilearned, worldnews, nfl) with diverse topics and genres. In our experiments, in order to have long enough discussion threads, we filter out discussion trees with fewer than 100 comments. For each subreddit, we randomly partition 90% of the data for online training, and 10% of the data for testing (deployment). The basic subreddit statistics are shown in Table 1. We report the random policy performances and heuristic upper bound performances (averaged over 10,000 episodes) in Table 2 and Table 3.3 The upper bound performances are obtained using stabilized karma scores and offline constructed tree structure. The mean and standard deviation are obtained by 5 independent runs.",
        "sentences": [
            "Our data consists of 5 subreddits (askscience, askmen, todayilearned, worldnews, nfl) with diverse topics and genres.",
            "In our experiments, in order to have long enough discussion threads, we filter out discussion trees with fewer than 100 comments.",
            "For each subreddit, we randomly partition 90% of the data for online training, and 10% of the data for testing (deployment).",
            "The basic subreddit statistics are shown in Table 1.",
            "We report the random policy performances and heuristic upper bound performances (averaged over 10,000 episodes) in Table 2 and Table 3.3 The upper bound performances are obtained using stabilized karma scores and offline constructed tree structure.",
            "The mean and standard deviation are obtained by 5 independent runs."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null,
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_2",
        "paper_id": "D16-1189",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1199table_3",
        "description": "Table 3 summarizes the results of experiments on INFOBOXQA. The performance of the baselines indicates that unigram bag-of-words models are not sufficiently expressive for matching, Tri-CNN makes use of larger semantic units through its multiple channels. Tri-CNN makes use of larger semantic units through its multiple channels. The attention mechanism and the combination of an RNN and CNN in ATTN1511 achieves better results than RNN, but still performs slightly worse than the CNN model with weightsharing. The Siamese architecture allows an input\u2019s representation to be influenced by the other inputs. The convolution feature maps are thus encoded in a comparable scheme that is more amenable to a matching task. Our Tri-CNN model built on top of this weight-sharing architecture achieves the best performance. Tri-CNN computes the match by comparing the similarities between question\u2013attribute and answer\u2013attribute, which leads to improved results over models that compare the question and answer directly.",
        "sentences": [
            "Table 3 summarizes the results of experiments on INFOBOXQA.",
            "The performance of the baselines indicates that unigram bag-of-words models are not sufficiently expressive for matching, Tri-CNN makes use of larger semantic units through its multiple channels.",
            "Tri-CNN makes use of larger semantic units through its multiple channels.",
            "The attention mechanism and the combination of an RNN and CNN in ATTN1511 achieves better results than RNN, but still performs slightly worse than the CNN model with weightsharing.",
            "The Siamese architecture allows an input\u2019s representation to be influenced by the other inputs.",
            "The convolution feature maps are thus encoded in a comparable scheme that is more amenable to a matching task.",
            "Our Tri-CNN model built on top of this weight-sharing architecture achieves the best performance.",
            "Tri-CNN computes the match by comparing the similarities between question\u2013attribute and answer\u2013attribute, which leads to improved results over models that compare the question and answer directly."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            2,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Tri-CNN"
            ],
            null,
            [
                "RNN",
                "CNN",
                "ATTN1511",
                "MAP",
                "MRR"
            ],
            null,
            null,
            [
                "Tri-CNN",
                "MAP",
                "MRR"
            ],
            null
        ],
        "n_sentence": 8.0,
        "table_id": "table_3",
        "paper_id": "D16-1199",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1008table_8",
        "description": "Results. To perform our evaluation we chose the most recent multilingual task (SemEval 2015 task 13) which includes gold data for Italian and Spanish. As can be seen from Table 8 TrainO-Matic enabled IMS to perform better than the best participating system (Manion and Sainudiin, 2014, SUDOKU) in all three settings (All domains, Maths & Computer and Biomedicine). Its performance was in fact, 1 to 3 points higher, with a 6-point peak on Maths & Computer in Spanish and on Biomedicine in Italian. This demonstrates the ability of Train-O-Matic to enable supervised WSD systems to surpass state-of-theart knowledge-based WSD approaches in lowresourced languages without relying on manually curated data for training.",
        "sentences": [
            "Results.",
            "To perform our evaluation we chose the most recent multilingual task (SemEval 2015 task 13) which includes gold data for Italian and Spanish.",
            "As can be seen from Table 8 TrainO-Matic enabled IMS to perform better than the best participating system (Manion and Sainudiin, 2014, SUDOKU) in all three settings (All domains, Maths & Computer and Biomedicine).",
            "Its performance was in fact, 1 to 3 points higher, with a 6-point peak on Maths & Computer in Spanish and on Biomedicine in Italian.",
            "This demonstrates the ability of Train-O-Matic to enable supervised WSD systems to surpass state-of-theart knowledge-based WSD approaches in lowresourced languages without relying on manually curated data for training."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Train-O-Matic",
                "ALL",
                "Dataset"
            ],
            [
                "Best System",
                "F1",
                "Train-O-Matic",
                "Computers & Math",
                "Spanish",
                "Biomedicine",
                "Italian"
            ],
            [
                "Train-O-Matic"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_8",
        "paper_id": "D17-1008",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1215table_2",
        "description": "Table 2 shows the performance of the Match LSTM and BiDAF models against all four adversaries. Each model incurred a significant accuracy drop under every form of adversarial evaluation. ADDSENT made average F1 score across the four models fall from 75.7% to 31.3%. ADDANY was even more effective, making average F1 score fall to 6.7%. ADDONESENT retained much of the effectiveness of ADDSENT, despite being model independent. Finally, ADDCOMMON caused average F1 score to fall to46.1%, despite only adding common words.",
        "sentences": [
            "Table 2 shows the performance of the Match LSTM and BiDAF models against all four adversaries.",
            "Each model incurred a significant accuracy drop under every form of adversarial evaluation.",
            "ADDSENT made average F1 score across the four models fall from 75.7% to 31.3%.",
            "ADDANY was even more effective, making average F1 score fall to 6.7%.",
            "ADDONESENT retained much of the effectiveness of ADDSENT, despite being model independent.",
            "Finally, ADDCOMMON caused average F1 score to fall to46.1%, despite only adding common words."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "ADDSENT",
                "Match",
                "Single",
                "Ens.",
                "BiDAF"
            ],
            [
                "ADDANY"
            ],
            [
                "ADDONESENT",
                "ADDSENT"
            ],
            [
                "ADDCOMMON"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_2",
        "paper_id": "D17-1215",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1224table_3",
        "description": "Table 3 shows the comparison results based on this evaluation protocol (two-part evaluation II). We can see from Tables 2 and 3 that our proposed approach performs much better than the baseline methods over all three metrics.",
        "sentences": [
            "Table 3 shows the comparison results based on this evaluation protocol (two-part evaluation II).",
            "We can see from Tables 2 and 3 that our proposed approach performs much better than the baseline methods over all three metrics."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Our Approach",
                "Submodular",
                "R-1",
                "R-2",
                "R-SU4"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_3",
        "paper_id": "D17-1224",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1231table_6",
        "description": "Table 6 summarizes the results for different systems, including the baseline CNN model without using context information (this baseline uses pretrained embeddings and speaker change feature), and four different ways of using context: (a) predicted DA information (posterior probabilities) is combined with the current sentence's CNN-based representation; (b) applying a BLSTM on top of the sentence CNN representation; (c) hierarchical CNN that combines the current sentence's CNN representation with its neighbors; (d) sequence decoding by combining CNN posteriors with DA transition scores. From the results, we can see the positive effect when context information is used. All the methods using context yield significant improvement over the baseline (statistically significant based on t-test). Comparing representing context information via the DA labels of the previous utterances vs using the hierarchical CNN or RNN model, we see there is not much difference.",
        "sentences": [
            "Table 6 summarizes the results for different systems, including the baseline CNN model without using context information (this baseline uses pretrained embeddings and speaker change feature), and four different ways of using context: (a) predicted DA information (posterior probabilities) is combined with the current sentence's CNN-based representation; (b) applying a BLSTM on top of the sentence CNN representation; (c) hierarchical CNN that combines the current sentence's CNN representation with its neighbors; (d) sequence decoding by combining CNN posteriors with DA transition scores.",
            "From the results, we can see the positive effect when context information is used.",
            "All the methods using context yield significant improvement over the baseline (statistically significant based on t-test).",
            "Comparing representing context information via the DA labels of the previous utterances vs using the hierarchical CNN or RNN model, we see there is not much difference."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            [
                "CNN baseline no context",
                "CNN + DA predictions",
                "CNN + RNN/BLSTM",
                "CNN + CNN",
                "CNN prob + DA transition"
            ],
            [
                "CNN + DA predictions",
                "CNN + RNN/BLSTM",
                "CNN + CNN",
                "CNN prob + DA transition"
            ],
            [
                "CNN baseline no context",
                "CNN + DA predictions",
                "CNN + RNN/BLSTM",
                "CNN + CNN",
                "CNN prob + DA transition"
            ],
            [
                "CNN prob + DA transition"
            ]

        ],
        "n_sentence": 4.0,
        "table_id": "table_6",
        "paper_id": "D17-1231",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1252table_3",
        "description": "Table 3 summarizes the results of the MMRNbased semantic parsing systems and other strong baselines. The SP column reports the averaged F1 scores. Compared to the pipeline approach (MMRN-PIPELINE), the joint learning framework (MMRN-JOINT) improves significantly, reaching 68.1% F1. To compare different learning methods, we also apply REINFORCE (Williams, 1992), a popular policy gradient algorithm, to train our joint model using the same setting and reward function. MMRN-JOINT outperforms REIN-FORCE  and  its  variant,  REINFORCE+,  which renormalizes the probabilities of the sampled candidate  sequences. Its result is also better than the state-of-the-art STAGG system. Note that we use the same architectures and initialization procedures for MMRN-PIPELINE/JOINT and REINFORCE/REINFORCE+. Therefore, the superior performance of MMRN-JOINT shows that the joint learning plays a crucial role in addition to the choices of architecture. Comparing to STAGG, note that Yih et al.(2016) did not jointly train the entity linker and semantic parser together, but they did improve the results by taking the top 10 predictions of their entity linking system for re-ranking parses. Our algorithm further allows to update the entity linker with the labels for semantic parsing and shows superior performance.",
        "sentences": [
            "Table 3 summarizes the results of the MMRNbased semantic parsing systems and other strong baselines.",
            "The SP column reports the averaged F1 scores.",
            "Compared to the pipeline approach (MMRN-PIPELINE), the joint learning framework (MMRN-JOINT) improves significantly, reaching 68.1% F1.",
            "To compare different learning methods, we also apply REINFORCE (Williams, 1992), a popular policy gradient algorithm, to train our joint model using the same setting and reward function.",
            "MMRN-JOINT outperforms REIN-FORCE  and  its  variant,  REINFORCE+,  which renormalizes the probabilities of the sampled candidate  sequences.",
            "Its result is also better than the state-of-the-art STAGG system.",
            "Note that we use the same architectures and initialization procedures for MMRN-PIPELINE/JOINT and REINFORCE/REINFORCE+.",
            "Therefore, the superior performance of MMRN-JOINT shows that the joint learning plays a crucial role in addition to the choices of architecture.",
            "Comparing to STAGG, note that Yih et al.(2016) did not jointly train the entity linker and semantic parser together, but they did improve the results by taking the top 10 predictions of their entity linking system for re-ranking parses.",
            "Our algorithm further allows to update the entity linker with the labels for semantic parsing and shows superior performance."
        ],
        "class_sentence": [
            1,
            2,
            1,
            2,
            1,
            1,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "SP",
                "Avg. F1"
            ],
            [
                "MMRN-PIPELINE",
                "MMRN-JOINT",
                "SP",
                "Avg. F1"
            ],
            [
                "REINFORCE",
                "REINFORCE+"
            ],
            [
                "MMRN-JOINT",
                "REINFORCE",
                "REINFORCE+",
                "Avg. F1"
            ],
            [
                "MMRN-JOINT",
                "STAGG",
                "SP",
                "Avg. F1"
            ],
            null,
            [
                "MMRN-JOINT"
            ],
            [
                "STAGG",
                "SP",
                "Avg. F1"
            ],
            [
                "MMRN-JOINT",
                "EL",
                "F1"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_3",
        "paper_id": "D17-1252",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1274table_5",
        "description": "Previous studies (Tamang and Ji, 2011; Rodriguez et al., 2015; Zhi et al., 2015; Viswanathan et al., 2015; Rajani and Mooney, 2016a; Yu et al., 2014a; Rajani and Mooney, 2016b) for SFV trained supervised classifiers based on features such as confidence score of each response and system credibility. For comparison, we developed a new SFV approach: a new SVM classifier based on a set of features (docId, filler string, original predicted slot type and confidence score, new predicted slot type confidence score based on our neural architecture) for each response to take advantage of the redundant information from various system runs. Table 5 compares our SFV performance against previous reported scores on judging each response as true or false. We can see that our approach advances state-of-the-art methods.",
        "sentences": [
            "Previous studies (Tamang and Ji, 2011; Rodriguez et al., 2015; Zhi et al., 2015; Viswanathan et al., 2015; Rajani and Mooney, 2016a; Yu et al., 2014a; Rajani and Mooney, 2016b) for SFV trained supervised classifiers based on features such as confidence score of each response and system credibility.",
            "For comparison, we developed a new SFV approach: a new SVM classifier based on a set of features (docId, filler string, original predicted slot type and confidence score, new predicted slot type confidence score based on our neural architecture) for each response to take advantage of the redundant information from various system runs.",
            "Table 5 compares our SFV performance against previous reported scores on judging each response as true or false.",
            "We can see that our approach advances state-of-the-art methods."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Our Approach"
            ],
            [
                "F-score (%)"
            ],
            [
                "Our Approach",
                "F-score (%)"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_5",
        "paper_id": "D17-1274",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1276table_2",
        "description": "Table 2 shows the results on the ACE datasets, and these are our main results. Following previous works (Finkel and Manning, 2009; Lu and Roth, 2015), we report standard precision (P), recall (R) and F1-score percentage scores. The highest results (F1-score) and those results that are not significantly different from the highest results are highlighted in bold (based on bootstrap resampling test (Koehn, 2004), where p > 0.01). For ACE datasets, we make comparisons with the two versions of the linear-chain CRF baseline: LCRF (single) which does not support overlapping mentions at all and LCRF (multiple) which does not support overlapping mentions of the same type, as well as our implementation of the mention hypergraph baseline (Lu and Roth, 2015).",
        "sentences": [
            "Table 2 shows the results on the ACE datasets, and these are our main results.",
            "Following previous works (Finkel and Manning, 2009; Lu and Roth, 2015), we report standard precision (P), recall (R) and F1-score percentage scores.",
            "The highest results (F1-score) and those results that are not significantly different from the highest results are highlighted in bold (based on bootstrap resampling test (Koehn, 2004), where p > 0.01).",
            "For ACE datasets, we make comparisons with the two versions of the linear-chain CRF baseline: LCRF (single) which does not support overlapping mentions at all and LCRF (multiple) which does not support overlapping mentions of the same type, as well as our implementation of the mention hypergraph baseline (Lu and Roth, 2015)."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "P",
                "R",
                "F1"
            ],
            [
                "F1",
                "This work (STATE)",
                "This work (EDGE)"
            ],
            [
                "LCRF (single)",
                "LCRF (multiple)",
                "Lu and Roth (2015)",
                "This work (STATE)",
                "This work (EDGE)",
                "F1"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "D17-1276",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1279table_5",
        "description": "Table 5 details the performance of our method on the three categories at the span and token level. We observe significant improvement by using ULM+GRAPHINTERP and  ULM+GRAPHFEAT over  best  SemEval  and  our  best  supervised  system on all three categories at both token and span levels. We further observe that systems\u2019 performance on TASK classification is much lower than PROCESS and MATERIAL. This is in part because TASK is much less frequent than the other types. In addition, TASK keyphrases of ten include verb phrases while the other two do mains mainly consists of noun phrases. An analysis of confusion patterns show that the most frequent type confusions are between PROCESS and MATERIAL. However, we observe that ULM+GRAPHFEAT* can greatly reduce the confusion, with 3.5% relative improvement of PROCESSand 3.6% relative improvement of PROCESS over ULM+GRAPHINTERP on token level.",
        "sentences": [
            "Table 5 details the performance of our method on the three categories at the span and token level.",
            "We observe significant improvement by using ULM+GRAPHINTERP and  ULM+GRAPHFEAT over  best  SemEval  and  our  best  supervised  system on all three categories at both token and span levels.",
            "We further observe that systems\u2019 performance on TASK classification is much lower than PROCESS and MATERIAL.",
            "This is in part because TASK is much less frequent than the other types.",
            "In addition, TASK keyphrases of ten include verb phrases while the other two do mains mainly consists of noun phrases.",
            "An analysis of confusion patterns show that the most frequent type confusions are between PROCESS and MATERIAL.",
            "However, we observe that ULM+GRAPHFEAT* can greatly reduce the confusion, with 3.5% relative improvement of PROCESSand 3.6% relative improvement of PROCESS over ULM+GRAPHINTERP on token level."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "ULM+GRAPHINTERP",
                "ULM+GRAPHFEAT",
                "Best SemEval",
                "supervised",
                "Token Level",
                "Span Level"
            ],
            [
                "T",
                "P",
                "M"
            ],
            [
                "T",
                "P",
                "M",
                "K"
            ],
            [
                "T"
            ],
            [
                "P",
                "M"
            ],
            [
                "ULM+GRAPHFEAT",
                "P",
                "ULM+GRAPHINTERP"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_5",
        "paper_id": "D17-1279",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1284table_1",
        "description": "In Table 1 we present linking accuracy for our models that vary in the information they use. We see that the model that only encodes the contextinformation, Model C (L = Ltext) consistently performs better than picking the entity with the highest prior probability from CrossWikis, indicating that the model is able to utilize the context across datasets. On incorporating the description with context (Model CD) we see improvement in the performance on ACE-2005, but slight decrease in CoNLL, suggesting the entity descriptions are not extremely useful for the latter (it contains rare entities, many short and incomplete sentences, and specific entities as annotations for metonymic mentions, as also observed by Ling et al.(2015)). On introducing the entity type-aware loss in Model CT to the context-only model, we see significantly improved results for all datasets, demonstrating that explicitly modeling fine-grained types helps learning a better context encoder and, in turn, typeaware entity representations. Combining descriptions with this model (Model CDT) shows further gains in accuracy indicating that our model is able to exploit complementary information from the two sources. Finally, on introducing explicit entity-type encoding, Model CDTE performs the best on two of the four datasets. As we will see in \u00a7 6.2, encoding entity-type information also allows our models to easily generalize to new entities.",
        "sentences": [
            "In Table 1 we present linking accuracy for our models that vary in the information they use.",
            "We see that the model that only encodes the contextinformation, Model C (L = Ltext) consistently performs better than picking the entity with the highest prior probability from CrossWikis, indicating that the model is able to utilize the context across datasets.",
            "On incorporating the description with context (Model CD) we see improvement in the performance on ACE-2005, but slight decrease in CoNLL, suggesting the entity descriptions are not extremely useful for the latter (it contains rare entities, many short and incomplete sentences, and specific entities as annotations for metonymic mentions, as also observed by Ling et al.(2015)).",
            "On introducing the entity type-aware loss in Model CT to the context-only model, we see significantly improved results for all datasets, demonstrating that explicitly modeling fine-grained types helps learning a better context encoder and, in turn, typeaware entity representations.",
            "Combining descriptions with this model (Model CDT) shows further gains in accuracy indicating that our model is able to exploit complementary information from the two sources.",
            "Finally, on introducing explicit entity-type encoding, Model CDTE performs the best on two of the four datasets.",
            "As we will see in \u00a7 6.2, encoding entity-type information also allows our models to easily generalize to new entities."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1,
            0
        ],
        "header_mention": [
            null,
            [
                "Model C",
                "Priors",
                "Wiki"
            ],
            [
                "Model CD",
                "ACE05",
                "CoNLL"
            ],
            [
                "Model CT",
                "CoNLL",
                "ACE05",
                "Wiki"
            ],
            [
                "Model CDT"
            ],
            [
                "Model CDTE",
                "Model C",
                "Model CD",
                "Model CT",
                "Model CDT"
            ],
            null
        ],
        "n_sentence": 7.0,
        "table_id": "table_1",
        "paper_id": "D17-1284",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1310table_2",
        "description": "Table 2 depicts experiment results. Compared to the best systems in SemEval challenge, MIN achieves 3.0% and 1.1% absolute gains on D1 and D2 respectively. Besides, our MIN outperforms WDEmb, a strong CRF-based system benefiting from several kinds of useful word embeddings, by 2.1% on D1. With memory interactions and consideration of sentimental sentence, our MIN boosts the performance of vanilla bi-directional LSTM (+2.0% and +1.7% respectively). It validates the effectiveness of the manually designed memory operations and the proposed memory interaction mechanism. MIN also outperforms the state-of-the-art RNCRF on each dataset suggesting that memory interactions can be an alternative strategy instead of syntactic parsing. To further study the impact of each element in MIN, we conduct ablation experiments.",
        "sentences": [
            "Table 2 depicts experiment results.",
            "Compared to the best systems in SemEval challenge, MIN achieves 3.0% and 1.1% absolute gains on D1 and D2 respectively.",
            "Besides, our MIN outperforms WDEmb, a strong CRF-based system benefiting from several kinds of useful word embeddings, by 2.1% on D1.",
            "With memory interactions and consideration of sentimental sentence, our MIN boosts the performance of vanilla bi-directional LSTM (+2.0% and +1.7% respectively).",
            "It validates the effectiveness of the manually designed memory operations and the proposed memory interaction mechanism.",
            "MIN also outperforms the state-of-the-art RNCRF on each dataset suggesting that memory interactions can be an alternative strategy instead of syntactic parsing.",
            "To further study the impact of each element in MIN, we conduct ablation experiments."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Our Work",
                "D1",
                "D2"
            ],
            [
                "Our Work",
                "WDEmb",
                "D1"
            ],
            [
                "Our Work",
                "LSTM",
                "D1",
                "D2"
            ],
            [
                "Our Work"
            ],
            [
                "Our Work",
                "RNCRF"
            ],
            [
                "Our Work"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_2",
        "paper_id": "D17-1310",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1320table_4",
        "description": "Table 4 shows the performance of the baseline. An analysis of the single pipeline steps revealed major bottlenecks of the method and challenges of the task. First, we observed that around 76% of gold concepts are covered by the extraction (step 1+2), while the top 25 concepts (step 5) only contain 17% of the gold concepts. Hence, content selection is a major challenge, stemming from the large cluster sizes in the corpus. Second, while also 17% of gold concepts are contained in the final maps (step 6), scores for strict proposition matching are low, indicating a poor performance of the relation extraction (step 3). The propagation of these errors along the pipeline contributes to overall low scores.",
        "sentences": [
            "Table 4 shows the performance of the baseline.",
            "An analysis of the single pipeline steps revealed major bottlenecks of the method and challenges of the task.",
            "First, we observed that around 76% of gold concepts are covered by the extraction (step 1+2), while the top 25 concepts (step 5) only contain 17% of the gold concepts.",
            "Hence, content selection is a major challenge, stemming from the large cluster sizes in the corpus.",
            "Second, while also 17% of gold concepts are contained in the final maps (step 6), scores for strict proposition matching are low, indicating a poor performance of the relation extraction (step 3).",
            "The propagation of these errors along the pipeline contributes to overall low scores."
        ],
        "class_sentence": [
            1,
            2,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "METEOR",
                "F1"
            ],
            null,
            [
                "Strict Match"
            ],
            [
                "ROUGE-2"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_4",
        "paper_id": "D17-1320",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1016table_3",
        "description": "Table 3 shows the model performances of the training set that is annotated once, and the training set of the merged annotation. The performance difference is quite significant. Furthermore, the difference is consistent with Table 2: the \"AB-merge\" model has a similar precision as the \"Once\" model, but it has a much higher recall. It indicates that a further refinement of the training set such as DEF-voting could be essential.",
        "sentences": [
            "Table 3 shows the model performances of the training set that is annotated once, and the training set of the merged annotation.",
            "The performance difference is quite significant.",
            "Furthermore, the difference is consistent with Table 2: the \"AB-merge\" model has a similar precision as the \"Once\" model, but it has a much higher recall.",
            "It indicates that a further refinement of the training set such as DEF-voting could be essential."
        ],
        "class_sentence": [
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            [
                "Once",
                "AB-merge"
            ],
            null,
            [
                "Once",
                "AB-merge",
                "Avg. Prec",
                "Avg. Rec"
            ],
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "D18-1016",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1020table_4",
        "description": "The results in Table 4 demonstrate that compared to the baseline BiGRU, adding the reconstruction loss (\"VSL-G, no VR\") yields only 0.1 improvement for both Twitter and NER. Although adding hierarchical structure further improves performance, the improvements are small (+0.1 and +0.2 for Twitter and NER respectively). For VSL-GG-Hier, variational regularization accounts for relatively large differences of 0.6 for Twitter and 0.5 for NER. These results show that the improvements do not come solely from adding a reconstruction objective to the learning procedure. In limited preliminary experiments, we did not find a benefit from adding unlabeled data under the \"no VR\" setting.",
        "sentences": [
            "The results in Table 4 demonstrate that compared to the baseline BiGRU, adding the reconstruction loss (\"VSL-G, no VR\") yields only 0.1 improvement for both Twitter and NER.",
            "Although adding hierarchical structure further improves performance, the improvements are small (+0.1 and +0.2 for Twitter and NER respectively).",
            "For VSL-GG-Hier, variational regularization accounts for relatively large differences of 0.6 for Twitter and 0.5 for NER.",
            "These results show that the improvements do not come solely from adding a reconstruction objective to the learning procedure.",
            "In limited preliminary experiments, we did not find a benefit from adding unlabeled data under the \"no VR\" setting."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            [
                "BiGRU baseline",
                "VSL-G",
                "acc.",
                "F1",
                "no VR",
                "Twitter",
                "NER"
            ],
            [
                "VSL-GG-Hier",
                "VSL-G",
                "no VR",
                "Twitter",
                "NER"
            ],
            [
                "VSL-GG-Hier",
                "Twitter",
                "NER",
                "acc.",
                "F1",
                "no VR"
            ],
            [
                "VSL-GG-Hier"
            ],
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_4",
        "paper_id": "D18-1020",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1023table_6",
        "description": "Monolingual embedding quality evaluation. Table 6 shows the name tagging performance for each language using the original monolingual embeddings and multilingual embeddings. For all languages, the multilingual embeddings learned from our approach significantly outperform those learned from MultiCCA and MultiCluster, which shows the effectiveness of our approach. More importantly, the multilingual embeddings learned from our approach also outperform original monolingual embeddings, which demonstrates that by projecting multiple languages into one common space, the monolingual embedding quality can be further improved.",
        "sentences": [
            "Monolingual embedding quality evaluation.",
            "Table 6 shows the name tagging performance for each language using the original monolingual embeddings and multilingual embeddings.",
            "For all languages, the multilingual embeddings learned from our approach significantly outperform those learned from MultiCCA and MultiCluster, which shows the effectiveness of our approach.",
            "More importantly, the multilingual embeddings learned from our approach also outperform original monolingual embeddings, which demonstrates that by projecting multiple languages into one common space, the monolingual embedding quality can be further improved."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Monolingual",
                "Multilingual"
            ],
            [
                "Amh",
                "Tig",
                "Uig",
                "Tur",
                "Multilingual",
                "MultiCCA",
                "MultiCluster",
                "CorrNet",
                "W+N+C+L"
            ],
            [
                "Monolingual",
                "CorrNet",
                "W+N+C+L"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_6",
        "paper_id": "D18-1023",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1048table_2",
        "description": "4.2 Results on Chinese-English Translation. The experimental results of our model and baseline models on Chinese-English machine translation datasets are depicted in Table 2. Parameters. RNNSearch, Deliberation Network and ABDNMT have 83.99M, 125.16M and 122.86M parameters, respectively. And the parameter size of our {2,3,4,5}-pass decoder and adaptive multi-pass decoder are about 87.81M and 96.01M, respectively. Fixed Decoding Depth. {2,3,4,5}-pass decoders perform the left-to-right decoding by the multipass decoder with a fixed number of decoding passes. In contrast to the related machine translation systems, our fixed number-pass decoder significantly outperforms Moses and RNNSearch by 7.53 and 1.05 BLEU points at least, as Table 2 presents. More importantly, our proposed multipass decoder obtains much better performance with an increase of only 3.82M parameters over RNNSearch. As a comparison with Deliberation Network involves two-pass decoding, the multipass decoder has a minimum increase of 0.24 BLEU score. Nevertheless, our multi-pass decoder proves its effectiveness due to the less parameters consumption of 37.35M in contrast to Deliberation Network. These results verify our hypothesis that the more decoding passes can polish the generated output to improve the translation quality. The underlying reason is that the attention component attndec within our multi-pass decoder can capture the extra target-side contexts to obtain a global understanding to assist the translation procedure. Towards the effect of the decoding depth set {2,3,4,5}, our multi-pass decoders obtain the approximate results, but the whole curve of BLEU is on an upward trend. Specifically, the multi-pass decoder with decoding depth 5 achieves the best performance with 38.64 BLEU, while the one with decoding depth 3 performs the worst among the decoding depth set with 38.55 BLEU. Although the average results of {2,3,4,5}-pass decoder are approximate, the distinction of {2,3,4,5}-pass decoder on NIST03, NIST04, NIST05, NIST06 and NIST08 is not negligible. These results indirectly prove the necessity of flexibility mechanism. Adaptive Decoding Depth. our proposed adaptive multi-pass decoder involves an extra policy network which controls the decoding depth according to the complexity of the source sentence and the differences between the consecutive generated translations. As shown in Table 2, the proposed adaptive multi-pass decoder obtains an improvement about 0.41 to 0.5 BLEU on average over the {2,3,4,5}-pass decoder, which demonstrates the effectiveness of the policy network. Specifically, the adaptive multi-pass decoder outperforms the multi-pass decoder with a fixed decoding depth by 0.69, 0.71, 0.68 and 0.45 BLEU scores on NIST03, NIST04, NIST05 and NIST06 datasets at most. In contrast to the Moses, RNNSearch, Deliberation Network and ABDNMT, the adaptive multi-pass decoder has the corresponding improvement about 8.03, 1.55, 0.74 and 0.34 BLEU points, respectively. More importantly, our adaptive multi-pass decoder outperforms ABDNMT, Deliberation Network model with a decrease of 26.85M, 29.15M parameters.",
        "sentences": [
            "4.2 Results on Chinese-English Translation.",
            "The experimental results of our model and baseline models on Chinese-English machine translation datasets are depicted in Table 2.",
            "Parameters.",
            "RNNSearch, Deliberation Network and ABDNMT have 83.99M, 125.16M and 122.86M parameters, respectively.",
            "And the parameter size of our {2,3,4,5}-pass decoder and adaptive multi-pass decoder are about 87.81M and 96.01M, respectively.",
            "Fixed Decoding Depth.",
            "{2,3,4,5}-pass decoders perform the left-to-right decoding by the multipass decoder with a fixed number of decoding passes.",
            "In contrast to the related machine translation systems, our fixed number-pass decoder significantly outperforms Moses and RNNSearch by 7.53 and 1.05 BLEU points at least, as Table 2 presents.",
            "More importantly, our proposed multipass decoder obtains much better performance with an increase of only 3.82M parameters over RNNSearch.",
            "As a comparison with Deliberation Network involves two-pass decoding, the multipass decoder has a minimum increase of 0.24 BLEU score.",
            "Nevertheless, our multi-pass decoder proves its effectiveness due to the less parameters consumption of 37.35M in contrast to Deliberation Network.",
            "These results verify our hypothesis that the more decoding passes can polish the generated output to improve the translation quality.",
            "The underlying reason is that the attention component attndec within our multi-pass decoder can capture the extra target-side contexts to obtain a global understanding to assist the translation procedure.",
            "Towards the effect of the decoding depth set {2,3,4,5}, our multi-pass decoders obtain the approximate results, but the whole curve of BLEU is on an upward trend.",
            "Specifically, the multi-pass decoder with decoding depth 5 achieves the best performance with 38.64 BLEU, while the one with decoding depth 3 performs the worst among the decoding depth set with 38.55 BLEU.",
            "Although the average results of {2,3,4,5}-pass decoder are approximate, the distinction of {2,3,4,5}-pass decoder on NIST03, NIST04, NIST05, NIST06 and NIST08 is not negligible.",
            "These results indirectly prove the necessity of flexibility mechanism.",
            "Adaptive Decoding Depth.",
            "our proposed adaptive multi-pass decoder involves an extra policy network which controls the decoding depth according to the complexity of the source sentence and the differences between the consecutive generated translations.",
            "As shown in Table 2, the proposed adaptive multi-pass decoder obtains an improvement about 0.41 to 0.5 BLEU on average over the {2,3,4,5}-pass decoder, which demonstrates the effectiveness of the policy network.",
            "Specifically, the adaptive multi-pass decoder outperforms the multi-pass decoder with a fixed decoding depth by 0.69, 0.71, 0.68 and 0.45 BLEU scores on NIST03, NIST04, NIST05 and NIST06 datasets at most.",
            "In contrast to the Moses, RNNSearch, Deliberation Network and ABDNMT, the adaptive multi-pass decoder has the corresponding improvement about 8.03, 1.55, 0.74 and 0.34 BLEU points, respectively.",
            "More importantly, our adaptive multi-pass decoder outperforms ABDNMT, Deliberation Network model with a decrease of 26.85M, 29.15M parameters."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            1,
            2,
            1,
            1,
            1,
            1,
            1,
            2,
            2,
            2,
            1,
            2,
            2,
            2,
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Moses",
                "RNNSearch",
                "Deliberation Network",
                "ABDNMT",
                "2-pass decoder",
                "3-pass decoder",
                "4-pass decoder",
                "5-pass decoder",
                "Adaptive multi-pass decoder"
            ],
            null,
            [
                "RNNSearch",
                "Deliberation Network",
                "ABDNMT",
                "#Params"
            ],
            [
                "2-pass decoder",
                "3-pass decoder",
                "4-pass decoder",
                "5-pass decoder",
                "Adaptive multi-pass decoder",
                "#Params"
            ],
            null,
            [
                "2-pass decoder",
                "3-pass decoder",
                "4-pass decoder",
                "5-pass decoder",
                "RNNSearch"
            ],
            [
                "2-pass decoder",
                "3-pass decoder",
                "4-pass decoder",
                "5-pass decoder",
                "Moses",
                "RNNSearch",
                "Ave."
            ],
            [
                "2-pass decoder",
                "3-pass decoder",
                "4-pass decoder",
                "5-pass decoder",
                "RNNSearch",
                "#Params"
            ],
            [
                "2-pass decoder",
                "Deliberation Network"
            ],
            [
                "2-pass decoder",
                "3-pass decoder",
                "4-pass decoder",
                "5-pass decoder",
                "Deliberation Network",
                "#Params"
            ],
            null,
            null,
            [
                "2-pass decoder",
                "3-pass decoder",
                "4-pass decoder",
                "5-pass decoder"
            ],
            [
                "3-pass decoder",
                "5-pass decoder",
                "Ave."
            ],
            [
                "2-pass decoder",
                "3-pass decoder",
                "4-pass decoder",
                "5-pass decoder"
            ],
            null,
            null,
            [
                "Adaptive multi-pass decoder"
            ],
            [
                "Adaptive multi-pass decoder",
                "2-pass decoder",
                "3-pass decoder",
                "4-pass decoder",
                "5-pass decoder",
                "Ave."
            ],
            [
                "Adaptive multi-pass decoder",
                "2-pass decoder",
                "3-pass decoder",
                "4-pass decoder",
                "5-pass decoder",
                "MT03",
                "MT04",
                "MT05",
                "MT06"
            ],
            [
                "Adaptive multi-pass decoder",
                "Moses",
                "RNNSearch",
                "Deliberation Network",
                "ABDNMT",
                "Ave."
            ],
            [
                "Adaptive multi-pass decoder",
                "RNNSearch",
                "Deliberation Network",
                "ABDNMT",
                "#Params"
            ]
        ],
        "n_sentence": 23.0,
        "table_id": "table_2",
        "paper_id": "D18-1048",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1101table_1",
        "description": "Table 1 shows the results. LM improves wordby-word baselines consistently in all four tasks, giving at least +3% BLEU. When our denoising model is applied on top of it, we have additional gain around +3% BLEU. Note that our methods do not involve any decoding steps to generate pseudo-parallel training data, but still perform better than unsupervised MT systems that rely on repetitive back-translations (Artetxe et al. 2018; Lample et al. 2018) by up to +3.9% BLEU.",
        "sentences": [
            "Table 1 shows the results.",
            "LM improves wordby-word baselines consistently in all four tasks, giving at least +3% BLEU.",
            "When our denoising model is applied on top of it, we have additional gain around +3% BLEU.",
            "Note that our methods do not involve any decoding steps to generate pseudo-parallel training data, but still perform better than unsupervised MT systems that rely on repetitive back-translations (Artetxe et al. 2018; Lample et al. 2018) by up to +3.9% BLEU."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "+LM",
                "Word-by-Word",
                "BLEU [%]",
                "de-en",
                "en-de",
                "fr-en",
                "en-fr"
            ],
            [
                "+ Denoising",
                "+LM",
                "BLEU [%]"
            ],
            [
                "+ Denoising",
                "(Lample et al. 2018)",
                "(Artetxe et al. 2018)"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_1",
        "paper_id": "D18-1101",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1180table_2",
        "description": "In Table 2 we report the disambiguation accuracy on semtest, a new state-of-the-art result. Results. In both the unsupervised and supervised disambiguation settings, the best performance is achieved by using all three features, v`, vr and vi. As summarized in Table 2, our unsupervised method achieves a 2.4% improvement over stateof-art (Hovy et al., 2011). The results in the supervised setting, tabulated in Table 3, reveal that the weighted k-NN classifier performs best. Denoting left, right and interplay features by `, r, i respectively, Table 2 and 3 report our experimental results using only subset combinations of these features on the two disambiguation tasks.",
        "sentences": [
            "In Table 2 we report the disambiguation accuracy on semtest, a new state-of-the-art result.",
            "Results.",
            "In both the unsupervised and supervised disambiguation settings, the best performance is achieved by using all three features, v`, vr and vi.",
            "As summarized in Table 2, our unsupervised method achieves a 2.4% improvement over stateof-art (Hovy et al., 2011).",
            "The results in the supervised setting, tabulated in Table 3, reveal that the weighted k-NN classifier performs best.",
            "Denoting left, right and interplay features by `, r, i respectively, Table 2 and 3 report our experimental results using only subset combinations of these features on the two disambiguation tasks."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            0,
            1
        ],
        "header_mention": [
            [
                "Accuracy"
            ],
            null,
            [
                "(l r i)"
            ],
            [
                "State-of-art (Hovy et al. 2011)",
                "(l r i)",
                "Accuracy"
            ],
            null,
            [
                "(l r)",
                "(l i)",
                "(r i)",
                "(l r i)"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_2",
        "paper_id": "D18-1180",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1222table_4",
        "description": "Our datasets have three kinds of triples. Hence, we do experiments on them respectively. Experimental results for relational triples, instanceOf triples, and subClassOf triples are shown in Table 2, Table 3, and Table 4 respectively. In Table 3 and Table 4, a rising arrow means performance of this model have a promotion from YAGO39K to M-YAGO39K and a down arrow means a drop. From Table 2, we can learn that: (1) TransC outperforms all previous work in relational triple classification. (2) The \u201cbern\u201d sampling trick works better than \u201cunif\u201d in TransC. From Table 3 and Table 4, we can conclude that: (1) On YAGO39K, some compared models perform better than TransC in instanceOf triple classification. This is because that instanceOf has most triples (53.5%) among all relations in YAGO39K. This relation is trained superabundant times and nearly achieves the best performance, which has an adverse effect on other triples. TransC can find a balance between them and all triples achieve a good performance. (2) On YAGO39K, TransC outperforms other models in subClassOf triple classification. As shown in Table 1, subClassOf triples are much less than instanceOf triples. Hence, other models can not achieve the best performance under the bad influence of instanceOf triples. (3) On M-YAGO39K, TransC outperforms previous work in both instanceOf triple classification and subClassOf triple classification, which indicates that TransC can handle the transitivity of isA relations much better than other models. (4) After comparing experimental results in YAGO39K and M-YAGO39K, we can find that most previous work\u2019s performance suffers a big drop in instanceOf triple classification and a small drop in subClassOf triple classification. This shows that previous work can not deal with instanceOf-subClassOf transitivity well. (5) In TransC, nearly all performances have a significant promotion from YAGO39K to MYAGO39K. Both instanceOf-subClassOf transitivity and subClassOf-subClassOf transitivity are solved well in TransC.",
        "sentences": [
            "Our datasets have three kinds of triples.",
            "Hence, we do experiments on them respectively.",
            "Experimental results for relational triples, instanceOf triples, and subClassOf triples are shown in Table 2, Table 3, and Table 4 respectively.",
            "In Table 3 and Table 4, a rising arrow means performance of this model have a promotion from YAGO39K to M-YAGO39K and a down arrow means a drop.",
            "From Table 2, we can learn that: (1) TransC outperforms all previous work in relational triple classification. (2) The \u201cbern\u201d sampling trick works better than \u201cunif\u201d in TransC.",
            "From Table 3 and Table 4, we can conclude that: (1) On YAGO39K, some compared models perform better than TransC in instanceOf triple classification.",
            "This is because that instanceOf has most triples (53.5%) among all relations in YAGO39K.",
            "This relation is trained superabundant times and nearly achieves the best performance, which has an adverse effect on other triples.",
            "TransC can find a balance between them and all triples achieve a good performance.",
            "(2) On YAGO39K, TransC outperforms other models in subClassOf triple classification.",
            "As shown in Table 1, subClassOf triples are much less than instanceOf triples.",
            "Hence, other models can not achieve the best performance under the bad influence of instanceOf triples.",
            "(3) On M-YAGO39K, TransC outperforms previous work in both instanceOf triple classification and subClassOf triple classification, which indicates that TransC can handle the transitivity of isA relations much better than other models.",
            "(4) After comparing experimental results in YAGO39K and M-YAGO39K, we can find that most previous work\u2019s performance suffers a big drop in instanceOf triple classification and a small drop in subClassOf triple classification.",
            "This shows that previous work can not deal with instanceOf-subClassOf transitivity well.",
            "(5) In TransC, nearly all performances have a significant promotion from YAGO39K to MYAGO39K.",
            "Both instanceOf-subClassOf transitivity and subClassOf-subClassOf transitivity are solved well in TransC."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            2,
            2,
            2,
            2,
            1,
            1,
            2,
            2,
            1,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "YAGO39K",
                "M-YAGO39K"
            ],
            [
                "TransC (unif)",
                "TransC (bern)"
            ],
            [
                "YAGO39K",
                "TransE",
                "TransH",
                "TransR",
                "TransD",
                "HolE",
                "DistMult",
                "ComplEx",
                "TransC (unif)",
                "TransC (bern)"
            ],
            null,
            null,
            [
                "TransC (unif)",
                "TransC (bern)"
            ],
            [
                "YAGO39K",
                "TransC (unif)",
                "TransC (bern)"
            ],
            null,
            null,
            [
                "M-YAGO39K",
                "TransC (unif)",
                "TransC (bern)"
            ],
            null,
            null,
            [
                "TransC (unif)",
                "TransC (bern)",
                "YAGO39K",
                "M-YAGO39K"
            ],
            [
                "TransC (unif)",
                "TransC (bern)"
            ]
        ],
        "n_sentence": 17.0,
        "table_id": "table_4",
        "paper_id": "D18-1222",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1229table_1",
        "description": "In Table 1 we also show recall, computed per-category and averaged over categories, at the last round of bootstrapping and similarly find the scatterplot outperforms the list. The recall performance in Table 1 also confirms the performance gains of the scatterplot.",
        "sentences": [
            "In Table 1 we also show recall, computed per-category and averaged over categories, at the last round of bootstrapping and similarly find the scatterplot outperforms the list.",
            "The recall performance in Table 1 also confirms the performance gains of the scatterplot."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "bootstrapping",
                "scatterplot",
                "list"
            ],
            [
                "extrapolation",
                "scatterplot"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_1",
        "paper_id": "D18-1229",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1275table_3",
        "description": "Table 3 lists the results of different taggers evaluated on NPSChat. Our method was tested using the same setup as the experiments in (Forsyth, 2007). The training part contained 90% of the data. The testing part contained the remaining 10%. Based on the results, we can see that our method could achieve the best accuracy (94.0%), which was significantly better than 90.8% (Forsyth, 2007). They trained the tagger on a mix of several corpora tagged with the Penn Treebank tag set. Our method also outperformed the ARK tagger, which applies various external corpus and features, e.g., Brown clustering, PTB, Freebase lists of celebrities, and video games.",
        "sentences": [
            "Table 3 lists the results of different taggers evaluated on NPSChat.",
            "Our method was tested using the same setup as the experiments in (Forsyth, 2007).",
            "The training part contained 90% of the data.",
            "The testing part contained the remaining 10%.",
            "Based on the results, we can see that our method could achieve the best accuracy (94.0%), which was significantly better than 90.8% (Forsyth, 2007).",
            "They trained the tagger on a mix of several corpora tagged with the Penn Treebank tag set.",
            "Our method also outperformed the ARK tagger, which applies various external corpus and features, e.g., Brown clustering, PTB, Freebase lists of celebrities, and video games."
        ],
        "class_sentence": [
            1,
            2,
            2,
            2,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "DCNN"
            ],
            null,
            null,
            [
                "DCNN",
                "Forsyth (2007)",
                "Acc."
            ],
            [
                "Forsyth (2007)"
            ],
            [
                "DCNN",
                "ARK Tagger",
                "Acc."
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_3",
        "paper_id": "D18-1275",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1333table_2",
        "description": "4.2 Results. Table 2 shows the translation results. It is clear that the proposed models significantly outperform the baselines in all cases, although there are considerable differences among different variations. Baselines (Rows 1-4):. The three baselines (Rows 1, 2, and 4) differ regarding the training data used. \u201cSeparate-Recs\u21d2(+DPs)\u201d (Row 3) is the best model reported in Wang et al. (2018), which we employed as another strong baseline. The baseline trained on the DPP-annotated data (\u201cBaseline (+DPPs)\u201d, Row 4) outperforms the other two counterparts, indicating that the error propagation problem does affect the performance of translating DPs. It suggests the necessity of jointly learning to translate and predict DPs. Our Models (Rows 5-8):. Using our shared reconstructor (Row 5) not only outperforms the corresponding baseline (Row 4), but also surpasses its separate reconstructor counterpart (Row 3). Introducing a joint prediction objective (Row 6) can achieve a further improvement of +0.61 BLEU points. These results verify that shared reconstructor and jointly predicting DPs can accumulatively improve translation performance. Among the variations of shared reconstructors (Rows 6-8), we found that an interaction attention from encoder to decoder (Row 7) achieves the best performance, which is +3.45 BLEU points better than our baseline (Row 4) and +1.45 BLEU points better than the best result reported by Wang et al. (2018) (Row 3). We attribute the superior performance of \u201cShared-Recenc\u2192dec\u201d to the fact that the attention context over encoder representations embeds useful DP information, which can help to better attend to the representations of the corresponding pronouns in the decoder side. Similar to Wang et al. (2018), the proposed approach improves BLEU scores at the cost of decreased training and decoding speed, which is due to the large number of newly introduced parameters resulting from the incorporation of reconstructors into the NMT model.",
        "sentences": [
            "4.2 Results.",
            "Table 2 shows the translation results.",
            "It is clear that the proposed models significantly outperform the baselines in all cases, although there are considerable differences among different variations.",
            "Baselines (Rows 1-4):.",
            "The three baselines (Rows 1, 2, and 4) differ regarding the training data used.",
            "\u201cSeparate-Recs\u21d2(+DPs)\u201d (Row 3) is the best model reported in Wang et al. (2018), which we employed as another strong baseline.",
            "The baseline trained on the DPP-annotated data (\u201cBaseline (+DPPs)\u201d, Row 4) outperforms the other two counterparts, indicating that the error propagation problem does affect the performance of translating DPs.",
            "It suggests the necessity of jointly learning to translate and predict DPs.",
            "Our Models (Rows 5-8):.",
            "Using our shared reconstructor (Row 5) not only outperforms the corresponding baseline (Row 4), but also surpasses its separate reconstructor counterpart (Row 3).",
            "Introducing a joint prediction objective (Row 6) can achieve a further improvement of +0.61 BLEU points.",
            "These results verify that shared reconstructor and jointly predicting DPs can accumulatively improve translation performance.",
            "Among the variations of shared reconstructors (Rows 6-8), we found that an interaction attention from encoder to decoder (Row 7) achieves the best performance, which is +3.45 BLEU points better than our baseline (Row 4) and +1.45 BLEU points better than the best result reported by Wang et al. (2018) (Row 3).",
            "We attribute the superior performance of \u201cShared-Recenc\u2192dec\u201d to the fact that the attention context over encoder representations embeds useful DP information, which can help to better attend to the representations of the corresponding pronouns in the decoder side.",
            "Similar to Wang et al. (2018), the proposed approach improves BLEU scores at the cost of decreased training and decoding speed, which is due to the large number of newly introduced parameters resulting from the incorporation of reconstructors into the NMT model."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            1,
            1,
            1,
            2,
            2,
            1,
            1,
            2,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "Shared-Recindependent\u21d2(+DPPs)",
                "Shared-Recindependent\u21d2(+DPPs) + joint prediction",
                "Shared-Recenc\u2192dec\u21d2(+DPPs) + joint prediction",
                "Shared-Recdec\u2192enc\u21d2(+DPPs) + joint prediction",
                "BLEU"
            ],
            null,
            [
                "Baseline",
                "Baseline (+DPs)",
                "Baseline (+DPPs)"
            ],
            [
                "Separate-Recs\u21d2(+DPs)"
            ],
            [
                "Baseline (+DPPs)",
                "Baseline",
                "Baseline (+DPs)"
            ],
            null,
            null,
            [
                "Shared-Recindependent\u21d2(+DPPs)",
                "Baseline (+DPPs)",
                "Separate-Recs\u21d2(+DPs)"
            ],
            [
                "Shared-Recindependent\u21d2(+DPPs) + joint prediction",
                "Shared-Recindependent\u21d2(+DPPs)",
                "BLEU"
            ],
            [
                "Shared-Recindependent\u21d2(+DPPs)",
                "Shared-Recindependent\u21d2(+DPPs) + joint prediction"
            ],
            [
                "Separate-Recs\u21d2(+DPs)",
                "Baseline (+DPPs)",
                "Shared-Recindependent\u21d2(+DPPs) + joint prediction",
                "Shared-Recenc\u2192dec\u21d2(+DPPs) + joint prediction",
                "Shared-Recdec\u2192enc\u21d2(+DPPs) + joint prediction",
                "BLEU"
            ],
            [
                "Shared-Recenc\u2192dec\u21d2(+DPPs) + joint prediction"
            ],
            [
                "Shared-Recindependent\u21d2(+DPPs)",
                "Shared-Recindependent\u21d2(+DPPs) + joint prediction",
                "Shared-Recenc\u2192dec\u21d2(+DPPs) + joint prediction",
                "Shared-Recdec\u2192enc\u21d2(+DPPs) + joint prediction",
                "# Params",
                "Speed",
                "Train",
                "Decode"
            ]
        ],
        "n_sentence": 15.0,
        "table_id": "table_2",
        "paper_id": "D18-1333",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1344table_1",
        "description": "Result:. Table 1 shows the results on Sentiment task. While all embedding models significantly improve over the baseline (\u2018None\u2019), \u03c1gCM-Skip and BiCVM perform the best. Instead of linguistically motivated data, we tried with CM data created by random juxtaposition of monolingual fragments and it gave a F1 score of 56.0% and the minimal gain is possibly only because of the monolingual fragments in the embedding training data.",
        "sentences": [
            "Result:.",
            "Table 1 shows the results on Sentiment task.",
            "While all embedding models significantly improve over the baseline (\u2018None\u2019), \u03c1gCM-Skip and BiCVM perform the best.",
            "Instead of linguistically motivated data, we tried with CM data created by random juxtaposition of monolingual fragments and it gave a F1 score of 56.0% and the minimal gain is possibly only because of the monolingual fragments in the embedding training data."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "None",
                "BiCCA",
                "BiCVM",
                "BiSkip",
                "\u03c7-gCM-Skip",
                "\u03c1-gCM-Skip"
            ],
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_1",
        "paper_id": "D18-1344",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1352table_4",
        "description": "In Table 4 we compare the P@10, R@10, and macro-F1 measures across all three groups (the union of S, F, and Z) on the MIMIC III dataset. We emphasize that the evaluation metrics are calculated over all labels and are not averages of the metrics computed independently for each group. We find that R@10 is nearly equivalent to the R@10 on the frequent group in Table 3. Furthermore, we find that ACNN outperforms ZAGCNN in P@10 by almost 4%. To compare all methods with respect to macro-F1, we simply threshold each label at 0.5. Both R@k and P@k give more weight to frequent labels, thus it is expected that ACNN outperforms ZAGCNN for frequent labels. However, we also find that ACNN outperforms our methods with respect to Macro-F1.",
        "sentences": [
            "In Table 4 we compare the P@10, R@10, and macro-F1 measures across all three groups (the union of S, F, and Z) on the MIMIC III dataset.",
            "We emphasize that the evaluation metrics are calculated over all labels and are not averages of the metrics computed independently for each group.",
            "We find that R@10 is nearly equivalent to the R@10 on the frequent group in Table 3.",
            "Furthermore, we find that ACNN outperforms ZAGCNN in P@10 by almost 4%.",
            "To compare all methods with respect to macro-F1, we simply threshold each label at 0.5.",
            "Both R@k and P@k give more weight to frequent labels, thus it is expected that ACNN outperforms ZAGCNN for frequent labels.",
            "However, we also find that ACNN outperforms our methods with respect to Macro-F1."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            [
                "P@10",
                "R@10",
                "Macro-F1"
            ],
            null,
            [
                "R@10"
            ],
            [
                "ACNN",
                "ZAGCNN",
                "P@10"
            ],
            [
                "Macro-F1"
            ],
            [
                "ACNN",
                "ZAGCNN"
            ],
            [
                "ACNN",
                "ZAGCNN",
                "Macro-F1"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_4",
        "paper_id": "D18-1352",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1354table_1",
        "description": "5.1 Quantitative Analysis. The experimental results evaluated by automatic metrics on the OpenSubtitles and the Reddit datasets are shown in Table 1 and 2, respectively. It is observed that both CVAE-based models and our proposed models outperform Seq2Seq by a large margin, showing the effectiveness of adding variational latent variable for response generation. However, using different structure of variational models leads to differences in performance on both plausibility and diversity. Our model with or without the SBOW auxiliary loss outperforms CVAE as observed by the significant boost in semantic relevance-oriented metrics (embedding similarities and RUBER score) and diversity-oriented metrics. This is mainly due to the different strategy employed for representing latent variables. CVAE only uses a unimodal latent variable as the semantic signal of the whole response sequence which limits its capability of capturing variability of response sequences. By incorporating a series of time-varying latent variables into each step of autoregressive decoder, our model is able to model more complicated multimodal distributions of response sequences and capture more detailed semantic information. Since adding the auxiliary loss could alleviate the model collapse problem, we found that CVAE model with the BOW auxiliary loss outperforms our basic model without auxiliary loss, especially on the diversity metrics. When adding the proposed SBOW auxiliary loss into our model, we found that our generated responses have shown better diversity compared to those generated by CVAE+BOW loss.",
        "sentences": [
            "5.1 Quantitative Analysis.",
            "The experimental results evaluated by automatic metrics on the OpenSubtitles and the Reddit datasets are shown in Table 1 and 2, respectively.",
            "It is observed that both CVAE-based models and our proposed models outperform Seq2Seq by a large margin, showing the effectiveness of adding variational latent variable for response generation.",
            "However, using different structure of variational models leads to differences in performance on both plausibility and diversity.",
            "Our model with or without the SBOW auxiliary loss outperforms CVAE as observed by the significant boost in semantic relevance-oriented metrics (embedding similarities and RUBER score) and diversity-oriented metrics.",
            "This is mainly due to the different strategy employed for representing latent variables.",
            "CVAE only uses a unimodal latent variable as the semantic signal of the whole response sequence which limits its capability of capturing variability of response sequences.",
            "By incorporating a series of time-varying latent variables into each step of autoregressive decoder, our model is able to model more complicated multimodal distributions of response sequences and capture more detailed semantic information.",
            "Since adding the auxiliary loss could alleviate the model collapse problem, we found that CVAE model with the BOW auxiliary loss outperforms our basic model without auxiliary loss, especially on the diversity metrics.",
            "When adding the proposed SBOW auxiliary loss into our model, we found that our generated responses have shown better diversity compared to those generated by CVAE+BOW loss."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            1,
            2,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "CVAE",
                "Ours",
                "Seq2Seq"
            ],
            null,
            [
                "Ours (without SBOW)",
                "Ours",
                "CVAE",
                "Embedding Similarity",
                "RUBER",
                "Diversity"
            ],
            null,
            [
                "CVAE"
            ],
            [
                "Ours"
            ],
            [
                "CVAE+BOW loss",
                "Ours (without SBOW)",
                "Diversity"
            ],
            [
                "Ours",
                "CVAE+BOW loss",
                "Diversity"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_1",
        "paper_id": "D18-1354",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1360table_3",
        "description": "Ablations. We evaluate the effect of multi-task learning in each of the three tasks defined in our dataset. Table 3 reports the results for individual tasks when additional tasks are included in the learning objective function. We observe that performance improves with each added task in the objective. For example, Entity recognition (65.7) benefits from both coreference resolution (67.5) and relation extraction (66.8). Relation extraction (37.9) significantly benefits when multi-tasked with coreference resolution (7.1% relative improvement). Coreference resolution benefits when multitasked with relation extraction, with 4.9% relative improvement.",
        "sentences": [
            "Ablations.",
            "We evaluate the effect of multi-task learning in each of the three tasks defined in our dataset.",
            "Table 3 reports the results for individual tasks when additional tasks are included in the learning objective function.",
            "We observe that performance improves with each added task in the objective.",
            "For example, Entity recognition (65.7) benefits from both coreference resolution (67.5) and relation extraction (66.8).",
            "Relation extraction (37.9) significantly benefits when multi-tasked with coreference resolution (7.1% relative improvement).",
            "Coreference resolution benefits when multitasked with relation extraction, with 4.9% relative improvement."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Entity Rec.",
                "Relation",
                "Coref."
            ],
            [
                "Entity Rec.",
                "Relation",
                "Coref.",
                "Single Task+Entity Rec.",
                "Single Task+Relation",
                "Single Task+Coreference"
            ],
            [
                "Single Task",
                "Single Task+Entity Rec.",
                "Single Task+Relation",
                "Single Task+Coreference"
            ],
            [
                "Single Task",
                "Single Task+Relation",
                "Single Task+Coreference",
                "Entity Rec."
            ],
            [
                "Single Task",
                "Relation",
                "Single Task+Coreference"
            ],
            [
                "Single Task",
                "Coref."
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_3",
        "paper_id": "D18-1360",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1370table_5",
        "description": "In MTL experiments, we consider the respective task performances from single-task experiments and MTL with a joint loss function with equal weighting of the task losses as baselines. Single-Task Experiments. We first report the model performances for individual tasks in STL settings. Results for token-level tasks are shown in Table 4, whereas Table 5 displays results for sentence-level tasks. The scores (precision, recall, and F1 score) are reported as macro-averages over all task labels. Expectedly, our neural architectures substantially outperform the traditional machine learning baselines on all tasks. For the three sentence-level tasks, the Hierarchical architecture outperforms the Simple model only when classifying sentences by summary relevance (SRC). This result seems intuitive a Very relevant sentence is likely to be surrounded with Relevant and May appear sentences (and an Irrelevant sentence with other Irrelevant and Should not appear sentences).",
        "sentences": [
            "In MTL experiments, we consider the respective task performances from single-task experiments and MTL with a joint loss function with equal weighting of the task losses as baselines.",
            "Single-Task Experiments.",
            "We first report the model performances for individual tasks in STL settings.",
            "Results for token-level tasks are shown in Table 4, whereas Table 5 displays results for sentence-level tasks.",
            "The scores (precision, recall, and F1 score) are reported as macro-averages over all task labels.",
            "Expectedly, our neural architectures substantially outperform the traditional machine learning baselines on all tasks.",
            "For the three sentence-level tasks, the Hierarchical architecture outperforms the Simple model only when classifying sentences by summary relevance (SRC).",
            "This result seems intuitive a Very relevant sentence is likely to be surrounded with Relevant and May appear sentences (and an Irrelevant sentence with other Irrelevant and Should not appear sentences)."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null,
            [
                "Neural: Simple",
                "Neural: Hierarchical"
            ],
            [
                "Neural: Hierarchical",
                "Neural: Simple",
                "SRC"
            ],
            null
        ],
        "n_sentence": 8.0,
        "table_id": "table_5",
        "paper_id": "D18-1370",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1423table_3",
        "description": "5.1 Quantitative Analysis. Table 3 reports the results of both automatic and human evaluations. We analyze the results from the following aspects. 5.1.1 The effect of CVAE. This study is to investigate whether using latent variable and variational inference can improve the diversity and novelty of terms in generated poems. There are two main observations. CVAE significantly improves term novelty. As illustrated in Table 3, CVAE outperforms all baselines significantly in terms of distinctness. With diversified terms, the aesthetics scores also confirm that CVAE can generate poems that correspond to better user experiences. Although Mem-AS2S can generate a rather high distinctness score, it requires a more complicated structure in learning and generating poems. The results confirm the effectiveness of CVAE in addressing the issue of term duplications that occurred in RNN. CVAE cannot control thematic consistency of generated poems. Recall that thematic consistency and term diversity are usually mutually exclusive, CVAE produces the worst result in thematic consistency, which is confirmed in Table 3 by the similarity score in automatic evaluation and the consistency score in human evaluation. 5.1.2 The Influence of the Discriminator. As previously stated, introducing a discriminator with adversarial training is expected to bring positive effect on thematic consistency. We investigate the influence of discriminator with two groups of comparison, i.e., CVAE-D v.s. CVAE, GAN v.s. S2S. Following observations are made in this investigation, which confirm that adversarial learning is an effective add-on to existing models for thematics control, without affecting other aspects. The discriminator effectively enhances poem generation with thematic information. When the discriminator is introduced, CVAE and S2S model are capable of generating thematically consistent poems, as illustrated by the similarity and meaning scores in Table 3. The BLEU results also confirm that the discriminator can improve the overlapping between generated poems and the ground truth, which serves as thematic consistent cases. The extra discriminator does not affect base models on irrelevant merits. For any base model, e.g., S2S and CVAE, when adding a discriminator, it is expected that it can bring help on thematic consistency while limiting any inferior effects on other evaluations. This is confirmed in the results, e.g., for distinctness, CVAE-D and GAN are comparable to CVAE and S2S. 5.1.3 The Performance of CVAE-D. Overall, the CVAE-D model substantially outperforms all other models in all metrics. Especially for term novelty and thematic consistency, CVAE-D illustrates an extraordinary balance between them, with observable improvements on both sides. This balance is mainly contributed from the proposed framework that seamlessly integrates CVAE and the discriminator.",
        "sentences": [
            "5.1 Quantitative Analysis.",
            "Table 3 reports the results of both automatic and human evaluations.",
            "We analyze the results from the following aspects.",
            "5.1.1 The effect of CVAE.",
            "This study is to investigate whether using latent variable and variational inference can improve the diversity and novelty of terms in generated poems.",
            "There are two main observations.",
            "CVAE significantly improves term novelty.",
            "As illustrated in Table 3, CVAE outperforms all baselines significantly in terms of distinctness.",
            "With diversified terms, the aesthetics scores also confirm that CVAE can generate poems that correspond to better user experiences.",
            "Although Mem-AS2S can generate a rather high distinctness score, it requires a more complicated structure in learning and generating poems.",
            "The results confirm the effectiveness of CVAE in addressing the issue of term duplications that occurred in RNN.",
            "CVAE cannot control thematic consistency of generated poems.",
            "Recall that thematic consistency and term diversity are usually mutually exclusive, CVAE produces the worst result in thematic consistency, which is confirmed in Table 3 by the similarity score in automatic evaluation and the consistency score in human evaluation.",
            "5.1.2 The Influence of the Discriminator.",
            "As previously stated, introducing a discriminator with adversarial training is expected to bring positive effect on thematic consistency.",
            "We investigate the influence of discriminator with two groups of comparison, i.e., CVAE-D v.s. CVAE, GAN v.s. S2S.",
            "Following observations are made in this investigation, which confirm that adversarial learning is an effective add-on to existing models for thematics control, without affecting other aspects.",
            "The discriminator effectively enhances poem generation with thematic information.",
            "When the discriminator is introduced, CVAE and S2S model are capable of generating thematically consistent poems, as illustrated by the similarity and meaning scores in Table 3.",
            "The BLEU results also confirm that the discriminator can improve the overlapping between generated poems and the ground truth, which serves as thematic consistent cases.",
            "The extra discriminator does not affect base models on irrelevant merits.",
            "For any base model, e.g., S2S and CVAE, when adding a discriminator, it is expected that it can bring help on thematic consistency while limiting any inferior effects on other evaluations.",
            "This is confirmed in the results, e.g., for distinctness, CVAE-D and GAN are comparable to CVAE and S2S.",
            "5.1.3 The Performance of CVAE-D.",
            "Overall, the CVAE-D model substantially outperforms all other models in all metrics.",
            "Especially for term novelty and thematic consistency, CVAE-D illustrates an extraordinary balance between them, with observable improvements on both sides.",
            "This balance is mainly contributed from the proposed framework that seamlessly integrates CVAE and the discriminator."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            2,
            2,
            2,
            1,
            1,
            1,
            2,
            2,
            1,
            2,
            2,
            2,
            1,
            2,
            1,
            1,
            2,
            2,
            1,
            2,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "Automatic Evaluation",
                "Human Evaluation"
            ],
            null,
            null,
            null,
            null,
            null,
            [
                "CVAE",
                "Dist-1",
                "Dist-2",
                "Dist-3",
                "Dist-4"
            ],
            [
                "CVAE",
                "BLEU-1",
                "BLEU-2"
            ],
            [
                "AS2S",
                "BLEU-1",
                "BLEU-2"
            ],
            [
                "CVAE"
            ],
            null,
            [
                "CVAE",
                "Automatic Evaluation",
                "Human Evaluation",
                "Sim",
                "Con."
            ],
            null,
            null,
            [
                "CVAE-D",
                "CVAE",
                "GAN",
                "S2S"
            ],
            null,
            null,
            [
                "CVAE",
                "S2S",
                "Sim",
                "Mea."
            ],
            [
                "BLEU-1",
                "BLEU-2"
            ],
            null,
            [
                "S2S",
                "CVAE"
            ],
            [
                "S2S",
                "GAN",
                "CVAE",
                "CVAE-D",
                "Dist-1",
                "Dist-2",
                "Dist-3",
                "Dist-4"
            ],
            null,
            [
                "CVAE-D"
            ],
            [
                "CVAE-D",
                "Con."
            ],
            null
        ],
        "n_sentence": 27.0,
        "table_id": "table_3",
        "paper_id": "D18-1423",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1434table_2",
        "description": "5.2 Baseline and State-of-the-Art. The comparison of our method with various baselines and state-of-the-art methods is provided in table 2 for VQA 1.0 and table 3 for VQG-COCO dataset. The comparable baselines for our method are the image based and caption based models in which we use either only the image or the caption embedding and generate the question. In both the tables, the first block consists of the current stateof-the-art methods on that dataset and the second contains the baselines. We observe that for the VQA dataset we achieve an improvement of 8% in BLEU and 7% in METEOR metric scores over the baselines, whereas for VQG-COCO dataset this is 15% for both the metrics. We improve over the previous state-of-the-art (Yang et al., 2015) for VQA dataset by around 6% in BLEU score and 10% in METEOR score.",
        "sentences": [
            "5.2 Baseline and State-of-the-Art.",
            "The comparison of our method with various baselines and state-of-the-art methods is provided in table 2 for VQA 1.0 and table 3 for VQG-COCO dataset.",
            "The comparable baselines for our method are the image based and caption based models in which we use either only the image or the caption embedding and generate the question.",
            "In both the tables, the first block consists of the current stateof-the-art methods on that dataset and the second contains the baselines.",
            "We observe that for the VQA dataset we achieve an improvement of 8% in BLEU and 7% in METEOR metric scores over the baselines, whereas for VQG-COCO dataset this is 15% for both the metrics.",
            "We improve over the previous state-of-the-art (Yang et al., 2015) for VQA dataset by around 6% in BLEU score and 10% in METEOR score."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Sample (Yang 2015)",
                "Max (Yang 2015)",
                "Image Only",
                "Caption Only",
                "MDN-Attention",
                "MDN-Hadamard",
                "MDN-Addition",
                "MDN-Joint (Ours)"
            ],
            [
                "Image Only",
                "Caption Only"
            ],
            [
                "Sample (Yang 2015)",
                "Max (Yang 2015)",
                "Image Only",
                "Caption Only"
            ],
            [
                "MDN-Joint (Ours)",
                "Image Only",
                "Caption Only",
                "BLEU1",
                "METEOR"
            ],
            [
                "MDN-Joint (Ours)",
                "Max (Yang 2015)",
                "BLEU1",
                "METEOR"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_2",
        "paper_id": "D18-1434",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1441table_5",
        "description": "Results on the test set are shown in Table 5. Our method much outperforms all the compared systems, which verifies the effectiveness of each component of our model. Note that, both the structural-compression and structural-coverage regularization significantly affect the summarization performance. The higher structuralcompression and structural-coverage scores will lead to higher ROUGE scores. Therefore, we can conclude that the structural-compression and structural-coverage regularization based on our hierarchical model have significant contributions to the increase of ROUGE scores. 5.3 Effects of Structural Regularization. The structural regularization based on our hierarchical encoder-decoder with hybrid attention model improves the quality of summaries from two aspects: (1) The summary covers more salient information and contains very few repetitions, which can be seen both qualitatively (Table 1 and Figure 1) and quantitatively (Table 5 and Figure 4).",
        "sentences": [
            "Results on the test set are shown in Table 5.",
            "Our method much outperforms all the compared systems, which verifies the effectiveness of each component of our model.",
            "Note that, both the structural-compression and structural-coverage regularization significantly affect the summarization performance.",
            "The higher structuralcompression and structural-coverage scores will lead to higher ROUGE scores.",
            "Therefore, we can conclude that the structural-compression and structural-coverage regularization based on our hierarchical model have significant contributions to the increase of ROUGE scores.",
            "5.3 Effects of Structural Regularization.",
            "The structural regularization based on our hierarchical encoder-decoder with hybrid attention model improves the quality of summaries from two aspects: (1) The summary covers more salient information and contains very few repetitions, which can be seen both qualitatively (Table 1 and Figure 1) and quantitatively (Table 5 and Figure 4)."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "+hierD"
            ],
            null,
            [
                "R-1",
                "R-2",
                "R-L"
            ],
            [
                "Hierarchical-b.",
                "R-1",
                "R-2",
                "R-L"
            ],
            null,
            null
        ],
        "n_sentence": 7.0,
        "table_id": "table_5",
        "paper_id": "D18-1441",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1458table_2",
        "description": "4.2 Overall. Results Table 2 reports the BLEU scores on newstest2014 and newstest2017, the perplexity on the validation set, and the accuracy on long-range dependencies. Transformer achieves the highest accuracy on this task and the highest BLEU scores on both newstest2014 and newstest2017.\". Compared to RNNS2S, ConvS2S has slightly better results regarding BLEU scores, but a much lower accuracy on long-range dependencies. The RNN-bideep model achieves distinctly better BLEU scores and a higher accuracy on long-range dependencies. However, it still cannot outperform Transformers on any of the tasks.",
        "sentences": [
            "4.2 Overall.",
            "Results Table 2 reports the BLEU scores on newstest2014 and newstest2017, the perplexity on the validation set, and the accuracy on long-range dependencies.",
            "Transformer achieves the highest accuracy on this task and the highest BLEU scores on both newstest2014 and newstest2017.\".",
            "Compared to RNNS2S, ConvS2S has slightly better results regarding BLEU scores, but a much lower accuracy on long-range dependencies.",
            "The RNN-bideep model achieves distinctly better BLEU scores and a higher accuracy on long-range dependencies.",
            "However, it still cannot outperform Transformers on any of the tasks."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "BLEU 2014",
                "BLEU 2017",
                "PPL",
                "Acc (%)"
            ],
            [
                "Transformer",
                "BLEU 2014",
                "BLEU 2017",
                "Acc (%)"
            ],
            [
                "Transformer",
                "RNNS2S",
                "ConvS2S",
                "BLEU 2014",
                "BLEU 2017",
                "Acc (%)"
            ],
            [
                "RNN-bideep",
                "BLEU 2014",
                "BLEU 2017",
                "Acc (%)"
            ],
            [
                "Transformer",
                "RNN-bideep"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_2",
        "paper_id": "D18-1458",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1458table_5",
        "description": "5.2 Overall. Results Table 5 gives the performance of all the architectures, including the perplexity on validation sets, the BLEU scores on newstest, and the accuracy on ContraWSD. Transformers distinctly outperform RNNS2S and ConvS2S models on DE\u2192EN and DE\u2192FR. Moreover, the Transformer model on DE\u2192EN also achieves higher accuracy than uedin-wmt17, although the BLEU score on newstest2017 is 1.4 lower than uedin-wmt17. We attribute this discrepancy between BLEU and WSD performance to the use of synthetic news training data in uedin-wmt17, which causes a large boost in BLEU due to better domain adaptation to newstest, but which is less helpful for ContraWSD, whose test set is drawn from a variety of domains. For DE\u2192EN, RNNS2S and ConvS2S have the same BLEU score on newstest2014, ConvS2S has a higher score on newstest2017. However, the WSD accuracy of ConvS2S is 1.7% lower than RNNS2S. For DE\u2192FR, ConvS2S achieves slightly better results on both BLEU scores and accuracy than RNNS2S. The Transformer model strongly outperforms the other architectures on this WSD task, with a gap of 4\u20138 percentage points. This affirms our hypothesis that Transformers are strong semantic features extractors. The results (in Table 5) show that TransRNN performs better than RNNS2S, but worse than the pure Transformer, both in terms of BLEU and WSD accuracy. This indicates that WSD is not only done in the encoder, but that the decoder also affects WSD performance.",
        "sentences": [
            "5.2 Overall.",
            "Results Table 5 gives the performance of all the architectures, including the perplexity on validation sets, the BLEU scores on newstest, and the accuracy on ContraWSD.",
            "Transformers distinctly outperform RNNS2S and ConvS2S models on DE\u2192EN and DE\u2192FR.",
            "Moreover, the Transformer model on DE\u2192EN also achieves higher accuracy than uedin-wmt17, although the BLEU score on newstest2017 is 1.4 lower than uedin-wmt17.",
            "We attribute this discrepancy between BLEU and WSD performance to the use of synthetic news training data in uedin-wmt17, which causes a large boost in BLEU due to better domain adaptation to newstest, but which is less helpful for ContraWSD, whose test set is drawn from a variety of domains.",
            "For DE\u2192EN, RNNS2S and ConvS2S have the same BLEU score on newstest2014, ConvS2S has a higher score on newstest2017.",
            "However, the WSD accuracy of ConvS2S is 1.7% lower than RNNS2S.",
            "For DE\u2192FR, ConvS2S achieves slightly better results on both BLEU scores and accuracy than RNNS2S.",
            "The Transformer model strongly outperforms the other architectures on this WSD task, with a gap of 4\u20138 percentage points.",
            "This affirms our hypothesis that Transformers are strong semantic features extractors.",
            "The results (in Table 5) show that TransRNN performs better than RNNS2S, but worse than the pure Transformer, both in terms of BLEU and WSD accuracy.",
            "This indicates that WSD is not only done in the encoder, but that the decoder also affects WSD performance."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2,
            1,
            1,
            1,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "RNNS2S",
                "ConvS2S",
                "Transformer",
                "uedin-wmt17",
                "TransRNN",
                "PPL",
                "BLEU 2014",
                "BLEU 2017",
                "BLEU 2012",
                "Acc (%)"
            ],
            [
                "Transformer",
                "RNNS2S",
                "ConvS2S",
                "DE\u2192EN",
                "DE\u2192FR"
            ],
            [
                "Transformer",
                "DE\u2192EN",
                "uedin-wmt17",
                "BLEU 2017",
                "Acc (%)"
            ],
            [
                "uedin-wmt17",
                "BLEU 2017",
                "Acc (%)"
            ],
            [
                "DE\u2192EN",
                "RNNS2S",
                "ConvS2S",
                "BLEU 2014",
                "BLEU 2017"
            ],
            [
                "DE\u2192EN",
                "RNNS2S",
                "ConvS2S",
                "Acc (%)"
            ],
            [
                "DE\u2192FR",
                "RNNS2S",
                "ConvS2S",
                "BLEU 2012",
                "Acc (%)"
            ],
            [
                "Transformer",
                "RNNS2S",
                "ConvS2S",
                "uedin-wmt17",
                "Acc (%)"
            ],
            [
                "Transformer"
            ],
            [
                "TransRNN",
                "RNNS2S",
                "Transformer",
                "BLEU 2012",
                "BLEU 2014",
                "BLEU 2017",
                "Acc(%)"
            ],
            null
        ],
        "n_sentence": 12.0,
        "table_id": "table_5",
        "paper_id": "D18-1458",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1477table_3",
        "description": "Results. Table 3 shows the translation results. When SRU is incorporated into the architecture, both the 4-layer and 5-layer model outperform the Transformer base model. For instance, our 5layer model obtains an average improvement of 0.7 test BLEU score and an improvement of 0.5 BLEU score by comparing the best results of each model achieved across three runs. SRU also exhibits more stable performance, with smaller variance over 3 runs. Figure 4 further compares the validation accuracy of different models. These results confirm that SRU is better at sequence modeling compared to the original feed-forward network (FFN), requiring fewer layers to achieve similar accuracy. Finally, adding SRU does not affect the parallelization or speed of Transformer \u00e2\u20ac\u201c the 4-layer model exhibits 10% speed improvement, while the 5-layer model is only 5% slower compared to the base model.",
        "sentences": [
            "Results.",
            "Table 3 shows the translation results.",
            "When SRU is incorporated into the architecture, both the 4-layer and 5-layer model outperform the Transformer base model.",
            "For instance, our 5layer model obtains an average improvement of 0.7 test BLEU score and an improvement of 0.5 BLEU score by comparing the best results of each model achieved across three runs.",
            "SRU also exhibits more stable performance, with smaller variance over 3 runs.",
            "Figure 4 further compares the validation accuracy of different models.",
            "These results confirm that SRU is better at sequence modeling compared to the original feed-forward network (FFN), requiring fewer layers to achieve similar accuracy.",
            "Finally, adding SRU does not affect the parallelization or speed of Transformer \u00e2\u20ac\u201c the 4-layer model exhibits 10% speed improvement, while the 5-layer model is only 5% slower compared to the base model."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Transformer (base)",
                "Transformer (+SRU)",
                "# layers",
                "4",
                "5"
            ],
            [
                "Transformer (base)",
                "Transformer (+SRU)",
                "# layers",
                "5",
                "BLEU score",
                "Valid",
                "Test"
            ],
            [
                "Transformer (+SRU)"
            ],
            null,
            [
                "Transformer (+SRU)"
            ],
            [
                "Transformer (+SRU)",
                "# layers",
                "4",
                "5",
                "Speed (toks/sec)",
                "Transformer (base)"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_3",
        "paper_id": "D18-1477",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1482table_4",
        "description": "Results. Table 4 shows that WME consistently matches or outperforms other unsupervised and supervised methods except the SIF method. Indeed, compared with ST and nbow, WME improves Pearson\u2019s scores substantially by 10% to 33% as a result of the consideration of word alignments and the use of TF-IDF weighting scheme. tf-idf also improves over these two methods but is slightly worse than our method, indicating the importance of taking into account the alignments between the words. SIF method is a strong baseline for textual similarity tasks but WME still can beat it on STS\u201912 and achieve close performance in other cases. Interestingly, WME is on a par with three supervised methods RNN, LSTM(no), and LSTM(o.g.) in most cases. The final remarks stem from the fact that, WME can gain significantly benefit from the supervised word embeddings similar to SIF, both showing strong performance on PSL.",
        "sentences": [
            "Results.",
            "Table 4 shows that WME consistently matches or outperforms other unsupervised and supervised methods except the SIF method.",
            "Indeed, compared with ST and nbow, WME improves Pearson\u2019s scores substantially by 10% to 33% as a result of the consideration of word alignments and the use of TF-IDF weighting scheme.",
            "tf-idf also improves over these two methods but is slightly worse than our method, indicating the importance of taking into account the alignments between the words.",
            "SIF method is a strong baseline for textual similarity tasks but WME still can beat it on STS\u201912 and achieve close performance in other cases.",
            "Interestingly, WME is on a par with three supervised methods RNN, LSTM(no), and LSTM(o.g.) in most cases.",
            "The final remarks stem from the fact that, WME can gain significantly benefit from the supervised word embeddings similar to SIF, both showing strong performance on PSL."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "SIF",
                "WME",
                "Supervised",
                "Unsupervised"
            ],
            [
                "WME",
                "ST",
                "nbow"
            ],
            [
                "ST",
                "nbow",
                "tf-idf",
                "WME"
            ],
            [
                "SIF",
                "WME",
                "STS\u201912"
            ],
            [
                "WME",
                "Supervised",
                "RNN",
                "LSTM(no)",
                "LSTM(o.g.)"
            ],
            [
                "SIF",
                "WME",
                "Supervised",
                "PSL"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_4",
        "paper_id": "D18-1482",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1018table_2",
        "description": "Table 2 presents results for our binary classification task. The BERT classifier has higher F1-score and precision than other classifiers. The BERTSA model after three epochs only achieves an F1score of 0.491, which confirms the difference between sentiment analysis and our good/bad task, i.e., even if the segment has positive sentiment, it might be not suitable as a justification.",
        "sentences": [
            "Table 2 presents results for our binary classification task.",
            "The BERT classifier has higher F1-score and precision than other classifiers.",
            "The BERTSA model after three epochs only achieves an F1score of 0.491, which confirms the difference between sentiment analysis and our good/bad task, i.e., even if the segment has positive sentiment, it might be not suitable as a justification."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "BERT",
                "F1",
                "Precision"
            ],
            [
                "BERT-SA (three epoch)",
                "F1"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_2",
        "paper_id": "D19-1018",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1054table_4",
        "description": "Performance & Trade-off: To see if the selector affects performance, we also ask human annotators to judge the text fluency. The fluency score is computed as the average number of text being judged as fluent. We include generations from the standard Enc-Dec model. Table 4 shows the best fluency is achieved for Enc-Dec. Imposing a content selector always affects the fluency a bit. The main reason is that when the controllability is strong, the change of selection will directly affect the text realization so that a tiny error of content selection might lead to unrealistic text.",
        "sentences": [
            "Performance & Trade-off: To see if the selector affects performance, we also ask human annotators to judge the text fluency.",
            "The fluency score is computed as the average number of text being judged as fluent.",
            "We include generations from the standard Enc-Dec model.",
            "Table 4 shows the best fluency is achieved for Enc-Dec.",
            "Imposing a content selector always affects the fluency a bit.",
            "The main reason is that when the controllability is strong, the change of selection will directly affect the text realization so that a tiny error of content selection might lead to unrealistic text."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "Enc-Dec"
            ],
            [
                "Fluency",
                "Enc-Dec"
            ],
            [
                "Fluency"
            ],
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_4",
        "paper_id": "D19-1054",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1069table_3",
        "description": "Baseline performance. Table 3 presents the performance of our baselines. Our best baseline (Baseline 5) significantly outperforms the standard pipeline approach (Baseline 1) in both the text and link evaluation. However, the performance of Baseline 5 is well below the human performance. The most impactful improvements over Baseline 1 are due to (a) incorporating coreference when choosing candidate links for pronouns in Baseline 2; (b) allowing more candidate facts and links to be classified by the relation extraction component in Baseline 4; and (c) incorporating BERT\u00e2\u20ac\u2122s pretrained model in Baseline 5.",
        "sentences": [
            "Baseline performance.",
            "Table 3 presents the performance of our baselines.",
            "Our best baseline (Baseline 5) significantly outperforms the standard pipeline approach (Baseline 1) in both the text and link evaluation.",
            "However, the performance of Baseline 5 is well below the human performance.",
            "The most impactful improvements over Baseline 1 are due to (a) incorporating coreference when choosing candidate links for pronouns in Baseline 2; (b) allowing more candidate facts and links to be classified by the relation extraction component in Baseline 4; and (c) incorporating BERT\u00e2\u20ac\u2122s pretrained model in Baseline 5."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "Baseline 5",
                "Baseline 1",
                "Text Evaluation",
                "Link Evaluation"
            ],
            [
                "Baseline 5",
                "Human"
            ],
            [
                "Baseline 1",
                "Baseline 2",
                "Baseline 4",
                "Baseline 5"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_3",
        "paper_id": "D19-1069",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1124table_1",
        "description": "4.3.1 Negative Log-likelihood Evaluation . Table 1 reports the comparisons of the per-word negative log-likelihood (NLL) composition of different models. NLL consists of the reconstruction loss (reconstruction) and the KL-divergence (KL-div.). KL-divergence indicates the information encoded in z, and reconstruction loss represents the loss of reconstructing x through z. As can be seen from Table 1, with almost the same KL-divergence, Dir-VHRED achieves the lowest NLL on both datasets, implying better performance compared with VHRED. Dir-VHRED also has the lower reconstruction loss, which indicates that Dirichlet prior is better than the Gaussian prior for reconstructing the responses.",
        "sentences": [
            "4.3.1 Negative Log-likelihood Evaluation .",
            "Table 1 reports the comparisons of the per-word negative log-likelihood (NLL) composition of different models.",
            "NLL consists of the reconstruction loss (reconstruction) and the KL-divergence (KL-div.).",
            "KL-divergence indicates the information encoded in z, and reconstruction loss represents the loss of reconstructing x through z.",
            "As can be seen from Table 1, with almost the same KL-divergence, Dir-VHRED achieves the lowest NLL on both datasets, implying better performance compared with VHRED.",
            "Dir-VHRED also has the lower reconstruction loss, which indicates that Dirichlet prior is better than the Gaussian prior for reconstructing the responses."
        ],
        "class_sentence": [
            2,
            2,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "NLL"
            ],
            [
                "NLL"
            ],
            [
                "NLL",
                "reconstruction",
                "KL-div."
            ],
            [
                "KL-div."
            ],
            [
                "Dir-VHRED",
                "VHRED"
            ],
            [
                "Dir-VHRED",
                "reconstruction"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_1",
        "paper_id": "D19-1124",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1154table_2",
        "description": "Table 2 lists the standard Pearson correlation coefficients r between the system predictions and the STS gold-standard scores. We report the best scores achieved by paraphrastic embeddings (Wieting et al., 2017) (text only) and the VSE models in the previous section. Note that the compared VSE models are all with RNN as the text encoder and no STS data is used for training. Our models achieve the best performance and the pre-trained word embeddings are preferred.",
        "sentences": [
            "Table 2 lists the standard Pearson correlation coefficients r between the system predictions and the STS gold-standard scores.",
            "We report the best scores achieved by paraphrastic embeddings (Wieting et al., 2017) (text only) and the VSE models in the previous section.",
            "Note that the compared VSE models are all with RNN as the text encoder and no STS data is used for training.",
            "Our models achieve the best performance and the pre-trained word embeddings are preferred."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "GRAN (Wieting et al. 2017)",
                "VSE (Kiros et al. 2014)"
            ],
            [
                "VSE (Kiros et al. 2014)"
            ],
            [
                "Ours (w/ FastText)",
                "Ours (w/ Random)",
                "Ours (w/ BERT)"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "D19-1154",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1158table_1",
        "description": "3.3 Results. Table 1 shows the results. Compared with WM18, our RGB model outperforms under all conditions. As we have stated, our model is a generalization of their approach. The more complex transformation matrix in our RGB model is able to learn more information, such as the effects of covariance between color channels, and thus achieves a better performance than WM18. Note that our reimplementation of the original WM18 system lead to significantly better performance.5. According to the cosine similarity, the HSV model is superior for most test conditions (confirming our hypothesis about simpler modifier behaviour in this space). However for Delta-E, the RGB model and ensemble perform better. Unlike cosine, Delta-E is sensitive to differences in vector length, and we would argue it is the most appropriate metric because lengths are critical to measuring the extent of lightness and darkness of colors. Accordingly the HSV model does worse under this metric, as it more directly models the direction of color modifiers, but as a consequence this leads to errors in its length predictions. Overall the ensemble does well according to both metrics, and has the best performance for several test conditions with Delta-E.",
        "sentences": [
            "3.3 Results.",
            "Table 1 shows the results.",
            "Compared with WM18, our RGB model outperforms under all conditions.",
            "As we have stated, our model is a generalization of their approach.",
            "The more complex transformation matrix in our RGB model is able to learn more information, such as the effects of covariance between color channels, and thus achieves a better performance than WM18.",
            "Note that our reimplementation of the original WM18 system lead to significantly better performance.5.",
            "According to the cosine similarity, the HSV model is superior for most test conditions (confirming our hypothesis about simpler modifier behaviour in this space).",
            "However for Delta-E, the RGB model and ensemble perform better.",
            "Unlike cosine, Delta-E is sensitive to differences in vector length, and we would argue it is the most appropriate metric because lengths are critical to measuring the extent of lightness and darkness of colors.",
            "Accordingly the HSV model does worse under this metric, as it more directly models the direction of color modifiers, but as a consequence this leads to errors in its length predictions.",
            "Overall the ensemble does well according to both metrics, and has the best performance for several test conditions with Delta-E."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "RGB",
                "WM18*"
            ],
            [
                "RGB"
            ],
            [
                "WM18*",
                "RGB"
            ],
            [
                "WM18*"
            ],
            [
                "WM18 HSV"
            ],
            [
                "RGB",
                "Delta-E \u00b1 SD",
                "Ensemble"
            ],
            [
                "Delta-E \u00b1 SD"
            ],
            [
                "WM18 HSV"
            ],
            [
                "Delta-E \u00b1 SD"
            ]
        ],
        "n_sentence": 11.0,
        "table_id": "table_1",
        "paper_id": "D19-1158",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1160table_2",
        "description": "Table 2 shows the results for Experiment 2 (using Dundee gaze data and PTB dependencies). Under this setup, the gains decrease in comparison with the results on the parallel setup. This could be partially caused by not using parallel data. The improvements seem to be more consistent between the development and test sets. This could be related to the fact that we use a larger (more representative) treebank. When looking at the dev set scores, the most discriminative gaze feature is mean fix dur that increases LAS by +0.17 and first fix dur by +0.14. On the other hand, evaluation on the test set shows that the most informative gaze features are from the context group learned as multiple auxiliary tasks (context feats aux) and they improve the LAS score by +0.18, followed by fix prob and w\u22121 fix prob with +0.13, total fix dur with +0.12 and late feats aux with +0.10. Results from both datasets suggest that grouping gaze features and treating them as multiple auxiliary tasks can improve the model\u2019s learning.",
        "sentences": [
            "Table 2 shows the results for Experiment 2 (using Dundee gaze data and PTB dependencies).",
            "Under this setup, the gains decrease in comparison with the results on the parallel setup.",
            "This could be partially caused by not using parallel data.",
            "The improvements seem to be more consistent between the development and test sets.",
            "This could be related to the fact that we use a larger (more representative) treebank.",
            "When looking at the dev set scores, the most discriminative gaze feature is mean fix dur that increases LAS by +0.17 and first fix dur by +0.14.",
            "On the other hand, evaluation on the test set shows that the most informative gaze features are from the context group learned as multiple auxiliary tasks (context feats aux) and they improve the LAS score by +0.18, followed by fix prob and w\u22121 fix prob with +0.13, total fix dur with +0.12 and late feats aux with +0.10.",
            "Results from both datasets suggest that grouping gaze features and treating them as multiple auxiliary tasks can improve the model\u2019s learning."
        ],
        "class_sentence": [
            1,
            2,
            2,
            2,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null,
            [
                "LAS",
                "first fix dur"
            ],
            [
                "LAS",
                "fix prob",
                "total fix dur",
                "mean fix dur",
                "first fix dur",
                "w \u2212 1 fix dur",
                "w + 1 fix dur"
            ],
            [
                "Gaze features"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_2",
        "paper_id": "D19-1160",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1174table_6",
        "description": "Table 6 shows results for one run of our best method across different numbers of labels per post (1 to 5). Entries for values 6 and 7, which have less than 10 associated test samples, are omitted. The best results are observed for values 2 to 4, suggesting that our approach performs better on multi-label samples.",
        "sentences": [
            "Table 6 shows results for one run of our best method across different numbers of labels per post (1 to 5).",
            "Entries for values 6 and 7, which have less than 10 associated test samples, are omitted.",
            "The best results are observed for values 2 to 4, suggesting that our approach performs better on multi-label samples."
        ],
        "class_sentence": [
            1,
            2,
            1
        ],
        "header_mention": [
            [
                "#Labels per post"
            ],
            null,
            [
                "2",
                "4"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_6",
        "paper_id": "D19-1174",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1189table_3",
        "description": "Dialog. Table 3 shows the results of the evaluation of the baseline models and our proposed method in dialog generation. In the evaluation of perplexity, Transformer has much lower perplexity (18.0) compared to REDIAL (28.1), and KBRD can reach the best performance in perplexity. This demonstrates the power of Transformer in modeling natural language. In the evaluation of diversity, we find that the models based on Transformer significantly outperform REDIAL from the results of distinct 3-gram and 4-gram. Besides, it can be found that KBRD has a clear advantage in diversity over the baseline Transformer. This shows that our model can generate more diverse contents without decreasing fluency.",
        "sentences": [
            "Dialog.",
            "Table 3 shows the results of the evaluation of the baseline models and our proposed method in dialog generation.",
            "In the evaluation of perplexity, Transformer has much lower perplexity (18.0) compared to REDIAL (28.1), and KBRD can reach the best performance in perplexity.",
            "This demonstrates the power of Transformer in modeling natural language.",
            "In the evaluation of diversity, we find that the models based on Transformer significantly outperform REDIAL from the results of distinct 3-gram and 4-gram.",
            "Besides, it can be found that KBRD has a clear advantage in diversity over the baseline Transformer.",
            "This shows that our model can generate more diverse contents without decreasing fluency."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "PPL",
                "Transformer",
                "REDIAL",
                "KBRD"
            ],
            [
                "Transformer"
            ],
            [
                "Transformer",
                "REDIAL",
                "Dist-3",
                "Dist-4"
            ],
            [
                "KBRD",
                "Transformer"
            ],
            null
        ],
        "n_sentence": 7.0,
        "table_id": "table_3",
        "paper_id": "D19-1189",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1197table_1",
        "description": "5.4 Evaluation Results. Table 1 report the results of automatic evaluation on the question response generation task. We can see that S2S-Temp outperforms all baseline models in terms of all metrics, and the improvements are statistically significant (t-test with p-value< 0.01). The results demonstrate that when only limited pairs are available, S2STemp can effectively leverage unpaired data to enhance the quality of response generation. Although lacking fine-grained check, from the comparison among S2S-Temp-None, S2S-Temp-50%, and S2S-Temp, we can conclude that the performance of S2S-Temp improves with more unpaired data. Moreover, without unpaired data, our model is even worse than CVAE since the structured templates cannot be accurately estimated from such a few data, and as long as half of the unpaired data are available, the model outperforms the baseline models on most metrics. The results further verified the important role the unpaired data plays in learning of a response generation model from low resources. S2S-Temp is better than S2S-TempMLE, indicating that the adversarial learning approach can indeed enhance the relevance of responses regarding to messages.",
        "sentences": [
            "5.4 Evaluation Results.",
            "Table 1 report the results of automatic evaluation on the question response generation task.",
            "We can see that S2S-Temp outperforms all baseline models in terms of all metrics, and the improvements are statistically significant (t-test with p-value< 0.01).",
            "The results demonstrate that when only limited pairs are available, S2STemp can effectively leverage unpaired data to enhance the quality of response generation.",
            "Although lacking fine-grained check, from the comparison among S2S-Temp-None, S2S-Temp-50%, and S2S-Temp, we can conclude that the performance of S2S-Temp improves with more unpaired data.",
            "Moreover, without unpaired data, our model is even worse than CVAE since the structured templates cannot be accurately estimated from such a few data, and as long as half of the unpaired data are available, the model outperforms the baseline models on most metrics.",
            "The results further verified the important role the unpaired data plays in learning of a response generation model from low resources.",
            "S2S-Temp is better than S2S-TempMLE, indicating that the adversarial learning approach can indeed enhance the relevance of responses regarding to messages."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "S2S-Temp"
            ],
            [
                "S2S-Temp"
            ],
            [
                "S2S-Temp-None",
                "S2S-Temp-50%",
                "S2S-Temp"
            ],
            [
                "CVAE",
                "S2S-Temp"
            ],
            null,
            [
                "S2S-Temp",
                "S2S-Temp-MLE"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_1",
        "paper_id": "D19-1197",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1203table_2",
        "description": "Results. Table 2 shows performance comparison on the test set. Note that only the full supervised model (+Decide) and the fine-tuned model (+Plan) can appropriately operate every function required of an expert agent such as producing utterances, recommending items, and deciding to speak or recommend. Compared to recommendation-only models, our prediction Predict modules show significant improvements over the recommendation baselines on both per-turn and per-chat recommendations: 52% on Turn@1 and 34% on Turn@3. Chat scores are always higher than Turn, indicating that recommendations get better as more dialogue context is provided. The Decide module yields additional improvements over the Predict model in both generation and recommendation, with 67.6% decision accuracy, suggesting that the supervised signal of decisions to speak or recommend can contribute to better overall representations. In generation, our proposed models show comparable performance as the IR baseline models (e.g., BERT-Ranker). The +Decide model improves on the F1 generation score because it learns when to predict the templated recommendation utterance. As expected, +Plan slightly hurts most metrics of supervised evaluation, because it optimizes a different objective (the game objective), which might not systematically align with the supervised metrics. For example, a system optimized to maximize game objective should try to avoid incorrect recommendations even if humans made them. Game-related evaluations are shown in \u0e22\u0e074.3.",
        "sentences": [
            "Results.",
            "Table 2 shows performance comparison on the test set.",
            "Note that only the full supervised model (+Decide) and the fine-tuned model (+Plan) can appropriately operate every function required of an expert agent such as producing utterances, recommending items, and deciding to speak or recommend.",
            "Compared to recommendation-only models, our prediction Predict modules show significant improvements over the recommendation baselines on both per-turn and per-chat recommendations: 52% on Turn@1 and 34% on Turn@3.",
            "Chat scores are always higher than Turn, indicating that recommendations get better as more dialogue context is provided.",
            "The Decide module yields additional improvements over the Predict model in both generation and recommendation, with 67.6% decision accuracy, suggesting that the supervised signal of decisions to speak or recommend can contribute to better overall representations.",
            "In generation, our proposed models show comparable performance as the IR baseline models (e.g., BERT-Ranker).",
            "The +Decide model improves on the F1 generation score because it learns when to predict the templated recommendation utterance.",
            "As expected, +Plan slightly hurts most metrics of supervised evaluation, because it optimizes a different objective (the game objective), which might not systematically align with the supervised metrics.",
            "For example, a system optimized to maximize game objective should try to avoid incorrect recommendations even if humans made them.",
            "Game-related evaluations are shown in \u0e22\u0e074.3."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "+DECIDE",
                "+PLAN"
            ],
            [
                "Ours",
                "+PREDICT",
                "Turn@1",
                "Turn@3"
            ],
            [
                "Chat@1",
                "Chat@3",
                "Turn@1",
                "Turn@3"
            ],
            [
                "+DECIDE",
                "+PREDICT",
                "ACC"
            ],
            [
                "Ours",
                "BERT-RANKER"
            ],
            [
                "F1",
                "+DECIDE"
            ],
            [
                "+PLAN"
            ],
            null,
            null
        ],
        "n_sentence": 11.0,
        "table_id": "table_2",
        "paper_id": "D19-1203",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1210table_3",
        "description": "We adopt the same experimental protocols as in \u00a74, but increase the maximum sentence tokenlength from 20 to 50;. Table 3 shows the test-set results. In general, the algorithms we introduce again outperform the NoStruct baseline. In contrast to the crowdlabeled experiments, AP (slightly) outperformed the other algorithms.17 . DIY is the most difficult among the datasets we consider.",
        "sentences": [
            "We adopt the same experimental protocols as in \u00a74, but increase the maximum sentence tokenlength from 20 to 50;.",
            "Table 3 shows the test-set results.",
            "In general, the algorithms we introduce again outperform the NoStruct baseline.",
            "In contrast to the crowdlabeled experiments, AP (slightly) outperformed the other algorithms.17 .",
            "DIY is the most difficult among the datasets we consider."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "AP",
                "NoStruct"
            ],
            [
                "AP"
            ],
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_3",
        "paper_id": "D19-1210",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1230table_8",
        "description": "Impact of Different Features. Table 8 shows the comparison of the BiLSTM-CRF models with different feature embeddings. The reason we employ the basic model is to avoid that the impacts of different features are covered by contextual attention mechanisms. The results indicate that the semantic role feature is the most effective for negative focus detection, which confirms our analysis above. In addition, the performance of the system with all features is lower than that only using the semantic role feature. It might be that other features cannot capture more effective information than the semantic role features.",
        "sentences": [
            "Impact of Different Features.",
            "Table 8 shows the comparison of the BiLSTM-CRF models with different feature embeddings.",
            "The reason we employ the basic model is to avoid that the impacts of different features are covered by contextual attention mechanisms.",
            "The results indicate that the semantic role feature is the most effective for negative focus detection, which confirms our analysis above.",
            "In addition, the performance of the system with all features is lower than that only using the semantic role feature.",
            "It might be that other features cannot capture more effective information than the semantic role features."
        ],
        "class_sentence": [
            0,
            1,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Embeddings"
            ],
            null,
            [
                "word+semantic role"
            ],
            [
                "all features"
            ],
            [
                "word+semantic role"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_8",
        "paper_id": "D19-1230",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1271table_4",
        "description": "Table 4 reports our results on the MultiNLI dataset. Similar to Table 3, the first category of methods are single models and the second category of methods are ensemble models. On MultiNLI, we compare on two test sets (matched and mismatched) which represent in-domain and out-domain performance. ADIN significantly outperforms ESIM, a strong baseline on the both test sets. An ensemble of ADIN models also achieve competitive result on the MultiNLI dataset.",
        "sentences": [
            "Table 4 reports our results on the MultiNLI dataset.",
            "Similar to Table 3, the first category of methods are single models and the second category of methods are ensemble models.",
            "On MultiNLI, we compare on two test sets (matched and mismatched) which represent in-domain and out-domain performance.",
            "ADIN significantly outperforms ESIM, a strong baseline on the both test sets.",
            "An ensemble of ADIN models also achieve competitive result on the MultiNLI dataset."
        ],
        "class_sentence": [
            1,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Single Models",
                "Ensemble Models"
            ],
            [
                "Matched",
                "Mismatched"
            ],
            [
                "ADIN (ours)",
                "ESIM (Chen et al., 2016)"
            ],
            [
                "ADIN (ours)"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_4",
        "paper_id": "D19-1271",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1345table_3",
        "description": "3.4 Analysis of Memory Consumption . Table 3 reports the comparison of memory consumption and edges numbers between Text-GCN and our model. Results show that our model has a significant advantage in memory consumption.",
        "sentences": [
            "3.4 Analysis of Memory Consumption .",
            "Table 3 reports the comparison of memory consumption and edges numbers between Text-GCN and our model.",
            "Results show that our model has a significant advantage in memory consumption."
        ],
        "class_sentence": [
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                " Text-GCN",
                "Our Model"
            ],
            [
                "Our Model"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_3",
        "paper_id": "D19-1345",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1386table_5",
        "description": "Table 5 also contains an ablation study for the SUMFOCUS in which the topic did make a significant difference. This suggests that it may be easier to use the topic in a symbolic model and a model may benefit from combining symbolic and continuous representations.",
        "sentences": [
            "Table 5 also contains an ablation study for the SUMFOCUS in which the topic did make a significant difference.",
            "This suggests that it may be easier to use the topic in a symbolic model and a model may benefit from combining symbolic and continuous representations."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "SUMFOCUS -TOPIC"
            ],
            null
        ],
        "n_sentence": 2.0,
        "table_id": "table_5",
        "paper_id": "D19-1386",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1387table_4",
        "description": "Table  4  summarizes  our  results  on  the  Xsum dataset. Recall that summaries in this dataset are highly abstractive (see Table 1) consisting of a single sentence conveying the gist of the document. Extractive models here perform poorly as corroborated by the low performance of the LEAD baseline  (which  simply  selects  the  leading  sentence from the document), and the ORACLE (which selects a single-best sentence in each document) in Table  4. As  a  result,  we  do  not  report  results for extractive models on this dataset. The second block in Table 4 presents the results of various abstractive models taken from Narayan et al. (2018a) and also includes our own abstractive Transformer baseline. In the third block we show the results of our BERT summarizers which again are superior to all previously reported models (by a wide margin).",
        "sentences": [
            "Table  4  summarizes  our  results  on  the  Xsum dataset.",
            "Recall that summaries in this dataset are highly abstractive (see Table 1) consisting of a single sentence conveying the gist of the document.",
            "Extractive models here perform poorly as corroborated by the low performance of the LEAD baseline  (which  simply  selects  the  leading  sentence from the document), and the ORACLE (which selects a single-best sentence in each document) in Table  4.",
            "As  a  result,  we  do  not  report  results for extractive models on this dataset.",
            "The second block in Table 4 presents the results of various abstractive models taken from Narayan et al. (2018a) and also includes our own abstractive Transformer baseline.",
            "In the third block we show the results of our BERT summarizers which again are superior to all previously reported models (by a wide margin)."
        ],
        "class_sentence": [
            1,
            2,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "LEAD",
                "ORACLE.1"
            ],
            null,
            [
                "Abstractive",
                "TransformerABS"
            ],
            [
                "BERT-based"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_4",
        "paper_id": "D19-1387",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1399table_3",
        "description": "Table 3 shows the performance comparison between our work and previous work on the OntoNotes English dataset. Without the LSTM layers (i.e., L = 0), the proposed model with dependency information significantly improves the NER performance with more than 2 points in F1 compared to the baseline BiLSTMCRF (L = 0), which demonstrate the effective ness of dependencies for the NER task. Our best performing BiLSTM-CRF baseline (with Glove) achieves a F1 score of 87.78 which is better than or on par with previous works (Chiu and Nichols, 2016; Li et al., 2017; Ghaddar and Langlais, 2018) with extra features. This baseline also outperforms the CNN-based models (Strubell et al., 2017; Li et al., 2017). The BiLSTM-GCN-CRF model outperforms the BiLSTM-CRF model but achieves inferior performance compared to the proposed DGLSTM-CRF model. We believe it is challenging to preserve the surrounding context information with stacking GCN layers while contextual information is important for NER (Peters et al., 2018b). Overall, the 2-layer DGLSTMCRF model significantly (with p < 0.01) outperforms the best BiLSTM-CRF baseline and the BiLSTM-GCN-CRF model. As we can see from the table, increasing the number of layers (e.g., L = 3) does not give us further improvements for both BiLSTM-CRF and DGLSTM-CRF because such third-order information (e.g., the relationship among a words parent, its grandparent, and greatgrandparent) does not play an important role in indicating the presence of named entities.",
        "sentences": [
            "Table 3 shows the performance comparison between our work and previous work on the OntoNotes English dataset.",
            "Without the LSTM layers (i.e., L = 0), the proposed model with dependency information significantly improves the NER performance with more than 2 points in F1 compared to the baseline BiLSTMCRF (L = 0), which demonstrate the effective ness of dependencies for the NER task.",
            "Our best performing BiLSTM-CRF baseline (with Glove) achieves a F1 score of 87.78 which is better than or on par with previous works (Chiu and Nichols, 2016; Li et al., 2017; Ghaddar and Langlais, 2018) with extra features.",
            "This baseline also outperforms the CNN-based models (Strubell et al., 2017; Li et al., 2017).",
            "The BiLSTM-GCN-CRF model outperforms the BiLSTM-CRF model but achieves inferior performance compared to the proposed DGLSTM-CRF model.",
            "We believe it is challenging to preserve the surrounding context information with stacking GCN layers while contextual information is important for NER (Peters et al., 2018b).",
            "Overall, the 2-layer DGLSTMCRF model significantly (with p < 0.01) outperforms the best BiLSTM-CRF baseline and the BiLSTM-GCN-CRF model.",
            "As we can see from the table, increasing the number of layers (e.g., L = 3) does not give us further improvements for both BiLSTM-CRF and DGLSTM-CRF because such third-order information (e.g., the relationship among a words parent, its grandparent, and greatgrandparent) does not play an important role in indicating the presence of named entities."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "DGLSTM-CRF (L = 0)"
            ],
            [
                "BiLSTM-CRF (L = 2)",
                "Chiu and Nichols (2016)",
                "Li et al. (2017)",
                "Ghaddar and Langlais (2018)"
            ],
            [
                "BiLSTM-CRF (L = 2)",
                "Strubell et al. (2017)"
            ],
            [
                "BiLSTM-GCN-CRF",
                "BiLSTM-CRF (L = 0)",
                "BiLSTM-CRF (L = 1)",
                "BiLSTM-CRF (L = 2)",
                "BiLSTM-CRF (L = 3)",
                "DGLSTM-CRF (L = 0)",
                "DGLSTM-CRF (L = 1)",
                "DGLSTM-CRF (L = 2)",
                "DGLSTM-CRF (L = 3)"
            ],
            null,
            [
                "DGLSTM-CRF (L = 2)",
                "BiLSTM-GCN-CRF",
                "BiLSTM-CRF (L = 0)",
                "BiLSTM-CRF (L = 1)",
                "BiLSTM-CRF (L = 2)",
                "BiLSTM-CRF (L = 3)"
            ],
            [
                "BiLSTM-CRF (L = 3)",
                "DGLSTM-CRF (L = 3)",
                "BiLSTM-CRF (L = 2)",
                "DGLSTM-CRF (L = 2)"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_3",
        "paper_id": "D19-1399",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1399table_8",
        "description": "Effect of Dependency Quality To evaluate how the quality of dependency trees affect the performance, we train a state-of-the-art dependency parser (Dozat and Manning, 2017) using our training set and make prediction on the development/test set. We implemented the dependency parser using the AllenNLP package (Gardner et al., 2017). Table 8 shows the performance (LAS) of the dependency parser on four languages (i.e., OntoNotes English, OntoNotes Chinese, Catalan and Spanish) and the performance of DGLSTM-CRF against the best performing BiLSTM-CRF with ELMo. DGLSTMCRF even with predicted dependencies is able to consistently outperform the BiLSTM-CRF on four languages. However, the performance is still worse than the DGLSTM-CRF with gold dependencies, especially on the Catalan and Spanish. Such results suggest that it is essential to have high-quality dependency annotations available for the proposed model.",
        "sentences": [
            "Effect of Dependency Quality To evaluate how the quality of dependency trees affect the performance, we train a state-of-the-art dependency parser (Dozat and Manning, 2017) using our training set and make prediction on the development/test set.",
            "We implemented the dependency parser using the AllenNLP package (Gardner et al., 2017).",
            "Table 8 shows the performance (LAS) of the dependency parser on four languages (i.e., OntoNotes English, OntoNotes Chinese, Catalan and Spanish) and the performance of DGLSTM-CRF against the best performing BiLSTM-CRF with ELMo.",
            "DGLSTMCRF even with predicted dependencies is able to consistently outperform the BiLSTM-CRF on four languages.",
            "However, the performance is still worse than the DGLSTM-CRF with gold dependencies, especially on the Catalan and Spanish.",
            "Such results suggest that it is essential to have high-quality dependency annotations available for the proposed model."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "BiLSTM-CRF",
                "DGLSTM-CRF (Predicted)",
                "DGLSTM-CRF (Gold)",
                "English",
                "Catalan",
                "Chinese",
                "Spanish"
            ],
            [
                "DGLSTM-CRF (Predicted)",
                "BiLSTM-CRF"
            ],
            [
                "DGLSTM-CRF (Predicted)",
                "DGLSTM-CRF (Gold)",
                "Catalan",
                "Spanish"
            ],
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_8",
        "paper_id": "D19-1399",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1401table_2",
        "description": "Table 2 shows the IMDB test-set results of our best rationale-biased methods and their non-rationale counterparts (chosen based on dev set performance), as well as all of the baselines. Our bag-of-words models, RB-BOW-PROTO and RB-BOW-SVM, which combine rationale-biasing with pre-trained word embeddings, outperform all other baselines by a large margin of up to more than 20 absolute accuracy points for training sets of size 20 or smaller. On these particularly small datasets, the other rationale-aware baselines, RASVM and RA-CNN, do not perform as well. For larger training sets of size 60 and more, all systems perform better, as expected. In particular, the more complex CNN and BERT-based models have significantly improved performance with more supervision. Both RA-CNN and our RB-BERT, which combine pre-training with rationales, benefit substantially from rationale supervision, with up to more than 30 absolute accuracy points improvement. In absolute terms, our BERT models outperform their CNN counterparts and every other baseline in this training size range.",
        "sentences": [
            "Table 2 shows the IMDB test-set results of our best rationale-biased methods and their non-rationale counterparts (chosen based on dev set performance), as well as all of the baselines.",
            "Our bag-of-words models, RB-BOW-PROTO and RB-BOW-SVM, which combine rationale-biasing with pre-trained word embeddings, outperform all other baselines by a large margin of up to more than 20 absolute accuracy points for training sets of size 20 or smaller.",
            "On these particularly small datasets, the other rationale-aware baselines, RASVM and RA-CNN, do not perform as well.",
            "For larger training sets of size 60 and more, all systems perform better, as expected.",
            "In particular, the more complex CNN and BERT-based models have significantly improved performance with more supervision.",
            "Both RA-CNN and our RB-BERT, which combine pre-training with rationales, benefit substantially from rationale supervision, with up to more than 30 absolute accuracy points improvement.",
            "In absolute terms, our BERT models outperform their CNN counterparts and every other baseline in this training size range."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "RB-BOW-PROTO",
                "RB-BOW-SVM"
            ],
            [
                "RA-CNN"
            ],
            [
                "Number of Training Instances",
                "60"
            ],
            [
                "AVG-BERT",
                "CNN"
            ],
            [
                "RB-WAVG-BERT",
                "RA-CNN"
            ],
            [
                "RB-WAVG-BERT",
                "RA-CNN"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_2",
        "paper_id": "D19-1401",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1403table_3",
        "description": "We also evaluate our method with a real-world intent classification dataset ODIC. The experiment results are listed in Table 3. We can see that our proposed Induction Networks achieves best classification performances on all of the four experiments. In the distance metric learning models (Matching Networks, Prototypical Networks, Graph Network and Relation Network), all the learning occurs in representing features and measuring distances at the sample-wise level. Our work builds an induction module focusing on the class-wise level of representation, which we claim to be more robust to variation of samples in the support set. Our model also outperforms the latest optimization-based method-SNAIL. The difference between Induction Networks and SNAIL shown in Table 3 is statistically significant under the paired at the 99% significance level. In addition, the performance difference between our model and other baselines in the 10-shot scenario is more significant than in the 5-shot scenario. This is because in the 10-shot scenario, for the baseline models the improvement brought by a bigger data size is also diminished by more sample level noises.",
        "sentences": [
            "We also evaluate our method with a real-world intent classification dataset ODIC.",
            "The experiment results are listed in Table 3.",
            "We can see that our proposed Induction Networks achieves best classification performances on all of the four experiments.",
            "In the distance metric learning models (Matching Networks, Prototypical Networks, Graph Network and Relation Network), all the learning occurs in representing features and measuring distances at the sample-wise level.",
            "Our work builds an induction module focusing on the class-wise level of representation, which we claim to be more robust to variation of samples in the support set.",
            "Our model also outperforms the latest optimization-based method-SNAIL.",
            "The difference between Induction Networks and SNAIL shown in Table 3 is statistically significant under the paired at the 99% significance level.",
            "In addition, the performance difference between our model and other baselines in the 10-shot scenario is more significant than in the 5-shot scenario.",
            "This is because in the 10-shot scenario, for the baseline models the improvement brought by a bigger data size is also diminished by more sample level noises."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            2,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "Induction Networks (ours)"
            ],
            [
                "Matching Networks (Vinyals et al., 2016)",
                "Prototypical Networks (Snell et al., 2017)",
                "Graph Network (Garcia and Bruna, 2017)",
                "Relation Network (Sung et al., 2018)"
            ],
            [
                "Induction Networks (ours)"
            ],
            [
                "Induction Networks (ours)",
                "SNAIL (Mishra et al., 2018)"
            ],
            null,
            null,
            null
        ],
        "n_sentence": 9.0,
        "table_id": "table_3",
        "paper_id": "D19-1403",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1437table_2",
        "description": "Table 2 illustrates the BLEU scores of FlowSeq and baselines with advanced decoding methods such as iterative refinement, IWD and NPD rescoring. The first block in Table 2 includes the baseline results from autoregressive Transformer. For the sampling procedure in IWD and NPD, we sampled from a reduced-temperature model (Kingma and Dhariwal, 2018) to obtain high-quality samples. We vary the temperature within {0.1, 0.2, 0.3, 0.4, 0.5, 1.0} and select the best temperature based on the performance on development sets. The analysis of the impact of sampling temperature and other hyper-parameters on samples is in \u00a7 4.4. For FlowSeq, NPD obtains better results than IWD, showing that FlowSeq still falls behind auto-regressive Transformer on model data distributions. Comparing with CMLM (Ghazvininejad et al., 2019) with 10 iterations of refinement, which is a contemporaneous work that achieves state-of-the-art translation performance, FlowSeq obtains competitive performance on both WMT2014 and WMT2016 corpora, with only slight degradation in translation quality.",
        "sentences": [
            "Table 2 illustrates the BLEU scores of FlowSeq and baselines with advanced decoding methods such as iterative refinement, IWD and NPD rescoring.",
            "The first block in Table 2 includes the baseline results from autoregressive Transformer.",
            "For the sampling procedure in IWD and NPD, we sampled from a reduced-temperature model (Kingma and Dhariwal, 2018) to obtain high-quality samples.",
            "We vary the temperature within {0.1, 0.2, 0.3, 0.4, 0.5, 1.0} and select the best temperature based on the performance on development sets.",
            "The analysis of the impact of sampling temperature and other hyper-parameters on samples is in \u00a7 4.4.",
            "For FlowSeq, NPD obtains better results than IWD, showing that FlowSeq still falls behind auto-regressive Transformer on model data distributions.",
            "Comparing with CMLM (Ghazvininejad et al., 2019) with 10 iterations of refinement, which is a contemporaneous work that achieves state-of-the-art translation performance, FlowSeq obtains competitive performance on both WMT2014 and WMT2016 corpora, with only slight degradation in translation quality."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Autoregressive Methods"
            ],
            null,
            null,
            null,
            [
                "FlowSeq-base (NPD n = 15)",
                "FlowSeq-base (NPD n = 30)",
                "FlowSeq-base (IWD n = 15)"
            ],
            [
                "FlowSeq-base (IWD n = 15)",
                "FlowSeq-base (NPD n = 15)",
                "FlowSeq-base (NPD n = 30)",
                "CMLM-base (refinement 10)"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_2",
        "paper_id": "D19-1437",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1440table_2",
        "description": "In the experiment, DBM was instantiated using WordNet 3.0 (only glosses structured into its semantic roles), VFM used VisualGenome 1.4 and CKG used ConceptNet 5. Table 2 depicts the outcome of the evaluation of the combined model. In order to answer Q1, we compared our model to a set of reference systems in the task of discriminative attribute identification (see table 1. While EDAM performs lower than the state-ofthe-art (F1-Score 0.76 vs 0.69), from existing approaches EDAM provides the only explainable model. Compared to EDAM, all the top performing models used distributional methods derived from large-scale corpora, while EDAM uses the combination of definition, visual features and commonsense structured KBs. In order to answer Q2, we evaluated the performance of each component of the model for the six dual-categories. The goal is to provide a quantitative basis to understand the contribution of each component in addressing the task.",
        "sentences": [
            "In the experiment, DBM was instantiated using WordNet 3.0 (only glosses structured into its semantic roles), VFM used VisualGenome 1.4 and CKG used ConceptNet 5.",
            "Table 2 depicts the outcome of the evaluation of the combined model.",
            "In order to answer Q1, we compared our model to a set of reference systems in the task of discriminative attribute identification (see table 1.",
            "While EDAM performs lower than the state-ofthe-art (F1-Score 0.76 vs 0.69), from existing approaches EDAM provides the only explainable model.",
            "Compared to EDAM, all the top performing models used distributional methods derived from large-scale corpora, while EDAM uses the combination of definition, visual features and commonsense structured KBs.",
            "In order to answer Q2, we evaluated the performance of each component of the model for the six dual-categories.",
            "The goal is to provide a quantitative basis to understand the contribution of each component in addressing the task."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "EDAM(DBM+CKG+VFM)"
            ],
            [
                "EDAM(DBM+CKG+VFM)"
            ],
            [
                "EDAM(DBM+CKG+VFM)"
            ],
            null,
            null
        ],
        "n_sentence": 7.0,
        "table_id": "table_2",
        "paper_id": "D19-1440",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1460table_7",
        "description": "5.1 Results . DA/IC/SL Results. Table 7 presents the MFC, LSTM, and ELMO results for each domain, on the subset of 15,000 conversations annotated at both the turn and sentence levels. In general for both granularities Turn and Sentence, both LSTM, and ELMO outperform MFC significantly across all domains. Relative to the LSTM, we find that ELMO obtains a modest increase in IC accuracy of 0.41 to 2.20 F1 points and a significant increase in SL F1 score on all domains.",
        "sentences": [
            "5.1 Results .",
            "DA/IC/SL Results.",
            "Table 7 presents the MFC, LSTM, and ELMO results for each domain, on the subset of 15,000 conversations annotated at both the turn and sentence levels.",
            "In general for both granularities Turn and Sentence, both LSTM, and ELMO outperform MFC significantly across all domains.",
            "Relative to the LSTM, we find that ELMO obtains a modest increase in IC accuracy of 0.41 to 2.20 F1 points and a significant increase in SL F1 score on all domains."
        ],
        "class_sentence": [
            0,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "LSTM",
                "ELMO"
            ],
            [
                "ELMO",
                "IC",
                "SL"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_7",
        "paper_id": "D19-1460",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1477table_2",
        "description": "We evaluate the performance of the models using the same metrics used for the optimization process (see Section 4.4). In Table 2 we report the results, that we compute as the average of ten runs with random parameter initialization. We use the unpaired Welch s t test to check for statistically significant difference between models. LING+random never improves over LING. We believe this is due to the fact that most of the authors have just one tweet, which hinders the possibility to learn at training time the representations of the users used at test time. We find that both PV and N2V user representations lead to an improvement over LING. N2V vectors are especially effective for the Stance detection task, where LING+N2V outperforms LING+PV, while for Hate Speech the performance\r\nof the two models is comparable (the difference between LING+PV and LING+N2V is not statistically significative due to the high variance of the LING+PV results - see extended results table in the supplementary material). Finally, our model outperforms any other model on both Stance and Hate Speech detection. This result confirms our initial hypothesis that a social attention mechanism which is able to assign different relevance to different neighbours allows for a more dynamic encoding of homophily relations in author embeddings and, in turn, leads to better results on the prediction tasks.",
        "sentences": [
            "We evaluate the performance of the models using the same metrics used for the optimization process (see Section 4.4).",
            "In Table 2 we report the results, that we compute as the average of ten runs with random parameter initialization.",
            "We use the unpaired Welch s t test to check for statistically significant difference between models.",
            "LING+random never improves over LING.",
            "We believe this is due to the fact that most of the authors have just one tweet, which hinders the possibility to learn at training time the representations of the users used at test time.",
            "We find that both PV and N2V user representations lead to an improvement over LING.",
            "N2V vectors are especially effective for the Stance detection task, where LING+N2V outperforms LING+PV, while for Hate Speech the performance\r\nof the two models is comparable (the difference between LING+PV and LING+N2V is not statistically significative due to the high variance of the LING+PV results - see extended results table in the supplementary material).",
            "Finally, our model outperforms any other model on both Stance and Hate Speech detection.",
            "This result confirms our initial hypothesis that a social attention mechanism which is able to assign different relevance to different neighbours allows for a more dynamic encoding of homophily relations in author embeddings and, in turn, leads to better results on the prediction tasks."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "LING",
                "LING+random"
            ],
            null,
            [
                "LING+PV",
                "LING+N2V",
                "LING"
            ],
            [
                "LING+N2V",
                "LING+PV",
                "Hate"
            ],
            [
                "LING+GAT"
            ],
            null
        ],
        "n_sentence": 9.0,
        "table_id": "table_2",
        "paper_id": "D19-1477",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1486table_3",
        "description": "5.4 Result Analysis Zero-shot Intent Classification. Table 3 summarizes the average results over 10 runs, where the top 2 results are highlighted in bold. The baseline results on SNIPS-NLU are taken from Xia et al. (2018). The results show that ReCapsNet-ZS outperforms all the baselines, demonstrating its superiority in tackling zero-shot intent classification. We can also see that ReCapsNet-ZS performs better than either ReCapsNet-ZS-Dim or ReCapsNetZS-TM, which shows the effectiveness of both of the dimensional attention mechanism and the transformation matrix construction method.",
        "sentences": [
            "5.4 Result Analysis Zero-shot Intent Classification.",
            "Table 3 summarizes the average results over 10 runs, where the top 2 results are highlighted in bold.",
            "The baseline results on SNIPS-NLU are taken from Xia et al. (2018).",
            "The results show that ReCapsNet-ZS outperforms all the baselines, demonstrating its superiority in tackling zero-shot intent classification.",
            "We can also see that ReCapsNet-ZS performs better than either ReCapsNet-ZS-Dim or ReCapsNetZS-TM, which shows the effectiveness of both of the dimensional attention mechanism and the transformation matrix construction method."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "SNIPS-NLU"
            ],
            [
                "ReCapsNet-ZS"
            ],
            [
                "ReCapsNet-ZS",
                "ReCapsNet-ZS-Dim",
                "ReCapsNet-ZS-TM"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_3",
        "paper_id": "D19-1486",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1496table_7",
        "description": "Embedding Estimator. Although DISP is not required to recover the ground-truth perturbed tokens, the embedding estimator plays an important role to derive appropriate embedding vectors that obtain the original semantics. We first evaluate the performance of embedding estimator as a regression task. The RMSE scores of estimated embeddings are 0.0442 and 0.1030 in SST-2 and IMDb datasets, which are small enough to derive satisfactory tokens. To further demonstrate the robustness of the embedding estimator and estimated embeddings, we identify the perturbations with our discriminator and replace them with the ground-truth tokens. Table 7 shows the accuracy scores over different types of attacks in the SST-2 dataset. DISP and DISPG denotes the recovery performance with our estimator and goundtruth tokens, respectively. More specifically, the accuracy of DISPG presents the upperbound performance gained by the embedding estimator. The experimental results demonstrate the robustness of the embedding estimator while the estimated embeddings only slightly lower the accuracy of DISP.",
        "sentences": [
            "Embedding Estimator.",
            "Although DISP is not required to recover the ground-truth perturbed tokens, the embedding estimator plays an important role to derive appropriate embedding vectors that obtain the original semantics.",
            "We first evaluate the performance of embedding estimator as a regression task.",
            "The RMSE scores of estimated embeddings are 0.0442 and 0.1030 in SST-2 and IMDb datasets, which are small enough to derive satisfactory tokens.",
            "To further demonstrate the robustness of the embedding estimator and estimated embeddings, we identify the perturbations with our discriminator and replace them with the ground-truth tokens.",
            "Table 7 shows the accuracy scores over different types of attacks in the SST-2 dataset.",
            "DISP and DISPG denotes the recovery performance with our estimator and goundtruth tokens, respectively.",
            "More specifically, the accuracy of DISPG presents the upperbound performance gained by the embedding estimator.",
            "The experimental results demonstrate the robustness of the embedding estimator while the estimated embeddings only slightly lower the accuracy of DISP."
        ],
        "class_sentence": [
            0,
            2,
            2,
            2,
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null,
            null,
            [
                "DISP G",
                "DISP"
            ],
            [
                "DISP G",
                "Embed"
            ],
            [
                "DISP G",
                "DISP",
                "Embed"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_7",
        "paper_id": "D19-1496",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1498table_3",
        "description": "We first analyse the performance of our main model (EoG) using different pre-trained word embeddings. Table 3 shows the performance difference between domain-specific (PubMed) (Chiu et al., 2016), general-domain (GloVe) (Pennington et al., 2014) and randomly initialized (random) word embeddings. As observed, our proposed model performs consistently with both in-domain and out-of-domain pre-trained word embeddings. The low performance of random embeddings is due to the small size of the dataset, which results in lower quality embeddings.",
        "sentences": [
            "We first analyse the performance of our main model (EoG) using different pre-trained word embeddings.",
            "Table 3 shows the performance difference between domain-specific (PubMed) (Chiu et al., 2016), general-domain (GloVe) (Pennington et al., 2014) and randomly initialized (random) word embeddings.",
            "As observed, our proposed model performs consistently with both in-domain and out-of-domain pre-trained word embeddings.",
            "The low performance of random embeddings is due to the small size of the dataset, which results in lower quality embeddings."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "EoG (PubMed)",
                "EoG (GloVe)",
                "EoG (random)"
            ],
            [
                "Intra",
                "Inter"
            ],
            [
                "EoG (random)"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "D19-1498",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1511table_2",
        "description": "As shown in Table2, CVAE(qt) incorporates question type information, and slightly improves the performance compared with CVAE, because it could help generate questions with reasonable type. Furthermore, answer information could further improve the overall performance. First, RL-CVAE and A-CVAE perform fairly well with lower perplexities, because they could generate fluent questions. Second, they can obtain higher RubG and RubA scores, showing that taking advantage of coherence between question and answer could further enhance semantic coherence between post and question. Third, they obtain higher distinct values, because answer information provides extra clues to guide question generation. Our RL-CVAE performs better than A-CVAE. This is because the decoder\u2019s hidden states are utilized to represent generated question word tokens in A-CVAE, which is asymmetric to the groundtruth question in adversarial process. Instead, the question sentence is observed in RL-CVAE after the model arrives at the end of a sequence, and it is then used to calculate coherence degree directly.",
        "sentences": [
            "As shown in Table2, CVAE(qt) incorporates question type information, and slightly improves the performance compared with CVAE, because it could help generate questions with reasonable type.",
            "Furthermore, answer information could further improve the overall performance.",
            "First, RL-CVAE and A-CVAE perform fairly well with lower perplexities, because they could generate fluent questions.",
            "Second, they can obtain higher RubG and RubA scores, showing that taking advantage of coherence between question and answer could further enhance semantic coherence between post and question.",
            "Third, they obtain higher distinct values, because answer information provides extra clues to guide question generation.",
            "Our RL-CVAE performs better than A-CVAE.",
            "This is because the decoder\u2019s hidden states are utilized to represent generated question word tokens in A-CVAE, which is asymmetric to the groundtruth question in adversarial process.",
            "Instead, the question sentence is observed in RL-CVAE after the model arrives at the end of a sequence, and it is then used to calculate coherence degree directly."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            1,
            2,
            2
        ],
        "header_mention": [
            [
                "CVAE (qt)",
                "CVAE"
            ],
            [
                "A-CVAE",
                "CVAE"
            ],
            [
                "RL-CVAE",
                "A-CVAE",
                "PPL"
            ],
            [
                "RL-CVAE",
                "A-CVAE",
                "RubG",
                "RubA"
            ],
            null,
            [
                "RL-CVAE",
                "A-CVAE"
            ],
            [
                "A-CVAE"
            ],
            [
                "RL-CVAE"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_2",
        "paper_id": "D19-1511",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1515table_3",
        "description": "Besides the Viterbi decoding algorithm used in prediction in CRFs, we also experiment with a smoothing decoding algorithm. While Viterbi finds the MAP label sequence conditioned on the input sequence arg maxy p(y|x), smoothing decoding finds the best label for each input xt: arg maxyt p(yt|x). This makes sense in some scenarios where we want to refine the predicted grounding of one entity by referring to the context instead of attempting to ground all entities mentioned in the description. Table 3 shows that in both Hard-Label Chain CRF and Soft-Label Chain CRF, smoothing decoding gives a prediction accuracy 0.04% higher than Viterbi decoding.",
        "sentences": [
            "Besides the Viterbi decoding algorithm used in prediction in CRFs, we also experiment with a smoothing decoding algorithm.",
            "While Viterbi finds the MAP label sequence conditioned on the input sequence arg maxy p(y|x), smoothing decoding finds the best label for each input xt: arg maxyt p(yt|x).",
            "This makes sense in some scenarios where we want to refine the predicted grounding of one entity by referring to the context instead of attempting to ground all entities mentioned in the description.",
            "Table 3 shows that in both Hard-Label Chain CRF and Soft-Label Chain CRF, smoothing decoding gives a prediction accuracy 0.04% higher than Viterbi decoding."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "Viterbi (MAP)"
            ],
            null,
            [
                "HL-CCRF",
                "SL-CCRF",
                "Smoothing",
                "Viterbi (MAP)"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "D19-1515",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1531table_1",
        "description": "Results . Table 1 shows that Hybrid Ori significantly decreases the difference of association between two genders as indicated by MWEAT_Diff and p-value. Other methods only show marginal mitigation effects in terms of p-value. We also find that the performance of our mitigation methods on word similarity is largely preserved as indicated by the Pearson correlation scores.",
        "sentences": [
            "Results .",
            "Table 1 shows that Hybrid Ori significantly decreases the difference of association between two genders as indicated by MWEAT_Diff and p-value.",
            "Other methods only show marginal mitigation effects in terms of p-value.",
            "We also find that the performance of our mitigation methods on word similarity is largely preserved as indicated by the Pearson correlation scores."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Hybrid_Ori",
                "MWEAT\u2013Diff",
                "MWEAT\u2013p-value"
            ],
            [
                "MWEAT\u2013p-value"
            ],
            [
                "Hybrid_Ori",
                "Word Similarity"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_1",
        "paper_id": "D19-1531",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1535table_2",
        "description": "3.2.2 Variant Analysis. Besides baselines, we also conduct experiments with several variants of STAR to further validate the design of our model. As shown in Table 2, there are three variants with ablation: \u201c\u2013 Phase I\u201d takes out SplitNet and performs Phase II on word level; \u201c\u2013 Phase II\u201d performs random guess in the recombination process for testing; and \u201c\u2013 RL\u201d only contains pre-training. The SymAcc drops from about 55% to 40% by ablating Phase I, and to 23% by ablating Phase II. Their poor performances indicate both of the two phases are indispensable. \u201c\u2013 RL\u201d also performs worse, which again demonstrates the rationality of applying RL. Three more variants are presented with different designs of R(q, z) to prove the efficiency and effectiveness of Equation 5 as a reward. \u201c+ Basic Reward\u201d represents the most straightforward REINFORCE algorithm, which samples both q \u2208 Q and \u02dcz\u2208Z, then takes r(z, \u02dcz) as R(q, z). \u201c+ Oracle Reward\u201d assumes the con\ufb02icts are always correct and rewrites R(q, z) as max\u02dcz\u2208Z (r(z, \u02dcz)). \u201c+ Uniform Reward\u201d assigns the same probability to all \u02dcz\u2208Z r(z, \u02dcz)). As shown in Table 2 and Figure 4, STAR learns better and faster than the variants due to the reasonable reward design. In fact, as mentioned in Section 2.1, the vast action space of the most straightforward REINFORCE algorithm leads to poor learning. STAR shrinks the space from |Q|\u00b7|Z| down to |Q| by enumerating z\u02dc. Meanwhile, statistics show that STAR obtains a 15\u00d7 speedup over \u201c+ Basic Reward\u201d on the convergence time.",
        "sentences": [
            "3.2.2 Variant Analysis.",
            "Besides baselines, we also conduct experiments with several variants of STAR to further validate the design of our model.",
            "As shown in Table 2, there are three variants with ablation: \u201c\u2013 Phase I\u201d takes out SplitNet and performs Phase II on word level; \u201c\u2013 Phase II\u201d performs random guess in the recombination process for testing; and \u201c\u2013 RL\u201d only contains pre-training.",
            "The SymAcc drops from about 55% to 40% by ablating Phase I, and to 23% by ablating Phase II.",
            "Their poor performances indicate both of the two phases are indispensable.",
            "\u201c\u2013 RL\u201d also performs worse, which again demonstrates the rationality of applying RL.",
            "Three more variants are presented with different designs of R(q, z) to prove the efficiency and effectiveness of Equation 5 as a reward.",
            "\u201c+ Basic Reward\u201d represents the most straightforward REINFORCE algorithm, which samples both q \u2208 Q and \u02dcz\u2208Z, then takes r(z, \u02dcz) as R(q, z).",
            "\u201c+ Oracle Reward\u201d assumes the con\ufb02icts are always correct and rewrites R(q, z) as max\u02dcz\u2208Z (r(z, \u02dcz)).",
            "\u201c+ Uniform Reward\u201d assigns the same probability to all \u02dcz\u2208Z r(z, \u02dcz)).",
            "As shown in Table 2 and Figure 4, STAR learns better and faster than the variants due to the reasonable reward design.",
            "In fact, as mentioned in Section 2.1, the vast action space of the most straightforward REINFORCE algorithm leads to poor learning.",
            "STAR shrinks the space from |Q|\u00b7|Z| down to |Q| by enumerating z\u02dc. Meanwhile, statistics show that STAR obtains a 15\u00d7 speedup over \u201c+ Basic Reward\u201d on the convergence time."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1,
            1,
            2,
            2,
            2,
            2,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "\u2013 Phase I",
                "\u2013 Phase II",
                "\u2013 RL"
            ],
            [
                "SymAcc (%)",
                "\u2013 Phase I",
                "\u2013 Phase II"
            ],
            [
                "\u2013 Phase I",
                "\u2013 Phase II"
            ],
            [
                "\u2013 RL"
            ],
            null,
            [
                "+ Basic Reward"
            ],
            [
                "+ Oracle Reward"
            ],
            [
                "+ Uniform Reward"
            ],
            [
                "STAR"
            ],
            null,
            [
                "STAR",
                "+ Basic Reward"
            ]
        ],
        "n_sentence": 13.0,
        "table_id": "table_2",
        "paper_id": "D19-1535",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1630table_4",
        "description": "Results . Table 4 shows the results. CBOW does not perform well on either datasets. On the CB data, the heuristics based on linguistic generalizations is a strong baseline, performing better than MNLIB. We gain a lot of performance with supervision from CB only (CBB), which aligns with McCoy et al.\u2019s observation that BERT performs very well when trained with in-domain data. The best results are obtained by MNLI+CBB on CB and by MNLIB on MultiNLI, but still lag behind human performance. While MNLI+CBB gives the best performance on CB, it does not perform well on MultiNLI. This is in line with Liu et al. (2019) who found that fine-tuning on datasets that test for a specific linguistic phenomenon decrease the performance on the original dataset.",
        "sentences": [
            "Results .",
            "Table 4 shows the results. CBOW does not perform well on either datasets.",
            "On the CB data, the heuristics based on linguistic generalizations is a strong baseline, performing better than MNLIB.",
            "We gain a lot of performance with supervision from CB only (CBB), which aligns with McCoy et al.\u2019s observation that BERT performs very well when trained with in-domain data.",
            "The best results are obtained by MNLI+CBB on CB and by MNLIB on MultiNLI, but still lag behind human performance.",
            "While MNLI+CBB gives the best performance on CB, it does not perform well on MultiNLI.",
            "This is in line with Liu et al. (2019) who found that fine-tuning on datasets that test for a specific linguistic phenomenon decrease the performance on the original dataset."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "CBOW"
            ],
            [
                "CB",
                "Heuristics",
                "MNLIB"
            ],
            [
                "CBB"
            ],
            [
                "MNLI+CBB",
                "CB",
                "MNLIB",
                "MultiNLI"
            ],
            [
                "MNLI+CBB",
                "CB",
                "MultiNLI"
            ],
            null
        ],
        "n_sentence": 7.0,
        "table_id": "table_4",
        "paper_id": "D19-1630",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1654table_3",
        "description": "4.2 Experimental Results and Analysis. Experimental results are reported in Table 3. From Table 3 we draw the following conclusions. First, sentence-level sentiment classifiers (TextCNN and LSTM) achieve competitive results on SemEval14 Restaurant Review dataset but perform poorly on MAMS datasets. This verifies that MAMS datasets can alleviate the task degeneration problem encountered in Restaurant dataset for ABSA. Second, most advanced and complex ABAS methods, which achieve impressive results on Restaurant dataset, perform poorly on the MAMS-small dataset. This verifies that MAMS (small) is more challenging than SemEval-14 Restaurant Review dataset. Third, attention based models without effectively modeling word order (e.g., MemNet and AEN) perform worst on MAMS since they lose word order information and cannot identify which part of context describes the given aspect. Fourth, CapsNet outperforms non-BERT baselines on 4 of 6 datasets, showing the potential of applying capsule networks to aspect-based sentiment analysis task. In addition, CapsNet-BERT performs significantly better than other models including BERT, indicating that combining capsule network and BERT can obtain additional improvement compared to vanilla BERT.",
        "sentences": [
            "4.2 Experimental Results and Analysis.",
            "Experimental results are reported in Table 3.",
            "From Table 3 we draw the following conclusions.",
            "First, sentence-level sentiment classifiers (TextCNN and LSTM) achieve competitive results on SemEval14 Restaurant Review dataset but perform poorly on MAMS datasets.",
            "This verifies that MAMS datasets can alleviate the task degeneration problem encountered in Restaurant dataset for ABSA.",
            "Second, most advanced and complex ABAS methods, which achieve impressive results on Restaurant dataset, perform poorly on the MAMS-small dataset.",
            "This verifies that MAMS (small) is more challenging than SemEval-14 Restaurant Review dataset.",
            "Third, attention based models without effectively modeling word order (e.g., MemNet and AEN) perform worst on MAMS since they lose word order information and cannot identify which part of context describes the given aspect.",
            "Fourth, CapsNet outperforms non-BERT baselines on 4 of 6 datasets, showing the potential of applying capsule networks to aspect-based sentiment analysis task.",
            "In addition, CapsNet-BERT performs significantly better than other models including BERT, indicating that combining capsule network and BERT can obtain additional improvement compared to vanilla BERT."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "TextCNN",
                "LSTM",
                "MAMS",
                "Restaurant"
            ],
            [
                "MAMS",
                "Restaurant"
            ],
            [
                "MAMS-small",
                "Restaurant"
            ],
            [
                "MAMS-small",
                "Restaurant"
            ],
            [
                "MemNet",
                "AEN"
            ],
            [
                "CapsNet"
            ],
            [
                "CapsNet-BERT",
                "BERT"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_3",
        "paper_id": "D19-1654",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1113table_6",
        "description": "Finally, Table 6 shows results for nominal and verbal predicates as well as for different (gold) role labels. In comparison to mate-tools, we can see that PathLSTM improves precision for all argument types of nominal predicates. For verbal predicates, improvements can be observed in terms of recall of proto-agent (A0) and protopatient (A1) roles, with slight gains in precision for the A2 role. Overall, PathLSTM does slightly worse with respect to modifier roles, which it labels with higher precision but at the cost of recall.",
        "sentences": [
            "Finally, Table 6 shows results for nominal and verbal predicates as well as for different (gold) role labels.",
            "In comparison to mate-tools, we can see that PathLSTM improves precision for all argument types of nominal predicates.",
            "For verbal predicates, improvements can be observed in terms of recall of proto-agent (A0) and protopatient (A1) roles, with slight gains in precision for the A2 role.",
            "Overall, PathLSTM does slightly worse with respect to modifier roles, which it labels with higher precision but at the cost of recall."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "Predicate & Role Label"
            ],
            [
                "POS Improvement over mate-tools",
                "PathLSTM"
            ],
            [
                "verb / A0",
                "verb / A1",
                "R (%)",
                "P (%)"
            ],
            [
                "PathLSTM",
                "P (%)",
                "R (%)"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_6",
        "paper_id": "P16-1113",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1136table_1",
        "description": "Table 1 summarizes the knowledge base (KB) completion results on the NCI-PID test. The rows compare our compositional learning approach ALL-PATHS+NODES with prior approaches. The comparison of the two columns demonstrates the impact when text is jointly embedded with KB. Our compositional learning approach significantly outperforms all other approaches in both evaluation metrics (MAP and HITS@10). Moreover, jointly embedding text and KB led to substantial improvement, compared to embedding KB only. Finally, modeling nodes in the paths offers significant gains (ALLPATHS+NODE gains 3 points in MAP over ALLPATHS).",
        "sentences": [
            "Table 1 summarizes the knowledge base (KB) completion results on the NCI-PID test.",
            "The rows compare our compositional learning approach ALL-PATHS+NODES with prior approaches.",
            "The comparison of the two columns demonstrates the impact when text is jointly embedded with KB.",
            "Our compositional learning approach significantly outperforms all other approaches in both evaluation metrics (MAP and HITS@10).",
            "Moreover, jointly embedding text and KB led to substantial improvement, compared to embedding KB only.",
            "Finally, modeling nodes in the paths offers significant gains (ALLPATHS+NODE gains 3 points in MAP over ALLPATHS)."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "ALL-PATHS+NODES d=100"
            ],
            [
                "KB",
                "KB and Text"
            ],
            [
                "ALL-PATHS+NODES d=100",
                "MAP",
                "HITS@10"
            ],
            [
                "KB",
                "KB and Text"
            ],
            [
                "ALL-PATHS+NODES d=100",
                "ALL-PATHS d=100"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_1",
        "paper_id": "P16-1136",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1159table_8",
        "description": "Table 8 shows the results on English-German translation. Our approach still significantly outperforms MLE and achieves comparable results with state-of-the-art systems even though Luong et al. (2015a) used a much deeper neural network. We believe that our work can be applied to their architecture easily.",
        "sentences": [
            "Table 8 shows the results on English-German translation.",
            "Our approach still significantly outperforms MLE and achieves comparable results with state-of-the-art systems even though Luong et al. (2015a) used a much deeper neural network.",
            "We believe that our work can be applied to their architecture easily."
        ],
        "class_sentence": [
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "this work",
                "gated RNN with search + PosUnk",
                "MLE",
                "Existing end-to-end NMT systems",
                "Luong et al. (2015a)"
            ],
            [
                "this work"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_8",
        "paper_id": "P16-1159",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1181table_2",
        "description": "Table 2 shows the accuracies of all baseline systems on the development sets. For WSJ all three algorithms achieve similar results which shows that MARMOT is a competitive baseline. As can be seen, predicting sentence boundaries for the Switchboard dataset is a more difficult task than for well-formatted text like the WSJ.",
        "sentences": [
            "Table 2 shows the accuracies of all baseline systems on the development sets.",
            "For WSJ all three algorithms achieve similar results which shows that MARMOT is a competitive baseline.",
            "As can be seen, predicting sentence boundaries for the Switchboard dataset is a more difficult task than for well-formatted text like the WSJ."
        ],
        "class_sentence": [
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "WSJ",
                "MARMOT"
            ],
            [
                "Switchboard",
                "WSJ"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_2",
        "paper_id": "P16-1181",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1195table_5",
        "description": "Table 5 shows the MRR on the RET task for CODE-NN and RET-IR, averaged over 20 runs for C# and SQL. CODE-NN outperforms the baseline by about 16% for C# and SQL. RET-IR can only output code snippets that are annotated with NL as potential matches. On the other hand, CODENN can rank even unannotated code snippets and nominate them as potential candidates. Hence, it can leverage vast amounts of such code available in online repositories like Github. To speed up retrieval when using CODE-NN , it could be one of the later stages in a multi-stage retrieval system and candidates may also be ranked in parallel.",
        "sentences": [
            "Table 5 shows the MRR on the RET task for CODE-NN and RET-IR, averaged over 20 runs for C# and SQL.",
            "CODE-NN outperforms the baseline by about 16% for C# and SQL.",
            "RET-IR can only output code snippets that are annotated with NL as potential matches.",
            "On the other hand, CODENN can rank even unannotated code snippets and nominate them as potential candidates.",
            "Hence, it can leverage vast amounts of such code available in online repositories like Github.",
            "To speed up retrieval when using CODE-NN , it could be one of the later stages in a multi-stage retrieval system and candidates may also be ranked in parallel."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            2,
            2
        ],
        "header_mention": [
            [
                "CODE-NN",
                "RET-IR",
                "C#",
                "SQL"
            ],
            [
                "CODE-NN",
                "RET-IR",
                "C#",
                "SQL"
            ],
            [
                "RET-IR"
            ],
            [
                "CODE-NN"
            ],
            [
                "CODE-NN"
            ],
            [
                "CODE-NN"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_5",
        "paper_id": "P16-1195",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1203table_5",
        "description": "The phonological features are important separation criteria as evidenced by the drop in performance when they are excluded from the experimental setup (Table 5). Specifically, using all features except phonological features is equivalent to using phonological features alone (about F = 79% in both cases) and slightly worse that using all name-intrinsic features (about F = 80%).",
        "sentences": [
            "The phonological features are important separation criteria as evidenced by the drop in performance when they are excluded from the experimental setup (Table 5).",
            "Specifically, using all features except phonological features is equivalent to using phonological features alone (about F = 79% in both cases) and slightly worse that using all name-intrinsic features (about F = 80%)."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Without phonological features",
                "Only phonological features"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_5",
        "paper_id": "P16-1203",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1209table_4",
        "description": "Next we investigated the results obtained by the various models using only 50 dim word embedding features. Table 4 shows the results obtained by different RNNs and the window based neural network (NN). In this case RNN models are giving better results than the NN model for both the tasks. In particular performance of Bi-LSTM models are best than others in both the tasks. We observe that for the task A, RNN models obtained 1.2% to 3% improvement in F1-score than the baseline NN performance. Similarly 2.55% to 4% improvement in F1-score are observed for the task B, with Bi-LSTM model obtaining more than 4% improvement.",
        "sentences": [
            "Next we investigated the results obtained by the various models using only 50 dim word embedding features.",
            "Table 4 shows the results obtained by different RNNs and the window based neural network (NN).",
            "In this case RNN models are giving better results than the NN model for both the tasks.",
            "In particular performance of Bi-LSTM models are best than others in both the tasks.",
            "We observe that for the task A, RNN models obtained 1.2% to 3% improvement in F1-score than the baseline NN performance.",
            "Similarly 2.55% to 4% improvement in F1-score are observed for the task B, with Bi-LSTM model obtaining more than 4% improvement."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Bi-RNN+WE",
                "NN+WE",
                "Task A",
                "Task B"
            ],
            [
                "Bi-LSTM+WE"
            ],
            [
                "Task A",
                "Bi-RNN+WE",
                "NN+WE",
                "F1 Score"
            ],
            [
                "F1 Score",
                "Task B",
                "Bi-RNN+WE",
                "NN+WE",
                "Bi-LSTM+WE"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_4",
        "paper_id": "P16-1209",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1230table_1",
        "description": "4.3 Dialogue Policy Evaluation. In order to compare performance, the averaged results obtained between 400-500 training dialogues are shown in the first section of Table 1 along with one standard error. For the 400-500 interval, the Subj, off-line RNN and on-line GP systems achieved comparable results without statistical differences. The results of continuing training on the Subj and on-line GP systems from 500 to 850 training dialogues are also shown. As can be seen, the on-line GP system was significantly better presumably because it is more robust to erroneous user feedback compared to the Subj system.",
        "sentences": [
            "4.3 Dialogue Policy Evaluation.",
            "In order to compare performance, the averaged results obtained between 400-500 training dialogues are shown in the first section of Table 1 along with one standard error.",
            "For the 400-500 interval, the Subj, off-line RNN and on-line GP systems achieved comparable results without statistical differences.",
            "The results of continuing training on the Subj and on-line GP systems from 500 to 850 training dialogues are also shown.",
            "As can be seen, the on-line GP system was significantly better presumably because it is more robust to erroneous user feedback compared to the Subj system."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "400-500",
                "Subj",
                "off-line RNN",
                "on-line GP"
            ],
            [
                "500-850",
                "Subj",
                "on-line GP"
            ],
            [
                "Subj",
                "on-line GP"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_1",
        "paper_id": "P16-1230",
        "valid": 1
    },
    {
        "table_id_paper": "P16-2006table_4",
        "description": "Table 4 summarizes the Chinese dependency parsing results. Again, our work is competitive with the state-of-the-art greedy parsers.",
        "sentences": [
            "Table 4 summarizes the Chinese dependency parsing results.",
            "Again, our work is competitive with the state-of-the-art greedy parsers."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Bi-LSTM",
                "2-Layer Bi-LSTM"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_4",
        "paper_id": "P16-2006",
        "valid": 1
    },
    {
        "table_id_paper": "P17-1171table_4",
        "description": "Table 4 presents our evaluation results on both development and test sets. SQuAD has been a very competitive machine comprehension benchmark since its creation and we only list the best-performing systems in the table. Our system (single model) can achieve 70.0% exact match and 79.0% F1 scores on the test set, which surpasses all the published results and can match the top performance on the SQuAD leaderboard at the time of writing. Additionally, we think that our model is conceptually simpler than most of the existing systems.",
        "sentences": [
            "Table 4 presents our evaluation results on both development and test sets.",
            "SQuAD has been a very competitive machine comprehension benchmark since its creation and we only list the best-performing systems in the table.",
            "Our system (single model) can achieve 70.0% exact match and 79.0% F1 scores on the test set, which surpasses all the published results and can match the top performance on the SQuAD leaderboard at the time of writing.",
            "Additionally, we think that our model is conceptually simpler than most of the existing systems."
        ],
        "class_sentence": [
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "DrQA (Our model Document Reader Only)"
            ],
            [
                "DrQA (Our model Document Reader Only)"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_4",
        "paper_id": "P17-1171",
        "valid": 1
    },
    {
        "table_id_paper": "P17-2007table_2",
        "description": "Table 2 shows the average performance on multi-source parsing by combining 3 to 4 languages for GEO and 2 to 3 languages for ATIS. For RANKING, we combine the predictions from each language by selecting the one with the highest probability. Indeed, we observe that system combination at the model level is able to give better performance on average (up to 4.29% on GEO) than doing so at the output level. Combining at the word level and sentence level shows comparable performance on both datasets. It can be seen that the benefit is more apparent when we include English in the system combination.",
        "sentences": [
            "Table 2 shows the average performance on multi-source parsing by combining 3 to 4 languages for GEO and 2 to 3 languages for ATIS.",
            "For RANKING, we combine the predictions from each language by selecting the one with the highest probability.",
            "Indeed, we observe that system combination at the model level is able to give better performance on average (up to 4.29% on GEO) than doing so at the output level.",
            "Combining at the word level and sentence level shows comparable performance on both datasets.",
            "It can be seen that the benefit is more apparent when we include English in the system combination."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "RANKING"
            ],
            [
                "MULTI",
                "GEO"
            ],
            [
                "word",
                "sentence"
            ],
            [
                "en+de+el",
                "en+de+th",
                "en+el+th",
                "en+id",
                "en+zh",
                "en+id+zh"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_2",
        "paper_id": "P17-2007",
        "valid": 1
    },
    {
        "table_id_paper": "P17-2021table_1",
        "description": "As shown in Table 1, for the resource-rich setting, the single models (bpe2bpe, bpe2tree) perform similarly in terms of BLEU on newstest2015. On newstest2016 we witness an advantage to the bpe2tree model. A similar trend is found when evaluating the model ensembles: while they improve results for both models, we again see an advantage to the bpe2tree model on newstest2016.",
        "sentences": [
            "As shown in Table 1, for the resource-rich setting, the single models (bpe2bpe, bpe2tree) perform similarly in terms of BLEU on newstest2015.",
            "On newstest2016 we witness an advantage to the bpe2tree model.",
            "A similar trend is found when evaluating the model ensembles: while they improve results for both models, we again see an advantage to the bpe2tree model on newstest2016."
        ],
        "class_sentence": [
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "bpe2bpe",
                "bpe2tree",
                "newstest2015"
            ],
            [
                "bpe2tree",
                "newstest2016"
            ],
            [
                "bpe2tree ens.",
                "bpe2tree",
                "newstest2016"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_1",
        "paper_id": "P17-2021",
        "valid": 1
    },
    {
        "table_id_paper": "P17-2042table_3",
        "description": "Results in Table 3 show that using DSWE gains significant improvements (one-tailed t-test with p<0.05) over using GloVe or word2vec, on both Accuracy and Macro F1. Furthermore, using DSWE achieves better performance across all relations on the F1 score, especially for minority relations (Temp, Comp and Cont). Overall, our DSWE can effectively incorporate discourse information in explicit data, and thus benefits implicit discourse relation recognition.",
        "sentences": [
            "Results in Table 3 show that using DSWE gains significant improvements (one-tailed t-test with p<0.05) over using GloVe or word2vec, on both Accuracy and Macro F1.",
            "Furthermore, using DSWE achieves better performance across all relations on the F1 score, especially for minority relations (Temp, Comp and Cont).",
            "Overall, our DSWE can effectively incorporate discourse information in explicit data, and thus benefits implicit discourse relation recognition."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "+DSWE",
                "+GloVe",
                "+word2vec",
                "Accuracy",
                "Macro F1"
            ],
            [
                "+DSWE",
                "F1",
                "Temp",
                "Comp",
                "Cont"
            ],
            [
                "+DSWE"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_3",
        "paper_id": "P17-2042",
        "valid": 1
    },
    {
        "table_id_paper": "P17-2042table_4",
        "description": "Results in Table 4 show the superiority of our method. Although Liu2016 performs slightly better on Macro F1, it uses the additional labeled RST-DT corpus. For R&X2015 and Liu2016, they both incorporate relatively small explicit data because of the noise problem, for example, 20000 and 40000 instances respectively. By contrast, our method benefits from about 4.9M explicit instances. While B&D2016 uses massive explicit data, it is limited by the fact that the maximum dimension of word representations is restricted to the number of connectives, for example 96 in their work. Overall, our method can effectively utilize massive explicit data, and thus is more powerful than baselines.",
        "sentences": [
            "Results in Table 4 show the superiority of our method.",
            "Although Liu2016 performs slightly better on Macro F1, it uses the additional labeled RST-DT corpus.",
            "For R&X2015 and Liu2016, they both incorporate relatively small explicit data because of the noise problem, for example, 20000 and 40000 instances respectively.",
            "By contrast, our method benefits from about 4.9M explicit instances.",
            "While B&D2016 uses massive explicit data, it is limited by the fact that the maximum dimension of word representations is restricted to the number of connectives, for example 96 in their work.",
            "Overall, our method can effectively utilize massive explicit data, and thus is more powerful than baselines."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "Liu2016",
                "Macro F1"
            ],
            [
                "R&X2015",
                "Liu2016"
            ],
            [
                "CDRR+DSWE"
            ],
            [
                "B&D2016"
            ],
            [
                "CDRR+DSWE"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_4",
        "paper_id": "P17-2042",
        "valid": 1
    },
    {
        "table_id_paper": "P17-2079table_3",
        "description": "To further evaluate our approach, we compared it with a publicly available chatbot. We select 878 out of the 1047 testing questions (used in the A/B test) by removing questions relevant to our chatbot, and use it to test the public one. To compare their answers with ours, two business analysts were asked to choose a better response for each testing question. Table 3 shows the averaged results from the two analysts, clearly our chatbot has a better performance (better on 37.64% of the 878 questions and worse on 18.84%). The Kappa measure between the analysts is 0.71, which shows a substantial agreement.",
        "sentences": [
            "To further evaluate our approach, we compared it with a publicly available chatbot.",
            "We select 878 out of the 1047 testing questions (used in the A/B test) by removing questions relevant to our chatbot, and use it to test the public one.",
            "To compare their answers with ours, two business analysts were asked to choose a better response for each testing question.",
            "Table 3 shows the averaged results from the two analysts, clearly our chatbot has a better performance (better on 37.64% of the 878 questions and worse on 18.84%).",
            "The Kappa measure between the analysts is 0.71, which shows a substantial agreement."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "Win",
                "Lose",
                "Number",
                "Percentage"
            ],
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_3",
        "paper_id": "P17-2079",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1003table_2",
        "description": "To analyze the benefit of our proposed word embedding variant, Table 2 shows the results that were obtained when we use standard word embedding models. In particular, we show results for the standard GloVe model, SkipGram and the Continuous Bag of Words (CBOW) model. As can be observed, our variant leads to better results than the original GloVe model, even for the baselines. The difference is particularly noticeable for DiffVec. The difference is also larger for our relation vectors than for the baselines, which is expected as our method is based on the assumption that context word vectors can be interpreted in terms of MI scores, which is only true for our variant.",
        "sentences": [
            "To analyze the benefit of our proposed word embedding variant, Table 2 shows the results that were obtained when we use standard word embedding models.",
            "In particular, we show results for the standard GloVe model, SkipGram and the Continuous Bag of Words (CBOW) model.",
            "As can be observed, our variant leads to better results than the original GloVe model, even for the baselines.",
            "The difference is particularly noticeable for DiffVec.",
            "The difference is also larger for our relation vectors than for the baselines, which is expected as our method is based on the assumption that context word vectors can be interpreted in terms of MI scores, which is only true for our variant."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "R1ik",
                "R2ik",
                "R3ik",
                "R4ik"
            ],
            [
                "GloVe",
                "SkipGram",
                "CBOW"
            ],
            [
                "R1ik",
                "R2ik",
                "R3ik",
                "R4ik",
                "GloVe"
            ],
            [
                "R1ik",
                "R2ik",
                "R3ik",
                "R4ik",
                "DiffVec"
            ],
            [
                "R1ik",
                "R2ik",
                "R3ik",
                "R4ik"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_2",
        "paper_id": "P18-1003",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1014table_1",
        "description": "Table 1 shows the performance of SWAP-NET, state-of-the-art baselines NN and SummaRuNNer and other baselines, using ROUGE recall with summary length of 75 bytes, on the entire Daily Mail test set. The performance of SWAP-NET is comparable to that of SummaRuNNer and better than NN and other baselines.",
        "sentences": [
            "Table 1 shows the performance of SWAP-NET, state-of-the-art baselines NN and SummaRuNNer and other baselines, using ROUGE recall with summary length of 75 bytes, on the entire Daily Mail test set.",
            "The performance of SWAP-NET is comparable to that of SummaRuNNer and better than NN and other baselines."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "SWAP-NET",
                "NN",
                "SummaRuNNer",
                "Lead-3",
                "SummaRuNNer-abs"
            ],
            [
                "SWAP-NET",
                "NN",
                "SummaRuNNer",
                "Lead-3",
                "SummaRuNNer-abs"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_1",
        "paper_id": "P18-1014",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1044table_5",
        "description": "Table 5 lists the experimental results. Our models (Gen and Gen+Adv) outperformed the previous models. Furthermore, the proposed model with adversarial training (Gen+Adv) was significantly better than the supervised model (Gen).",
        "sentences": [
            "Table 5 lists the experimental results.",
            "Our models (Gen and Gen+Adv) outperformed the previous models.",
            "Furthermore, the proposed model with adversarial training (Gen+Adv) was significantly better than the supervised model (Gen)."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Gen",
                "Gen+Adv",
                "Ouchi+ 2015",
                "Shibata+ 2016"
            ],
            [
                "Gen",
                "Gen+Adv"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_5",
        "paper_id": "P18-1044",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1045table_3",
        "description": "Table 3 shows performance comparisons of our ILP systems with other event coreference resolution approaches including the recent joint learning approach (Lu and Ng, 2017) which is the best performing model on the KBP 2016 corpus. For both datasets, the full discourse structure augmented model achieved superior performance compared to the local classifier based system. The improvement is observed across all metrics with average F1 gain of 3.1 for KBP 2016 and 2.17 for KBP 2017. Most interestingly, we see over 28% improvement in MUC F1 score which directly evaluates the pairwise coreference link predictions. This implies that the document level structures, indeed, helps in linking more coreferent event mentions, which otherwise are difficult with the local classifier trained on lexical and surface features. Our ILP based system also outperforms the previous best model on the KBP 2016 corpus (Lu and Ng, 2017) consistently using all the evaluation metrics, with an overall improvement of 1.21 based on the average F1 scores. In Table 3, we also report the F1 scores when we increasingly add each type of structure in the ILP baseline. Among different scoring metrics, all structures positively contributed to the MUC and BLANC scores for KBP 2016 corpus. However, subevent based constraints slightly reduced the F1 scores on KBP 2017 corpus. Based on our preliminary analysis, this can be accounted to the simple method applied for subevent extraction. We only extracted 31 subevents in KBP 2017 corpus compared to 211 in KBP 2016 corpus.",
        "sentences": [
            "Table 3 shows performance comparisons of our ILP systems with other event coreference resolution approaches including the recent joint learning approach (Lu and Ng, 2017) which is the best performing model on the KBP 2016 corpus.",
            "For both datasets, the full discourse structure augmented model achieved superior performance compared to the local classifier based system.",
            "The improvement is observed across all metrics with average F1 gain of 3.1 for KBP 2016 and 2.17 for KBP 2017.",
            "Most interestingly, we see over 28% improvement in MUC F1 score which directly evaluates the pairwise coreference link predictions.",
            "This implies that the document level structures, indeed, helps in linking more coreferent event mentions, which otherwise are difficult with the local classifier trained on lexical and surface features.",
            "Our ILP based system also outperforms the previous best model on the KBP 2016 corpus (Lu and Ng, 2017) consistently using all the evaluation metrics, with an overall improvement of 1.21 based on the average F1 scores.",
            "In Table 3, we also report the F1 scores when we increasingly add each type of structure in the ILP baseline.",
            "Among different scoring metrics, all structures positively contributed to the MUC and BLANC scores for KBP 2016 corpus.",
            "However, subevent based constraints slightly reduced the F1 scores on KBP 2017 corpus.",
            "Based on our preliminary analysis, this can be accounted to the simple method applied for subevent extraction.",
            "We only extracted 31 subevents in KBP 2017 corpus compared to 211 in KBP 2016 corpus."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            2,
            1,
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            [
                "Local classifier",
                "Clustering",
                "Basic ILP",
                "Joint learning"
            ],
            [
                "KBP 2016",
                "KBP 2017",
                "Local classifier",
                "Basic ILP",
                "Basic ILP + Topic structure",
                "Basic ILP + Cross-chain",
                "Basic ILP + Distribution",
                "Basic ILP + Subevent"
            ],
            [
                "Basic ILP + Subevent",
                "Local classifier",
                "AVG"
            ],
            [
                "MUC"
            ],
            [
                "Local classifier",
                "Basic ILP"
            ],
            [
                "Local classifier",
                "Basic ILP",
                "Basic ILP + Topic structure",
                "Basic ILP + Cross-chain",
                "Basic ILP + Distribution",
                "Basic ILP + Subevent"
            ],
            [
                "Basic ILP + Topic structure",
                "Basic ILP + Cross-chain",
                "Basic ILP + Distribution",
                "Basic ILP + Subevent"
            ],
            [
                "Basic ILP + Topic structure",
                "Basic ILP + Cross-chain",
                "Basic ILP + Distribution",
                "Basic ILP + Subevent",
                "MUC",
                "BLANC"
            ],
            [
                "Basic ILP + Subevent",
                "KBP 2017"
            ],
            [
                "Basic ILP + Subevent",
                "KBP 2017"
            ],
            [
                "Basic ILP + Subevent",
                "KBP 2017"
            ]
        ],
        "n_sentence": 11.0,
        "table_id": "table_3",
        "paper_id": "P18-1045",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1054table_3",
        "description": "Table 3 shows the result on the development set. We found that, the path embedding was effective for PA, and the string match was effective for CR. The sentence distance for both CR and PA was effective for News, but not for Web since the Web evaluation corpus consists of three-sentence documents.",
        "sentences": [
            "Table 3 shows the result on the development set.",
            "We found that, the path embedding was effective for PA, and the string match was effective for CR.",
            "The sentence distance for both CR and PA was effective for News, but not for Web since the Web evaluation corpus consists of three-sentence documents."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "PA - path embedding",
                "CR - string match",
                "delta"
            ],
            [
                "CR - sentence distance",
                "PA - sentence distance",
                "Web",
                "News",
                "delta"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_3",
        "paper_id": "P18-1054",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1097table_5",
        "description": "Table 5 shows the evaluation results on the CoNLL-2014 dataset. Without using the nonpublic training data from Lang-8.com, our single model obtains 50.04 F0.5, larlgely outperforming the other seq2seq models and only inferior to CAMB17 (AMU16 based) and NUS17. It should be noted, however, that the CAMB17 and NUS17 are actually re-rankers built on top of an SMTbased GEC system (AMU16\u0081fs framework); thus, they are ensemble models. When we build our approach on top of AMU16 (i.e., we take AMU16\u0081fs outputs as the input to our GEC system to edit on top of its outputs), we achieve 53.30 F0.5 score. With introducing the non-public training data, our single and ensemble system obtain 52.72 and 54.51 F0.5 score respectively, which is a stateof-the-art result7 on CoNLL-2014 dataset.",
        "sentences": [
            "Table 5 shows the evaluation results on the CoNLL-2014 dataset.",
            "Without using the nonpublic training data from Lang-8.com, our single model obtains 50.04 F0.5, larlgely outperforming the other seq2seq models and only inferior to CAMB17 (AMU16 based) and NUS17.",
            "It should be noted, however, that the CAMB17 and NUS17 are actually re-rankers built on top of an SMTbased GEC system (AMU16\u0081fs framework); thus, they are ensemble models.",
            "When we build our approach on top of AMU16 (i.e., we take AMU16\u0081fs outputs as the input to our GEC system to edit on top of its outputs), we achieve 53.30 F0.5 score.",
            "With introducing the non-public training data, our single and ensemble system obtain 52.72 and 54.51 F0.5 score respectively, which is a stateof-the-art result7 on CoNLL-2014 dataset."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "dual-boost (single)",
                "Char-seq2seq",
                "Nested-seq2seq",
                "Adapt-seq2seq",
                "CAMB17 (AMU16 based)"
            ],
            [
                "CAMB17 (AMU16 based)",
                "NUS17"
            ],
            [
                "dual-boost (AMU16 based)",
                "F0.5"
            ],
            [
                "dual-boost (single) *",
                "dual-boost (AMU16 based) *",
                "F0.5"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_5",
        "paper_id": "P18-1097",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1120table_3",
        "description": "The first two columns of Table 3 show the MRR results under Exact Match and Partial Match conditions. The first 3 rows show the results for the baseline systems, and the remaining rows show results for our Activity Profile (AP) semi-supervised learning method. We show results for 5 variations of the algorithm: AP uses Algorithm 1, and the others use Algorithm 2 with different Activity Similarity measures: AP+AL (location profile similarity), AP+AO (overlap similarity), AP+AE (embedding similarity), and AP+AL+E (location profiles plus embeddings). Table 3 shows that our AP algorithm outperforms all 3 baseline methods. When adding Activity Similarity into the algorithm, we find that AL slightly improves performance, but AO and AE do not. However, we also tried combining them and obtained improved results by using AL and AE together, yielding an MRRP score of 0.42. To gain more insight about the behavior of the models, Table 3 also shows results for the topranked 1, 2, and 3 answers. For these experiments, the system gets full credit if any of its top k answers exactly matches the gold standard, or 50% credit if a partial match is among its top k answers. These results show that our AP method produces more correct answers at the top of the list than the baseline methods.",
        "sentences": [
            "The first two columns of Table 3 show the MRR results under Exact Match and Partial Match conditions.",
            "The first 3 rows show the results for the baseline systems, and the remaining rows show results for our Activity Profile (AP) semi-supervised learning method.",
            "We show results for 5 variations of the algorithm: AP uses Algorithm 1, and the others use Algorithm 2 with different Activity Similarity measures: AP+AL (location profile similarity), AP+AO (overlap similarity), AP+AE (embedding similarity), and AP+AL+E (location profiles plus embeddings).",
            "Table 3 shows that our AP algorithm outperforms all 3 baseline methods.",
            "When adding Activity Similarity into the algorithm, we find that AL slightly improves performance, but AO and AE do not.",
            "However, we also tried combining them and obtained improved results by using AL and AE together, yielding an MRRP score of 0.42.",
            "To gain more insight about the behavior of the models, Table 3 also shows results for the topranked 1, 2, and 3 answers.",
            "For these experiments, the system gets full credit if any of its top k answers exactly matches the gold standard, or 50% credit if a partial match is among its top k answers.",
            "These results show that our AP method produces more correct answers at the top of the list than the baseline methods."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            [
                "MRRE",
                "MRRP"
            ],
            [
                "EMBED",
                "PMI",
                "FREQ",
                "AP",
                "AP+AL",
                "AP+AO",
                "AP+AE",
                "AP+AL+E"
            ],
            [
                "AP",
                "AP+AL",
                "AP+AO",
                "AP+AE",
                "AP+AL+E"
            ],
            [
                "AP",
                "AP+AL",
                "AP+AO",
                "AP+AE",
                "AP+AL+E"
            ],
            [
                "AP+AL",
                "AP+AO",
                "AP+AE"
            ],
            [
                "AP+AL+E"
            ],
            [
                "TOP1",
                "TOP2",
                "TOP3"
            ],
            null,
            [
                "AP",
                "AP+AL",
                "AP+AO",
                "AP+AE",
                "AP+AL+E"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_3",
        "paper_id": "P18-1120",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1126table_6",
        "description": "Table 6 provides averages of the classification and similarity results, along with the results of selected tasks (SNLI, SICK-E). As the baseline for classifications tasks, we assign the most frequent class to all test examples.8. The de models are generally worse, most likely due to the higher OOV rate and overall simplicity of the training sentences. On cs, we see a clear pattern that more heads hurt the performance. The de set has more variations to consider but the results are less conclusive. For the similarity results, it is worth noting that cs-ATTN-ATTN performs very well with 1 attention head but fails miserably with more heads. Otherwise, the relation to the number of heads is less clear. Table 6 also provides our measurements based on sentence paraphrases. For paraphrase retrieval (NN), we found cosine distance to work better than L2 distance. We therefore do not list L2- based results (except in the supplementary material). This evaluation seems less stable and discerning than the previous two, but we can again confirm the victory of InferSent followed by our nonattentive cs models. cs and de models are no longer clearly separated.",
        "sentences": [
            "Table 6 provides averages of the classification and similarity results, along with the results of selected tasks (SNLI, SICK-E).",
            "As the baseline for classifications tasks, we assign the most frequent class to all test examples.8.",
            "The de models are generally worse, most likely due to the higher OOV rate and overall simplicity of the training sentences.",
            "On cs, we see a clear pattern that more heads hurt the performance.",
            "The de set has more variations to consider but the results are less conclusive.",
            "For the similarity results, it is worth noting that cs-ATTN-ATTN performs very well with 1 attention head but fails miserably with more heads.",
            "Otherwise, the relation to the number of heads is less clear.",
            "Table 6 also provides our measurements based on sentence paraphrases.",
            "For paraphrase retrieval (NN), we found cosine distance to work better than L2 distance.",
            "We therefore do not list L2- based results (except in the supplementary material).",
            "This evaluation seems less stable and discerning than the previous two, but we can again confirm the victory of InferSent followed by our nonattentive cs models.",
            "cs and de models are no longer clearly separated."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "SNLI",
                "SICK-E",
                "AvgAcc",
                "AvgSim"
            ],
            null,
            [
                "de-MAXPOOL-CTX",
                "de-ATTN-CTX",
                "de-AVGPOOL-CTX",
                "de-FINAL",
                "de-ATTN-ATTN",
                "de-FINAL-CTX",
                "de-TRF-ATTN-ATTN"
            ],
            [
                "cs-FINAL-CTX",
                "cs-ATTN-ATTN",
                "cs-FINAL",
                "cs-MAXPOOL",
                "cs-AVGPOOL",
                "cs-ATTN-CTX",
                "H."
            ],
            [
                "de-MAXPOOL-CTX",
                "de-ATTN-CTX",
                "de-AVGPOOL-CTX",
                "de-FINAL",
                "de-ATTN-ATTN",
                "de-FINAL-CTX",
                "de-TRF-ATTN-ATTN"
            ],
            [
                "cs-ATTN-ATTN",
                "AvgSim",
                "H."
            ],
            [
                "H."
            ],
            null,
            [
                "Hy-NN",
                "CO-NN"
            ],
            null,
            [
                "InferSent",
                "cs-FINAL-CTX",
                "cs-FINAL",
                "cs-MAXPOOL"
            ],
            [
                "de-MAXPOOL-CTX",
                "de-ATTN-CTX",
                "de-AVGPOOL-CTX",
                "de-FINAL",
                "de-ATTN-ATTN",
                "de-FINAL-CTX",
                "de-TRF-ATTN-ATTN",
                "cs-FINAL-CTX",
                "cs-ATTN-ATTN",
                "cs-FINAL",
                "cs-MAXPOOL",
                "cs-AVGPOOL",
                "cs-ATTN-CTX"
            ]
        ],
        "n_sentence": 12.0,
        "table_id": "table_6",
        "paper_id": "P18-1126",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1131table_4",
        "description": "Table 4 shows these calculations for the 15 relation types for which the performance gap was highest and which had at least 15 instances in each of the AA and WH tweet sets, along with the corresponding calculation under the ARK Tagger model. The amount by which the performance gap is reduced from the first setting to the second setting is also reported. Of the 15 relations shown, the gap was reduced for 14, and 7 saw a reduction of at least 10%.",
        "sentences": [
            "Table 4 shows these calculations for the 15 relation types for which the performance gap was highest and which had at least 15 instances in each of the AA and WH tweet sets, along with the corresponding calculation under the ARK Tagger model.",
            "The amount by which the performance gap is reduced from the first setting to the second setting is also reported.",
            "Of the 15 relations shown, the gap was reduced for 14, and 7 saw a reduction of at least 10%."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "Relation",
                "Gap (WH - AA)",
                "ARK Tagger"
            ],
            [
                "Morpho-Tagger",
                "ARK Tagger",
                "Reduction"
            ],
            [
                "Reduction"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_4",
        "paper_id": "P18-1131",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1185table_3",
        "description": "Table 3 shows the performance of the baseline, which is BLSTM-CRF with sentences as input only, and our proposed models on both datasets. BLSTM-CRF + Global Image Vector: use global image vector to initialize the BLSTM-CRF. BLSTM-CRF + Visual attention: use attention based visual context vector to initialize the BLSTM-CRF. BLSTM-CRF + Visual attention + Gate: modulate word representations with visual vector. Our final model BLSTM-CRF + VISUAL ATTENTION + GATE, which has visual attention component and modulation gate, obtains the best F1 scores on both datasets. Visual features successfully play a role of validating entity types. For example, when there is a person in the image, it is more likely to include a person name in the associated sentence, but when there is a soccer field\nin the image, it is more likely to include a sports\nteam name.\n. All the models get better scores on Twitter dataset than on Snap dataset, because the average length of the sentences in Snap dataset (8.1 tokens) is much smaller than that of Twitter dataset (16.0 tokens), which means there is much less contextual information in Snap dataset. Also comparing the gains from visual features on different datasets, we find that the model benefits more from visual features on Twitter dataset, considering the much higher baseline scores on Twitter dataset.",
        "sentences": [
            "Table 3 shows the performance of the baseline, which is BLSTM-CRF with sentences as input only, and our proposed models on both datasets.",
            "BLSTM-CRF + Global Image Vector: use global image vector to initialize the BLSTM-CRF.",
            "BLSTM-CRF + Visual attention: use attention based visual context vector to initialize the BLSTM-CRF.",
            "BLSTM-CRF + Visual attention + Gate: modulate word representations with visual vector.",
            "Our final model BLSTM-CRF + VISUAL ATTENTION + GATE, which has visual attention component and modulation gate, obtains the best F1 scores on both datasets.",
            "Visual features successfully play a role of validating entity types.",
            "For example, when there is a person in the image, it is more likely to include a person name in the associated sentence, but when there is a soccer field\nin the image, it is more likely to include a sports\nteam name.\n.",
            "All the models get better scores on Twitter dataset than on Snap dataset, because the average length of the sentences in Snap dataset (8.1 tokens) is much smaller than that of Twitter dataset (16.0 tokens), which means there is much less contextual information in Snap dataset.",
            "Also comparing the gains from visual features on different datasets, we find that the model benefits more from visual features on Twitter dataset, considering the much higher baseline scores on Twitter dataset."
        ],
        "class_sentence": [
            1,
            2,
            2,
            2,
            1,
            2,
            0,
            1,
            2
        ],
        "header_mention": [
            [
                "BLSTM-CRF",
                "BLSTM-CRF + Global Image Vector",
                "BLSTM-CRF + Visual attention",
                "BLSTM-CRF + Visual attention + Gate"
            ],
            [
                "BLSTM-CRF + Global Image Vector"
            ],
            [
                "BLSTM-CRF + Visual attention"
            ],
            [
                "BLSTM-CRF + Visual attention + Gate"
            ],
            [
                "BLSTM-CRF + Visual attention + Gate",
                "Snap Captions",
                "Twitter",
                "F1"
            ],
            [
                "BLSTM-CRF + Visual attention + Gate"
            ],
            null,
            [
                "Snap Captions",
                "Twitter"
            ],
            [
                "Twitter"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_3",
        "paper_id": "P18-1185",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1193table_4",
        "description": "Table 4 shows development results, including model ablation studies. Removing previous instructions (-previous instructions) or both states (-current and initial state) reduces performance across all domains. Removing only the initial state (-initial state) or the current state (-current state) shows mixed results across the domains. Providing access to both initial and current states increases performance for ALCHEMY, but reduces performance on the other domains. We hypothesize that this is due to the increase in the number of parameters outweighing what is relatively marginal information for these domains. In our development and test results we use a single architecture across the three domains, the full approach, which has the highest interactive-level accuracy when averaged across the three domains (62.7 5utts).",
        "sentences": [
            "Table 4 shows development results, including model ablation studies.",
            "Removing previous instructions (-previous instructions) or both states (-current and initial state) reduces performance across all domains.",
            "Removing only the initial state (-initial state) or the current state (-current state) shows mixed results across the domains.",
            "Providing access to both initial and current states increases performance for ALCHEMY, but reduces performance on the other domains.",
            "We hypothesize that this is due to the increase in the number of parameters outweighing what is relatively marginal information for these domains.",
            "In our development and test results we use a single architecture across the three domains, the full approach, which has the highest interactive-level accuracy when averaged across the three domains (62.7 5utts)."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "Our approach -previous instructions",
                "Our approach -current and initial state"
            ],
            [
                "Our approach -initial state",
                "Our approach -current state"
            ],
            [
                "Our approach",
                "ALCHEMY",
                "SCENE",
                "TANGRAMS"
            ],
            null,
            [
                "Our approach",
                "ALCHEMY",
                "SCENE",
                "TANGRAMS",
                "5utts"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_4",
        "paper_id": "P18-1193",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1212table_4",
        "description": "Table 4 demonstrates the improvement of the joint framework over individual components. The temporal only baseline is the improved temporal extraction system for which the joint inference with causal links has NOT been applied. The causal only baseline is to use s c (?) alone for the prediction of each pair. That is, for a pair i, if s c{i 7 causes} > sc{i 7 caused by}, we then assign causes to pair i; otherwise, we assign caused by to pair i. Note that the causal accuracy column in Table 4 was evaluated only on gold causal pairs. In the proposed joint system, the temporal and causal scores were added up for all event pairs. The temporal performance got strictly better in precision, recall, and F1, and the causal performance also got improved by a large margin from 70.5% to 77.3%, indicating that temporal signals and causal signals are helpful to each other. According to the McNemar test, both improvements are significant with p<0.05. The second part of Table 4 shows that if gold relations were used, how well each component would possibly perform. Technically, these gold temporal/causal relations were enforced via adding extra constraints to ILP in Eq. (3) (imagine these gold relations as a special rule, and convert them into constraints like what we did in Eq. (2)). When using gold temporal relations, causal accuracy went up to 91.9%. First, this number is much higher than the 77.3% on line 3, so there is still room for improvement. Second, it means in this dataset, there were 8.1% of the C-Links in which the cause is temporally after its effect.",
        "sentences": [
            "Table 4 demonstrates the improvement of the joint framework over individual components.",
            "The temporal only baseline is the improved temporal extraction system for which the joint inference with causal links has NOT been applied.",
            "The causal only baseline is to use s c (?) alone for the prediction of each pair.",
            "That is, for a pair i, if s c{i 7 causes} > sc{i 7 caused by}, we then assign causes to pair i; otherwise, we assign caused by to pair i.",
            "Note that the causal accuracy column in Table 4 was evaluated only on gold causal pairs.",
            "In the proposed joint system, the temporal and causal scores were added up for all event pairs.",
            "The temporal performance got strictly better in precision, recall, and F1, and the causal performance also got improved by a large margin from 70.5% to 77.3%, indicating that temporal signals and causal signals are helpful to each other.",
            "According to the McNemar test, both improvements are significant with p<0.05.",
            "The second part of Table 4 shows that if gold relations were used, how well each component would possibly perform.",
            "Technically, these gold temporal/causal relations were enforced via adding extra constraints to ILP in Eq. (3) (imagine these gold relations as a special rule, and convert them into constraints like what we did in Eq. (2)).",
            "When using gold temporal relations, causal accuracy went up to 91.9%.",
            "First, this number is much higher than the 77.3% on line 3, so there is still room for improvement.",
            "Second, it means in this dataset, there were 8.1% of the C-Links in which the cause is temporally after its effect."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            1,
            1,
            1,
            2,
            1,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "1. Temporal Only"
            ],
            [
                "2. Causal Only"
            ],
            [
                "1. Temporal Only",
                "2. Causal Only"
            ],
            [
                "Causal",
                "Accuracy"
            ],
            [
                "3. Joint System",
                "Temporal",
                "Causal"
            ],
            [
                "Temporal",
                "P",
                "R",
                "F1",
                "Causal"
            ],
            null,
            [
                "Enforcing Gold Relations in Joint System"
            ],
            null,
            [
                "4. Gold Temporal",
                "Causal",
                "Accuracy"
            ],
            [
                "Causal",
                "Accuracy",
                "3. Joint System"
            ],
            null
        ],
        "n_sentence": 13.0,
        "table_id": "table_4",
        "paper_id": "P18-1212",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1228table_3",
        "description": "Table 3 shows the results of the analogy test using our Reddit20M in comparison with Google300D, which is the Google News embedding used in previous sections. As one can expect, Reddit20M shows worse performance than Google300D. However, the four categories (capital-common-countries, capital-world, currency, and city-in-state denoted by World), which require some general knowledge on the world, drive the 10% decrease in overall accuracy. Other categories show comparable or even better accuracy. For example, Reddit20M outperforms Google300D by 4.52% in the family category. Since Reddit is a US-based online community, the Reddit model may not be able to properly capture semantic relationships in World category. By contrast, for the categories for testing grammar (denoted by Gram1-9), Reddit20M shows comparable performances with Google300D (70.21 vs. 73.4). In this study, we use Reddit20M as a reference model and update it with new target corpora.",
        "sentences": [
            "Table 3 shows the results of the analogy test using our Reddit20M in comparison with Google300D, which is the Google News embedding used in previous sections.",
            "As one can expect, Reddit20M shows worse performance than Google300D.",
            "However, the four categories (capital-common-countries, capital-world, currency, and city-in-state denoted by World), which require some general knowledge on the world, drive the 10% decrease in overall accuracy.",
            "Other categories show comparable or even better accuracy.",
            "For example, Reddit20M outperforms Google300D by 4.52% in the family category.",
            "Since Reddit is a US-based online community, the Reddit model may not be able to properly capture semantic relationships in World category.",
            "By contrast, for the categories for testing grammar (denoted by Gram1-9), Reddit20M shows comparable performances with Google300D (70.21 vs. 73.4).",
            "In this study, we use Reddit20M as a reference model and update it with new target corpora."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            [
                "Reddit20M",
                "Google300D"
            ],
            [
                "Reddit20M",
                "Google300D"
            ],
            [
                "World",
                "Total"
            ],
            [
                "family",
                "Gram1-9"
            ],
            [
                "Reddit20M",
                "family"
            ],
            [
                "Reddit20M",
                "World"
            ],
            [
                "Gram1-9",
                "Reddit20M",
                "Google300D"
            ],
            [
                "Reddit20M"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_3",
        "paper_id": "P18-1228",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1244table_3",
        "description": "Table 3 shows the character error rates (CERs), where the baseline model showed 2.6% on WSJ and 7.8% on CSJ. Since the model was trained and evaluated with unmixed speech data, these CERs are considered lower bounds for the CERs in the succeeding experiments with mixed speech data.",
        "sentences": [
            "Table 3 shows the character error rates (CERs), where the baseline model showed 2.6% on WSJ and 7.8% on CSJ.",
            "Since the model was trained and evaluated with unmixed speech data, these CERs are considered lower bounds for the CERs in the succeeding experiments with mixed speech data."
        ],
        "class_sentence": [
            1,
            2
        ],
        "header_mention": [
            [
                "WSJ",
                "CSJ"
            ],
            null
        ],
        "n_sentence": 2.0,
        "table_id": "table_3",
        "paper_id": "P18-1244",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1246table_4",
        "description": "Table 4 shows the results. Our models tend to produce significantly better results than the winners of the CoNLL 2017 Shared Task (i.e., 1.8% absolute improvement on average, corresponding to a RRIE of 21.20%). The only cases for which this is not true are again languages that require significant segmentation efforts (i.e., Hebrew, Chinese, Vietnamese and Japanese) or when the task was trivial. Given the fact that Dozat et al. (2017) obtained the best results in part-of-speech tagging by a significant margin in the CoNLL 2017 Shared Task, it would be expected that their model would also perform significantly well in morphological tagging since the tasks are very similar. Since they did not participate in this particular challenge, we decided to reimplement their system to serve as a strong baseline. As expected, our reimplementation of Dozat et al. (2017) tends to significantly outperform the winners of the CONLL 2017 Shared Task. However, in general, our models still obtain better results, outperforming Dozat et al. on 43 of the 54 treebanks, with an absolute difference of 0.42% on average.",
        "sentences": [
            "Table 4 shows the results.",
            "Our models tend to produce significantly better results than the winners of the CoNLL 2017 Shared Task (i.e., 1.8% absolute improvement on average, corresponding to a RRIE of 21.20%).",
            "The only cases for which this is not true are again languages that require significant segmentation efforts (i.e., Hebrew, Chinese, Vietnamese and Japanese) or when the task was trivial.",
            "Given the fact that Dozat et al. (2017) obtained the best results in part-of-speech tagging by a significant margin in the CoNLL 2017 Shared Task, it would be expected that their model would also perform significantly well in morphological tagging since the tasks are very similar.",
            "Since they did not participate in this particular challenge, we decided to reimplement their system to serve as a strong baseline.",
            "As expected, our reimplementation of Dozat et al. (2017) tends to significantly outperform the winners of the CONLL 2017 Shared Task.",
            "However, in general, our models still obtain better results, outperforming Dozat et al. on 43 of the 54 treebanks, with an absolute difference of 0.42% on average."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "ours",
                "CONLL Winner"
            ],
            [
                "ours",
                "CONLL Winner",
                "he",
                "zh",
                "vi",
                "ja"
            ],
            null,
            [
                "DQM Reimpl."
            ],
            [
                "DQM Reimpl.",
                "CONLL Winner"
            ],
            [
                "ours",
                "DQM Reimpl."
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_4",
        "paper_id": "P18-1246",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1256table_2",
        "description": "Table 2 shows the performance obtained by the different models with and without POS tags. Overall, our attention model WP outperforms all other models in 10 out of 14 scenarios (combinations of datasets and whether or not POS tags are used). Importantly, our model outperforms the regular LSTM model without introducing additional parameters to the model, which highlights the advantage of WP\u0081fs attention-based pooling method. For all models listed in Table 2, we find that including POS tags benefits the detection of adverbial presupposition triggers in Gigaword and PTB datasets. Note that, in Table 2, we bolded accuracy figures that were within 0.1% of the best performing WP model as McNemar\u0081fs test did not show that WP significantly outperformed the other model in these cases (p value > 0.05).",
        "sentences": [
            "Table 2 shows the performance obtained by the different models with and without POS tags.",
            "Overall, our attention model WP outperforms all other models in 10 out of 14 scenarios (combinations of datasets and whether or not POS tags are used).",
            "Importantly, our model outperforms the regular LSTM model without introducing additional parameters to the model, which highlights the advantage of WP\u0081fs attention-based pooling method.",
            "For all models listed in Table 2, we find that including POS tags benefits the detection of adverbial presupposition triggers in Gigaword and PTB datasets.",
            "Note that, in Table 2, we bolded accuracy figures that were within 0.1% of the best performing WP model as McNemar\u0081fs test did not show that WP significantly outperformed the other model in these cases (p value > 0.05)."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "+ POS",
                "- POS"
            ],
            [
                "WP",
                "MFC",
                "LogReg",
                "CNN",
                "LSTM",
                "+ POS",
                "- POS"
            ],
            [
                "WP",
                "LSTM"
            ],
            [
                "Gigaword",
                "+ POS"
            ],
            [
                "WP"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_2",
        "paper_id": "P18-1256",
        "valid": 1
    },
    {
        "table_id_paper": "P18-2042table_2",
        "description": "Table 2 presents METEOR (Denkowski and Lavie, 2014) and ROUGE-L (Lin, 2004) scores for each method, where we can see score gains on both metrics from the Editing Mechanism. Additionally, 10 NLP researchers manually assess the quality of each method. We randomly selected 50 titles and applied each model to generate an abstract. We then asked human judges to choose the best generated abstract for each title and computed the overall percentage of each model being preferred by human, which we record as Human Preference. The criteria the human judges adopt include topical relevance, logical coherence, and conciseness. Table 2 shows that the human judges strongly favor the abstracts from our ED(2) method.",
        "sentences": [
            "Table 2 presents METEOR (Denkowski and Lavie, 2014) and ROUGE-L (Lin, 2004) scores for each method, where we can see score gains on both metrics from the Editing Mechanism.",
            "Additionally, 10 NLP researchers manually assess the quality of each method.",
            "We randomly selected 50 titles and applied each model to generate an abstract.",
            "We then asked human judges to choose the best generated abstract for each title and computed the overall percentage of each model being preferred by human, which we record as Human Preference.",
            "The criteria the human judges adopt include topical relevance, logical coherence, and conciseness.",
            "Table 2 shows that the human judges strongly favor the abstracts from our ED(2) method."
        ],
        "class_sentence": [
            1,
            2,
            2,
            2,
            2,
            1
        ],
        "header_mention": [
            [
                "METEOR",
                "ROUGE-L"
            ],
            [
                "PREFERENCE"
            ],
            [
                "PREFERENCE"
            ],
            [
                "PREFERENCE"
            ],
            [
                "PREFERENCE"
            ],
            [
                "ED(2)",
                "PREFERENCE"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_2",
        "paper_id": "P18-2042",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1002table_1",
        "description": "Table 1 shows the automatic and manual evaluation results for both the baseline and our models. In manual evaluation, among baselines, Wizard Transformer and RNN without knowledge have the highest fluency of 1.62 and Wizard obtains the highest knowledge relevance of 0.47 while Transformer without knowledge gets the highest context coherence of 0.67. For all models, ITE+CKAD obtains the highest fluency of 1.68 and ITE+DD has the highest Knowledge Relevance of 0.56 and highest Context Coherence of 0.90. In automatic evaluation, our proposed model has lower perplexity and higher BLEU scores than baselines. For BLEU, HRED with knowledge obtains the highest BLEU score of 0.77 among the baselines. And ITE+DD gets 0.95 BLEU score, which is the highest among all the models. For perplexity, Wizard Transformer obtains the lowest perplexity of 70.30 among baseline models and ITE+DD has remarkably lower perplexity of 15.11 than all the other models. A detailed analysis is in Section 3.6.",
        "sentences": [
            "Table 1 shows the automatic and manual evaluation results for both the baseline and our models.",
            "In manual evaluation, among baselines, Wizard Transformer and RNN without knowledge have the highest fluency of 1.62 and Wizard obtains the highest knowledge relevance of 0.47 while Transformer without knowledge gets the highest context coherence of 0.67.",
            "For all models, ITE+CKAD obtains the highest fluency of 1.68 and ITE+DD has the highest Knowledge Relevance of 0.56 and highest Context Coherence of 0.90.",
            "In automatic evaluation, our proposed model has lower perplexity and higher BLEU scores than baselines.",
            "For BLEU, HRED with knowledge obtains the highest BLEU score of 0.77 among the baselines.",
            "And ITE+DD gets 0.95 BLEU score, which is the highest among all the models.",
            "For perplexity, Wizard Transformer obtains the lowest perplexity of 70.30 among baseline models and ITE+DD has remarkably lower perplexity of 15.11 than all the other models.",
            "A detailed analysis is in Section 3.6."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0
        ],
        "header_mention": [
            null,
            [
                "Wizard Transformer",
                "Transformer without knowledge"
            ],
            [
                "ITE+CKAD (ours)",
                "ITE+DD (ours)",
                "Knowledge Relevance",
                "Context Coherence"
            ],
            [
                "KAT (ours)",
                "BLEU(%)",
                "PPL"
            ],
            [
                "BLEU(%)"
            ],
            [
                "ITE+DD (ours)",
                "BLEU(%)"
            ],
            [
                "PPL"
            ],
            null
        ],
        "n_sentence": 8.0,
        "table_id": "table_1",
        "paper_id": "P19-1002",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1012table_1",
        "description": "Table 1 shows the accuracy of all the parsers. Comparing the simple and extended architectures we see that dropping the structural features does not hurt the performance, neither for transition-based nor graph-based parsers. Figure 2 displays the accuracy relative to dependency length in terms of recall8. It shows that the differences between models are not restricted to arcs of particular lengths.",
        "sentences": [
            "Table 1 shows the accuracy of all the parsers.",
            "Comparing the simple and extended architectures we see that dropping the structural features does not hurt the performance, neither for transition-based nor graph-based parsers.",
            "Figure 2 displays the accuracy relative to dependency length in terms of recall8.",
            "It shows that the differences between models are not restricted to arcs of particular lengths."
        ],
        "class_sentence": [
            1,
            1,
            0,
            0
        ],
        "header_mention": [
            null,
            [
                "TBMIN",
                "TBEXT",
                "GBMIN",
                "GBSIBL"
            ],
            null,
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_1",
        "paper_id": "P19-1012",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1048table_3",
        "description": "4.3 Results and Analysis. Main results. Table 3 shows the comparison results. Note that IMN performs co-extraction of aspect and opinion terms in AE, which utilizes additional opinion term labels during training, while the baseline methods except CMLA do not consider this information in their original models. To enable fair comparison, we slightly modify those baselines to perform co-extraction as well, with opinion term labels provided. Further details on model comparison are provided in Appendix B. From Table 3, we observe that IMN\u2212d is able to significantly outperform other baselines on F1-I. IMN further boosts the performance and outperforms the best F1-I results from the baselines by 2.29%, 1.77%, and 2.61% on D1, D2, and D3. Specifically, for AE (F1-a and F1-o), IMN -d performs the best in most cases. For AS (acc-s and F1-s), IMN outperforms other methods by large margins. PIPELINE, IMN -d, and the pipeline methods with dTrans also perform reasonably well on this task, outperforming other baselines by moderate margins. All these models utilize knowledge from larger corpora by either joint training of document-level tasks or using domain-specific embeddings. This suggests that domain-specific knowledge is very helpful, and both joint training and domain-specific embeddings are effective ways to transfer such knowledge. We also show the results of IMN -d and IMN when only the general-purpose embeddings (without domain-specific embeddings) are used for initialization. They are denoted as IMN -d/IMN wo DE. IMN wo DE performs only marginally below IMN. This indicates that the knowledge captured by domain-specific embeddings could be similar to that captured by joint training of the IMN\u2212d is more affected document-level tasks.  IMN ?d is  more  affected without domain-specific embeddings, while it still outperforms  all  other  baselines  except  DECNN-dTrans. DECNN-dTrans  is  a  very  strong  base-line as it exploits additional knowledge from larger corpora for both tasks.  IMN?dwo DE is compet-itive with DECNN-dTrans even without utilizing additional  knowledge,  which  suggests  the  effectiveness of the proposed network structure.",
        "sentences": [
            "4.3 Results and Analysis.",
            "Main results.",
            "Table 3 shows the comparison results.",
            "Note that IMN performs co-extraction of aspect and opinion terms in AE, which utilizes additional opinion term labels during training, while the baseline methods except CMLA do not consider this information in their original models.",
            "To enable fair comparison, we slightly modify those baselines to perform co-extraction as well, with opinion term labels provided.",
            "Further details on model comparison are provided in Appendix B.",
            "From Table 3, we observe that IMN\u2212d is able to significantly outperform other baselines on F1-I.",
            "IMN further boosts the performance and outperforms the best F1-I results from the baselines by 2.29%, 1.77%, and 2.61% on D1, D2, and D3.",
            "Specifically, for AE (F1-a and F1-o), IMN -d performs the best in most cases.",
            "For AS (acc-s and F1-s), IMN outperforms other methods by large margins.",
            "PIPELINE, IMN -d, and the pipeline methods with dTrans also perform reasonably well on this task, outperforming other baselines by moderate margins.",
            "All these models utilize knowledge from larger corpora by either joint training of document-level tasks or using domain-specific embeddings.",
            "This suggests that domain-specific knowledge is very helpful, and both joint training and domain-specific embeddings are effective ways to transfer such knowledge.",
            "We also show the results of IMN -d and IMN when only the general-purpose embeddings (without domain-specific embeddings) are used for initialization.",
            "They are denoted as IMN -d/IMN wo DE.",
            "IMN wo DE performs only marginally below IMN.",
            "This indicates that the knowledge captured by domain-specific embeddings could be similar to that captured by joint training of the IMN\u2212d is more affected document-level tasks.",
            " IMN ?d is  more  affected without domain-specific embeddings, while it still outperforms  all  other  baselines  except  DECNN-dTrans.",
            "DECNN-dTrans  is  a  very  strong  base-line as it exploits additional knowledge from larger corpora for both tasks.",
            " IMN?dwo DE is compet-itive with DECNN-dTrans even without utilizing additional  knowledge,  which  suggests  the  effectiveness of the proposed network structure."
        ],
        "class_sentence": [
            2,
            2,
            1,
            2,
            2,
            2,
            1,
            1,
            1,
            1,
            1,
            2,
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null,
            null,
            [
                "IMN -d"
            ],
            [
                "IMN"
            ],
            [
                "IMN -d",
                "F1-a",
                "F1-o"
            ],
            [
                "IMN",
                "acc-s",
                "F1-s"
            ],
            [
                "PIPELINE",
                "IMN -d",
                "DECNN-dTrans"
            ],
            null,
            null,
            [
                "IMN -d",
                "IMN"
            ],
            [
                "IMN -d",
                "IMN wo DE"
            ],
            [
                "IMN",
                "IMN wo DE"
            ],
            [
                "IMN -d"
            ],
            [
                "IMN -d",
                "DECNN-dTrans"
            ],
            null,
            [
                "IMN wo DE"
            ]
        ],
        "n_sentence": 20.0,
        "table_id": "table_3",
        "paper_id": "P19-1048",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1085table_3",
        "description": "We use the OFEVER metric to evaluate the document retrieval component. Table 3 shows the OFEVER scores of our model and models from other teams. After running the same model proposed by Hanselowski et al. (2018), we find our OFEVER score is slightly lower, which may due to the random factors.",
        "sentences": [
            "We use the OFEVER metric to evaluate the document retrieval component.",
            "Table 3 shows the OFEVER scores of our model and models from other teams.",
            "After running the same model proposed by Hanselowski et al. (2018), we find our OFEVER score is slightly lower, which may due to the random factors."
        ],
        "class_sentence": [
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "OFEVER"
            ],
            [
                "OFEVER"
            ],
            [
                "Athene",
                "Our Model",
                "OFEVER"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_3",
        "paper_id": "P19-1085",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1198table_2",
        "description": "Table 2 shows evaluation results of our proposed approaches along with existing supervised and unsupervised alternatives. We observe that unsupervised baselines such as UNMT and USMT often, after attaining convergence, recreates sentences similar to the inputs. This explains why they achieve higher BLEU and reduced worddifference scores. The ST system did not converge for our dataset after significant number of epochs which affected the performance metrics. The system often produces short sentences which are simple but do not retain important phrases. Other supervised systems such as SBMT and NTS achieve better content reduction as shown through SARI, BLEU and FE-diff scores; this is expected. However, it is still a good sign that the scores for the unsupervised system UNTS are not far from the supervised skylines. The higher word-diff scores for the unsupervised system also indicate that it is able to perform content reduction (a form of syntactic simplification), which is crucial to TS. This is unlike the existing unsupervised LIGHTLS system which often replaces nouns with related non-synonymous nouns; sometimes increasing the complexity and affecting the meaning. Finally, it is worth noting that aiding the system with a very small amount of labeled data can also benefit our unsupervised pipeline, as suggested by the scores for the UNTS+10K system.",
        "sentences": [
            "Table 2 shows evaluation results of our proposed approaches along with existing supervised and unsupervised alternatives.",
            "We observe that unsupervised baselines such as UNMT and USMT often, after attaining convergence, recreates sentences similar to the inputs.",
            "This explains why they achieve higher BLEU and reduced worddifference scores.",
            "The ST system did not converge for our dataset after significant number of epochs which affected the performance metrics.",
            "The system often produces short sentences which are simple but do not retain important phrases.",
            "Other supervised systems such as SBMT and NTS achieve better content reduction as shown through SARI, BLEU and FE-diff scores; this is expected.",
            "However, it is still a good sign that the scores for the unsupervised system UNTS are not far from the supervised skylines.",
            "The higher word-diff scores for the unsupervised system also indicate that it is able to perform content reduction (a form of syntactic simplification), which is crucial to TS.",
            "This is unlike the existing unsupervised LIGHTLS system which often replaces nouns with related non-synonymous nouns; sometimes increasing the complexity and affecting the meaning.",
            "Finally, it is worth noting that aiding the system with a very small amount of labeled data can also benefit our unsupervised pipeline, as suggested by the scores for the UNTS+10K system."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            2,
            1,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "UNMT",
                "USMT"
            ],
            [
                "BLEU",
                "Word-diff"
            ],
            [
                "ST"
            ],
            null,
            [
                "SBMT",
                "NTS",
                "SARI"
            ],
            [
                "UNTS"
            ],
            [
                "Word-diff",
                "UNTS+10K",
                "UNTS",
                "NTS"
            ],
            null,
            [
                "UNTS+10K"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_2",
        "paper_id": "P19-1198",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1228table_1",
        "description": "5 Results and Discussion . Table 1 shows the unlabeled F1 scores for our models and various baselines. All models soundly outperform right branching baselines, and we find that the neural PCFG/compound PCFG are strong models for grammar induction. In particular the compound PCFG outperforms other models by an appreciable margin on both English and Chinese. We again note that we were unable to induce meaningful grammars through a traditional PCFG with the scalar parameterization.11 . See appendix A.2 for the full results (including corpuslevel F1) broken down by sentence length.",
        "sentences": [
            "5 Results and Discussion .",
            "Table 1 shows the unlabeled F1 scores for our models and various baselines.",
            "All models soundly outperform right branching baselines, and we find that the neural PCFG/compound PCFG are strong models for grammar induction.",
            "In particular the compound PCFG outperforms other models by an appreciable margin on both English and Chinese.",
            "We again note that we were unable to induce meaningful grammars through a traditional PCFG with the scalar parameterization.11 .",
            "See appendix A.2 for the full results (including corpuslevel F1) broken down by sentence length."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "Neural PCFG",
                "Compound PCFG"
            ],
            [
                "Compound PCFG"
            ],
            null,
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_1",
        "paper_id": "P19-1228",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1291table_2",
        "description": "Table 2 reports results on the baseline model and our models under different beta. NIST 06 is used as dev set to select the best models, and NIST 2002, 2003, 2004, 2005 and 2008 datasets are used as test sets. There are some interesting observations. First, combing textual and phonetic information improves the performance of translation. Although the real reason is unknown, we suspect that it is because some kind of regularization effects from phonetic embeddings. Second, the phonetic information plays a very important role in translation. Even when beta = 0.95, that is, most weights are put on phonetic embedding, the performance is still very good. In fact, our best BLEU score (48.91), is achieved when beta = 0.95. However, word embedding is still important. In fact, when we use only phonetic information (beta = 1.0), the performance degrades, almost the same as baseline (only using textual information).",
        "sentences": [
            "Table 2 reports results on the baseline model and our models under different beta.",
            "NIST 06 is used as dev set to select the best models, and NIST 2002, 2003, 2004, 2005 and 2008 datasets are used as test sets.",
            "There are some interesting observations.",
            "First, combing textual and phonetic information improves the performance of translation.",
            "Although the real reason is unknown, we suspect that it is because some kind of regularization effects from phonetic embeddings.",
            "Second, the phonetic information plays a very important role in translation.",
            "Even when beta = 0.95, that is, most weights are put on phonetic embedding, the performance is still very good.",
            "In fact, our best BLEU score (48.91), is achieved when beta = 0.95.",
            "However, word embedding is still important.",
            "In fact, when we use only phonetic information (beta = 1.0), the performance degrades, almost the same as baseline (only using textual information)."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "NIST06 (Dev Set)",
                "NIST02",
                "NIST03",
                "NIST04",
                "NIST08"
            ],
            null,
            null,
            null,
            null,
            [
                "beta = 0.95"
            ],
            [
                "beta = 0.95"
            ],
            null,
            [
                "beta = 1.0"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_2",
        "paper_id": "P19-1291",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1329table_4",
        "description": "4.2 Evaluating Retrained Embeddings . Table 4 presents the performance of retrained embeddings and a random embedding baseline on magnitude and numeration tests. There is no significant difference in performance between Num and All variants, suggesting that seeing more numerals during training does not necessarily result in better representations. Results follow similar trends as off-the-shelf embeddings. All models capture an approximate notion of magnitude (high performance on BC-MAG), but do not capture numeration. Across models, FastText variants fare best.",
        "sentences": [
            "4.2 Evaluating Retrained Embeddings .",
            "Table 4 presents the performance of retrained embeddings and a random embedding baseline on magnitude and numeration tests.",
            "There is no significant difference in performance between Num and All variants, suggesting that seeing more numerals during training does not necessarily result in better representations.",
            "Results follow similar trends as off-the-shelf embeddings.",
            "All models capture an approximate notion of magnitude (high performance on BC-MAG), but do not capture numeration.",
            "Across models, FastText variants fare best."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "random",
                "GloVe-Num",
                "GloVe-All",
                "FastText-Num",
                "FastText-All",
                "Word2Vec-Num",
                "Word2Vec-All",
                "Magnitude",
                "Numeration"
            ],
            [
                "GloVe-Num",
                "GloVe-All",
                "FastText-Num",
                "FastText-All",
                "Word2Vec-Num",
                "Word2Vec-All"
            ],
            null,
            [
                "BC-MAG",
                "Magnitude",
                "Numeration"
            ],
            [
                "FastText-Num",
                "FastText-All"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_4",
        "paper_id": "P19-1329",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1343table_6",
        "description": "4.9 Hate speech detection results . Table 6 shows hate speech detection results. Training with only synthetic text after thresholding and stratified sampling outperforms training with only gold-tagged text by 4% F1, and using both gold and synthetic text gives a F1 boost of 6% beyond using gold alone. Remarkably, synthetic text alone outperforms gold text, because gold text has high class imbalance, leading to poorer prediction. Because we can create arbitrary amounts of synthetic text, we can balance the labels to achieve better prediction.",
        "sentences": [
            "4.9 Hate speech detection results .",
            "Table 6 shows hate speech detection results.",
            "Training with only synthetic text after thresholding and stratified sampling outperforms training with only gold-tagged text by 4% F1, and using both gold and synthetic text gives a F1 boost of 6% beyond using gold alone.",
            "Remarkably, synthetic text alone outperforms gold text, because gold text has high class imbalance, leading to poorer prediction.",
            "Because we can create arbitrary amounts of synthetic text, we can balance the labels to achieve better prediction."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "Only synthetic",
                "Synthetic +Gold",
                "Gold"
            ],
            [
                "Only synthetic"
            ],
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_6",
        "paper_id": "P19-1343",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1345table_3",
        "description": "4.3 Experimental Results. Table 3 shows the performances of different approaches to ASC-QA. From this table, we can see that all the three state-of-the-art ASC approaches, i.e., RAM, GCAE and S-LSTM, perform better than LSTM. This confirms the usefulness of considering aspect information in ASC. Besides, both the attention based approaches RAM and S-LSTM achieve comparable or better performance than GCAE. This result demonstrates the usefulness of a proper attention mechanism to model aspect information. The two QA matching approaches, i.e., BIDAF and HMN could achieve comparable performance with the three state-of-the-art ASC approaches, and MAMC even beats all of them. This indicates the appropriateness of treating question and answer in a QA style review as two parallel units instead of a single sequence in ASC-QA. Furthermore, our RBAN w/o RAWS approach (i.e., without considering aspect information) performs consistently better than MAMC. This encourages to employ bidirectional attention to learn the representation vectors of both the question and answer in order to capture the sentiment information therein. Besides, it\u00e2\u20ac\u2122s interesting to notice that RBAN w/o A2Q (i.e., without question vector sq) performs much better than RBAN w/o Q2A (i.e., without answer vector sa). This is due to the fact that the main sentiment polarity towards aspect is usually expressed in the answer text.\".",
        "sentences": [
            "4.3 Experimental Results.",
            "Table 3 shows the performances of different approaches to ASC-QA.",
            "From this table, we can see that all the three state-of-the-art ASC approaches, i.e., RAM, GCAE and S-LSTM, perform better than LSTM.",
            "This confirms the usefulness of considering aspect information in ASC.",
            "Besides, both the attention based approaches RAM and S-LSTM achieve comparable or better performance than GCAE.",
            "This result demonstrates the usefulness of a proper attention mechanism to model aspect information.",
            "The two QA matching approaches, i.e., BIDAF and HMN could achieve comparable performance with the three state-of-the-art ASC approaches, and MAMC even beats all of them.",
            "This indicates the appropriateness of treating question and answer in a QA style review as two parallel units instead of a single sequence in ASC-QA.",
            "Furthermore, our RBAN w/o RAWS approach (i.e., without considering aspect information) performs consistently better than MAMC.",
            "This encourages to employ bidirectional attention to learn the representation vectors of both the question and answer in order to capture the sentiment information therein.",
            "Besides, it\u00e2\u20ac\u2122s interesting to notice that RBAN w/o A2Q (i.e., without question vector sq) performs much better than RBAN w/o Q2A (i.e., without answer vector sa).",
            "This is due to the fact that the main sentiment polarity towards aspect is usually expressed in the answer text.\"."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            1,
            2,
            1,
            2,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Term-level ASC-QA",
                "Category-level ASC-QA"
            ],
            [
                "RAM (Chen et al.,2017)",
                "GCAE (Xue and Li, 2018)",
                "S-LSTM (Wang and Lu, 2018)",
                "LSTM (Wang et al.,2016)"
            ],
            null,
            [
                "RAM (Chen et al.,2017)",
                "S-LSTM (Wang and Lu, 2018)",
                "GCAE (Xue and Li, 2018)"
            ],
            null,
            [
                "BIDAF (Seo et al., 2016)",
                "HMN (Shen et al., 2018a)",
                "MAMC (Yin et al., 2017)"
            ],
            null,
            [
                "RBAN w/o RAWS",
                "MAMC (Yin et al., 2017)"
            ],
            null,
            [
                "RBAN w/o A2Q",
                "RBAN w/o Q2A"
            ],
            null
        ],
        "n_sentence": 12.0,
        "table_id": "table_3",
        "paper_id": "P19-1345",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1407table_2",
        "description": "We use the task of question generation: Given a structured representation of a query against a knowledge base or a database (e.g. a logical form), produce the corresponding natural language question. We use two datasets consisting of (question, logical form) pairs: WebQuestionsSP (Yih et al., 2016) (a standard dataset for semantic parsing, where the logical form is in SPARQL), and WikiSQL (Zhong et al., 2017) (where the logical form is SQL). Both datasets are small, with the former having 3098 training and 1639 testing examples, and the latter being an order of magnitude larger with 56346 training and 15873 testing examples. We evaluate metrics at both a corpus level (to indicate how natural output questions are) and at a per-sentence level (to demonstrate how well output questions exactly match the gold question). BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) are chosen for precision and recall-based metrics. METEOR (Banerjee and Lavie, 2005) is chosen to deal with stemming and synonyms. We noticed that many tokens that appear in the logical form are also present in the natural language form for each example. In fact, nearly half of the tokens in the question appear in the corresponding SPARQL of the WebQuestionSP dataset (Yih et al., 2016), implying that a network with the ability to copy from the input could see significant gains on the task. Accordingly, we compare our Scratchpad Mechanism against three baselines: (1) Seq2Seq, (2) Copynet and (3) Coverage, a method introduced by Tu et al. (2016) that aims to solve attention-related problems. Seq2Seq is the standard approach introduced in Sutskever et al. (2014). The Copynet (He et al., 2017) baseline additionally gives the Seq2Seq model the ability to copy vocabulary from the source to the target. From Table 2 it is clear that our approach, Scratchpad outperforms all baselines on all the metrics.",
        "sentences": [
            "We use the task of question generation: Given a structured representation of a query against a knowledge base or a database (e.g. a logical form), produce the corresponding natural language question.",
            "We use two datasets consisting of (question, logical form) pairs: WebQuestionsSP (Yih et al., 2016) (a standard dataset for semantic parsing, where the logical form is in SPARQL), and WikiSQL (Zhong et al., 2017) (where the logical form is SQL).",
            "Both datasets are small, with the former having 3098 training and 1639 testing examples, and the latter being an order of magnitude larger with 56346 training and 15873 testing examples.",
            "We evaluate metrics at both a corpus level (to indicate how natural output questions are) and at a per-sentence level (to demonstrate how well output questions exactly match the gold question).",
            "BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) are chosen for precision and recall-based metrics.",
            "METEOR (Banerjee and Lavie, 2005) is chosen to deal with stemming and synonyms.",
            "We noticed that many tokens that appear in the logical form are also present in the natural language form for each example.",
            "In fact, nearly half of the tokens in the question appear in the corresponding SPARQL of the WebQuestionSP dataset (Yih et al., 2016), implying that a network with the ability to copy from the input could see significant gains on the task.",
            "Accordingly, we compare our Scratchpad Mechanism against three baselines: (1) Seq2Seq, (2) Copynet and (3) Coverage, a method introduced by Tu et al. (2016) that aims to solve attention-related problems.",
            "Seq2Seq is the standard approach introduced in Sutskever et al. (2014).",
            "The Copynet (He et al., 2017) baseline additionally gives the Seq2Seq model the ability to copy vocabulary from the source to the target.",
            "From Table 2 it is clear that our approach, Scratchpad outperforms all baselines on all the metrics."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            2,
            2,
            2,
            2,
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "WebQSP",
                "WikiSQL"
            ],
            null,
            [
                "Per-Sentence",
                "Corpus-Level"
            ],
            [
                "Bleu",
                "Rouge-L"
            ],
            [
                "Meteor"
            ],
            null,
            null,
            [
                "Baseline",
                "Copynet",
                "Copy + Coverage"
            ],
            [
                "Baseline"
            ],
            [
                "Copynet"
            ],
            [
                "Copy + Scratchpad"
            ]
        ],
        "n_sentence": 12.0,
        "table_id": "table_2",
        "paper_id": "P19-1407",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1428table_1",
        "description": "The evaluation was performed in the eHealthKD corpus (Piad-Morffis et al., 2019a), using the training and development collections for training and the test collection for evaluation. Thus, the training data is comprised of 844 sentences resulting in a total of 9540 annotations among key phrases and relations. The test data is comprised of 100 sentences (for Scenario 1) with 1100 total annotations. After 60 generations, the best performing pipeline (actually found in generation 18) achieved a F1 score of 0.754 in Scenario 1 of the eHealth-KD challenge. This represents a 1% absolute improvement from the top result presented in the eHealth-KD challenge, and a 4.2% absolute improvement over the average result of the top 3 alternatives (F1 = 0.711). Table 1 shows this result in a comparison with the rest of the approaches presented in Section 3.",
        "sentences": [
            "The evaluation was performed in the eHealthKD corpus (Piad-Morffis et al., 2019a), using the training and development collections for training and the test collection for evaluation.",
            "Thus, the training data is comprised of 844 sentences resulting in a total of 9540 annotations among key phrases and relations.",
            "The test data is comprised of 100 sentences (for Scenario 1) with 1100 total annotations.",
            "After 60 generations, the best performing pipeline (actually found in generation 18) achieved a F1 score of 0.754 in Scenario 1 of the eHealth-KD challenge.",
            "This represents a 1% absolute improvement from the top result presented in the eHealth-KD challenge, and a 4.2% absolute improvement over the average result of the top 3 alternatives (F1 = 0.711).",
            "Table 1 shows this result in a comparison with the rest of the approaches presented in Section 3."
        ],
        "class_sentence": [
            1,
            2,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "Our Proposal",
                "F 1 (Scenario 1)"
            ],
            [
                "Our Proposal",
                "F 1 (Scenario 1)"
            ],
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_1",
        "paper_id": "P19-1428",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1513table_7",
        "description": "Table 7 reports P@1, P@3, P@5, and P@10 for each leaderboard (i.e., TDM triple). The macro average P@1 and P@3 are 0.70 and 0.67, respectively, which is encouraging. Overall, 86% of papers are related to the target task T. We found that most false positives are due to the fact that these papers conduct research on the target task T, but report results on a different dataset or use the target dataset D as a resource to extract features. For instance, most predicted papers for the leaderboard <Machine translation, WMT 2014 EN-FR, BLEU> are papers about Machine translation but these papers report results on the dataset WMT 2012 EN-FR or WMT 2014 EN-DE.",
        "sentences": [
            "Table 7 reports P@1, P@3, P@5, and P@10 for each leaderboard (i.e., TDM triple).",
            "The macro average P@1 and P@3 are 0.70 and 0.67, respectively, which is encouraging.",
            "Overall, 86% of papers are related to the target task T.",
            "We found that most false positives are due to the fact that these papers conduct research on the target task T, but report results on a different dataset or use the target dataset D as a resource to extract features.",
            "For instance, most predicted papers for the leaderboard <Machine translation, WMT 2014 EN-FR, BLEU> are papers about Machine translation but these papers report results on the dataset WMT 2012 EN-FR or WMT 2014 EN-DE."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            [
                "P@1",
                "P@3",
                "P@5",
                "P@10"
            ],
            [
                "Macro-average",
                "P@1",
                "P@3"
            ],
            null,
            null,
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_7",
        "paper_id": "P19-1513",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1557table_3",
        "description": "Table 3 shows that the system obtains superior results in the Hate Speech dataset and yields competitive results on the Kaggle data in comparison to some sate-of-the-art baseline systems. Table 4 shows the results of our system on the DBpedia and AG News datasets. Using the same model without any tuning, we managed to obtain competitive results again compared to previous stateof-the-art systems.",
        "sentences": [
            "Table 3 shows that the system obtains superior results in the Hate Speech dataset and yields competitive results on the Kaggle data in comparison to some sate-of-the-art baseline systems.",
            "Table 4 shows the results of our system on the DBpedia and AG News datasets.",
            "Using the same model without any tuning, we managed to obtain competitive results again compared to previous stateof-the-art systems."
        ],
        "class_sentence": [
            1,
            0,
            0
        ],
        "header_mention": [
            [
                "Hate Speech dataset",
                "Kaggle dataset",
                "This work + chisquare50 ",
                "This work + chisuare100 ",
                "This work + ANOVA50 ",
                "This work + ANOVA100 ",
                "This work + chisquare100 "
            ],
            null,
            null
        ],
        "n_sentence": 3.0,
        "table_id": "table_3",
        "paper_id": "P19-1557",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1570table_3",
        "description": "5.2.3 Word entailment. Given two words w1 and w2, w2 entails w1 (denoted by w1 |= w2) if all instances of w1 are w2. We compare Word2Sense embeddings with Word2GM on the entailment dataset provided by (Baroni et al., 2012). We use KL divergence to generate entailment scores between words w1 and w2. For Word2GM, we use both cosine similarity and KL divergence, as used in the original paper. We report the F1 scores and Average Precision(AP) scores for reporting the quality of prediction. Table 3 compares the performance of our embedding with Word2GM. We notice that Word2Sense embeddings with \u00b5 = k(cid:48) (denoted Word2Sense-full in the table), i.e., with no truncation, yields the best results. We do not compare with hyperbolic embeddings (Tifrea et al., 2019; Dhingra et al., 2018) because these embeddings are designed mainly to perform well on entailment tasks, but are far off from the performance of Euclidean embeddings on similarity tasks.",
        "sentences": [
            "5.2.3 Word entailment.",
            "Given two words w1 and w2, w2 entails w1 (denoted by w1 |= w2) if all instances of w1 are w2.",
            "We compare Word2Sense embeddings with Word2GM on the entailment dataset provided by (Baroni et al., 2012).",
            "We use KL divergence to generate entailment scores between words w1 and w2.",
            "For Word2GM, we use both cosine similarity and KL divergence, as used in the original paper.",
            "We report the F1 scores and Average Precision(AP) scores for reporting the quality of prediction.",
            "Table 3 compares the performance of our embedding with Word2GM.",
            "We notice that Word2Sense embeddings with \u00b5 = k(cid:48) (denoted Word2Sense-full in the table), i.e., with no truncation, yields the best results.",
            "We do not compare with hyperbolic embeddings (Tifrea et al., 2019; Dhingra et al., 2018) because these embeddings are designed mainly to perform well on entailment tasks, but are far off from the performance of Euclidean embeddings on similarity tasks."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            2,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "(Baroni et al., 2012)"
            ],
            null,
            [
                "Word2GM (10)-Cos",
                "Word2GM (10)-KL"
            ],
            [
                "Best AP",
                "Best F1"
            ],
            [
                "Word2Sense",
                "Word2Sense -full",
                "Word2GM (10)-Cos",
                "Word2GM (10)-KL"
            ],
            [
                "Word2Sense -full"
            ],
            null
        ],
        "n_sentence": 9.0,
        "table_id": "table_3",
        "paper_id": "P19-1570",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1570table_5",
        "description": "We split the 300 senses into ten sets of 30 senses, and assigned 3 judges to annotate the intruder in each of the 30 senses in a set (we used a total of 30 judges). For each question, we take the majority voted word as the predicted intruder. If a question has 3 different annotations, we count that dimension as non interpretable6. Since, we followed the procedure as in (Subramanian et al., 2018), we compare our performance with the results reported in their paper. Table 5 shows that W ord2Sense is competitive with the best interpretable embeddings.",
        "sentences": [
            "We split the 300 senses into ten sets of 30 senses, and assigned 3 judges to annotate the intruder in each of the 30 senses in a set (we used a total of 30 judges).",
            "For each question, we take the majority voted word as the predicted intruder.",
            "If a question has 3 different annotations, we count that dimension as non interpretable6.",
            "Since, we followed the procedure as in (Subramanian et al., 2018), we compare our performance with the results reported in their paper.",
            "Table 5 shows that W ord2Sense is competitive with the best interpretable embeddings."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            [
                "Word2Sense"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_5",
        "paper_id": "P19-1570",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1586table_6",
        "description": "Active Learning Sampling Strategies. As discussed in a previous section, we adopted highconfidence sampling and a partition mechanism for our active learning. Here we analyze the effect of the two methods. Table 6 shows deep transfer active learning performance in DBLP-ACM with varying sampling strategies. We can observe that high-confidence sampling and the partition mechanism contribute to high and stable performance as well as good precision-recall balance. Notice that there is a huge jump in recall by adding partition while precision stays the same (row 4 to row 3). This is due to the fact that the partition mechanism succeeds in finding more false negatives. The breakdown of labeled examples (Table 7) shows that is indeed the case. It is noteworthy that the partition mechanism lowers the ratio of misclassified examples (FP+FN) in the labeled sample set because partitioning encourages us to choose likely false negatives more aggressively, yet false negatives tend to be more challenging to find in entity resolution due to the skewness toward the negative (Qian et al., 2017). We observed similar patterns in DBLP-Scholar and Cora.",
        "sentences": [
            "Active Learning Sampling Strategies.",
            "As discussed in a previous section, we adopted highconfidence sampling and a partition mechanism for our active learning.",
            "Here we analyze the effect of the two methods.",
            "Table 6 shows deep transfer active learning performance in DBLP-ACM with varying sampling strategies.",
            "We can observe that high-confidence sampling and the partition mechanism contribute to high and stable performance as well as good precision-recall balance.",
            "Notice that there is a huge jump in recall by adding partition while precision stays the same (row 4 to row 3).",
            "This is due to the fact that the partition mechanism succeeds in finding more false negatives.",
            "The breakdown of labeled examples (Table 7) shows that is indeed the case.",
            "It is noteworthy that the partition mechanism lowers the ratio of misclassified examples (FP+FN) in the labeled sample set because partitioning encourages us to choose likely false negatives more aggressively, yet false negatives tend to be more challenging to find in entity resolution due to the skewness toward the negative (Qian et al., 2017).",
            "We observed similar patterns in DBLP-Scholar and Cora."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            1,
            1,
            2,
            0,
            0,
            0
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            [
                "High-Conf.+Part."
            ],
            [
                "High-Conf.+Part.",
                "Top K Entrop"
            ],
            null,
            null,
            null,
            null
        ],
        "n_sentence": 10.0,
        "table_id": "table_6",
        "paper_id": "P19-1586",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1596table_6",
        "description": "Table 6 presents the average SER rates for each model, where lower rates mean fewer mistakes (indicated by ?). It is important to note here that we compute errors over value and adjective slots only, since these are the ones that we are able to identify lexically (we cannot identify whether an output makes an error on sentiment in this way, so we measure that with a human evaluation in Section 4.3). This means that the BASE outputs errors are computed over only value slots (since they don\u2019t contain adjectives), and the rest of the errors are computed over both value and adjective slots. Amazingly, overall, Table 6 results show the SER is extremely low, even while achieving a large amount of stylistic variation. Naturally, BASE, with no access to style information, has the best (lowest) SER. But we note that there is not a large increase in SER as more information is added \u2013 even for the most difficult setting, +STYLE, the models make an error on less than 10% of the slots in a given MR, on average.",
        "sentences": [
            "Table 6 presents the average SER rates for each model, where lower rates mean fewer mistakes (indicated by ?).",
            "It is important to note here that we compute errors over value and adjective slots only, since these are the ones that we are able to identify lexically (we cannot identify whether an output makes an error on sentiment in this way, so we measure that with a human evaluation in Section 4.3).",
            "This means that the BASE outputs errors are computed over only value slots (since they don\u2019t contain adjectives), and the rest of the errors are computed over both value and adjective slots.",
            "Amazingly, overall, Table 6 results show the SER is extremely low, even while achieving a large amount of stylistic variation.",
            "Naturally, BASE, with no access to style information, has the best (lowest) SER.",
            "But we note that there is not a large increase in SER as more information is added \u2013 even for the most difficult setting, +STYLE, the models make an error on less than 10% of the slots in a given MR, on average."
        ],
        "class_sentence": [
            1,
            2,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "Avg SER"
            ],
            null,
            null,
            [
                "Avg SER"
            ],
            [
                "BASE",
                "Avg SER"
            ],
            [
                "Avg SER",
                " +STYLE"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_6",
        "paper_id": "P19-1596",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1630table_1",
        "description": "Table 1 (top) presents results of models trained and tested on the synthetic multi-aspect dataset. All aspect-aware models beat both baselines by a large margin. For classical summarization, the lead-3 baseline remains a challenge to beat even by state-of-the-art systems, and also on multiaspect documents we observe that, unlike our systems, PG-net performs worse than lead-3. Unsurprisingly, the extractive aspect-aware models outperform their abstractive counterparts in terms of ROUGE, and the decoder attention distributions are more amenable to extraction than encoder attention scores. Overall, our structured models enable both abstractive and extractive aspectaware summarization at a quality clearly exceeding structure-agnostic baselines.",
        "sentences": [
            "Table 1 (top) presents results of models trained and tested on the synthetic multi-aspect dataset.",
            "All aspect-aware models beat both baselines by a large margin.",
            "For classical summarization, the lead-3 baseline remains a challenge to beat even by state-of-the-art systems, and also on multiaspect documents we observe that, unlike our systems, PG-net performs worse than lead-3.",
            "Unsurprisingly, the extractive aspect-aware models outperform their abstractive counterparts in terms of ROUGE, and the decoder attention distributions are more amenable to extraction than encoder attention scores.",
            "Overall, our structured models enable both abstractive and extractive aspectaware summarization at a quality clearly exceeding structure-agnostic baselines."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "enc-attn-extract",
                "dec-attn-extract"
            ],
            [
                "PG-net",
                "lead-3"
            ],
            [
                "enc-attn-extract",
                "dec-attn-extract",
                "enc-attn",
                "dec-attn",
                "Rouge 1",
                "Rouge 2",
                "Rouge L"
            ],
            [
                "enc-attn-extract",
                "dec-attn-extract",
                "sf"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_1",
        "paper_id": "P19-1630",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1635table_4",
        "description": "Table 4 summarizes the experimental results. Logistic regression (LR) with bag of words, which are composed of top-1K frequent words, sets a baseline for the proposed task. The BiGRU model beats the other models with a micro-averaged F1 score of 80.16%, and the GRU-capsule model performs the best with a macro-averaged F1 score of 64.71%. The RNN-based models outperform the CNN-based models in both the general NN framework and the capsule network framework. The results account for the importance of the order of the context in market comments when inserting numerical information. Further evidence supporting this statement is that the CRNN model obtains a higher performance than the CNN model does.",
        "sentences": [
            "Table 4 summarizes the experimental results.",
            "Logistic regression (LR) with bag of words, which are composed of top-1K frequent words, sets a baseline for the proposed task.",
            "The BiGRU model beats the other models with a micro-averaged F1 score of 80.16%, and the GRU-capsule model performs the best with a macro-averaged F1 score of 64.71%.",
            "The RNN-based models outperform the CNN-based models in both the general NN framework and the capsule network framework.",
            "The results account for the importance of the order of the context in market comments when inserting numerical information.",
            "Further evidence supporting this statement is that the CRNN model obtains a higher performance than the CNN model does."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "LR"
            ],
            [
                "BiGRU",
                "Micro-F1",
                "GRU-capsule",
                "Macro-F1"
            ],
            [
                "CRNN",
                "CNN",
                "CNN-capsule"
            ],
            null,
            [
                "CRNN",
                "CNN"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_4",
        "paper_id": "P19-1635",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1638table_3",
        "description": "CNN-SC achieves an accuracy of 90.0%, outperforming CNN-R by a large margin. Additionally, sentence content is four times as fast to train as the computationally-expensive reconstruction objective. Are representations obtained using these objectives more useful when learned from in-domain data?. To examine the dataset effect, we repeat our experiments using paragraph embeddings pre-trained using these objectives on a subset of Wikipedia (560K paragraphs). The second row of Table 3 shows that both approaches suffer a drop in downstream accuracy when pre-trained on out-of-domain data. Interestingly, CNN-SC still performs best, indicating that sentence content is more suitable for downstream classification.",
        "sentences": [
            "CNN-SC achieves an accuracy of 90.0%, outperforming CNN-R by a large margin.",
            "Additionally, sentence content is four times as fast to train as the computationally-expensive reconstruction objective.",
            "Are representations obtained using these objectives more useful when learned from in-domain data?.",
            "To examine the dataset effect, we repeat our experiments using paragraph embeddings pre-trained using these objectives on a subset of Wikipedia (560K paragraphs).",
            "The second row of Table 3 shows that both approaches suffer a drop in downstream accuracy when pre-trained on out-of-domain data.",
            "Interestingly, CNN-SC still performs best, indicating that sentence content is more suitable for downstream classification."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "CNN-SC",
                "CNN-R"
            ],
            [
                "CNN-SC"
            ],
            null,
            [
                "On Wikipedia"
            ],
            [
                "On Wikipedia"
            ],
            [
                "CNN-SC"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_3",
        "paper_id": "P19-1638",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1648table_4",
        "description": "Results on VisDial test-std v1.0 . We also evaluate the proposed ReDAN on the blind test-std v1.0 set, by submitting results to the online evaluation server. Table 4 shows the comparison between our model and state-of-the-art visual dialog models. By using a diverse set of ensembles, ReDAN+ outperforms the state of the art method, DAN (Kottur et al., 2018), by a significant margin, lifting NDCG from 59.36% to 64.47%.",
        "sentences": [
            "Results on VisDial test-std v1.0 .",
            "We also evaluate the proposed ReDAN on the blind test-std v1.0 set, by submitting results to the online evaluation server.",
            "Table 4 shows the comparison between our model and state-of-the-art visual dialog models.",
            "By using a diverse set of ensembles, ReDAN+ outperforms the state of the art method, DAN (Kottur et al., 2018), by a significant margin, lifting NDCG from 59.36% to 64.47%."
        ],
        "class_sentence": [
            0,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "ReDAN+ (Diverse Ens.)",
                "NMN (Kottur et al., 2018)",
                "RvA (Niu et al., 2018)",
                "CorefNMN (Kottur et al., 2018)"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_4",
        "paper_id": "P19-1648",
        "valid": 1
    }
]