[
    {
        "table_id_paper": "D16-1007table_2",
        "description": "Table 2 summarizes the performances of proposed model when different position features are exploited. To concentrate on studying the effect of position features, we do not involve lexical features in this section. As the table shows, the position feature on plain text is still effective in our model and we accredit its satisfactory result to the dependency information and tree-based kernels. The F1 scores of tree-based position features are higher since they are \u201cspecially designed\u201d for our model. Contrary to our expectation, the more fine-grained TPF2 does not yield a better performance than TPF1, and two kinds of TPF give fairly close results. One possible reason is that the influence of a more elaborated definition of relative position is minimal. As most sentences in this dataset are of short length and their dependency trees are not so complicated, replacing TPF1 with TPF2 usually brings little new structural information and thus results in a similar F1 score. However, though the performances of different position features are close, tree-based position feature is an essential part of our model. The F1 score is severely reduced to 75.22 when we remove the tree-based position feature in PECNN.",
        "sentences": [
            "Table 2 summarizes the performances of proposed model when different position features are exploited.",
            "To concentrate on studying the effect of position features, we do not involve lexical features in this section.",
            "As the table shows, the position feature on plain text is still effective in our model and we accredit its satisfactory result to the dependency information and tree-based kernels.",
            "The F1 scores of tree-based position features are higher since they are \u201cspecially designed\u201d for our model.",
            "Contrary to our expectation, the more fine-grained TPF2 does not yield a better performance than TPF1, and two kinds of TPF give fairly close results.",
            "One possible reason is that the influence of a more elaborated definition of relative position is minimal.",
            "As most sentences in this dataset are of short length and their dependency trees are not so complicated, replacing TPF1 with TPF2 usually brings little new structural information and thus results in a similar F1 score.",
            "However, though the performances of different position features are close, tree-based position feature is an essential part of our model.",
            "The F1 score is severely reduced to 75.22 when we remove the tree-based position feature in PECNN."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            1,
            2,
            2,
            0,
            0
        ],
        "header_mention": [
            null,
            null,
            [
                "plain text PF",
                "TPF1",
                "TPF2"
            ],
            [
                "TPF1",
                "TPF2"
            ],
            [
                "TPF1",
                "TPF2"
            ],
            null,
            [
                "TPF1",
                "TPF2"
            ],
            null,
            null
        ],
        "n_sentence": 9.0,
        "table_id": "table_2",
        "paper_id": "D16-1007",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1010table_3",
        "description": "Table 3 presents the correlation results for the two models\u00e2\u20ac\u2122 preferences for each construction and the verb bias score. The AB model does not correlate with the judgments for the DO. However, the model produces significant positive correlations with the PD judgments and with the verb bias score. The BFS model, on the other hand, achieves significant positive correlations on all measures, by both levels. As in the earlier experiments, the best correlation with the verb bias score is produced by the second level of the BFS model, as Figure 3 demonstrates.",
        "sentences": [
            "Table 3 presents the correlation results for the two models\u00e2\u20ac\u2122 preferences for each construction and the verb bias score.",
            "The AB model does not correlate with the judgments for the DO.",
            "However, the model produces significant positive correlations with the PD judgments and with the verb bias score.",
            "The BFS model, on the other hand, achieves significant positive correlations on all measures, by both levels.",
            "As in the earlier experiments, the best correlation with the verb bias score is produced by the second level of the BFS model."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "AB (Connectionist)",
                "BFS (Bayesian)"
            ],
            [
                "AB (Connectionist)",
                "DO"
            ],
            [
                "AB (Connectionist)",
                "PD"
            ],
            [
                "DO",
                "PD",
                "DO-PD",
                "AB (Connectionist)",
                "BFS (Bayesian)"
            ],
            [
                "Level 2",
                "BFS (Bayesian)"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_3",
        "paper_id": "D16-1010",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1011table_4",
        "description": "Results. Table 4 presents the results of our rationale model. We explore a range of hyper-parameter values. We include two runs for each version. The first one achieves the highest MAP on the development set, The second run is selected to compare the models when they use roughly 10% of question text (7 words on average). We also show the results of different runs in Figure 6. The rationales achieve the MAP up to 56.5%, getting close to using the titles. The models also outperform the baseline of using the noisy question bodies, indicating the the models\u00e2\u20ac\u2122 capacity of extracting short but important fragments.",
        "sentences": [
            "Results.",
            "Table 4 presents the results of our rationale model.",
            "We explore a range of hyper-parameter values.",
            "We include two runs for each version.",
            "The first one achieves the highest MAP on the development set, The second run is selected to compare the models when they use roughly 10% of question text (7 words on average).",
            "We also show the results of different runs in Figure 6.",
            "The rationales achieve the MAP up to 56.5%, getting close to using the titles.",
            "The models also outperform the baseline of using the noisy question bodies, indicating the the models\u00e2\u20ac\u2122 capacity of extracting short but important fragments."
        ],
        "class_sentence": [
            2,
            1,
            2,
            2,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            [
                "Independent",
                "Dependent",
                "MAP (dev)"
            ],
            null,
            [
                "Dependent",
                "MAP (dev)",
                "Full title"
            ],
            [
                "Independent",
                "Dependent",
                "Full title",
                "Full body"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_4",
        "paper_id": "D16-1011",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1018table_2",
        "description": "Table 2 shows the results of our contextdependent sense embedding models on the SCWS dataset. In this table, \u03c1 refers to the Spearman\u2019s rank correlation and a higher value of \u03c1 indicates better performance. The baseline performances are from Huang et al. (2012), Chen et al. (2014), Neelakantan et al. (2014), Li and Jurafsky (2015), Tian et al. (2014) and Bartunov et al. (2016). Here Ours + CBOW denotes our model with a CBOW based energy function and Ours + Skip-gram denotes our model with a Skip-gram based energy function. The results above the thick line are the models based on context clustering methods and the results below the thick line are the probabilistic models including ours. The similarity metrics of context clustering based models are AvgSim and AvgSimC proposed by Reisinger and Mooney (2010). Tian et al. (2014) propose two metrics Model_M and Model_W which are similar to our HardSim and SoftSim metrics. From Table 2, we can observe that our model outperforms the other probabilistic models and is not as good as the best context clustering based model. The context clustering based models are overall better than the probabilistic models on this task. A possible reason is that most context clustering based methods make use of more external knowledge than probabilistic models. However, note that Faruqui et al. (2016) presented several problems associated with the evaluation of word vectors on word similarity datasets and pointed out that the use of word similarity tasks for evaluation of word vectors is not sustainable. Bartunov et al. (2016) also suggest that SCWS should be of limited use for evaluating word representation models. Therefore, the results on this task shall be taken with caution. We consider that more realistic natural language processing tasks like word sense induction are better for evaluating sense embedding models.",
        "sentences": [
            "Table 2 shows the results of our contextdependent sense embedding models on the SCWS dataset.",
            "In this table, \u03c1 refers to the Spearman\u2019s rank correlation and a higher value of \u03c1 indicates better performance.",
            "The baseline performances are from Huang et al. (2012), Chen et al. (2014), Neelakantan et al. (2014), Li and Jurafsky (2015), Tian et al. (2014) and Bartunov et al. (2016).",
            "Here Ours + CBOW denotes our model with a CBOW based energy function and Ours + Skip-gram denotes our model with a Skip-gram based energy function.",
            "The results above the thick line are the models based on context clustering methods and the results below the thick line are the probabilistic models including ours.",
            "The similarity metrics of context clustering based models are AvgSim and AvgSimC proposed by Reisinger and Mooney (2010).",
            "Tian et al. (2014) propose two metrics Model_M and Model_W which are similar to our HardSim and SoftSim metrics.",
            "From Table 2, we can observe that our model outperforms the other probabilistic models and is not as good as the best context clustering based model.",
            "The context clustering based models are overall better than the probabilistic models on this task.",
            "A possible reason is that most context clustering based methods make use of more external knowledge than probabilistic models.",
            "However, note that Faruqui et al. (2016) presented several problems associated with the evaluation of word vectors on word similarity datasets and pointed out that the use of word similarity tasks for evaluation of word vectors is not sustainable.",
            "Bartunov et al. (2016) also suggest that SCWS should be of limited use for evaluating word representation models.",
            "Therefore, the results on this task shall be taken with caution.",
            "We consider that more realistic natural language processing tasks like word sense induction are better for evaluating sense embedding models."
        ],
        "class_sentence": [
            1,
            2,
            2,
            2,
            2,
            2,
            2,
            1,
            2,
            2,
            2,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "Huang",
                "Chen",
                "Neelakantan",
                "Li",
                "Tian",
                "Bartunov"
            ],
            [
                "Ours + CBOW",
                "Ours + Skip-gram"
            ],
            null,
            [
                "AvgSim",
                "AvgSimC"
            ],
            [
                "Model_M",
                "Model_W",
                "HardSim",
                "SoftSim"
            ],
            [
                "Model"
            ],
            [
                "Model"
            ],
            [
                "Model"
            ],
            null,
            null,
            null,
            null
        ],
        "n_sentence": 14.0,
        "table_id": "table_2",
        "paper_id": "D16-1018",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1021table_4",
        "description": "From Table 4, we can find that in the first hop the context words \u201cgreat\u201d, \u201cbut\u201d and \u201cdreadful\u201d contribute equally to the aspect \u201cservice\u201d. While after the second hop, the weight of \u201cdreadful\u201d increases and finally the model correctly predict the polarity towards \u201cservice\u201d as negative. This case shows the effects of multiple hops. However, for food aspect, the content-based model also gives a larger weight to \u201cdreadful\u201d when the target we focus on is \u201cfood\u201d. As a result, the model incorrectly predicts the polarity towards \u201cfood\u201d as negative. This phenomenon might be caused by the neglect of location information.",
        "sentences": [
            "From Table 4, we can find that in the first hop the context words \u201cgreat\u201d, \u201cbut\u201d and \u201cdreadful\u201d contribute equally to the aspect \u201cservice\u201d.",
            "While after the second hop, the weight of \u201cdreadful\u201d increases and finally the model correctly predict the polarity towards \u201cservice\u201d as negative.",
            "This case shows the effects of multiple hops.",
            "However, for food aspect, the content-based model also gives a larger weight to \u201cdreadful\u201d when the target we focus on is \u201cfood\u201d.",
            "As a result, the model incorrectly predicts the polarity towards \u201cfood\u201d as negative.",
            "This phenomenon might be caused by the neglect of location information."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "great",
                "but",
                "dreadful",
                "service"
            ],
            null,
            [
                "dreadful",
                "service"
            ],
            [
                "dreadful",
                "food"
            ],
            [
                "food"
            ],
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_4",
        "paper_id": "D16-1021",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1025table_2",
        "description": "4 Overall Translation Quality. Table 2 presents overall system results according to HTER and mTER, as well as BLEU computed against the original TED Talks reference translation. We can see that NMT clearly outperforms all other approaches both in terms of BLEU and TER scores. Focusing on mTER results, the gain obtained by NMT over the second best system (PBSY) amounts to 26%. It is also worth noticing that mTER is considerably lower than HTER for each system. This reduction shows that exploiting all the available postedits as references for TER is a viable way to control and overcome post-editors variability, thus ensuring a more reliable and informative evaluation about the real overall performance of MT systems. For this reason, the two following analyses rely on mTER. In particular, we investigate how specific characteristics of input documents affect the system\u00e2\u20ac\u2122s overall translation quality, focusing on (i) sentence length and (ii) the different talks composing the dataset.",
        "sentences": [
            "4 Overall Translation Quality.",
            "Table 2 presents overall system results according to HTER and mTER, as well as BLEU computed against the original TED Talks reference translation.",
            "We can see that NMT clearly outperforms all other approaches both in terms of BLEU and TER scores.",
            "Focusing on mTER results, the gain obtained by NMT over the second best system (PBSY) amounts to 26%.",
            "It is also worth noticing that mTER is considerably lower than HTER for each system.",
            "This reduction shows that exploiting all the available postedits as references for TER is a viable way to control and overcome post-editors variability, thus ensuring a more reliable and informative evaluation about the real overall performance of MT systems.",
            "For this reason, the two following analyses rely on mTER.",
            "In particular, we investigate how specific characteristics of input documents affect the system\u00e2\u20ac\u2122s overall translation quality, focusing on (i) sentence length and (ii) the different talks composing the dataset."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            2,
            0,
            0
        ],
        "header_mention": [
            null,
            [
                "system",
                "HTER",
                "mTER",
                "BLEU"
            ],
            [
                "NMT",
                "BLEU",
                "HTER",
                "mTER"
            ],
            [
                "mTER",
                "NMT",
                "PBSY"
            ],
            [
                "mTER",
                "HTER"
            ],
            [
                "HTER",
                "mTER"
            ],
            null,
            null
        ],
        "n_sentence": 8.0,
        "table_id": "table_2",
        "paper_id": "D16-1025",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1025table_4",
        "description": "5.3 Word order errors. To analyse reordering errors, we start by focusing on shift operations identified by the HTER metrics. The first three columns of Table 4 show, respectively: (i) the number of words generated by each system (ii) the number of shifts required to align each system output to the corresponding post-edit; and (iii) the corresponding percentage of shift errors. Notice that the shift error percentages are incorporated in the HTER scores reported in Table 2. We can see in Table 4 that shift errors in NMT translations are definitely less than in the other systems. The error reduction of NMT with respect to the second best system (PBSY) is about 50% (173 vs. 354). It should be recalled that these numbers only refer to shifts detected by HTER, that is (groups of) words of the MT output and corresponding post-edit that are identical but occurring in different positions. Words that had to be moved and modified at the same time (for instance replaced by a synonym or a morphological variant) are not counted in HTER shift figures, but are detected as substitution, insertion or deletion operations. To ensure that our reordering evaluation is not biased towards the alignment between the MT output and the post-edit performed by HTER, we run an additional assessment using KRS \u2013 Kendall Reordering Score (Birch et al., 2010) \u2013 which measures the similarity between the source-reference reorderings and the source-MT output reorderings. Being based on bilingual word alignment via the source sentence, KRS detects reordering errors also when post-edit and MT words are not identical. Also unlike TER, KRS is sensitive to the distance between the position of a word in the MT output and that in the reference. Looking at the last column of Table 4, we can say that our observations on HTER are confirmed by the KRS results: the reorderings performed by NMT are much more accurate than those performed by any PBMT system. Moreover, according to the approximate randomization test, KRS differences are statistically significant between NMT and all other systems, but not among the three PBMT systems.",
        "sentences": [
            "5.3 Word order errors.",
            "To analyse reordering errors, we start by focusing on shift operations identified by the HTER metrics.",
            "The first three columns of Table 4 show, respectively: (i) the number of words generated by each system (ii) the number of shifts required to align each system output to the corresponding post-edit; and (iii) the corresponding percentage of shift errors. Notice that the shift error percentages are incorporated in the HTER scores reported in Table 2.",
            "We can see in Table 4 that shift errors in NMT translations are definitely less than in the other systems.",
            "The error reduction of NMT with respect to the second best system (PBSY) is about 50% (173 vs. 354).",
            "It should be recalled that these numbers only refer to shifts detected by HTER, that is (groups of) words of the MT output and corresponding post-edit that are identical but occurring in different positions.",
            "Words that had to be moved and modified at the same time (for instance replaced by a synonym or a morphological variant) are not counted in HTER shift figures, but are detected as substitution, insertion or deletion operations.",
            "To ensure that our reordering evaluation is not biased towards the alignment between the MT output and the post-edit performed by HTER, we run an additional assessment using KRS \u2013 Kendall Reordering Score (Birch et al., 2010) \u2013 which measures the similarity between the source-reference reorderings and the source-MT output reorderings.",
            "Being based on bilingual word alignment via the source sentence, KRS detects reordering errors also when post-edit and MT words are not identical.",
            "Also unlike TER, KRS is sensitive to the distance between the position of a word in the MT output and that in the reference.",
            "Looking at the last column of Table 4, we can say that our observations on HTER are confirmed by the KRS results: the reorderings performed by NMT are much more accurate than those performed by any PBMT system.",
            "Moreover, according to the approximate randomization test, KRS differences are statistically significant between NMT and all other systems, but not among the three PBMT systems."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            1,
            2,
            2,
            2,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "#words",
                "#shifts",
                "%shifts"
            ],
            [
                "NMT",
                "system",
                "#shifts",
                "%shifts"
            ],
            [
                "NMT",
                "PBSY"
            ],
            null,
            null,
            [
                "KRS"
            ],
            [
                "KRS"
            ],
            [
                "KRS"
            ],
            [
                "KRS",
                "NMT"
            ],
            [
                "KRS",
                "NMT",
                "system"
            ]
        ],
        "n_sentence": 12.0,
        "table_id": "table_4",
        "paper_id": "D16-1025",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1032table_2",
        "description": "Table 2 shows the averaged scores over the responses. The checklist models outperform all baselines in generating recipes that follow the provided agenda closely and accomplish the desired goal, where NN in particular often generates the wrong dish. Perhaps surprisingly, both the Attention and EncDec baselines and the Checklist model beat the true recipes in terms of having better grammar. This can partly be attributed to noise in the parsing of the true recipes, and partly because the neural models tend to generate shorter, simpler texts.",
        "sentences": [
            "Table 2 shows the averaged scores over the responses.",
            "The checklist models outperform all baselines in generating recipes that follow the provided agenda closely and accomplish the desired goal, where NN in particular often generates the wrong dish.",
            "Perhaps surprisingly, both the Attention and EncDec baselines and the Checklist model beat the true recipes in terms of having better grammar.",
            "This can partly be attributed to noise in the parsing of the true recipes, and partly because the neural models tend to generate shorter, simpler texts."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Checklist",
                "Checklist+",
                "NN",
                "Model"
            ],
            [
                "Attention",
                "EncDec",
                "Checklist"
            ],
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "D16-1032",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1035table_4",
        "description": "Table 4 shows the performance for our system and those systems. Our system achieves the best result in span and relatively lower performance in nucleus and relation identification comparing with the corresponding best results but still better than most systems. No system achieves the best result on all three metrics. To further show the effectiveness of the deep learning model itself without handcrafted features, we compare the performance between our model and the model proposed by Li et al. (2014a) without handcrafted features and the results are shown in Table 5. It shows our overall performance outperforms the model proposed by Li et al. (2014a) which illustrates our model is effective.",
        "sentences": [
            "Table 4 shows the performance for our system and those systems.",
            "Our system achieves the best result in span and relatively lower performance in nucleus and relation identification comparing with the corresponding best results but still better than most systems.",
            "No system achieves the best result on all three metrics.",
            "To further show the effectiveness of the deep learning model itself without handcrafted features, we compare the performance between our model and the model proposed by Li et al. (2014a) without handcrafted features and the results are shown in Table 5.",
            "It shows our overall performance outperforms the model proposed by Li et al. (2014a) which illustrates our model is effective."
        ],
        "class_sentence": [
            1,
            1,
            1,
            0,
            0
        ],
        "header_mention": [
            null,
            [
                "Ours",
                "System"
            ],
            [
                "System",
                "S",
                "N",
                "R"
            ],
            null,
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_4",
        "paper_id": "D16-1035",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1038table_7",
        "description": "4.7 Domain Transfer Evaluation. To demonstrate the superiority of the adaptation capabilities of the proposed MSEP system, we test its performance on new domains and compare with the supervised system. TAC-KBP corpus contains two genres: newswire (NW) and discussion forum (DF), and they have roughly equal number of documents. When trained on NW and tested on DF, supervised methods encounter out-of-domain situations. However, the MSEP system can adapt well. Table 7 shows that MSEP outperforms supervised methods in out-of-domain situations for both tasks. The differences are statistically significant with p < 0.05.",
        "sentences": [
            "4.7 Domain Transfer Evaluation.",
            "To demonstrate the superiority of the adaptation capabilities of the proposed MSEP system, we test its performance on new domains and compare with the supervised system.",
            "TAC-KBP corpus contains two genres: newswire (NW) and discussion forum (DF), and they have roughly equal number of documents.",
            "When trained on NW and tested on DF, supervised methods encounter out-of-domain situations.",
            "However, the MSEP system can adapt well.",
            "Table 7 shows that MSEP outperforms supervised methods in out-of-domain situations for both tasks.",
            "The differences are statistically significant with p < 0.05."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "MSEP",
                "Supervised"
            ],
            null,
            [
                "Train NW Test DF"
            ],
            [
                "MSEP"
            ],
            [
                "MSEP",
                "Supervised",
                "Out of Domain",
                "Event Detection",
                "Event Co-reference"
            ],
            null
        ],
        "n_sentence": 7.0,
        "table_id": "table_7",
        "paper_id": "D16-1038",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1039table_2",
        "description": "Table 2 shows the performance of the three supervised models in Experiment 1. Our approach achieves significantly better performance than Yu\u2019s method and Word2Vec method in terms of accuracy (t-test, p-value < 0.05) for both BLESS and ENTAILMENT datasets. Specifically, our approach improves the average accuracy by 4% compared to Yu\u2019s method, and by 9% compared to the Word2Vec method. The Word2Vec embeddings have the worst result because it is based only on co-occurrence based similarity, which is not effective for the classifier to accurately recognize all the taxonomic relations. Our approach performs better than Yu\u2019s method and it shows that our approach can learn embeddings more effectively. Our approach encodes not only hypernym and hyponym terms but also the contextual information between them, while Yu\u2019s method ignores the contextual information for taxonomic relation identification. Moreover, from the experimental results of SVM+Our and SVM+Ourshort, we can observe that the offset vector between hypernym and hyponym, which captures the contextual information, plays an important role in our approach as it helps to improve the performance in both datasets. However, the offset feature is not so important for the Word2Vec model. The reason is that the Word2Vec model is targeted for the analogy task rather than taxonomic relation identification.",
        "sentences": [
            "Table 2 shows the performance of the three supervised models in Experiment 1.",
            "Our approach achieves significantly better performance than Yu\u2019s method and Word2Vec method in terms of accuracy (t-test, p-value < 0.05) for both BLESS and ENTAILMENT datasets.",
            "Specifically, our approach improves the average accuracy by 4% compared to Yu\u2019s method, and by 9% compared to the Word2Vec method.",
            "The Word2Vec embeddings have the worst result because it is based only on co-occurrence based similarity, which is not effective for the classifier to accurately recognize all the taxonomic relations.",
            "Our approach performs better than Yu\u2019s method and it shows that our approach can learn embeddings more effectively.",
            "Our approach encodes not only hypernym and hyponym terms but also the contextual information between them, while Yu\u2019s method ignores the contextual information for taxonomic relation identification.",
            "Moreover, from the experimental results of SVM+Our and SVM+Ourshort, we can observe that the offset vector between hypernym and hyponym, which captures the contextual information, plays an important role in our approach as it helps to improve the performance in both datasets.",
            "However, the offset feature is not so important for the Word2Vec model.",
            "The reason is that the Word2Vec model is targeted for the analogy task rather than taxonomic relation identification."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            2,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "SVM+Ourshort",
                "SVM+Our",
                "BLESS",
                "ENTAIL",
                "Accuracy"
            ],
            [
                "SVM+Ourshort",
                "SVM+Our",
                "SVM+Yu",
                "SVM+Word2Vecshort",
                "SVM+Word2Vec"
            ],
            [
                "SVM+Word2Vecshort",
                "SVM+Word2Vec"
            ],
            [
                "SVM+Ourshort",
                "SVM+Our",
                "SVM+Yu"
            ],
            [
                "SVM+Yu"
            ],
            [
                "SVM+Ourshort",
                "SVM+Our"
            ],
            [
                "SVM+Word2Vecshort",
                "SVM+Word2Vec"
            ],
            [
                "SVM+Word2Vecshort",
                "SVM+Word2Vec"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_2",
        "paper_id": "D16-1039",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1039table_3",
        "description": "Experiment 2. This experiment aims to evaluate the generalization capability of our extracted term embeddings. In the experiment, we train the classifier on the BLESS dataset, test it on the ENTAILMENT dataset and vice versa. Similarly, we exclude from the training set any pair of terms that has one term appearing in the testing set. The experimental results in Table 3 show that our term embedding learning approach performs better than other methods in accuracy. It also shows that the taxonomic properties identified by our term embedding learning approach have great generalization capability (i.e. less dependent on the training set), and can be used generically for representing taxonomic relations.",
        "sentences": [
            "Experiment 2.",
            "This experiment aims to evaluate the generalization capability of our extracted term embeddings.",
            "In the experiment, we train the classifier on the BLESS dataset, test it on the ENTAILMENT dataset and vice versa.",
            "Similarly, we exclude from the training set any pair of terms that has one term appearing in the testing set.",
            "The experimental results in Table 3 show that our term embedding learning approach performs better than other methods in accuracy.",
            "It also shows that the taxonomic properties identified by our term embedding learning approach have great generalization capability (i.e. less dependent on the training set), and can be used generically for representing taxonomic relations."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "BLESS",
                "ENTAIL"
            ],
            null,
            [
                "SVM+Our",
                "Model"
            ],
            [
                "SVM+Our"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_3",
        "paper_id": "D16-1039",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1043table_5",
        "description": "5.2 Common subset comparison. Table 5 shows the results on the common subset of the evaluation datasets, where all word pairs have images in each of the data sources. First, note the same patterns as before: multi-modal representations perform better than linguistic ones. Even for the poorly performing ESP Game dataset, the VGGNet representations perform better on both SimLex and MEN (bottom right of the table). Visual representations from Google, Bing, Flickr and ImageNet all perform much better than ESP Game on this common covered subset. In a sense, the fullcoverage datasets were \u201cpunished\u201d for their ability to return images for abstract words in the previous experiment: on this subset, which is more concrete, the search engines do much better. To a certain extent, including linguistic information is actually detrimental to performance, with multi-modal performing worse than purely visual. Again, we see the marked improvement with VGGNet for ImageNet, while Google, Bing and Flickr all do very well, regardless of the architecture.",
        "sentences": [
            "5.2 Common subset comparison.",
            "Table 5 shows the results on the common subset of the evaluation datasets, where all word pairs have images in each of the data sources.",
            "First, note the same patterns as before: multi-modal representations perform better than linguistic ones.",
            "Even for the poorly performing ESP Game dataset, the VGGNet representations perform better on both SimLex and MEN (bottom right of the table).",
            "Visual representations from Google, Bing, Flickr and ImageNet all perform much better than ESP Game on this common covered subset.",
            "In a sense, the fullcoverage datasets were \u201cpunished\u201d for their ability to return images for abstract words in the previous experiment: on this subset, which is more concrete, the search engines do much better.",
            "To a certain extent, including linguistic information is actually detrimental to performance, with multi-modal performing worse than purely visual.",
            "Again, we see the marked improvement with VGGNet for ImageNet, while Google, Bing and Flickr all do very well, regardless of the architecture."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "ESPGame",
                "VGGNet",
                "SL",
                "MEN"
            ],
            [
                "Google",
                "Bing",
                "Flickr",
                "ImageNet",
                "ESPGame"
            ],
            null,
            null,
            [
                "VGGNet",
                "ImageNet",
                "Google",
                "Bing",
                "Flickr"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_5",
        "paper_id": "D16-1043",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1044table_1",
        "description": "4.3 Ablation Results. We compare the performance of non-bilinear and bilinear pooling methods in Table 1. We see that MCB pooling outperforms all non-bilinear pooling methods, such as eltwise sum, concatenation, and eltwise product. One could argue that the compact bilinear method simply has more parameters than the non-bilinear pooling methods, which contributes to its performance. We compensated for this by stacking fully connected layers (with 4096 units per layer, ReLU activation, and dropout) after the non-bilinear pooling methods to increase their number of parameters. However, even with similar parameter budgets, nonbilinear methods could not achieve the same accuracy as the MCB method. For example, the \u201cConcatenation + FC + FC\u201d pooling method has approximately 40962 + 40962 + 4096 \u00d7 3000 \u2248 46 million parameters, which matches the 48 million parameters available in MCB with d = 16000. However, the performance of the \u201cConcatenation + FC + FC\u201d method is only 57.10% compared to MCB\u2019s 59.83%. Section 2 in Table 1 also shows that compact bilinear pooling has no impact on accuracy compared to full bilinear pooling. Section 3 in Table 1 demonstrates that the MCB brings improvements regardless of the image CNN used. We primarily use ResNet152 in this paper, but MCB also improves performance if VGG-19 is used. Section 4 in Table 1 shows that our soft attention model works best with MCB pooling. In fact, attending to the Concatenation + FC layer has the same performance as not using attention at all, while attending to the MCB layer improves performance by 2.67 points.",
        "sentences": [
            "4.3 Ablation Results.",
            "We compare the performance of non-bilinear and bilinear pooling methods in Table 1.",
            "We see that MCB pooling outperforms all non-bilinear pooling methods, such as eltwise sum, concatenation, and eltwise product.",
            "One could argue that the compact bilinear method simply has more parameters than the non-bilinear pooling methods, which contributes to its performance.",
            "We compensated for this by stacking fully connected layers (with 4096 units per layer, ReLU activation, and dropout) after the non-bilinear pooling methods to increase their number of parameters.",
            "However, even with similar parameter budgets, nonbilinear methods could not achieve the same accuracy as the MCB method.",
            "For example, the \u201cConcatenation + FC + FC\u201d pooling method has approximately 40962 + 40962 + 4096 \u00d7 3000 \u2248 46 million parameters, which matches the 48 million parameters available in MCB with d = 16000.",
            "However, the performance of the \u201cConcatenation + FC + FC\u201d method is only 57.10% compared to MCB\u2019s 59.83%.",
            "Section 2 in Table 1 also shows that compact bilinear pooling has no impact on accuracy compared to full bilinear pooling.",
            "Section 3 in Table 1 demonstrates that the MCB brings improvements regardless of the image CNN used.",
            "We primarily use ResNet152 in this paper, but MCB also improves performance if VGG-19 is used. Section 4 in Table 1 shows that our soft attention model works best with MCB pooling.",
            "In fact, attending to the Concatenation + FC layer has the same performance as not using attention at all, while attending to the MCB layer improves performance by 2.67 points."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2,
            1,
            2,
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "MCB (2048 \u00d7 2048 \u2192 16K)",
                "MCB (128 \u00d7 128 \u2192 4K)",
                "MCB (d = 16K) with VGG-19",
                "MCB (d = 16K) with Attention",
                "Method"
            ],
            null,
            null,
            [
                "MCB (2048 \u00d7 2048 \u2192 16K)"
            ],
            [
                "MCB (2048 \u00d7 2048 \u2192 16K)"
            ],
            [
                "MCB (2048 \u00d7 2048 \u2192 16K)"
            ],
            null,
            [
                "MCB (d = 16K) with VGG-19"
            ],
            [
                "MCB (d = 16K) with Attention"
            ],
            [
                "MCB (d = 16K) with Attention"
            ]
        ],
        "n_sentence": 12.0,
        "table_id": "table_1",
        "paper_id": "D16-1044",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1045table_1",
        "description": "Synthetic Data. Table 1 presents our results. In all three setups an SWVP algorithm is superior. Averaged accuracy differences between the best performing algorithms and CSP are: 3.72 (B-WMR, (simple(++), learnable(+++))), 5.29 (B-WM, (simple(++), learnable(++))) and 5.18 (A-WM, (simple(+), learnable(+))). In all setups SWVP outperforms CSP in terms of averaged performance (except from B-WMR for (simple(+), learnable(+))). Moreover, the weighted models are more stable than CSP, as indicated by the lower standard deviation of their accuracy scores. Finally, for the more simple and learnable datasets the SWVP models outperform CSP in the majority of cases (7-10/10).",
        "sentences": [
            "Synthetic Data.",
            "Table 1 presents our results.",
            "In all three setups an SWVP algorithm is superior.",
            "Averaged accuracy differences between the best performing algorithms and CSP are: 3.72 (B-WMR, (simple(++), learnable(+++))), 5.29 (B-WM, (simple(++), learnable(++))) and 5.18 (A-WM, (simple(+), learnable(+))).",
            "In all setups SWVP outperforms CSP in terms of averaged performance (except from B-WMR for (simple(+), learnable(+))).",
            "Moreover, the weighted models are more stable than CSP, as indicated by the lower standard deviation of their accuracy scores.",
            "Finally, for the more simple and learnable datasets the SWVP models outperform CSP in the majority of cases (7-10/10)."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "B-WM",
                "B-WMR",
                "A-WM",
                "A-WMR"
            ],
            [
                "B-WM",
                "B-WMR",
                "A-WM",
                "A-WMR"
            ],
            [
                "B-WM",
                "B-WMR",
                "A-WM",
                "A-WMR",
                "CSP"
            ],
            [
                "CSP"
            ],
            [
                "B-WM",
                "B-WMR",
                "A-WM",
                "A-WMR",
                "CSP"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_1",
        "paper_id": "D16-1045",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1048table_2",
        "description": "5.2.2 Selection of Searching Modes. With the hyper-parameters given by the developing procedures, cross-lingual similarization is conducted on the whole FBIS dataset. All the searching mode configurations are tried and 6 pairs of grammars are generated. For each of the 6 Chinese dependency grammars, we also give the three indicators as described before. Table 2 shows that, cross-lingual similarization results in grammars with much higher cross-lingual similarity, and the adaptive accuracies given by the adapted grammars approach to those of the original grammars. It indicates that the proposed algorithm improve the crosslingual similarity without losing syntactic knowledge. To determine the best searching mode for treebased machine translation, we use the ChineseEnglish FBIS dataset as the small-scale bilingual corpus. A 4-gram language model is trained on the Xinhua portion of the Gigaword corpus with the SRILM toolkit (Stolcke and Andreas, 2002). For the analysis given by non-projective similarized grammars, The projective transformation should be conducted in order to produce projective dependency structures for rule extraction and translation decoding. In details, the projective transformation first traverses the non-projective dependency structures just as they are projective, then adjusts the order of the nodes according to the traversed word sequences. We take NIST MT Evaluation testing set 2002 (NIST 02) for developing , and use the casesensitive BLEU (Papineni et al., 2002) to measure the translation accuracy. The last column of Table 2 shows the performance of the grammars on machine translation. The cross-lingually similarized grammars corresponding to the configurations with projective searching for Chinese always improve the translation performance, while non-projective grammars always hurt the performance. It probably can be attributed to the low performance of non-projective parsing as well as the inappropriateness of the simple projective transformation method. In the final application in machine translation, we adopted the similarized grammar corresponding to the configuration with projective searching on the source side and nonprojective searching on the target side.",
        "sentences": [
            "5.2.2 Selection of Searching Modes.",
            "With the hyper-parameters given by the developing procedures, cross-lingual similarization is conducted on the whole FBIS dataset.",
            "All the searching mode configurations are tried and 6 pairs of grammars are generated.",
            "For each of the 6 Chinese dependency grammars, we also give the three indicators as described before.",
            "Table 2 shows that, cross-lingual similarization results in grammars with much higher cross-lingual similarity, and the adaptive accuracies given by the adapted grammars approach to those of the original grammars.",
            "It indicates that the proposed algorithm improve the crosslingual similarity without losing syntactic knowledge.",
            "To determine the best searching mode for treebased machine translation, we use the ChineseEnglish FBIS dataset as the small-scale bilingual corpus.",
            "A 4-gram language model is trained on the Xinhua portion of the Gigaword corpus with the SRILM toolkit (Stolcke and Andreas, 2002).",
            "For the analysis given by non-projective similarized grammars, The projective transformation should be conducted in order to produce projective dependency structures for rule extraction and translation decoding.",
            "In details, the projective transformation first traverses the non-projective dependency structures just as they are projective, then adjusts the order of the nodes according to the traversed word sequences.",
            "We take NIST MT Evaluation testing set 2002 (NIST 02) for developing , and use the casesensitive BLEU (Papineni et al., 2002) to measure the translation accuracy.",
            "The last column of Table 2 shows the performance of the grammars on machine translation.",
            "The cross-lingually similarized grammars corresponding to the configurations with projective searching for Chinese always improve the translation performance, while non-projective grammars always hurt the performance.",
            "It probably can be attributed to the low performance of non-projective parsing as well as the inappropriateness of the simple projective transformation method.",
            "In the final application in machine translation, we adopted the similarized grammar corresponding to the configuration with projective searching on the source side and nonprojective searching on the target side."
        ],
        "class_sentence": [
            0,
            0,
            0,
            0,
            1,
            1,
            2,
            2,
            2,
            2,
            2,
            1,
            1,
            2,
            0
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            null
        ],
        "n_sentence": 15.0,
        "table_id": "table_2",
        "paper_id": "D16-1048",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1048table_3",
        "description": "Table 3 shows the performance of the crosslingually similarized grammar on dependency treebased translation, compared with previous work (Xie et al., 2011). We also give the performance of constituency tree-based translation (Liu et al., 2006) and formal syntax-based translation (Chiang, 2007). The original grammar performs slightly worse than the previous work in dependency tree-based translation, this can ascribed to the difference between the implementation of the original grammar and the dependency parser used in the previous work. However, the similarized grammar achieves very significant improvement based on the original grammar, and also significant surpass the previous work. Note that there is no other modification on the translation model besides the replacement of the source parser.",
        "sentences": [
            "Table 3 shows the performance of the crosslingually similarized grammar on dependency treebased translation, compared with previous work (Xie et al., 2011).",
            "We also give the performance of constituency tree-based translation (Liu et al., 2006) and formal syntax-based translation (Chiang, 2007).",
            "The original grammar performs slightly worse than the previous work in dependency tree-based translation, this can ascribed to the difference between the implementation of the original grammar and the dependency parser used in the previous work.",
            "However, the similarized grammar achieves very significant improvement based on the original grammar, and also significant surpass the previous work.",
            "Note that there is no other modification on the translation model besides the replacement of the source parser."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "Similarized Grammar",
                "Original Grammar",
                "(Xie et al. 2011)"
            ],
            [
                "(Liu et al. 2006)",
                "(Chiang 2007)"
            ],
            [
                "Original Grammar",
                "(Xie et al. 2011)"
            ],
            [
                "Similarized Grammar",
                "Original Grammar",
                "(Xie et al. 2011)"
            ],
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_3",
        "paper_id": "D16-1048",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1050table_1",
        "description": "Table 1 summarizes the BLEU scores of different systems on the Chinese-English translation tasks. Clearly VNMT significantly improves translation quality in terms of BLEU on most cases, and obtains the best average results that gain 0.86 and 1.35 BLEU points over Moses and GroundHog respectively. Besides, without the KL objective, VNMT w/o KL obtains even worse results than GroundHog. These results indicate the following two points: 1) explicitly modeling underlying semantics by a latent variable indeed benefits neural machine translation, and 2) the improvements of our model are not from enlarging the network.",
        "sentences": [
            "Table 1 summarizes the BLEU scores of different systems on the Chinese-English translation tasks.",
            "Clearly VNMT significantly improves translation quality in terms of BLEU on most cases, and obtains the best average results that gain 0.86 and 1.35 BLEU points over Moses and GroundHog respectively.",
            "Besides, without the KL objective, VNMT w/o KL obtains even worse results than GroundHog.",
            "These results indicate the following two points: 1) explicitly modeling underlying semantics by a latent variable indeed benefits neural machine translation, and 2) the improvements of our model are not from enlarging the network."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "VNMT",
                "Moses",
                "GroundHog"
            ],
            [
                "VNMT w/o KL",
                "GroundHog"
            ],
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_1",
        "paper_id": "D16-1050",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1051table_1",
        "description": "Table 1 shows the alignment summary statistics for the 447 sentences present in the Hansard test data. We present alignments quality scores using either the FastAlign IBM Model 2, the GIZA++ HMM, and our model and its relaxation using either the \u201cHMM\u201d or \u201cJoint\u201d decoding. First, we note that in deciding the decoding style for IBM2-HMM, the HMM method is better than the Joint method. We expected this type of performance since HMM decoding introduces positional dependance among the entire set of words in the sentence, which is shown to be a good modeling assumption (Vogel et al., 1996). From the results in Table 1 we see that the HMM outperforms all other models, including IBM2HMM and its convex relaxation. However, IBM2- HMM is not far in AER performance from the HMM and both it and its relaxation do better than FastAlign or IBM Model 3 (the results for IBM Model 3 are not presented; a one-directional English-French run of 1 52 53 15 gave AER and F-Measure numbers of 0.1768 and 0.6588, respectively, and this was behind both the IBM Model 2 FastAlign and our models).",
        "sentences": [
            "Table 1 shows the alignment summary statistics for the 447 sentences present in the Hansard test data.",
            "We present alignments quality scores using either the FastAlign IBM Model 2, the GIZA++ HMM, and our model and its relaxation using either the \u201cHMM\u201d or \u201cJoint\u201d decoding.",
            "First, we note that in deciding the decoding style for IBM2-HMM, the HMM method is better than the Joint method.",
            "We expected this type of performance since HMM decoding introduces positional dependance among the entire set of words in the sentence, which is shown to be a good modeling assumption (Vogel et al., 1996).",
            "From the results in Table 1 we see that the HMM outperforms all other models, including IBM2-HMM and its convex relaxation.",
            "However, IBM2-HMM is not far in AER performance from the HMM and both it and its relaxation do better than FastAlign."
        ],
        "class_sentence": [
            1,
            2,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Training",
                "HMM",
                "Joint",
                "Decoding"
            ],
            [
                "2H",
                "HMM",
                "Joint"
            ],
            [
                "HMM"
            ],
            [
                "HMM",
                "2H",
                "2HC"
            ],
            [
                "2H",
                "IBM2",
                "AER",
                "HMM",
                "F-Measure",
                "FA"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_1",
        "paper_id": "D16-1051",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1062table_3",
        "description": "Table 3 shows that the level of agreement as measured by the Fleiss\u2019\u03ba score is much lower when the number of annotators is increased, particularly for the 4GS set of sentence pairs, as compared to scores noted in Bowman et al. (2015). The decrease in agreement is particularly large with regard to contradiction. This could occur for a number of reasons. Recognizing entailment is an inherently difficult task, and classifying a correct label, particularly for contradiction and neutral, can be difficult due to an individual\u2019s interpretation of the sentences and assumptions that an individual makes about the key facts of each sentence (e.g. coreference). It may also be the case that the individuals tasked with creating the sentence pairs on AMT created sentences that appeared to contradict a premise text, but can be interpreted differently given a different context.",
        "sentences": [
            "Table 3 shows that the level of agreement as measured by the Fleiss\u2019\u03ba score is much lower when the number of annotators is increased, particularly for the 4GS set of sentence pairs, as compared to scores noted in Bowman et al. (2015).",
            "The decrease in agreement is particularly large with regard to contradiction.",
            "This could occur for a number of reasons.",
            "Recognizing entailment is an inherently difficult task, and classifying a correct label, particularly for contradiction and neutral, can be difficult due to an individual\u2019s interpretation of the sentences and assumptions that an individual makes about the key facts of each sentence (e.g. coreference).",
            "It may also be the case that the individuals tasked with creating the sentence pairs on AMT created sentences that appeared to contradict a premise text, but can be interpreted differently given a different context."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            [
                "Fleiss\u2019\u03ba",
                "4GS",
                "Bowman et al. 2015"
            ],
            [
                "Contradiction"
            ],
            null,
            null,
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_3",
        "paper_id": "D16-1062",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1062table_5",
        "description": "The theta scores from IRT in Table 5 show that, compared to AMT users, the system performed well above average for contradiction items compared to human performance, and performed around the average for entailment and neutral items. For both the neutral and contradiction items, the theta scores are similar across the 4GS and 5GS sets, whereas the accuracy of the more difficult 4GS items is consistently lower. This shows the advantage of IRT to account for item characteristics in its ability estimates. A similar theta score across sets indicates that we can measure the \u201cability level\u201d regardless of whether the test set is easy or hard. Theta score is a consistent measurement, compared to accuracy which varies with the difficulty of the dataset.",
        "sentences": [
            "The theta scores from IRT in Table 5 show that, compared to AMT users, the system performed well above average for contradiction items compared to human performance, and performed around the average for entailment and neutral items.",
            "For both the neutral and contradiction items, the theta scores are similar across the 4GS and 5GS sets, whereas the accuracy of the more difficult 4GS items is consistently lower.",
            "This shows the advantage of IRT to account for item characteristics in its ability estimates.",
            "A similar theta score across sets indicates that we can measure the \u201cability level\u201d regardless of whether the test set is easy or hard.",
            "Theta score is a consistent measurement, compared to accuracy which varies with the difficulty of the dataset."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            [
                "Theta Score",
                "Contradiction",
                "Entailment",
                "Neutral"
            ],
            [
                "Neutral",
                "Contradiction",
                "Theta Score",
                "4GS",
                "5GS",
                "Test Acc."
            ],
            null,
            [
                "Theta Score"
            ],
            [
                "Theta Score",
                "Test Acc."
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_5",
        "paper_id": "D16-1062",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1063table_2",
        "description": "It can be seen from Table 2 that adding the weight rw,c improves performance in all the cases, especially on the word analogy task. Among the four \u03c1 functions, \u03c10 performs the best on the word similarity task but suffers notably on the analogy task, while \u03c11 = log performs the best overall. Given these observations, which are consistent with the results on large scale datasets, in the experiments that follow we only report WordRank with the best configuration, i.e., using \u03c11 with the weight rw,c as defined in (4).",
        "sentences": [
            "It can be seen from Table 2 that adding the weight rw,c improves performance in all the cases, especially on the word analogy task.",
            "Among the four \u03c1 functions, \u03c10 performs the best on the word similarity task but suffers notably on the analogy task, while \u03c11 = log performs the best overall.",
            "Given these observations, which are consistent with the results on large scale datasets, in the experiments that follow we only report WordRank with the best configuration, i.e., using \u03c11 with the weight rw,c as defined in (4)."
        ],
        "class_sentence": [
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "Analogy",
                "Similarity"
            ],
            [
                "\u03c10",
                "\u03c11",
                "Similarity",
                "Analogy"
            ],
            [
                "\u03c11"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_2",
        "paper_id": "D16-1063",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1065table_3",
        "description": "4.4 Joint Model vs. Pipelined Model. In this section, we compare the overall performance of our joint model to the pipelined model, JAMR. To give a fair comparison, we first implemented system 1 only using the same features (i.e., features 1- 4 in Table 1) as JAMR for concept fragments. Table 3 gives the results on the two datasets. In terms of F-measure, we gain a 6% absolute improvement, and a 5% absolute improvement over the results of JAMR on the two different experimental setups respectively. Next, we implemented system 2 by using more lexical features to capture the association between concept and the context (i.e., features 5-16 in Table 1). Intuitively, these lexical contextual features should be helpful in identifying concepts in parsing process. As expected, the results in Table 3 show that we gain 3% improvement over the two different datasets respectively, by adding only some additional lexical features.",
        "sentences": [
            "4.4 Joint Model vs. Pipelined Model.",
            "In this section, we compare the overall performance of our joint model to the pipelined model, JAMR.",
            "To give a fair comparison, we first implemented system 1 only using the same features (i.e., features 1- 4 in Table 1) as JAMR for concept fragments.",
            "Table 3 gives the results on the two datasets.",
            "In terms of F-measure, we gain a 6% absolute improvement, and a 5% absolute improvement over the results of JAMR on the two different experimental setups respectively.",
            "Next, we implemented system 2 by using more lexical features to capture the association between concept and the context (i.e., features 5-16 in Table 1).",
            "Intuitively, these lexical contextual features should be helpful in identifying concepts in parsing process.",
            "As expected, the results in Table 3 show that we gain 3% improvement over the two different datasets respectively, by adding only some additional lexical features."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            [
                "F1",
                "System 1",
                "JAMR (fixed)"
            ],
            [
                "System 2"
            ],
            null,
            [
                "System 2"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_3",
        "paper_id": "D16-1065",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1065table_4",
        "description": "We give a comparison between our approach and other state-of-the-art AMR parsers, including CCGbased parser (Artzi et al., 2015) and dependencybased parser (Wang et al., 2015b). For comparison purposes, we give two results from two different versions of dependency-based AMR parser: CAMR* and CAMR. Compared to the latter, the former denotes the system that does not use the extended features generated from the semantic role labeling system, word sense disambiguation system and so on, which is directly comparable to our system. From Table 4 we can see that our parser achieves better performance than other approaches, even without utilizing any external semantic resources.",
        "sentences": [
            "We give a comparison between our approach and other state-of-the-art AMR parsers, including CCGbased parser (Artzi et al., 2015) and dependency-based parser (Wang et al., 2015b).",
            "For comparison purposes, we give two results from two different versions of dependency-based AMR parser: CAMR* and CAMR.",
            "Compared to the latter, the former denotes the system that does not use the extended features generated from the semantic role labeling system, word sense disambiguation system and so on, which is directly comparable to our system.",
            "From Table 4 we can see that our parser achieves better performance than other approaches, even without utilizing any external semantic resources."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "CAMR*",
                "CAMR"
            ],
            null,
            [
                "Our approach",
                "System"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_4",
        "paper_id": "D16-1065",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1065table_5",
        "description": "We also evaluate our parser on the full LDC2014T12 dataset. We use the training/development/test split recommended in the release: 10,312 sentences for training, 1368 sentences for development and 1371 sentences for testing. For comparison, we include the results of JAMR, CAMR*, CAMR and SMBT-based parser (Pust et al., 2015), which are also trained on the same dataset. The results in Table 5 show that our approach outperforms CAMR*, and obtains comparable performance with CAMR. However, our approach achieves slightly lower performance, compared to the SMBT-based parser, which adds data and features drawn from various external semantic resources.",
        "sentences": [
            "We also evaluate our parser on the full LDC2014T12 dataset.",
            "We use the training/development/test split recommended in the release: 10,312 sentences for training, 1368 sentences for development and 1371 sentences for testing.",
            "For comparison, we include the results of JAMR, CAMR*, CAMR and SMBT-based parser (Pust et al., 2015), which are also trained on the same dataset.",
            "The results in Table 5 show that our approach outperforms CAMR*, and obtains comparable performance with CAMR.",
            "However, our approach achieves slightly lower performance, compared to the SMBT-based parser, which adds data and features drawn from various external semantic resources."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "LDC2014T12"
            ],
            null,
            [
                "JAMR (fixed)",
                "CAMR*",
                "CAMR",
                "SMBT-based",
                "Our approach"
            ],
            [
                "Our approach",
                "CAMR*",
                "CAMR"
            ],
            [
                "Our approach",
                "SMBT-based"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_5",
        "paper_id": "D16-1065",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1068table_2",
        "description": "Table 2 complements our results, providing UAS values for each of the 8 languages participating in this setup. The UAS difference between BGI-PP+i+b and the TurboParser are (+0.24)-(- 0.71) in first order parsing and (+0.18)-(-2.46) in second order parsing. In the latter case, combining these two models (BGI+PP+i+b+e) yields improvements over the TurboParser in 6 out of 8 languages.",
        "sentences": [
            "Table 2 complements our results, providing UAS values for each of the 8 languages participating in this setup.",
            "The UAS difference between BGI-PP+i+b and the TurboParser are (+0.24)-(- 0.71) in first order parsing and (+0.18)-(-2.46) in second order parsing.",
            "In the latter case, combining these two models (BGI+PP+i+b+e) yields improvements over the TurboParser in 6 out of 8 languages."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "language",
                "First Order",
                "Second Order"
            ],
            [
                "BGI-PP+i+b",
                "TurboParser",
                "First Order",
                "Second Order"
            ],
            [
                "BGI-PP+i+b+e",
                "TurboParser",
                "language"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_2",
        "paper_id": "D16-1068",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1071table_3",
        "description": "Results. The MRR results in the left half of Table 3 (\u201cunfiltered\u201d) show that for all languages and for all POS, form real has the worst performance among the form models. This comes at no surprise since this model does barely know anything about word forms and lemmata. The form opt model improves these results based on the additional information it has access to (the mapping from lemma to its most frequent form). form sum performs similar to form opt. For Czech, Hungarian and Spanish it is slightly better (or equally good), whereas for English and German there is no clear trend. There is a large difference between these two models on German nouns, with form sum performing considerably worse. We attribute this to the fact that many German noun forms are rare compounds and therefore lead to badly trained form embeddings, which summed up do not lead to high quality embeddings either. Among the stemming models, stem real also is the worst performing model. We can further see that for all languages and almost all POS,stem sum performs worse than stem opt. That indicates that stemming leads to many low-frequency stems or many words sharing the same stem. This is especially apparent in Spanish verbs. There, the stemming models are clearly inferior to form models. Overall, LAMB performs best for all languages and POS types. Most improvements of LAMB are significant. The improvement over the best formmodel reaches up to 6 points (e.g., Czech nouns). In contrast to form sum, LAMB improves over form opt on German nouns. This indicates that the sparsity issue is successfully addressed by LAMB.",
        "sentences": [
            "Results.",
            "The MRR results in the left half of Table 3 (\u201cunfiltered\u201d) show that for all languages and for all POS, form real has the worst performance among the form models.",
            "This comes at no surprise since this model does barely know anything about word forms and lemmata.",
            "The form opt model improves these results based on the additional information it has access to (the mapping from lemma to its most frequent form).",
            "form sum performs similar to form opt.",
            "For Czech, Hungarian and Spanish it is slightly better (or equally good), whereas for English and German there is no clear trend.",
            "There is a large difference between these two models on German nouns, with form sum performing considerably worse.",
            "We attribute this to the fact that many German noun forms are rare compounds and therefore lead to badly trained form embeddings, which summed up do not lead to high quality embeddings either.",
            "Among the stemming models, stem real also is the worst performing model.",
            "We can further see that for all languages and almost all POS,stem sum performs worse than stem opt.",
            "That indicates that stemming leads to many low-frequency stems or many words sharing the same stem.",
            "This is especially apparent in Spanish verbs.",
            "There, the stemming models are clearly inferior to form models.",
            "Overall, LAMB performs best for all languages and POS types.",
            "Most improvements of LAMB are significant.",
            "The improvement over the best formmodel reaches up to 6 points (e.g., Czech nouns).",
            "In contrast to form sum, LAMB improves over form opt on German nouns.",
            "This indicates that the sparsity issue is successfully addressed by LAMB."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "lang",
                "POS",
                "unfiltered"
            ],
            null,
            [
                "form"
            ],
            [
                "form",
                "sum",
                "opt"
            ],
            [
                "lang"
            ],
            [
                "form",
                "sum",
                "de"
            ],
            [
                "de"
            ],
            [
                "STEM"
            ],
            [
                "lang",
                "POS",
                "STEM",
                "sum",
                "opt"
            ],
            [
                "STEM"
            ],
            [
                "es"
            ],
            [
                "STEM"
            ],
            [
                "LAMB",
                "lang",
                "POS"
            ],
            [
                "LAMB"
            ],
            [
                "cz"
            ],
            [
                "LAMB",
                "form",
                "sum",
                "opt",
                "de"
            ],
            [
                "LAMB"
            ]
        ],
        "n_sentence": 18.0,
        "table_id": "table_3",
        "paper_id": "D16-1071",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1071table_5",
        "description": "Results. Table 5 lists the 10-fold cross-validation results (accuracy and macro F1) on the CSFD dataset. LAMB/STEM results are consistently better than form results. In our analysis, we found the following example for the benefit of normalization: \u201cpopis a nazev za- \u00b4 jmavy a film je takov \u00b4 a filma \u00b4 \u02c7rska pras \u00b4 arna\u201d (engl. \u00b4 \u201cdescription and title are interesting, but it is bad film-making\u201d). This example is correctly classified as negative by the LAMB model because it has an embedding for \u201cprasarna\u201d (bad, smut) whereas the \u00b4 form model does not. The out-of-vocabulary counts for form and LAMB on the first fold of the CSFD experiment are 26.3k and 25.5k, respectively. The similarity of these two numbers suggests that the quality of word embeddings (form vs. LAMB) are responsible for the performance gain. On the SemEval data, LAMB improves the results over form and stem (cf. Table 5). Hence, LAMB can still pick up additional information despite the simple morphology of English. This is probably due to better embeddings for rare words. The SemEval 2015 winner (Hagen et al., 2015) is a highly domaindependent and specialized system that we do not outperform.",
        "sentences": [
            "Results.",
            "Table 5 lists the 10-fold cross-validation results (accuracy and macro F1) on the CSFD dataset.",
            "LAMB/STEM results are consistently better than form results.",
            "In our analysis, we found the following example for the benefit of normalization: \u201cpopis a nazev za- \u00b4 jmavy a film je takov \u00b4 a filma \u00b4 \u02c7rska pras \u00b4 arna\u201d (engl. \u00b4 \u201cdescription and title are interesting, but it is bad film-making\u201d).",
            "This example is correctly classified as negative by the LAMB model because it has an embedding for \u201cprasarna\u201d (bad, smut) whereas the \u00b4 form model does not.",
            "The out-of-vocabulary counts for form and LAMB on the first fold of the CSFD experiment are 26.3k and 25.5k, respectively.",
            "The similarity of these two numbers suggests that the quality of word embeddings (form vs. LAMB) are responsible for the performance gain.",
            "On the SemEval data, LAMB improves the results over form and stem (cf. Table 5).",
            "Hence, LAMB can still pick up additional information despite the simple morphology of English.",
            "This is probably due to better embeddings for rare words.",
            "The SemEval 2015 winner (Hagen et al., 2015) is a highly domain-dependent and specialized system that we do not outperform."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            2,
            2,
            2,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "acc",
                "F1"
            ],
            [
                "LAMB",
                "STEM"
            ],
            null,
            [
                "LAMB"
            ],
            [
                "form",
                "LAMB"
            ],
            [
                "form",
                "LAMB"
            ],
            [
                "LAMB",
                "form",
                "STEM"
            ],
            [
                "LAMB",
                "en"
            ],
            null,
            [
                "Hagen et al. (2015)"
            ]
        ],
        "n_sentence": 11.0,
        "table_id": "table_5",
        "paper_id": "D16-1071",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1072table_2",
        "description": "5 Experiments on POS Tagging. 5.1 Parameter Tuning. For both online and offline pruning, we need to decide the maximum number of single-side tag candidates r and the accumulative probability threshold \u03bb for further truncating the candidates. Table 2 shows the tagging accuracies and the averaged numbers of single-side tags for each token after pruning. The first major row tunes the two hyperparameters for online pruning. We first fix \u03bb = 0.98 and increase r from 2 to 8, leading to consistently improved accuracies on both CTB5-dev and PDdev. No further improvement is gained with r = 16, indicating that tags below the top-8 are mostly very unlikely ones and thus insignificant for computing feature expectations. Then we fix r = 8 and try different \u03bb. We find that \u03bb has little effect on tagging accuracies but influences the numbers of remaining single-side tags. We choose r = 8 and \u03bb = 0.98 for final evaluation. The second major row tunes r and \u03bb for offline pruning. Different from online pruning, \u03bb has much greater effect on the number of remaining single-side tags. Under \u03bb = 0.9999, increasing r from 8 to 16 leads to 0.20% accuracy improvement on CTB5-dev, but using r = 32 has no further gain. Then we fix r = 16 and vary \u03bb from 0.99 to 0.99999. We choose r = 16 and \u03bb = 0.9999 for offline pruning for final evaluation, which leaves each word with about 5.2 CTB-tags and 7.6 PD-tags on average.",
        "sentences": [
            "5 Experiments on POS Tagging.",
            "5.1 Parameter Tuning.",
            "For both online and offline pruning, we need to decide the maximum number of single-side tag candidates r and the accumulative probability threshold \u03bb for further truncating the candidates.",
            "Table 2 shows the tagging accuracies and the averaged numbers of single-side tags for each token after pruning.",
            "The first major row tunes the two hyperparameters for online pruning.",
            "We first fix \u03bb = 0.98 and increase r from 2 to 8, leading to consistently improved accuracies on both CTB5-dev and PDdev.",
            "No further improvement is gained with r = 16, indicating that tags below the top-8 are mostly very unlikely ones and thus insignificant for computing feature expectations.",
            "Then we fix r = 8 and try different \u03bb.",
            "We find that \u03bb has little effect on tagging accuracies but influences the numbers of remaining single-side tags.",
            "We choose r = 8 and \u03bb = 0.98 for final evaluation.",
            "The second major row tunes r and \u03bb for offline pruning.",
            "Different from online pruning, \u03bb has much greater effect on the number of remaining single-side tags.",
            "Under \u03bb = 0.9999, increasing r from 8 to 16 leads to 0.20% accuracy improvement on CTB5-dev, but using r = 32 has no further gain.",
            "Then we fix r = 16 and vary \u03bb from 0.99 to 0.99999.",
            "We choose r = 16 and \u03bb = 0.9999 for offline pruning for final evaluation, which leaves each word with about 5.2 CTB-tags and 7.6 PD-tags on average."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Online Pruning",
                "Offline Pruning",
                "\u03bb",
                "r"
            ],
            null,
            [
                "Online Pruning"
            ],
            [
                "\u03bb",
                "r",
                "CTB5-dev",
                "PD-dev"
            ],
            [
                "r"
            ],
            [
                "\u03bb",
                "r"
            ],
            [
                "\u03bb"
            ],
            [
                "r",
                "\u03bb"
            ],
            [
                "r",
                "\u03bb"
            ],
            [
                "\u03bb"
            ],
            [
                "r",
                "\u03bb",
                "CTB5-dev"
            ],
            [
                "r",
                "\u03bb"
            ],
            [
                "r",
                "\u03bb",
                "CTB-side",
                "PD-side"
            ]
        ],
        "n_sentence": 15.0,
        "table_id": "table_2",
        "paper_id": "D16-1072",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1072table_3",
        "description": "5.2 Main Results. Table 3 summarizes the accuracies on the test data and the tagging speed during the test phase. \u201cCoupled (No Prune)\u201d refers to the coupled model with complete mapping in Li et al. (2015), which maps each one-side tag to all the-other-side tags. \u201cCoupled (Relaxed)\u201d refers the coupled model with relaxed mapping in Li et al. (2015), which maps a one-side tag to a manually-designed small set of the-otherside tags. Li et al. (2012b) report the state-of-theart accuracy on this CTB data, with a joint model of Chinese POS tagging and dependency parsing. It is clear that both online and offline pruning greatly improve the efficiency of the coupled model by about two magnitudes, without the need of a carefully predefined set of tag-to-tag mapping rules. Moreover, the coupled model with offline pruning achieves 0.76% accuracy improvement on CTB5- test over the baseline model, and 0.48% over our reimplemented guide-feature approach of Jiang et al. (2009). The gains on PD-test are marginal, possibly due to the large size of PD-train, similar to the results in Li et al. (2015).",
        "sentences": [
            "5.2 Main Results.",
            "Table 3 summarizes the accuracies on the test data and the tagging speed during the test phase.",
            "\u201cCoupled (No Prune)\u201d refers to the coupled model with complete mapping in Li et al. (2015), which maps each one-side tag to all the-other-side tags.",
            "\u201cCoupled (Relaxed)\u201d refers the coupled model with relaxed mapping in Li et al. (2015), which maps a one-side tag to a manually-designed small set of the-otherside tags.",
            "Li et al. (2012b) report the state-of-theart accuracy on this CTB data, with a joint model of Chinese POS tagging and dependency parsing.",
            "It is clear that both online and offline pruning greatly improve the efficiency of the coupled model by about two magnitudes, without the need of a carefully predefined set of tag-to-tag mapping rules.",
            "Moreover, the coupled model with offline pruning achieves 0.76% accuracy improvement on CTB5- test over the baseline model, and 0.48% over our reimplemented guide-feature approach of Jiang et al. (2009).",
            "The gains on PD-test are marginal, possibly due to the large size of PD-train, similar to the results in Li et al. (2015)."
        ],
        "class_sentence": [
            0,
            1,
            2,
            2,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Coupled (No Prune)"
            ],
            [
                "Coupled (Relaxed)"
            ],
            [
                "Li et al. (2012b)"
            ],
            [
                "Coupled (Offline)",
                "Coupled (Online)"
            ],
            [
                "Coupled (Offline)",
                "CTB5-test"
            ],
            [
                "PD-test"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_3",
        "paper_id": "D16-1072",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1072table_4",
        "description": "Table 4 shows results for tuning r and \u03bb. From the results, we can see that in the online pruning method, \u03bb seems useless and r becomes the only threshold for pruning unlikely single-side tags. The accuracies are much inferior to those from the offline pruning approach. We believe that the accuracies can be further improved with larger r, which would nevertheless lead to severe inefficiency issue. Based on the results, we choose r = 16 and \u03bb = 1.00 for final evaluation.",
        "sentences": [
            "Table 4 shows results for tuning r and \u03bb.",
            "From the results, we can see that in the online pruning method, \u03bb seems useless and r becomes the only threshold for pruning unlikely single-side tags.",
            "The accuracies are much inferior to those from the offline pruning approach.",
            "We believe that the accuracies can be further improved with larger r, which would nevertheless lead to severe inefficiency issue.",
            "Based on the results, we choose r = 16 and \u03bb = 1.00 for final evaluation."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "Online Pruning",
                "\u03bb"
            ],
            [
                "Online Pruning",
                "Offline Pruning"
            ],
            [
                "r"
            ],
            [
                "r",
                "\u03bb"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_4",
        "paper_id": "D16-1072",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1072table_5",
        "description": "6.2 Main Results. Table 5 summarizes the accuracies on the test data and the tagging speed (characters per second) during the test phase. \u201cCoupled (No Prune)\u201d is not tried due to the prohibitive tag set size in joint WS&POS tagging, and \u201cCoupled (Relaxed)\u201d is also skipped since it seems impossible to manually design reasonable tag-to-tag mapping rules in this case. In terms of efficiency, the coupled model with offline pruning is on par with the baseline single-side tagging model. In terms of F-score, the coupled model with offline pruning achieves 0.67% (WS) and 1.09% (WS&POS) gains on CTB5-test over the baseline model, and 0.48% (WS) and 0.79% (WS&POS) over our reimplemented guide-feature approach of Jiang et al. (2009). Similar to the case of POS tagging, the baseline model is very competitive on PD-test due to the large scale of PD-train.",
        "sentences": [
            "6.2 Main Results.",
            "Table 5 summarizes the accuracies on the test data and the tagging speed (characters per second) during the test phase.",
            "\u201cCoupled (No Prune)\u201d is not tried due to the prohibitive tag set size in joint WS&POS tagging, and \u201cCoupled (Relaxed)\u201d is also skipped since it seems impossible to manually design reasonable tag-to-tag mapping rules in this case.",
            "In terms of efficiency, the coupled model with offline pruning is on par with the baseline single-side tagging model.",
            "In terms of F-score, the coupled model with offline pruning achieves 0.67% (WS) and 1.09% (WS&POS) gains on CTB5-test over the baseline model, and 0.48% (WS) and 0.79% (WS&POS) over our reimplemented guide-feature approach of Jiang et al. (2009).",
            "Similar to the case of POS tagging, the baseline model is very competitive on PD-test due to the large scale of PD-train."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "Speed (Char/Sec)"
            ],
            [
                "F (%) on CTB5-test",
                "Coupled (Offline)"
            ],
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_5",
        "paper_id": "D16-1072",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1072table_6",
        "description": "6.4 Comparison with Previous Work. In order to compare with previous work, we also run our models on CTB5X and PD, where CTB5X adopts a different data split of CTB5 and is widely used in previous research on joint WS&POS tagging (Jiang et al., 2009; Sun and Wan, 2012). CTB5X-dev/test only contain 352/348 sentences respectively. Table 6 presents the F scores on CTB5X-test. We can see that the coupled model with offline pruning achieves 0.64% (WS) and 1.16% (WS&POS) F-score improvements over the baseline model, and 0.05% (WS) and 0.33% (WS&POS) over the guide-feature approach. The original guide-feature method in Jiang et al. (2009) achieves 98.23% and 94.03% F-score, which is very close to the results of our reimplemented model. The sub-word stacking approach of Sun and Wan (2012) can be understood as a more complex variant of the basic guide-feature method.",
        "sentences": [
            "6.4 Comparison with Previous Work.",
            "In order to compare with previous work, we also run our models on CTB5X and PD, where CTB5X adopts a different data split of CTB5 and is widely used in previous research on joint WS&POS tagging (Jiang et al., 2009; Sun and Wan, 2012).",
            "CTB5X-dev/test only contain 352/348 sentences respectively.",
            "Table 6 presents the F scores on CTB5X-test.",
            "We can see that the coupled model with offline pruning achieves 0.64% (WS) and 1.16% (WS&POS) F-score improvements over the baseline model, and 0.05% (WS) and 0.33% (WS&POS) over the guide-feature approach.",
            "The original guide-feature method in Jiang et al. (2009) achieves 98.23% and 94.03% F-score, which is very close to the results of our reimplemented model.",
            "The sub-word stacking approach of Sun and Wan (2012) can be understood as a more complex variant of the basic guide-feature method."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "F (%) on CTB5X-test"
            ],
            [
                "Coupled (Offline)",
                "Guide-feature",
                "Baseline",
                "F (%) on CTB5X-test"
            ],
            [
                "Jiang et al. (2009)",
                "F (%) on CTB5X-test",
                "Coupled (Offline)"
            ],
            [
                "Sun and Wan (2012)"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_6",
        "paper_id": "D16-1072",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1075table_3",
        "description": "The results are shown in Table 3. It can be clearly observed that BINet-based approaches outperform baselines and perform comparably to the state-ofthe-art model on generating the summaries on most topics: AreaRank achieves the significant improvement over the state-of-the-art model on sports and disasters, and performs comparably on politics and military and NodeRank\u2019s performance achieves the comparable performance to previous state-of-the-art model though it is inferior to AreaRank on most topics. Among these five topics, almost all models perform well on disaster and military topics because disaster and military reference summaries have more entries than the topics such as politics and sports and topics of event entries in the summaries are focused. The high-quality training data benefits models\u2019 performance especially for AreaRank which is purely data-driven. In contrast, on sports and politics, the number of entries in the reference summaries is small, which results in weaker supervision and affect the performance of models. It is notable that AreaRank does not perform well on generating the comprehensive summary in which topics of event entries are miscellaneous. The reason for the undesirable performance is that the topics of event entries in the comprehensive reference summary are not focused, which results in very few reference (positive) examples for each topic. As a result, the miscellaneousness of topics of positive examples makes them tend to be overwhelmed by large numbers of negative examples during training the model, leading to very week supervision and making it difficult for AreaRank to learn the patterns of positive examples. Compared to AreaRank, the strategy of selecting documents for generating event entries in other baselines and NodeRank use more or less heuristic knowledge, which makes these models perform stably even if the training examples are not sufficient.",
        "sentences": [
            "The results are shown in Table 3.",
            "It can be clearly observed that BINet-based approaches outperform baselines and perform comparably to the state-ofthe-art model on generating the summaries on most topics: AreaRank achieves the significant improvement over the state-of-the-art model on sports and disasters, and performs comparably on politics and military and NodeRank\u2019s performance achieves the comparable performance to previous state-of-the-art model though it is inferior to AreaRank on most topics.",
            "Among these five topics, almost all models perform well on disaster and military topics because disaster and military reference summaries have more entries than the topics such as politics and sports and topics of event entries in the summaries are focused.",
            "The high-quality training data benefits models\u2019 performance especially for AreaRank which is purely data-driven.",
            "In contrast, on sports and politics, the number of entries in the reference summaries is small, which results in weaker supervision and affect the performance of models.",
            "It is notable that AreaRank does not perform well on generating the comprehensive summary in which topics of event entries are miscellaneous.",
            "The reason for the undesirable performance is that the topics of event entries in the comprehensive reference summary are not focused, which results in very few reference (positive) examples for each topic.",
            "As a result, the miscellaneousness of topics of positive examples makes them tend to be overwhelmed by large numbers of negative examples during training the model, leading to very week supervision and making it difficult for AreaRank to learn the patterns of positive examples.",
            "Compared to AreaRank, the strategy of selecting documents for generating event entries in other baselines and NodeRank use more or less heuristic knowledge, which makes these models perform stably even if the training examples are not sufficient."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "BINet-NodeRank",
                "BINet-AreaRank"
            ],
            [
                "sports",
                "politics",
                "disaster",
                "military",
                "comprehensive"
            ],
            [
                "BINet-AreaRank"
            ],
            [
                "sports",
                "politics"
            ],
            [
                "BINet-AreaRank"
            ],
            null,
            [
                "BINet-AreaRank"
            ],
            [
                "BINet-NodeRank",
                "BINet-AreaRank"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_3",
        "paper_id": "D16-1075",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1078table_2",
        "description": "4.3 Experimental Results on Abstracts. Table 2 summarizes the performances of scope detection on Abstracts. In Table 2, CNN_C and CNN_D refer the CNN-based model with constituency paths and dependency paths, respectively (the same below). It shows that our CNN-based models (both CNN_C and CNN_D) can achieve better performances than the baseline in most measurements. This indicates that our CNN-based models can better extract and model effective features. Besides, compared to the baseline, our CNN-based models consider fewer features and need less human intervention. It also manifests that our CNN-based models improve significantly more on negation scope detection than on speculation scope detection. Much of this is due to the better ability of our CNN-based models in identifying the right boundaries of scopes than the left ones on negation scope detection, with the huge gains of 29.44% and 25.25% on PCRB using CNN_C and CNN_D, respectively. Table 2 illustrates that the performance of speculation scope detection is higher than that of negation (Best PCS: 85.75% vs 77.14%). It is mainly attributed to the shorter scopes of negation cues. Under the circumstances that the average length of negation sentences is almost as long as that of speculation ones (29.28 vs 29.77), shorter negation scopes mean that more tokens do not belong to the scopes, indicating more negative instances. The imbalance between positive and negative instances has negative effects on both the baseline and the CNN-based models for negation scope detection. Table 2 also shows that our CNN_D outperforms CNN_C in negation scope detection (PCS: 77.14% vs 70.86%), while our CNN_C performs better than CNN_D in speculation scope detection (PCS: 85.75% vs 74.43%). To explore the results of our CNN-based models in details, we present the analysis of top 10 speculative and negative cues below on CNN_C and CNN_D, respectively.",
        "sentences": [
            "4.3 Experimental Results on Abstracts.",
            "Table 2 summarizes the performances of scope detection on Abstracts.",
            "In Table 2, CNN_C and CNN_D refer the CNN-based model with constituency paths and dependency paths, respectively (the same below).",
            "It shows that our CNN-based models (both CNN_C and CNN_D) can achieve better performances than the baseline in most measurements.",
            "This indicates that our CNN-based models can better extract and model effective features.",
            "Besides, compared to the baseline, our CNN-based models consider fewer features and need less human intervention.",
            "It also manifests that our CNN-based models improve significantly more on negation scope detection than on speculation scope detection.",
            "Much of this is due to the better ability of our CNN-based models in identifying the right boundaries of scopes than the left ones on negation scope detection, with the huge gains of 29.44% and 25.25% on PCRB using CNN_C and CNN_D, respectively.",
            "Table 2 illustrates that the performance of speculation scope detection is higher than that of negation (Best PCS: 85.75% vs 77.14%).",
            "It is mainly attributed to the shorter scopes of negation cues.",
            "Under the circumstances that the average length of negation sentences is almost as long as that of speculation ones (29.28 vs 29.77), shorter negation scopes mean that more tokens do not belong to the scopes, indicating more negative instances.",
            "The imbalance between positive and negative instances has negative effects on both the baseline and the CNN-based models for negation scope detection.",
            "Table 2 also shows that our CNN_D outperforms CNN_C in negation scope detection (PCS: 77.14% vs 70.86%), while our CNN_C performs better than CNN_D in speculation scope detection (PCS: 85.75% vs 74.43%).",
            "To explore the results of our CNN-based models in details, we present the analysis of top 10 speculative and negative cues below on CNN_C and CNN_D, respectively."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            2,
            2,
            1,
            1,
            0
        ],
        "header_mention": [
            null,
            null,
            [
                "CNN_C",
                "CNN_D"
            ],
            [
                "CNN_C",
                "CNN_D",
                "Baseline"
            ],
            [
                "CNN_C",
                "CNN_D"
            ],
            [
                "CNN_C",
                "CNN_D",
                "Baseline"
            ],
            [
                "CNN_C",
                "CNN_D"
            ],
            [
                "CNN_C",
                "CNN_D",
                "PCRB (%)"
            ],
            [
                "PCS (%)"
            ],
            null,
            null,
            [
                "Negation",
                "CNN_C",
                "CNN_D"
            ],
            [
                "Negation",
                "CNN_C",
                "CNN_D"
            ],
            null
        ],
        "n_sentence": 14.0,
        "table_id": "table_2",
        "paper_id": "D16-1078",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1078table_4",
        "description": "Table 4 compares our CNN-based models with the  state-of-the-art  systems. It  shows  that  our  CNNbased  models  can  achieve  higher  PCSs  (+1.54%)  than those of the state-of-the-art systems for speculation  scope  detection  and  the  second  highest  PCS  for  negation  scope  detection  on  Abstracts,  and can get comparable PCSs on Clinical Records  (73.92%  vs  78.69%  for  speculation  scopes,  89.66%  vs  90.74%  for  negation  scopes). It  is  worth  noting  that  Abstracts  and  Clinical  Records  come from different genres. It also displays that our CNN-based models perform worse than the state-of-the-art on Full Papers due to the complex syntactic structures of the sentences and the cross-domain nature of our evaluation. Although our evaluation on Clinical Records is cross-domain, the sentences in Clinical Records are much simpler and the results on Clinical Records are satisfactory. Remind that our CNN-based models are all trained on Abstracts. Another reason is that those state-of-the-art systems on Full Papers (e.g., Li et al., 2010; Velldal et al., 2012) are tree-based, instead of token-based. Li et al. (2010) proposed a semantic parsing framework and focused on determining whether a constituent, rather than a word, is in the scope of a negative cue. Velldal et al. (2012) presented a hybrid framework, combining a rule-based approach using dependency structures and a data-driven approach for selecting appropriate subtrees in constituent structures. Normally, tree-based models can better capture long-distance syntactic dependency than token-based ones. Compared to those tree-based models, however, our CNN-based model needs less manual intervention. To improve the performances of scope detection task, we will explore this alternative in our future work.",
        "sentences": [
            "Table 4 compares our CNN-based models with the  state-of-the-art  systems.",
            "It  shows  that  our  CNNbased  models  can  achieve  higher  PCSs  (+1.54%)  than those of the state-of-the-art systems for speculation  scope  detection  and  the  second  highest  PCS  for  negation  scope  detection  on  Abstracts,  and can get comparable PCSs on Clinical Records  (73.92%  vs  78.69%  for  speculation  scopes,  89.66%  vs  90.74%  for  negation  scopes).",
            "It  is  worth  noting  that  Abstracts  and  Clinical  Records  come from different genres.",
            "It also displays that our CNN-based models perform worse than the state-of-the-art on Full Papers due to the complex syntactic structures of the sentences and the cross-domain nature of our evaluation.",
            "Although our evaluation on Clinical Records is cross-domain, the sentences in Clinical Records are much simpler and the results on Clinical Records are satisfactory.",
            "Remind that our CNN-based models are all trained on Abstracts.",
            "Another reason is that those state-of-the-art systems on Full Papers (e.g., Li et al., 2010; Velldal et al., 2012) are tree-based, instead of token-based.",
            "Li et al. (2010) proposed a semantic parsing framework and focused on determining whether a constituent, rather than a word, is in the scope of a negative cue.",
            "Velldal et al. (2012) presented a hybrid framework, combining a rule-based approach using dependency structures and a data-driven approach for selecting appropriate subtrees in constituent structures.",
            "Normally, tree-based models can better capture long-distance syntactic dependency than token-based ones.",
            "Compared to those tree-based models, however, our CNN-based model needs less manual intervention.",
            "To improve the performances of scope detection task, we will explore this alternative in our future work."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            0
        ],
        "header_mention": [
            [
                "System"
            ],
            [
                "Ours",
                "Abstracts",
                "Cli"
            ],
            [
                "Abstracts",
                "Cli"
            ],
            [
                "Ours",
                "System"
            ],
            [
                "Cli"
            ],
            [
                "Ours",
                "Abstracts"
            ],
            null,
            [
                "Li (2010)"
            ],
            [
                "Velldal (2012)"
            ],
            null,
            [
                "Ours"
            ],
            null
        ],
        "n_sentence": 12.0,
        "table_id": "table_4",
        "paper_id": "D16-1078",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1080table_4",
        "description": "Table 4 lists the effects of word embedding. We can see that the performance when updating the word embedding is better than when not updating, and the performance of word embedding is a little better than random word embedding. The main reason is that the vocabulary size is 147,377, but the number of words from tweets that exist in the word embedding trained on the Google News dataset is just 35,133. This means that 76.2% of the words are missing. This also confirms that the proposed jointlayer RNN is more suitable for keyphrase extraction on Twitter.",
        "sentences": [
            "Table 4 lists the effects of word embedding.",
            "We can see that the performance when updating the word embedding is better than when not updating, and the performance of word embedding is a little better than random word embedding.",
            "The main reason is that the vocabulary size is 147,377, but the number of words from tweets that exist in the word embedding trained on the Google News dataset is just 35,133.",
            "This means that 76.2% of the words are missing.",
            "This also confirms that the proposed jointlayer RNN is more suitable for keyphrase extraction on Twitter."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "WEU",
                "WENU",
                "REU",
                "RENU"
            ],
            null,
            null,
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_4",
        "paper_id": "D16-1080",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1083table_3",
        "description": "The compared results are shown in Table 3. We utilize our learnt embeddings of reviewers (Ours RE), both of reviewers\u2019 embeddings and products\u2019 embeddings (Ours RE+PE), respectively. Moreover, to perform fair comparison, like Mukherjee et al. (2013b), we add representations of the review text in classifier (Ours RE+PE+Bigram). From the results, we can observe that our method could outperform all state-of-the-arts in both the hotel and restaurant domains. It proves that our method is effective. Furthermore, the improvements in both the hotel and restaurant domains prove that our model possesses preferable domain-adaptability. It could represent the reviews more accurately and globally by learning from the original data, rather than the experts\u2019 knowledge or assumption.",
        "sentences": [
            "The compared results are shown in Table 3.",
            "We utilize our learnt embeddings of reviewers (Ours RE), both of reviewers\u2019 embeddings and products\u2019 embeddings (Ours RE+PE), respectively.",
            "Moreover, to perform fair comparison, like Mukherjee et al. (2013b), we add representations of the review text in classifier (Ours RE+PE+Bigram).",
            "From the results, we can observe that our method could outperform all state-of-the-arts in both the hotel and restaurant domains.",
            "It proves that our method is effective.",
            "Furthermore, the improvements in both the hotel and restaurant domains prove that our model possesses preferable domain-adaptability.",
            "It could represent the reviews more accurately and globally by learning from the original data, rather than the experts\u2019 knowledge or assumption."
        ],
        "class_sentence": [
            1,
            2,
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Ours_RE",
                "Ours_RE+PE"
            ],
            [
                "Ours_RE+PE+Bigram"
            ],
            [
                "Hotel",
                "Restaurant",
                "Ours_RE",
                "Ours_RE+PE",
                "Ours_RE+PE+Bigram"
            ],
            [
                "Ours_RE+PE",
                "Ours_RE+PE+Bigram"
            ],
            [
                "Hotel",
                "Restaurant",
                "Ours_RE",
                "Ours_RE+PE",
                "Ours_RE+PE+Bigram"
            ],
            null
        ],
        "n_sentence": 7.0,
        "table_id": "table_3",
        "paper_id": "D16-1083",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1083table_4",
        "description": "3.5 The Effects of Different Relations. We also drop relations of our method with a graceful degradation. Table 4 shows the performances of our method utilizing BF+PE+Bigram for hotel and restaurant domains. We found that dropping Relations 1, 2 and 10 results in a relatively gentle reduction (about 2.2%) in F1-score. According to our survey, the sparseness of the slices generated by Relation 1, 2 and 10 is about 99.9%. For this reason, the result is a relatively gentle reduction. Dropping other relations also result in a 2.5-4.0% performance reduction. It proves that each relation has an influence on the learning to represent reviews.",
        "sentences": [
            "3.5 The Effects of Different Relations.",
            "We also drop relations of our method with a graceful degradation.",
            "Table 4 shows the performances of our method utilizing BF+PE+Bigram for hotel and restaurant domains.",
            "We found that dropping Relations 1, 2 and 10 results in a relatively gentle reduction (about 2.2%) in F1-score.",
            "According to our survey, the sparseness of the slices generated by Relation 1, 2 and 10 is about 99.9%.",
            "For this reason, the result is a relatively gentle reduction.",
            "Dropping other relations also result in a 2.5-4.0% performance reduction.",
            "It proves that each relation has an influence on the learning to represent reviews."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            2,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "Hotel",
                "Restaurant"
            ],
            [
                "F1",
                "1",
                "2",
                "10"
            ],
            null,
            null,
            null,
            null
        ],
        "n_sentence": 8.0,
        "table_id": "table_4",
        "paper_id": "D16-1083",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1084table_4",
        "description": "Pre-Training. Table 4 shows the effect of unsupervised pre-training of word embeddings with a word2vec skip-gram model, and furthermore, the results of sharing of these representations between the tweets and targets, on the development set. The first set of results is with a uniformly Random embedding initialisation in [\u00e2\u02c6\u20190.1, 0.1]. PreFixed uses the pre-trained skip-gram word embeddings, whereas PreCont initialises the word embeddings with ones from SkipGram and continues training them during LSTM training. Our results show that, in the absence of a large labelled training dataset, pretraining of word embeddings is more helpful than random initialisation of embeddings. Sing vs Sep shows the difference between using shared vs two separate embeddings matrices for looking up the word embeddings. Sing means the word representations for tweet and target vocabularies are shared, whereas Sep means they are different. Using shared embeddings performs better, which we hypothesise is because the tweets contain some mentions of targets that are tested.",
        "sentences": [
            "Pre-Training.",
            "Table 4 shows the effect of unsupervised pre-training of word embeddings with a word2vec skip-gram model, and furthermore, the results of sharing of these representations between the tweets and targets, on the development set.",
            "The first set of results is with a uniformly Random embedding initialisation in [\u00e2\u02c6\u20190.1, 0.1].",
            "PreFixed uses the pre-trained skip-gram word embeddings, whereas PreCont initialises the word embeddings with ones from SkipGram and continues training them during LSTM training.",
            "Our results show that, in the absence of a large labelled training dataset, pretraining of word embeddings is more helpful than random initialisation of embeddings.",
            "Sing vs Sep shows the difference between using shared vs two separate embeddings matrices for looking up the word embeddings.",
            "Sing means the word representations for tweet and target vocabularies are shared, whereas Sep means they are different.",
            "Using shared embeddings performs better, which we hypothesise is because the tweets contain some mentions of targets that are tested."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "Random"
            ],
            [
                "PreFixed",
                "PreCont"
            ],
            [
                "Random",
                "PreCont",
                "PreFixed"
            ],
            [
                "Sing",
                "Sep"
            ],
            [
                "Sing",
                "Sep"
            ],
            null
        ],
        "n_sentence": 8.0,
        "table_id": "table_4",
        "paper_id": "D16-1084",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1084table_7",
        "description": "Table 7 shows all our results, including those using the unseen target setup, compared against the state-of-the-art on the stance detection corpus. Table 7 further lists baselines reported by Mohammad et al. (2016), namely a majority class baseline (Majority baseline), and a method using 1 to 3-gram bag-of-word and character n-gram features (SVM-ngrams-comb), which are extracted from the tweets and used to train a 3-way SVM classifier. Bag-of-word baselines (BoWV, SVM-ngramscomb) achieve results comparable to the majority baseline (F1 of 0.2972), which shows how difficult the task is. The baselines which only extract features from the tweets, SVM-ngrams-comb and TweetOnly perform worse than the baselines which also learn representations for the targets (BoWV, Concat). By training conditional encoding models on automatically labelled stance detection data we achieve state-of-the-art results. The best result (F1 of 0.5803) is achieved with the bi-directional conditional encoding model (BiCond). This shows that such models are suitable for unseen, as well as seen target stance detection.",
        "sentences": [
            "Table 7 shows all our results, including those using the unseen target setup, compared against the state-of-the-art on the stance detection corpus.",
            "Table 7 further lists baselines reported by Mohammad et al. (2016), namely a majority class baseline (Majority baseline), and a method using 1 to 3-gram bag-of-word and character n-gram features (SVM-ngrams-comb), which are extracted from the tweets and used to train a 3-way SVM classifier.",
            "Bag-of-word baselines (BoWV, SVM-ngrams-comb) achieve results comparable to the majority baseline (F1 of 0.2972), which shows how difficult the task is.",
            "The baselines which only extract features from the tweets, SVM-ngrams-comb and TweetOnly perform worse than the baselines which also learn representations for the targets (BoWV, Concat).",
            "By training conditional encoding models on automatically labelled stance detection data we achieve state-of-the-art results.",
            "The best result (F1 of 0.5803) is achieved with the bi-directional conditional encoding model (BiCond).",
            "This shows that such models are suitable for unseen, as well as seen target stance detection."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Majority baseline (Unseen Target)",
                "SVM-ngrams-comb (Unseen Target)"
            ],
            [
                "SVM-ngrams-comb (Unseen Target)",
                "Majority baseline (Unseen Target)"
            ],
            null,
            [
                "BiCond (Unseen Target)"
            ],
            [
                "BiCond (Unseen Target)"
            ],
            [
                "BiCond (Unseen Target)",
                "Stance"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_7",
        "paper_id": "D16-1084",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1088table_4",
        "description": "4 Evaluation. We show that our acquired subevent phrases are useful to discover articles that describe the main event and therefore improve event detection performance. For direct comparisons, we tested our subevents using the same test data and the same evaluation setting as the previous multi-faceted event recognition research by (Huang and Riloff, 2013). Specifically, they have annotated 300 new articles that each contains a civil unrest keyword and only 101 of them are actually civil unrest stories. They have shown that the multi-faceted event recognition approach can accurately identify civil unrest documents, by identifying a sentence in the documents where two types of facet phrases or one facet phrase and a main event expression were matched. The first row of Table 4 shows their multi-faceted event recognition performance. We compared our learned subevent phrases with the event phrases learned by (Huang and Riloff, 2013) and found that 559 out of our 610 unique phrases are not in their list. We augmented their provided event phrase list with our newly acquired subevent phrases and then used the exactly same evaluation procedure. Essentially, we used a longer event phrase dictionary which is a combination of main event expressions resulted from the previous research by (Huang and Riloff, 2013) and our learned subevent phrases. Row 2 shows the event recognition performance using the extended event phrase list. We can see that after incorporating subevent phrases, additional 10% of civil unrest stories were discovered, with a small precision loss, the F1-score on event detection was improved by 3%.",
        "sentences": [
            "4 Evaluation.",
            "We show that our acquired subevent phrases are useful to discover articles that describe the main event and therefore improve event detection performance.",
            "For direct comparisons, we tested our subevents using the same test data and the same evaluation setting as the previous multi-faceted event recognition research by (Huang and Riloff, 2013).",
            "Specifically, they have annotated 300 new articles that each contains a civil unrest keyword and only 101 of them are actually civil unrest stories.",
            "They have shown that the multi-faceted event recognition approach can accurately identify civil unrest documents, by identifying a sentence in the documents where two types of facet phrases or one facet phrase and a main event expression were matched.",
            "The first row of Table 4 shows their multi-faceted event recognition performance.",
            "We compared our learned subevent phrases with the event phrases learned by (Huang and Riloff, 2013) and found that 559 out of our 610 unique phrases are not in their list.",
            "We augmented their provided event phrase list with our newly acquired subevent phrases and then used the exactly same evaluation procedure.",
            "Essentially, we used a longer event phrase dictionary which is a combination of main event expressions resulted from the previous research by (Huang and Riloff, 2013) and our learned subevent phrases.",
            "Row 2 shows the event recognition performance using the extended event phrase list.",
            "We can see that after incorporating subevent phrases, additional 10% of civil unrest stories were discovered, with a small precision loss, the F1-score on event detection was improved by 3%."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "(Huang and Riloff 2013)"
            ],
            null,
            null,
            [
                "(Huang and Riloff 2013)"
            ],
            [
                "+Subevents",
                "(Huang and Riloff 2013)"
            ],
            null,
            [
                "(Huang and Riloff 2013)"
            ],
            [
                "+Subevents"
            ],
            [
                "F1-score",
                "+Subevents",
                "Precision"
            ]
        ],
        "n_sentence": 11.0,
        "table_id": "table_4",
        "paper_id": "D16-1088",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1089table_1",
        "description": "3.2 Results. Table 1 compares our results on the AWA benchmark against alternatives using the same visual features, and word vectors trained on the same corpus. We observe that: (i) Our Gaussian-embedding obtains the best performance overall. (ii) Our method outperforms CME which shares an objective function and optimisation strategy with ours, but operates on vectors rather than Gaussians. This suggests that our new distribution rather than vectorembedding does indeed bring significant benefit.",
        "sentences": [
            "3.2 Results.",
            "Table 1 compares our results on the AWA benchmark against alternatives using the same visual features, and word vectors trained on the same corpus.",
            "We observe that: (i) Our Gaussian-embedding obtains the best performance overall.",
            "(ii) Our method outperforms CME which shares an objective function and optimisation strategy with ours, but operates on vectors rather than Gaussians.",
            "This suggests that our new distribution rather than vectorembedding does indeed bring significant benefit."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Ours",
                "Gaussian",
                "Vector space models"
            ],
            [
                "Ours",
                "Gaussian"
            ],
            [
                "Ours",
                "Gaussian",
                "Vector space models"
            ],
            [
                "Ours",
                "Gaussian",
                "Vector space models"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_1",
        "paper_id": "D16-1089",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1096table_1",
        "description": "5.2 Translation Results. Table 1 shows the results of all systems on 5 million training set. The traditional syntax-based system achieves 9.45, 12.90, and 17.72 on MT06, MT08 News, and MT08 Web sets respectively, and 13.36 on average in terms of (TERBLEU)/2. The large vocabulary NMT (LVNMT), our baseline, achieves an average (TERBLEU)/2 score of 15.74, which is about 2 points worse than the hybrid system. We test four different settings for our coverage embedding models: \u2022 UGRU : updating with a GRU; \u2022 USub: updating as a subtraction; \u2022 UGRU + USub: combination of two methods (do not share coverage embedding vectors); \u2022 +Obj.: UGRU + USub plus an additional objective in Equation 6. UGRU improves the translation quality by 1.3 points on average over LVNMT. And UGRU + USub achieves the best average score of 13.14, which is about 2.6 points better than LVNMT. All the improvements of our coverage embedding models over LVNMT are statistically significant with the signtest of Collins et al. (2005). We believe that we need to explore more hyper-parameters of +Obj. in order to get even better results over UGRU + USub.",
        "sentences": [
            "5.2 Translation Results.",
            "Table 1 shows the results of all systems on 5 million training set.",
            "The traditional syntax-based system achieves 9.45, 12.90, and 17.72 on MT06, MT08 News, and MT08 Web sets respectively, and 13.36 on average in terms of (TERBLEU)/2.",
            "The large vocabulary NMT (LVNMT), our baseline, achieves an average (TERBLEU)/2 score of 15.74, which is about 2 points worse than the hybrid system.",
            "We test four different settings for our coverage embedding models: \u2022 UGRU : updating with a GRU; \u2022 USub: updating as a subtraction; \u2022 UGRU + USub: combination of two methods (do not share coverage embedding vectors); \u2022 +Obj.: UGRU + USub plus an additional objective in Equation 6.",
            "UGRU improves the translation quality by 1.3 points on average over LVNMT.",
            "And UGRU + USub achieves the best average score of 13.14, which is about 2.6 points better than LVNMT.",
            "All the improvements of our coverage embedding models over LVNMT are statistically significant with the signtest of Collins et al. (2005).",
            "We believe that we need to explore more hyper-parameters of +Obj. in order to get even better results over UGRU + USub."
        ],
        "class_sentence": [
            0,
            1,
            1,
            1,
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "MT06",
                "T-B",
                "MT08",
                "News",
                "Web",
                "avg."
            ],
            [
                "LVNMT",
                "avg.",
                "T-B"
            ],
            [
                "UGRU",
                "USub",
                "UGRU+USub",
                "+Obj."
            ],
            [
                "UGRU",
                "LVNMT",
                "avg."
            ],
            [
                "UGRU+USub",
                "avg.",
                "LVNMT"
            ],
            [
                "LVNMT"
            ],
            [
                "UGRU+USub"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_1",
        "paper_id": "D16-1096",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1096table_2",
        "description": "Table 2 shows the results of 11 million systems, LVNMT achieves an average (TER-BLEU)/2 of 13.27, which is about 2.5 points better than 5 million LVNMT. The result of our UGRU coverage model gives almost 1 point gain over LVNMT. Those results suggest that the more training data we use, the stronger the baseline system becomes, and the harder to get improvements. In order to get a reasonable or strong NMT system, we have to conduct experiments over a large-scale training set.",
        "sentences": [
            "Table 2 shows the results of 11 million systems, LVNMT achieves an average (TER-BLEU)/2 of 13.27, which is about 2.5 points better than 5 million LVNMT.",
            "The result of our UGRU coverage model gives almost 1 point gain over LVNMT.",
            "Those results suggest that the more training data we use, the stronger the baseline system becomes, and the harder to get improvements.",
            "In order to get a reasonable or strong NMT system, we have to conduct experiments over a large-scale training set."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            [
                "LVNMT",
                "avg."
            ],
            [
                "UGRU",
                "LVNMT"
            ],
            null,
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "D16-1096",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1099table_2",
        "description": "Table 2 (next side) shows the average results over the different frequency ranges for the various DSMs trained on the 1 billion-word ukWaC data. We also include the highest and lowest individual test scores (signified by \u2191 and \u2193), in order to get an idea about the consistency of the results. As can be seen in the table, the most consistent model is ISVD, which produces the best results in both the MEDIUM and MIXED frequency ranges. The neural network models SGNS and CBOW produce the best results in the HIGH and LOW range, respectively, with CBOW clearly outperforming SGNS in the latter case. The major difference between these models is that CBOW predicts a word based on a context, while SGNS predicts a context based on a word. Clearly, the former approach is more beneficial for low-frequent items. The PPMI, TSVD and RI models perform similarly across the frequency ranges, with RI producing somewhat lower results in the MEDIUM range, and TSVD producing somewhat lower results in the LOW range. The CO model underperforms in all frequency ranges. Worth noting is the fact that all models that are based on an explicit matrix (i.e. CO, PPMI, TSVD and ISVD) produce better results in the MEDIUM range than in the HIGH range. The arguably most interesting results are in the LOW range. Unsurprisingly, there is a general and significant drop in performance for low frequency items, but with interesting differences among the various models. As already mentioned, the CBOW model produces the best results, closely followed by PPMI and RI. It is noteworthy that the low-dimensional embeddings of the CBOW model only gives a modest improvement over the highdimensional explicit vectors of PPMI. The worst results are produced by the ISVD model, which scores even lower than the baseline CO model. This might be explained by the fact that ISVD removes the latent dimensions with largest variance, which are arguably the most important dimensions for very lowfrequent items. Increasing the number of latent dimensions with high variance in the ISVD model improves the results in the LOW range (16.59 when removing only the top 100 dimensions).",
        "sentences": [
            "Table 2 (next side) shows the average results over the different frequency ranges for the various DSMs trained on the 1 billion-word ukWaC data.",
            "We also include the highest and lowest individual test scores (signified by \u2191 and \u2193), in order to get an idea about the consistency of the results.",
            "As can be seen in the table, the most consistent model is ISVD, which produces the best results in both the MEDIUM and MIXED frequency ranges.",
            "The neural network models SGNS and CBOW produce the best results in the HIGH and LOW range, respectively, with CBOW clearly outperforming SGNS in the latter case.",
            "The major difference between these models is that CBOW predicts a word based on a context, while SGNS predicts a context based on a word.",
            "Clearly, the former approach is more beneficial for low-frequent items.",
            "The PPMI, TSVD and RI models perform similarly across the frequency ranges, with RI producing somewhat lower results in the MEDIUM range, and TSVD producing somewhat lower results in the LOW range.",
            "The CO model underperforms in all frequency ranges.",
            "Worth noting is the fact that all models that are based on an explicit matrix (i.e. CO, PPMI, TSVD and ISVD) produce better results in the MEDIUM range than in the HIGH range.",
            "The arguably most interesting results are in the LOW range.",
            "Unsurprisingly, there is a general and significant drop in performance for low frequency items, but with interesting differences among the various models.",
            "As already mentioned, the CBOW model produces the best results, closely followed by PPMI and RI.",
            "It is noteworthy that the low-dimensional embeddings of the CBOW model only gives a modest improvement over the highdimensional explicit vectors of PPMI.",
            "The worst results are produced by the ISVD model, which scores even lower than the baseline CO model.",
            "This might be explained by the fact that ISVD removes the latent dimensions with largest variance, which are arguably the most important dimensions for very lowfrequent items.",
            "Increasing the number of latent dimensions with high variance in the ISVD model improves the results in the LOW range (16.59 when removing only the top 100 dimensions)."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "ISVD",
                "MEDIUM",
                "MIXED"
            ],
            [
                "SGNS",
                "CBOW",
                "HIGH",
                "LOW"
            ],
            [
                "CBOW",
                "SGNS"
            ],
            null,
            [
                "PPMI",
                "TSVD",
                "RI",
                "MEDIUM",
                "LOW"
            ],
            [
                "CO",
                "HIGH",
                "MEDIUM",
                "LOW",
                "MIXED"
            ],
            [
                "CO",
                "PPMI",
                "TSVD",
                "ISVD",
                "MEDIUM",
                "HIGH"
            ],
            [
                "LOW"
            ],
            [
                "LOW"
            ],
            [
                "CBOW",
                "PPMI",
                "RI"
            ],
            [
                "CBOW",
                "PPMI"
            ],
            [
                "ISVD",
                "CO"
            ],
            [
                "ISVD"
            ],
            [
                "ISVD",
                "LOW"
            ]
        ],
        "n_sentence": 16.0,
        "table_id": "table_2",
        "paper_id": "D16-1099",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1102table_2",
        "description": "4.2 Results. The evaluation results are listed in Table 2. For comparison, we include evaluation results reported for three high-resource languages: German and Chinese, representing average high-resource results, as well as Hindi, a below-average outlier. We make the following observations: Lower annotation projection quality. We find that the F1-scores of Bengali, Malayalam and Tamil are 6, 11 and 31 pp below that of an average highresource language (as exemplified by German in Table 2). Bengali and Malayalam, however, do surpass Hindi, for which only a relatively poor dependency parser was used. This suggests that syntactic annotation projection may be a better method for identifying predicate-argument structures in languages that lack fully developed dependency parsers.",
        "sentences": [
            "4.2 Results.",
            "The evaluation results are listed in Table 2.",
            "For comparison, we include evaluation results reported for three high-resource languages: German and Chinese, representing average high-resource results, as well as Hindi, a below-average outlier.",
            "We make the following observations: Lower annotation projection quality.",
            "We find that the F1-scores of Bengali, Malayalam and Tamil are 6, 11 and 31 pp below that of an average highresource language (as exemplified by German in Table 2).",
            "Bengali and Malayalam, however, do surpass Hindi, for which only a relatively poor dependency parser was used.",
            "This suggests that syntactic annotation projection may be a better method for identifying predicate-argument structures in languages that lack fully developed dependency parsers."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "German (Akbik et al. 2015)",
                "Chinese (Akbik et al. 2015)",
                "Hindi (Akbik et al. 2015)"
            ],
            null,
            [
                "Bengali PROJECTED",
                "Malayalam PROJECTED",
                "Tamil PROJECTED",
                "German (Akbik et al. 2015)"
            ],
            [
                "Bengali PROJECTED",
                "Malayalam PROJECTED",
                "Hindi (Akbik et al. 2015)"
            ],
            null
        ],
        "n_sentence": 7.0,
        "table_id": "table_2",
        "paper_id": "D16-1102",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1104table_2",
        "description": "6 Results. Table 2 shows performance of sarcasm detection when our word embedding-based features are used on their own i.e, not as augmented features. The embedding in this case is Word2Vec. The four rows show baseline sets of features: unigrams, unweighted similarity using word embeddings (S), weighted similarity using word embeddings (WS) and both (i.e., unweighted plus weighted similarities using word embeddings). Using only unigrams as features gives a F-score of 72.53%, while only unweighted and weighted features gives F-score of 69.49% and 58.26% respectively. This validates our intuition that word embedding-based features alone are not sufficient, and should be augmented with other features.",
        "sentences": [
            "6 Results.",
            "Table 2 shows performance of sarcasm detection when our word embedding-based features are used on their own i.e, not as augmented features.",
            "The embedding in this case is Word2Vec.",
            "The four rows show baseline sets of features: unigrams, unweighted similarity using word embeddings (S), weighted similarity using word embeddings (WS) and both (i.e., unweighted plus weighted similarities using word embeddings).",
            "Using only unigrams as features gives a F-score of 72.53%, while only unweighted and weighted features gives F-score of 69.49% and 58.26% respectively.",
            "This validates our intuition that word embedding-based features alone are not sufficient, and should be augmented with other features."
        ],
        "class_sentence": [
            2,
            1,
            2,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "Features"
            ],
            [
                "Unigrams",
                "F",
                "S",
                "WS"
            ],
            [
                "S",
                "WS"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_2",
        "paper_id": "D16-1104",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1104table_3",
        "description": "Table 3 shows results for four kinds of word embeddings. All entries in the tables are higher than the simple unigrams baseline, i.e., F-score for each of the four is higher than unigrams - highlighting that these are better features for sarcasm detection than simple unigrams. Values in bold indicate the best F-score for a given prior work-embedding type combination. In case of Liebrecht et al. (2013) for Word2Vec, the overall improvement in F-score is 4%. Precision increases by 8% while recall remains nearly unchanged. For features given in Gonzalez- \u00b4 Ibanez et al. (2011a), there is a negligible degradation of \u00b4 0.91% when word embedding-based features based on Word2Vec are used. For Buschmeier et al. (2014) for Word2Vec, we observe an improvement in F-score from 76.61% to 78.09%. Precision remains nearly unchanged while recall increases. In case of Joshi et al. (2015) and Word2Vec, we observe a slight improvement of 0.20% when unweighted (S) features are used. This shows that word embedding-based features are useful, across four past works for Word2Vec. Table 3 also shows that the improvement holds across the four word embedding types as well. The maximum improvement is observed in case of Liebrecht et al. (2013). It is around 4% in case of LSA, 5% in case of GloVe, 6% in case of Dependency weight-based and 4% in case of Word2Vec. These improvements are not directly comparable because the four embeddings have different vocabularies (since they are trained on different datasets) and vocabulary sizes, their results cannot be directly compared.",
        "sentences": [
            "Table 3 shows results for four kinds of word embeddings.",
            "All entries in the tables are higher than the simple unigrams baseline, i.e., F-score for each of the four is higher than unigrams - highlighting that these are better features for sarcasm detection than simple unigrams.",
            "Values in bold indicate the best F-score for a given prior work-embedding type combination.",
            "In case of Liebrecht et al. (2013) for Word2Vec, the overall improvement in F-score is 4%.",
            "Precision increases by 8% while recall remains nearly unchanged.",
            "For features given in Gonzalez- \u00b4 Ibanez et al. (2011a), there is a negligible degradation of \u00b4 0.91% when word embedding-based features based on Word2Vec are used.",
            "For Buschmeier et al. (2014) for Word2Vec, we observe an improvement in F-score from 76.61% to 78.09%.",
            "Precision remains nearly unchanged while recall increases.",
            "In case of Joshi et al. (2015) and Word2Vec, we observe a slight improvement of 0.20% when unweighted (S) features are used.",
            "This shows that word embedding-based features are useful, across four past works for Word2Vec.",
            "Table 3 also shows that the improvement holds across the four word embedding types as well.",
            "The maximum improvement is observed in case of Liebrecht et al. (2013).",
            "It is around 4% in case of LSA, 5% in case of GloVe, 6% in case of Dependency weight-based and 4% in case of Word2Vec.",
            "These improvements are not directly comparable because the four embeddings have different vocabularies (since they are trained on different datasets) and vocabulary sizes, their results cannot be directly compared."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "F"
            ],
            [
                "F"
            ],
            [
                "L",
                "F",
                "Word2Vec"
            ],
            [
                "P"
            ],
            [
                "G",
                "Word2Vec"
            ],
            [
                "B",
                "Word2Vec",
                "F"
            ],
            [
                "P",
                "R"
            ],
            [
                "J",
                "Word2Vec",
                "+S"
            ],
            [
                "Word2Vec"
            ],
            [
                "LSA",
                "GloVe",
                "Dependency Weights",
                "Word2Vec"
            ],
            [
                "L"
            ],
            [
                "LSA",
                "GloVe",
                "Dependency Weights",
                "Word2Vec"
            ],
            null
        ],
        "n_sentence": 14.0,
        "table_id": "table_3",
        "paper_id": "D16-1104",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1108table_4",
        "description": "We compute a normalized community similarity score s\u02dci,j = si,j \u2212 si,m, where si,m is the corresponding score from the subreddit merged others. The correlation between s\u02dci,j and community feedback is reported for three models in Table 4 for the thread level, and in Table 5 for the user level. On the thread level, the hyb-500.30 style model consistently finds positive, statistically significant, correlation between the post\u2019s stylistic similarity score and its karma. This result suggests that language style adaptation does contribute to being well-received by the community. None of the other models explored in the previous section had this property, and for the topic models the correlation is mostly negative. On the user level, all correlations between a user\u2019s k-index and their style/topic match are statistically significant, though the hyb-500.30 style model shows more positive correlation than other models. In both cases, the word_only model gives results between the style and topic models. The hyb-15k model has results that are similar to the word_only model, and the tag_only model has mostly negative correlation.",
        "sentences": [
            "We compute a normalized community similarity score s\u02dci,j = si,j \u2212 si,m, where si,m is the corresponding score from the subreddit merged others.",
            "The correlation between s\u02dci,j and community feedback is reported for three models in Table 4 for the thread level, and in Table 5 for the user level.",
            "On the thread level, the hyb-500.30 style model consistently finds positive, statistically significant, correlation between the post\u2019s stylistic similarity score and its karma.",
            "This result suggests that language style adaptation does contribute to being well-received by the community.",
            "None of the other models explored in the previous section had this property, and for the topic models the correlation is mostly negative.",
            "On the user level, all correlations between a user\u2019s k-index and their style/topic match are statistically significant, though the hyb-500.30 style model shows more positive correlation than other models.",
            "In both cases, the word_only model gives results between the style and topic models.",
            "The hyb-15k model has results that are similar to the word_only model, and the tag_only model has mostly negative correlation."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            0,
            0,
            0
        ],
        "header_mention": [
            null,
            null,
            [
                "hyb-500.30"
            ],
            [
                "hyb-500.30"
            ],
            null,
            null,
            null,
            null
        ],
        "n_sentence": 8.0,
        "table_id": "table_4",
        "paper_id": "D16-1108",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1122table_2",
        "description": "Specifically, we used each method to construct a ranked list of time intervals. Then, for each method, we computed the discounted cumulative gain (DCG), which, in this context, is equivalent to computing X 39 eD1 1 log rank e; Lmethod T \u0001\u0001 ; (9) where L method T is the method\u2019s ranked list of time intervals and rank e; Lmethod T \u0001 is the rank of the e th well-known event in L method T. Finally, we divided the DCG by the ideal DCG\u2014i.e., P39 eD1 1 log .e/\u2014 .obtain the normalized DCG (nDCG). Table 2 shows that Capsule outperforms all four baseline methods.",
        "sentences": [
            "Specifically, we used each method to construct a ranked list of time intervals.",
            "Then, for each method, we computed the discounted cumulative gain (DCG), which, in this context, is equivalent to computing X 39 eD1 1 log rank e; Lmethod T \u0001\u0001 ; (9) where L method T is the method\u2019s ranked list of time intervals and rank e; Lmethod T \u0001 is the rank of the e th well-known event in L method T.",
            "Finally, we divided the DCG by the ideal DCG\u2014i.e., P39 eD1 1 log .e/\u2014 .obtain the normalized DCG (nDCG).",
            "Table 2 shows that Capsule outperforms all four baseline methods."
        ],
        "class_sentence": [
            0,
            0,
            0,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "Capsule (this paper)",
                "Method"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "D16-1122",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1129table_2",
        "description": "Results from Table 2 do not show significant differences between the two models. Putting the oneerror numbers into human performance context can be done only indirectly, as the data validation presented in Section 3.4 had a different set-up. Here we can see that the error rate of the most confident predicted label is about 30%, while human performed similarly by choosing from a two different label sets in a binary settings, so their task was inherently harder.",
        "sentences": [
            "Results from Table 2 do not show significant differences between the two models.",
            "Putting the one-error numbers into human performance context can be done only indirectly, as the data validation presented in Section 3.4 had a different set-up.",
            "Here we can see that the error rate of the most confident predicted label is about 30%, while human performed similarly by choosing from a two different label sets in a binary settings, so their task was inherently harder."
        ],
        "class_sentence": [
            1,
            2,
            1
        ],
        "header_mention": [
            [
                "BLSTM",
                "BLSTM/CNN/ATT"
            ],
            null,
            [
                "BLSTM",
                "BLSTM/CNN/ATT"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_2",
        "paper_id": "D16-1129",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1132table_2",
        "description": "The results in Table 2 show that our method using all the column sets achieved the best average precision among the combination of column sets that include at least the BASE column set. This suggests that all of the clues introduced by our four column sets are effective for performance improvement. Table 2 also demonstrates that our method using all the column sets obtained better average precision than the strongest baseline, Ouchi\u2019s method, in spite of an unfavorable condition for it. The results also show that our method with all of the column sets achieved a better F-score than Iida\u2019s method and the single-column baseline. However, it achieved a lower F-score than Ouchi\u2019s method. This was caused by the choice of different recall levels for computing the F-score. In contrast, the PR curves for these two methods in Figure 5 show that our method obtained higher precision than Ouchi\u2019s method at all recall levels. Particularly, it got high precision in a wide range of recall levels (e.g., around 0.8 in precision at 0.25 in recall and around 0.7 in precision at 0.4 in recall), while the precision obtained by Ouchi\u2019s method at 0.25 in recall was just around 0.65. We believe this difference becomes crucial when using the outputs of each method for developing accurate real-world NLP applications.",
        "sentences": [
            "The results in Table 2 show that our method using all the column sets achieved the best average precision among the combination of column sets that include at least the BASE column set.",
            "This suggests that all of the clues introduced by our four column sets are effective for performance improvement.",
            "Table 2 also demonstrates that our method using all the column sets obtained better average precision than the strongest baseline, Ouchi\u2019s method, in spite of an unfavorable condition for it.",
            "The results also show that our method with all of the column sets achieved a better F-score than Iida\u2019s method and the single-column baseline.",
            "However, it achieved a lower F-score than Ouchi\u2019s method.",
            "This was caused by the choice of different recall levels for computing the F-score.",
            "In contrast, the PR curves for these two methods in Figure 5 show that our method obtained higher precision than Ouchi\u2019s method at all recall levels.",
            "Particularly, it got high precision in a wide range of recall levels (e.g., around 0.8 in precision at 0.25 in recall and around 0.7 in precision at 0.4 in recall), while the precision obtained by Ouchi\u2019s method at 0.25 in recall was just around 0.65.",
            "We believe this difference becomes crucial when using the outputs of each method for developing accurate real-world NLP applications."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            2,
            0,
            1,
            2
        ],
        "header_mention": [
            [
                "MCNN"
            ],
            [
                "BASE+SURFSEQ+DEPTREE+PREDCONTEXT (Proposed)"
            ],
            [
                "BASE+SURFSEQ+DEPTREE+PREDCONTEXT (Proposed)",
                "Precision",
                "Ouchi et al. (ACL2015)"
            ],
            [
                "BASE+SURFSEQ+DEPTREE+PREDCONTEXT (Proposed)",
                "F-score",
                "Iida et al. (EMNLP2015)",
                "single column CNN (w/ position vec.)"
            ],
            [
                "BASE+SURFSEQ+DEPTREE+PREDCONTEXT (Proposed)",
                "F-score",
                "Ouchi et al. (ACL2015)"
            ],
            [
                "F-score"
            ],
            null,
            [
                "Recall",
                "Precision",
                "Ouchi et al. (ACL2015)"
            ],
            null
        ],
        "n_sentence": 9.0,
        "table_id": "table_2",
        "paper_id": "D16-1132",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1136table_4",
        "description": "We train the model as described in \u00a74, which is the combine embeddings setting from Table 3. Since the evaluation involves de and en word similarity, we train the CLWE for en-de pair. Table 4 shows the performance of our combined model compared with several baselines. Our combined model out-performed both Luong et al. (2015) and Gouws and S\u00c3\u00b8gaard (2015) which represent the best published crosslingual embeddings trained on bitext and monolingual data respectively.",
        "sentences": [
            "We train the model as described in \u00a74, which is the combine embeddings setting from Table 3.",
            "Since the evaluation involves de and en word similarity, we train the CLWE for en-de pair.",
            "Table 4 shows the performance of our combined model compared with several baselines.",
            "Our combined model out-performed both Luong et al. (2015) and Gouws and S\u00c3\u00b8gaard (2015) which represent the best published crosslingual embeddings trained on bitext and monolingual data respectively."
        ],
        "class_sentence": [
            0,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Model"
            ],
            [
                "Ours",
                "+combine",
                "Luong et al. (2015)",
                "Gouws and Sogaard (2015)"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_4",
        "paper_id": "D16-1136",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1136table_6",
        "description": "Table 6 shows the CLDC results for various CLWE. Despite its simplicity, our model achieves competitive performance. Note that aside from our model, all other models in Table 6 use a large bitext (Europarl) which may not exist for many lowresource languages, limiting their applicability.",
        "sentences": [
            "Table 6 shows the CLDC results for various CLWE.",
            "Despite its simplicity, our model achieves competitive performance.",
            "Note that aside from our model, all other models in Table 6 use a large bitext (Europarl) which may not exist for many lowresource languages, limiting their applicability."
        ],
        "class_sentence": [
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Our model"
            ],
            [
                "Model"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_6",
        "paper_id": "D16-1136",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1138table_3",
        "description": "Table 3 gives the average accuracy of the uniSSNT+, biSSNT+, vanilla encoder-decoder, and attention-based models. The model with the best previous average result \u2014 denoted as adaptedseq2seq (FTND16) (Faruqui et al., 2016) \u2014 is also included for comparison. Our biSSNT+ model outperforms the vanilla encoder-decoder by a large margin and almost matches the state-of-the-art result on this task. As mentioned earlier, a characteristic of these datasets is that the stems and their corresponding inflected forms mostly overlap. Compare to the vanilla encoder-decoder, our model is better at copying and finding correspondences between prefix, stem and suffix segments.",
        "sentences": [
            "Table 3 gives the average accuracy of the uniSSNT+, biSSNT+, vanilla encoder-decoder, and attention-based models.",
            "The model with the best previous average result \u2014 denoted as adaptedseq2seq (FTND16) (Faruqui et al., 2016) \u2014 is also included for comparison.",
            "Our biSSNT+ model outperforms the vanilla encoder-decoder by a large margin and almost matches the state-of-the-art result on this task.",
            "As mentioned earlier, a characteristic of these datasets is that the stems and their corresponding inflected forms mostly overlap.",
            "Compare to the vanilla encoder-decoder, our model is better at copying and finding correspondences between prefix, stem and suffix segments."
        ],
        "class_sentence": [
            1,
            2,
            1,
            2,
            2
        ],
        "header_mention": [
            [
                "Seq2Seq",
                "Seq2Seq w/ Attention",
                "uniSSNT+",
                "biSSNT+",
                "Avg. accuracy"
            ],
            [
                "Adapted-seq2seq (FTND16)"
            ],
            [
                "biSSNT+",
                "Seq2Seq"
            ],
            null,
            [
                "uniSSNT+",
                "biSSNT+",
                "Seq2Seq"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_3",
        "paper_id": "D16-1138",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1144table_4",
        "description": "Table 4 shows the results of AFET and its variants. Comparison with the other typing methods. AFET outperforms both FIGER and HYENA systems, demonstrating the predictive power of the learned embeddings, and the effectiveness of modeling type correlation information and noisy candidate types. We also observe that pruning methods do not always improve the performance, since they  ggressively filter out rare types in the corpus, which may lead to low Recall. ClusType is not as good as FIGER and HYENA because it is intended for coarse types and only utilizes relation phrases.",
        "sentences": [
            "Table 4 shows the results of AFET and its variants.",
            "Comparison with the other typing methods.",
            "AFET outperforms both FIGER and HYENA systems, demonstrating the predictive power of the learned embeddings, and the effectiveness of modeling type correlation information and noisy candidate types.",
            "We also observe that pruning methods do not always improve the performance, since they  ggressively filter out rare types in the corpus, which may lead to low Recall.",
            "ClusType is not as good as FIGER and HYENA because it is intended for coarse types and only utilizes relation phrases."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "AFET-NoCo",
                "AFET-NoPa",
                "AFET-CoH",
                "AFET"
            ],
            [
                "Typing Method"
            ],
            [
                "AFET",
                "FIGER (Ling and Weld 2012)",
                "HYENA (Yosef et al. 2012)"
            ],
            null,
            [
                "ClusType (Ren et al. 2015)",
                "FIGER (Ling and Weld 2012)",
                "HYENA (Yosef et al. 2012)"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_4",
        "paper_id": "D16-1144",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1147table_4",
        "description": "A breakdown by question type comparing the different data sources for KVMemNNs is given in Table 4. IE loses out especially to Doc (and KB) on Writer, Director and Actor to Movie, perhaps because coreference is difficult in these cases \u2013 although it has other losses elsewhere too. Note that only 56% of subject-object pairs in IE match the triples in the original KB, so losses are expected. Doc loses out to KB particularly on Tag to Movie, Movie to Tags, Movie to Writer and Movie to Actors.",
        "sentences": [
            "A breakdown by question type comparing the different data sources for KVMemNNs is given in Table 4.",
            "IE loses out especially to Doc (and KB) on Writer, Director and Actor to Movie, perhaps because coreference is difficult in these cases \u2013 although it has other losses elsewhere too.",
            "Note that only 56% of subject-object pairs in IE match the triples in the original KB, so losses are expected.",
            "Doc loses out to KB particularly on Tag to Movie, Movie to Tags, Movie to Writer and Movie to Actors."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "IE",
                "Doc",
                "KB",
                "Writer to Movie",
                "Director to Movie",
                "Actor to Movie"
            ],
            [
                "IE",
                "KB"
            ],
            [
                "Doc",
                "KB",
                "Tag to Movie",
                "Movie to Writer",
                "Movie to Actors"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_4",
        "paper_id": "D16-1147",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1149table_3",
        "description": "The convergence results are shown in Table 3 for four different temporal comparison intervals. Comparison of the significant game 1 results shows that teams entrained on pitch min, pitch max, shimmer, and jitter in at least one of the intervals. Both shimmer and jitter converged for all choices of temporal units. For pitch, convergence was instead only seen using the first and last 3 minutes, which are the intervals farthest in the game from each other. The only feature that diverged during game 1 is pitch-mean. The rest of the features did not show significant team-level partner differences during game 1 for any temporal interval and thus exhibited maintenance, meaning that the team members neither converged nor diverged. During game 2, we observed maintenance for all features except for intensity-mean and intensity-min, which diverged. Together our results suggest that when teams in our corpus converged on a feature, they did so earlier in the experiment (namely, just during the first game, and sometimes just in the earliest part of the first game).",
        "sentences": [
            "The convergence results are shown in Table 3 for four different temporal comparison intervals.",
            "Comparison of the significant game 1 results shows that teams entrained on pitch min, pitch max, shimmer, and jitter in at least one of the intervals.",
            "Both shimmer and jitter converged for all choices of temporal units.",
            "For pitch, convergence was instead only seen using the first and last 3 minutes, which are the intervals farthest in the game from each other.",
            "The only feature that diverged during game 1 is pitch-mean.",
            "The rest of the features did not show significant team-level partner differences during game 1 for any temporal interval and thus exhibited maintenance, meaning that the team members neither converged nor diverged.",
            "During game 2, we observed maintenance for all features except for intensity-mean and intensity-min, which diverged.",
            "Together our results suggest that when teams in our corpus converged on a feature, they did so earlier in the experiment (namely, just during the first game, and sometimes just in the earliest part of the first game)."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Game1",
                "Pitch-min",
                "Pitch-max",
                "Shimmer-local",
                "Jitter-local"
            ],
            [
                "Shimmer-local",
                "Jitter-local"
            ],
            [
                "Pitch-min",
                "Pitch-max",
                "Pitch-mean",
                "Pitch-sd"
            ],
            [
                "Game1",
                "Pitch-mean"
            ],
            [
                "Game1",
                "Feature"
            ],
            [
                "Game2",
                "Feature",
                "Intensity-mean",
                "Intensity-min"
            ],
            [
                "Feature"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_3",
        "paper_id": "D16-1149",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1150table_1",
        "description": "In our first study, we employed regression analysis to identify significant correlations between personal traits and aspect ranks. Specifically, we trained eight linear regression models, one for each of the eight car aspects. The dependent variable in each model is the rank of an aspect (from 1 to 8) and the independent variables are the ten user traits. In the regression analysis, we only focused on the main effects since a full interaction model with ten traits will require much more data to train. Since the raw scores of the personality and value traits use different scales, we normalized these scores so that they are all from 0 to 1. Table 1 shows the regression results. Several interesting patterns were discovered in this analysis: (a) a positive correlation between the rank of \u201cluxury\u201d and \u201cself-enhancement\u201d, a trait often associated with people who pursue self-interests and value social status, prestige and personal success (p < 0.0001). This pattern suggests that to promote a car to someone who scores high on \u201cselfenhancement\u201d, we need to highlight the \u201cluxury\u201d aspect of a car. (b) the rank of \u201csafety\u201d is positively correlated with \u201cconservation\u201d, a trait associated with people who conform to tradition and pursue safety, harmony, and stability (p < 0.005). This result suggests that for someone values \u201cconservation\u201d, it is better to emphasize \u201ccar safety\u201d in a personalized sales message. (c) \u201cself-transcendence\u201d, a trait often associated with people who pursue the protection of the welfare of others and the nature, is positively correlated with the rank of \u201cfuel economy\u201d (p < 0.005) but negatively correlated with the rank of \u201cstyle\u201d (p < 0.005). This suggests that for someone who values \u201cself-transcendence\u201d, it is better to emphasize \u201cfuel economy\u201d, but not so much on \u201cstyle\u201d. Other significant correlations uncovered in this analysis include a negative correlation between car \u201cprice\u201d and \u201cconservation\u201d (p < 0.005), a negative correlation between car \u201csafety\u201d and \u201cconscientiousness\u201d (p < 0.05), and a positive correlation between \u201copenness to change\u201d and car \u201cperformance\u201d (p < 0.05).",
        "sentences": [
            "In our first study, we employed regression analysis to identify significant correlations between personal traits and aspect ranks.",
            "Specifically, we trained eight linear regression models, one for each of the eight car aspects.",
            "The dependent variable in each model is the rank of an aspect (from 1 to 8) and the independent variables are the ten user traits.",
            "In the regression analysis, we only focused on the main effects since a full interaction model with ten traits will require much more data to train.",
            "Since the raw scores of the personality and value traits use different scales, we normalized these scores so that they are all from 0 to 1.",
            "Table 1 shows the regression results.",
            "Several interesting patterns were discovered in this analysis: (a) a positive correlation between the rank of \u201cluxury\u201d and \u201cself-enhancement\u201d, a trait often associated with people who pursue self-interests and value social status, prestige and personal success (p < 0.0001).",
            "This pattern suggests that to promote a car to someone who scores high on \u201cselfenhancement\u201d, we need to highlight the \u201cluxury\u201d aspect of a car.",
            "(b) the rank of \u201csafety\u201d is positively correlated with \u201cconservation\u201d, a trait associated with people who conform to tradition and pursue safety, harmony, and stability (p < 0.005).",
            "This result suggests that for someone values \u201cconservation\u201d, it is better to emphasize \u201ccar safety\u201d in a personalized sales message.",
            "(c) \u201cself-transcendence\u201d, a trait often associated with people who pursue the protection of the welfare of others and the nature, is positively correlated with the rank of \u201cfuel economy\u201d (p < 0.005) but negatively correlated with the rank of \u201cstyle\u201d (p < 0.005).",
            "This suggests that for someone who values \u201cself-transcendence\u201d, it is better to emphasize \u201cfuel economy\u201d, but not so much on \u201cstyle\u201d.",
            "Other significant correlations uncovered in this analysis include a negative correlation between car \u201cprice\u201d and \u201cconservation\u201d (p < 0.005), a negative correlation between car \u201csafety\u201d and \u201cconscientiousness\u201d (p < 0.05), and a positive correlation between \u201copenness to change\u201d and car \u201cperformance\u201d (p < 0.05)."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            2,
            1,
            1,
            2,
            1,
            2,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null,
            null,
            [
                "Luxury",
                "Self-enhancement"
            ],
            null,
            [
                "Safety",
                "Conservation"
            ],
            null,
            [
                "Self-transcendence",
                "Fuel",
                "Style"
            ],
            null,
            [
                "Price",
                "Conservation",
                "Safety",
                "Conscientiousness",
                "Openness to change",
                "Perf"
            ]
        ],
        "n_sentence": 13.0,
        "table_id": "table_1",
        "paper_id": "D16-1150",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1151table_9",
        "description": "Table 9 shows an ablation of the alignment classifier features. Entailment of arguments is the most informative feature for argument alignment. Adding lexical and syntactic context compatibilities adds significant boosts in precision and recall. Knowing that the arguments are retrieved by the same query pattern (sentence feature) only provides minor improvements. Even though the overall classification performance is far from perfect, cross sentence can benefit from alignment as long as it provides a higher score for argument pairs that should align compared to those that should not.",
        "sentences": [
            "Table 9 shows an ablation of the alignment classifier features.",
            "Entailment of arguments is the most informative feature for argument alignment.",
            "Adding lexical and syntactic context compatibilities adds significant boosts in precision and recall.",
            "Knowing that the arguments are retrieved by the same query pattern (sentence feature) only provides minor improvements.",
            "Even though the overall classification performance is far from perfect, cross sentence can benefit from alignment as long as it provides a higher score for argument pairs that should align compared to those that should not."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Entailment score only"
            ],
            [
                "+Lexical",
                "+Syntactic",
                "P",
                "R"
            ],
            [
                "+Sentence"
            ],
            [
                "+Sentence"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_9",
        "paper_id": "D16-1151",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1152table_4",
        "description": "Table 4 summarizes the empirical findings for our approach and S-MART (Yang and Chang, 2015) on the tweet entity linking task. For the systems with user-entity bilinear function, we report results obtained from embeddings trained on RETWEET+ in Table 4, and other results are available in Table 5. The best hyper-parameters are: the number of hidden units for the MLP is 40, the L2 regularization penalty for the composition parameters is 0.005, and the user embedding size is 100. For the word embedding size, we find 600 offers marginal improvements over 400 but requires longer training time. Thus, we choose 400 as the size of word embeddings. As presented in Table 4, NTEL-nonstruct performs 2.7% F1 worse than the NTEL baseline on the two test sets, which indicates the non-overlapping inference improves system performance on the task. With structured inference but without embeddings, NTEL performs roughly the same as S-MART, showing that a feedforward neural network offers similar expressivity to the regression trees employed by Yang and Chang (2015). Performance improves substantially with the incorporation of low-dimensional author, mention, and entity representations. As shown in Table 4, by learning the interactions between mention and entity representations, NTEL with mention-entity bilinear function outperforms the NTEL baseline system by 1.8% F1 on average. Specifically, the bilinear function results in considerable performance gains in recalls, with small compromise in precisions on the datasets.",
        "sentences": [
            "Table 4 summarizes the empirical findings for our approach and S-MART (Yang and Chang, 2015) on the tweet entity linking task.",
            "For the systems with user-entity bilinear function, we report results obtained from embeddings trained on RETWEET+ in Table 4, and other results are available in Table 5.",
            "The best hyper-parameters are: the number of hidden units for the MLP is 40, the L2 regularization penalty for the composition parameters is 0.005, and the user embedding size is 100.",
            "For the word embedding size, we find 600 offers marginal improvements over 400 but requires longer training time.",
            "Thus, we choose 400 as the size of word embeddings.",
            "As presented in Table 4, NTEL-nonstruct performs 2.7% F1 worse than the NTEL baseline on the two test sets, which indicates the non-overlapping inference improves system performance on the task.",
            "With structured inference but without embeddings, NTEL performs roughly the same as S-MART, showing that a feedforward neural network offers similar expressivity to the regression trees employed by Yang and Chang (2015).",
            "Performance improves substantially with the incorporation of low-dimensional author, mention, and entity representations.",
            "As shown in Table 4, by learning the interactions between mention and entity representations, NTEL with mention-entity bilinear function outperforms the NTEL baseline system by 1.8% F1 on average.",
            "Specifically, the bilinear function results in considerable performance gains in recalls, with small compromise in precisions on the datasets."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            2,
            1,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "System"
            ],
            null,
            null,
            null,
            null,
            [
                "NTEL-nonstruct",
                "NTEL",
                "F1"
            ],
            [
                "NTEL",
                "S-MART"
            ],
            null,
            [
                "NTEL",
                "Avg. F1"
            ],
            [
                "R",
                "P"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_4",
        "paper_id": "D16-1152",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1153table_6",
        "description": "Table 6 documents NIST evaluation results on an unseen Uyghur test set (with gold annotations) for the best transfer model configuration jointly trained on Turkish and Uzbek gold annotations and Uyghur training annotations produced by a non-speaker linguist (non-gold). Since Uyghur lacks helpful typelevel orthographic features such as capitalization, our transfer model in table 6 does not use any sparse features or attention but benefits from transfer via the phonological character representations we've proposed. Despite the noisy supervision provided in the target language, transferring from Turkish and Uzbek provides a +14.1 F1 improvement over a state of the art monolingual model trained on the same Uyghur annotations. It is worth pointing out that this transfer was achieved across 3 languages each with different scripts, morphology, phonology and lexicons.",
        "sentences": [
            "Table 6 documents NIST evaluation results on an unseen Uyghur test set (with gold annotations) for the best transfer model configuration jointly trained on Turkish and Uzbek gold annotations and Uyghur training annotations produced by a non-speaker linguist (non-gold).",
            "Since Uyghur lacks helpful typelevel orthographic features such as capitalization, our transfer model in table 6 does not use any sparse features or attention but benefits from transfer via the phonological character representations we've proposed.",
            "Despite the noisy supervision provided in the target language, transferring from Turkish and Uzbek provides a +14.1 F1 improvement over a state of the art monolingual model trained on the same Uyghur annotations.",
            "It is worth pointing out that this transfer was achieved across 3 languages each with different scripts, morphology, phonology and lexicons."
        ],
        "class_sentence": [
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "F1",
                "Model"
            ],
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_6",
        "paper_id": "D16-1153",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1154table_3",
        "description": "The results shown in Table 3 generally confirm the conclusions we drew from the PTB experiments above. In particular, we can see that the proposed LSRC model largely outperforms all other models. In particular, LSRC clearly outperforms LSTM with a negligible increase in the number of parameters (resulting from the additional 200 \u00d7 200 = 0.04M local connection weights Ulc) for the single layer results. We can also see that this improvement is maintained for deep models (2 hidden layers), where the LSRC model achieves a slightly better performance while reducing the number of parameters by \u2248 2.5M and speeding up the training time by \u2248 20% compared to deep LSTM.",
        "sentences": [
            "The results shown in Table 3 generally confirm the conclusions we drew from the PTB experiments above.",
            "In particular, we can see that the proposed LSRC model largely outperforms all other models.",
            "In particular, LSRC clearly outperforms LSTM with a negligible increase in the number of parameters (resulting from the additional 200 \u00d7 200 = 0.04M local connection weights Ulc) for the single layer results.",
            "We can also see that this improvement is maintained for deep models (2 hidden layers), where the LSRC model achieves a slightly better performance while reducing the number of parameters by \u2248 2.5M and speeding up the training time by \u2248 20% compared to deep LSTM."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "LSRC [200]-R600-80k",
                "LSRC [200]-R600-600-80k",
                "FFNN [M*200]-600-600-80k",
                "FOFE [M*200]-600-600-80k",
                "RNN [600]-R600-80k",
                "LSTM [200]-R600-80k",
                "LSTM [200]-R600-R600-80k"
            ],
            [
                "LSRC [200]-R600-80k",
                "LSRC [200]-R600-600-80k",
                "LSTM [200]-R600-80k",
                "LSTM [200]-R600-R600-80k"
            ],
            [
                "LSRC [200]-R600-80k",
                "LSRC [200]-R600-600-80k",
                "LSTM [200]-R600-80k",
                "LSTM [200]-R600-R600-80k"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "D16-1154",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1156table_2",
        "description": "We present our results in Table 2. Our approach significantly outperforms the Stanford Parser (De Marneffe et al., 2006) by 17.91% (28.69% relative) for PASCAL-50S, and 12.83% (25.28% relative) for PASCAL-Context-50S. We also make small improvements over DeepLabCRF (Chen et al., 2015) in the case of PASCAL-50S. To measure statistical significance of our results, we performed paired t-tests between MEDIATOR and INDEP. For both modules (and average), the null hypothesis (that the accuracies of our approach and baseline come from the same distribution) can be successfully rejected at p-value 0.05. For sake of completeness, we also compared MEDIATOR with our ablated system (CASCADE) and found statistically significant differences only in PPAR.",
        "sentences": [
            "We present our results in Table 2.",
            "Our approach significantly outperforms the Stanford Parser (De Marneffe et al., 2006) by 17.91% (28.69% relative) for PASCAL-50S, and 12.83% (25.28% relative) for PASCAL-Context-50S.",
            "We also make small improvements over DeepLabCRF (Chen et al., 2015) in the case of PASCAL-50S.",
            "To measure statistical significance of our results, we performed paired t-tests between MEDIATOR and INDEP.",
            "For both modules (and average), the null hypothesis (that the accuracies of our approach and baseline come from the same distribution) can be successfully rejected at p-value 0.05.",
            "For sake of completeness, we also compared MEDIATOR with our ablated system (CASCADE) and found statistically significant differences only in PPAR."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "Ours CASCADE",
                "Ours MEDIATOR",
                "Stanford Parser",
                "PASCAL-50S",
                "PASCAL-Context-50S"
            ],
            [
                "DeepLab-CRF",
                "PASCAL-50S"
            ],
            null,
            null,
            [
                "Ours CASCADE",
                "Ours MEDIATOR",
                "PPAR Acc."
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_2",
        "paper_id": "D16-1156",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1157table_4",
        "description": "The results are shown in Table 4. Performance is similar across models. We found that adding a second fully-connected 150 dimensional layer to the CHARAGRAM model improved results slightly.",
        "sentences": [
            "The results are shown in Table 4.",
            "Performance is similar across models.",
            "We found that adding a second fully-connected 150 dimensional layer to the CHARAGRAM model improved results slightly."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Accuracy (%)",
                "Model"
            ],
            [
                "CHARAGRAM (2-layer)",
                "Accuracy (%)"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_4",
        "paper_id": "D16-1157",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1160table_1",
        "description": "Table 1 reports the translation quality for different methods. Comparing the first two lines in Table 1, it is obvious that the NMT method RNNSearch performs much worse than the SMT model Moses on Chinese-to-English translation. The gap is as large as approximately 2.0 BLEU points (28.38 vs. 30.30). We speculate that the encoder-decoder network models of NMT are not well optimized due to insufficient bilingual training data. The focus of this work is to figure out whether the encoder model of NMT can be improved using source-side monolingual data and further boost the translation quality. The four lines (3-6 in Table 1) show the BLEU scores when applying self-learning algorithm to incorporate the source-side monolingual data. Clearly, RNNSearch-Mono-SL outperforms RNNSearch in most cases. The best performance is obtained if the top 50% monolingual data is used. The biggest improvement is up to 4.05 BLEU points (32.43 vs. 28.38 on MT03) and it also significantly outperforms Moses. When employing our multi-task learning framework to incorporate source-side monolingual data, the translation quality can be further improved (Lines 7-10 in Table 1). For example, RNNSearchMono-MTL using the top 50% monolingual data can remarkably outperform the baseline RNNSearch,\r\nwith an improvement up to 5.0 BLEU points (33.38 vs. 28.38 on MT03). Moreover, it also performs significantly better than the state-of-the-art phrasebased SMT Moses by the largest gains of 3.38 BLEU points (31.57 vs. 28.19 on MT05). The promising results demonstrate that source-side monolingual data can improve neural machine translation and our multi-task learning is more effective. From the last two lines in Table 1, we can see that RNNSearch-Mono-Autoencoder can also improve the translation quality by more than 1.0 BLEU points when using the most related monolingual data. However, it underperforms RNNSearch-MonoMTL by a large gap. It indicates that sentence reordering model is better than sentence reconstruction model for exploiting the source-side monolingual data.",
        "sentences": [
            "Table 1 reports the translation quality for different methods.",
            "Comparing the first two lines in Table 1, it is obvious that the NMT method RNNSearch performs much worse than the SMT model Moses on Chinese-to-English translation.",
            "The gap is as large as approximately 2.0 BLEU points (28.38 vs. 30.30).",
            "We speculate that the encoder-decoder network models of NMT are not well optimized due to insufficient bilingual training data.",
            "The focus of this work is to figure out whether the encoder model of NMT can be improved using source-side monolingual data and further boost the translation quality.",
            "The four lines (3-6 in Table 1) show the BLEU scores when applying self-learning algorithm to incorporate the source-side monolingual data.",
            "Clearly, RNNSearch-Mono-SL outperforms RNNSearch in most cases.",
            "The best performance is obtained if the top 50% monolingual data is used.",
            "The biggest improvement is up to 4.05 BLEU points (32.43 vs. 28.38 on MT03) and it also significantly outperforms Moses.",
            "When employing our multi-task learning framework to incorporate source-side monolingual data, the translation quality can be further improved (Lines 7-10 in Table 1).",
            "For example, RNNSearchMono-MTL using the top 50% monolingual data can remarkably outperform the baseline RNNSearch,\r\nwith an improvement up to 5.0 BLEU points (33.38 vs. 28.38 on MT03).",
            "Moreover, it also performs significantly better than the state-of-the-art phrasebased SMT Moses by the largest gains of 3.38 BLEU points (31.57 vs. 28.19 on MT05).",
            "The promising results demonstrate that source-side monolingual data can improve neural machine translation and our multi-task learning is more effective.",
            "From the last two lines in Table 1, we can see that RNNSearch-Mono-Autoencoder can also improve the translation quality by more than 1.0 BLEU points when using the most related monolingual data.",
            "However, it underperforms RNNSearch-MonoMTL by a large gap.",
            "It indicates that sentence reordering model is better than sentence reconstruction model for exploiting the source-side monolingual data."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            2,
            2,
            1,
            1,
            1,
            2,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Method",
                "RNNSearch",
                "Moses"
            ],
            [
                "Method",
                "RNNSearch",
                "Moses"
            ],
            null,
            null,
            [
                "RNNSearch-Mono-SL (25%)",
                "RNNSearch-Mono-SL (50%)",
                "RNNSearch-Mono-SL (75%)",
                "RNNSearch-Mono-SL (100%)"
            ],
            [
                "RNNSearch-Mono-SL (25%)",
                "RNNSearch-Mono-SL (50%)",
                "RNNSearch-Mono-SL (75%)",
                "RNNSearch-Mono-SL (100%)",
                "RNNSearch"
            ],
            [
                "RNNSearch-Mono-SL (50%)"
            ],
            [
                "RNNSearch-Mono-SL (50%)",
                "Moses"
            ],
            [
                "RNNSearch-Mono-MTL (25%)",
                "RNNSearch-Mono-MTL (50%)",
                "RNNSearch-Mono-MTL (75%)",
                "RNNSearch-Mono-MTL (100%)"
            ],
            [
                "RNNSearch-Mono-MTL (50%)",
                "RNNSearch",
                "MT03"
            ],
            [
                "RNNSearch-Mono-MTL (50%)",
                "Moses",
                "MT05"
            ],
            [
                "RNNSearch-Mono-MTL (25%)",
                "RNNSearch-Mono-MTL (50%)",
                "RNNSearch-Mono-MTL (75%)",
                "RNNSearch-Mono-MTL (100%)"
            ],
            [
                "RNNSearch-Mono-Autoencoder (50%)",
                "RNNSearch-Mono-Autoencoder (100%)"
            ],
            [
                "RNNSearch-Mono-Autoencoder (50%)",
                "RNNSearch-Mono-Autoencoder (100%)",
                "RNNSearch-Mono-MTL (25%)",
                "RNNSearch-Mono-MTL (50%)",
                "RNNSearch-Mono-MTL (75%)",
                "RNNSearch-Mono-MTL (100%)"
            ],
            [
                "RNNSearch-Mono-Autoencoder (50%)",
                "RNNSearch-Mono-Autoencoder (100%)",
                "RNNSearch-Mono-MTL (25%)",
                "RNNSearch-Mono-MTL (50%)",
                "RNNSearch-Mono-MTL (75%)",
                "RNNSearch-Mono-MTL (100%)"
            ]
        ],
        "n_sentence": 16.0,
        "table_id": "table_1",
        "paper_id": "D16-1160",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1160table_2",
        "description": "A natural question arises that is the source-side monolingual data still very helpful when we have much more bilingual training data. We conduct the large-scale experiments using our proposed multitask framework RNNSearch-Mono-MTL. Table 2 reports the results. We can see from the table that closely related source-side monolingual data (the top 50%) can also boost the translation quality on all of the test sets. The performance improvement can be more than 1.0 BLEU points. Compared to the results on small training data, the gains from source-side monolingual data are much smaller. It is reasonable since large-scale training data can make the parameters of the encoder-decoder parameters much stable. We can also observe the similar phenomenon that adding more unrelated monolingual data leads to decreased translation quality.",
        "sentences": [
            "A natural question arises that is the source-side monolingual data still very helpful when we have much more bilingual training data.",
            "We conduct the large-scale experiments using our proposed multitask framework RNNSearch-Mono-MTL.",
            "Table 2 reports the results.",
            "We can see from the table that closely related source-side monolingual data (the top 50%) can also boost the translation quality on all of the test sets.",
            "The performance improvement can be more than 1.0 BLEU points.",
            "Compared to the results on small training data, the gains from source-side monolingual data are much smaller.",
            "It is reasonable since large-scale training data can make the parameters of the encoder-decoder parameters much stable.",
            "We can also observe the similar phenomenon that adding more unrelated monolingual data leads to decreased translation quality."
        ],
        "class_sentence": [
            0,
            2,
            1,
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "RNNSearch-Mono-MTL (50%)",
                "MT03",
                "MT04",
                "MT05",
                "MT06"
            ],
            null,
            [
                "RNNSearch-Mono-MTL (50%)",
                "RNNSearch-Mono-MTL (100%)",
                "MT03",
                "MT04",
                "MT05",
                "MT06"
            ],
            null,
            null
        ],
        "n_sentence": 8.0,
        "table_id": "table_2",
        "paper_id": "D16-1160",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1161table_4",
        "description": "Table 4 summarizes the best results reported in this paper for the CoNLL-2014 test set (column 2014) before and after adding the Common Crawl n-gram language model. The vanilla Moses baseline with the Common Crawl model can be seen as a new simple baseline for unrestricted settings and is ahead of any previously published result. The combination of sparse features and web-scale monolingual data marks our best result, outperforming previously published results by 8% M2 using similar training data. While our sparse features cause a respectable gain when used with the smaller language model, the web-scale language model seems to cancel out part of the effect.",
        "sentences": [
            "Table 4 summarizes the best results reported in this paper for the CoNLL-2014 test set (column 2014) before and after adding the Common Crawl n-gram language model.",
            "The vanilla Moses baseline with the Common Crawl model can be seen as a new simple baseline for unrestricted settings and is ahead of any previously published result.",
            "The combination of sparse features and web-scale monolingual data marks our best result, outperforming previously published results by 8% M2 using similar training data.",
            "While our sparse features cause a respectable gain when used with the smaller language model, the web-scale language model seems to cancel out part of the effect."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "2014"
            ],
            [
                "Baseline"
            ],
            [
                "M2",
                "Best parse",
                "+CCLM"
            ],
            [
                "Best parse"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_4",
        "paper_id": "D16-1161",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1163table_3",
        "description": "We also use the NMT model with transfer learning as a feature when re-scoring output n-best lists (n = 1000) from the SBMT system. Table 3 shows the results of re-scoring. We compare re-scoring with transfer NMT to re-scoring with baseline (i.e. non-transfer) NMT and to re-scoring with a neural language model. The neural language model is an LSTM RNN with 2 layers and 1000 hidden states. It has a target vocabulary of 100K and is trained using noise-contrastive estimation (Mnih and Teh, 2012, Vaswani et al., 2013, Baltescu and Blunsom, 2015 Williams et al., 2015). Additionally, it is trained using dropout with a dropout probability of 0.2 as suggested by Zaremba et al. (2014). Re-scoring with the transfer NMT model yields an improvement of 1.1\u20131.6 BLEU points above the strong SBMT system, we find that transfer NMT is a better re-scoring feature than baseline NMT or neural language models.",
        "sentences": [
            "We also use the NMT model with transfer learning as a feature when re-scoring output n-best lists (n = 1000) from the SBMT system.",
            "Table 3 shows the results of re-scoring.",
            "We compare re-scoring with transfer NMT to re-scoring with baseline (i.e. non-transfer) NMT and to re-scoring with a neural language model.",
            "The neural language model is an LSTM RNN with 2 layers and 1000 hidden states.",
            "It has a target vocabulary of 100K and is trained using noise-contrastive estimation (Mnih and Teh, 2012, Vaswani et al., 2013, Baltescu and Blunsom, 2015 Williams et al., 2015).",
            "Additionally, it is trained using dropout with a dropout probability of 0.2 as suggested by Zaremba et al. (2014).",
            "Re-scoring with the transfer NMT model yields an improvement of 1.1\u20131.6 BLEU points above the strong SBMT system, we find that transfer NMT is a better re-scoring feature than baseline NMT or neural language models."
        ],
        "class_sentence": [
            0,
            1,
            2,
            2,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null,
            null,
            [
                "Xfer"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_3",
        "paper_id": "D16-1163",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1165table_2",
        "description": "Table 2 shows the results of an ablation study when removing some groups of features. More specifically, we drop lexical similarities, domain-specific features, and the complex semantic-syntactic interactions modeled in the hidden layer between the embeddings and the domain-specific features. We can see that the lexical similarity features (which we modeled by MT evaluation metrics), have a large impact: excluding them from the network yields a decrease of over eight MAP points. This can be explained as the strong dependence that relatedness has over strict word matching. Since questions are relatively short, a better related question will be one that matches better the original question. As expected, eliminating the domain-specific features also hurts the performance greatly: by six MAP points absolute. Eliminating the use of distributed representation has a lesser impact: 3.3 MAP points absolute. This is in line with our previous findings (Guzman et al., 2015, Guzm \u00b4 an et al., 2016a, \u00b4Guzman et al., 2016b) that semantic and syntactic \u00b4embeddings are useful to make a fine-grained distinction between comments (relevance, appropriateness), which are usually longer. We have also found that there is an interaction between features and similarity relations. For example, for relatedness, lexical similarity is 2.6 MAP points more informative10 than distributed representations. In contrast, forrelevance, distributed representations are 0.7 MAP points more informative than lexical similarities.",
        "sentences": [
            "Table 2 shows the results of an ablation study when removing some groups of features.",
            "More specifically, we drop lexical similarities, domain-specific features, and the complex semantic-syntactic interactions modeled in the hidden layer between the embeddings and the domain-specific features.",
            "We can see that the lexical similarity features (which we modeled by MT evaluation metrics), have a large impact: excluding them from the network yields a decrease of over eight MAP points.",
            "This can be explained as the strong dependence that relatedness has over strict word matching.",
            "Since questions are relatively short, a better related question will be one that matches better the original question.",
            "As expected, eliminating the domain-specific features also hurts the performance greatly: by six MAP points absolute.",
            "Eliminating the use of distributed representation has a lesser impact: 3.3 MAP points absolute.",
            "This is in line with our previous findings (Guzman et al., 2015, Guzm \u00b4 an et al., 2016a, \u00b4Guzman et al., 2016b) that semantic and syntactic \u00b4embeddings are useful to make a fine-grained distinction between comments (relevance, appropriateness), which are usually longer.",
            "We have also found that there is an interaction between features and similarity relations.",
            "For example, for relatedness, lexical similarity is 2.6 MAP points more informative10 than distributed representations.",
            "In contrast, forrelevance, distributed representations are 0.7 MAP points more informative than lexical similarities."
        ],
        "class_sentence": [
            1,
            2,
            1,
            2,
            0,
            1,
            1,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "- Lexical similarity",
                "MAP"
            ],
            null,
            null,
            [
                "MAP",
                "- Domain-specific"
            ],
            [
                "MAP",
                "- Distributed rep."
            ],
            null,
            null,
            [
                "- Lexical similarity",
                "- Distributed rep."
            ],
            [
                "- Distributed rep.",
                "- Lexical similarity"
            ]
        ],
        "n_sentence": 11.0,
        "table_id": "table_2",
        "paper_id": "D16-1165",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1168table_2",
        "description": "However, a pairwise comparison between FULL and -SYN (Table 2) reveals that human subjects consistently prefer the output of FULL instead of -SYN both for STARtest and CARTOON. Table 2 also reports that HUMAN outperforms the output of the FULL model, and a pairwise comparison of FULL and -SEM which yields a result in line with the METEOR scores.",
        "sentences": [
            "However, a pairwise comparison between FULL and -SYN (Table 2) reveals that human subjects consistently prefer the output of FULL instead of -SYN both for STARtest and CARTOON.",
            "Table 2 also reports that HUMAN outperforms the output of the FULL model, and a pairwise comparison of FULL and -SEM which yields a result in line with the METEOR scores."
        ],
        "class_sentence": [
            1,
            2
        ],
        "header_mention": [
            [
                "FULL",
                "HUMAN",
                "-SYN",
                "STARtest",
                "CARTOON"
            ],
            [
                "HUMAN",
                "FULL",
                "-SEM",
                "STARtest",
                "CARTOON"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_2",
        "paper_id": "D16-1168",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1168table_3",
        "description": "Table 3 shows the results of the detailed comparison of Thematicity, Coherence, and Solvability. This table clearly shows the strong contribution of the semantic component of our system. The specific contribution of the syntactic component is to pro duce overall more solvable and thematically satisfying problems, although it can slightly affect coherence especially when automatic parses fail. Finally, the overall high ratings for human-authored stories across all three dimensions, confirm the high quality of the crowd-sourced stories.",
        "sentences": [
            "Table 3 shows the results of the detailed comparison of Thematicity, Coherence, and Solvability.",
            "This table clearly shows the strong contribution of the semantic component of our system.",
            "The specific contribution of the syntactic component is to pro duce overall more solvable and thematically satisfying problems, although it can slightly affect coherence especially when automatic parses fail.",
            "Finally, the overall high ratings for human-authored stories across all three dimensions, confirm the high quality of the crowd-sourced stories."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "Thematicity",
                "Coherence",
                "Solvability"
            ],
            [
                "-SEM"
            ],
            [
                "-SYN"
            ],
            [
                "HUMAN",
                "Model"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "D16-1168",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1173table_1",
        "description": "Table 1 shows the classification performance on the SST2 dataset. From rows 1-3 we see that our proposed sentiment model that integrates the diverse set of knowledge (section 4) significantly outperforms the base CNN (Kim 2014). The improvement of the student network p validates the effectiveness of the iterative mutual distillation process. Consistent with the observations in (Hu et al. 2016), the regularized teacher model q provides further performance boost, though it imposes additional computational overhead for explicit knowledge representations. Note that our models are trained with only sentence-level annotations. Compared with the baselines trained in the same setting (rows 4-6), our model with the full knowledge, CNN+REL+LEX, performs the best. CNN+But-q (row 6) is the base CNN augmented with a logic rule that identifies contrastive sense through explicit occurrence of word \u201cbut\u201d (section 3.1) (Hu et al. 2016). Our enhanced framework enables richer knowledge and achieves much better performance. Our method further outperforms the base CNN that is additionally trained with dense phrase-level annotations (row 7), showing improved generalization of the knowledge-enhanced model from limited data. Figure 2 further studies the performance with varying training sizes. We can clearly observe that the incorporated knowledge tends to offer higher improvement with less training data. This property can be particularly desirable in applications of structured predictions where manual annotations are expensive while rich human knowledge is available.",
        "sentences": [
            "Table 1 shows the classification performance on the SST2 dataset.",
            "From rows 1-3 we see that our proposed sentiment model that integrates the diverse set of knowledge (section 4) significantly outperforms the base CNN (Kim 2014).",
            "The improvement of the student network p validates the effectiveness of the iterative mutual distillation process.",
            "Consistent with the observations in (Hu et al. 2016), the regularized teacher model q provides further performance boost, though it imposes additional computational overhead for explicit knowledge representations.",
            "Note that our models are trained with only sentence-level annotations.",
            "Compared with the baselines trained in the same setting (rows 4-6), our model with the full knowledge, CNN+REL+LEX, performs the best.",
            "CNN+But-q (row 6) is the base CNN augmented with a logic rule that identifies contrastive sense through explicit occurrence of word \u201cbut\u201d (section 3.1) (Hu et al. 2016).",
            "Our enhanced framework enables richer knowledge and achieves much better performance.",
            "Our method further outperforms the base CNN that is additionally trained with dense phrase-level annotations (row 7), showing improved generalization of the knowledge-enhanced model from limited data.",
            "Figure 2 further studies the performance with varying training sizes.",
            "We can clearly observe that the incorporated knowledge tends to offer higher improvement with less training data.",
            "This property can be particularly desirable in applications of structured predictions where manual annotations are expensive while rich human knowledge is available."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            2,
            1,
            2,
            1,
            1,
            0,
            0,
            0
        ],
        "header_mention": [
            null,
            [
                "CNN+REL q",
                "CNN+REL p",
                "CNN+REL+LEX q",
                "CNN+REL+LEX p",
                "CNN (Kim 2014)"
            ],
            null,
            null,
            null,
            [
                "CNN+REL+LEX q",
                "CNN+REL+LEX p"
            ],
            [
                "CNN+But-q (Hu et al. 2016)"
            ],
            [
                "CNN+REL+LEX q",
                "CNN+REL+LEX p"
            ],
            [
                "CNN (Kim 2014)"
            ],
            null,
            null,
            null
        ],
        "n_sentence": 12.0,
        "table_id": "table_1",
        "paper_id": "D16-1173",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1173table_2",
        "description": "Table 2 shows model performance on the CR dataset. Our model again surpasses the base network and several other competitive neural methods by a large margin. Though falling behind AdaSent (row 7) which has a more specialized and complex architecture than standard convolutional networks, the proposed framework indeed is general enough to apply on top of it for further enhancement. To further evaluate the proposed mutual distillation framework for learning knowledge, we compare to an extensive set of other possible knowledge optimization approaches. Table 3 shows the results. In row 2, the \u201copt-joint\u201d method optimizes the regularized joint model of Eq.(2) directly in terms of both the neural network and knowledge parameters. Row 3, \u201copt-knwl-pipeline\u201d, is an approach that first optimizes the standalone knowledge component and then inserts it into the previous framework of (Hu et al. 2016) as a fixed constraint. Without interaction between the knowledge and neural network learning, the pipelined method yields inferior results. Finally, rows 4-5 display a method that adapts the knowledge component at each iteration by optimizing the joint model q in terms of the knowledge parameters. We report the accuracy of both the student network p (row 4) and the joint teacher network q (row 5), and compare with our method in row 6 and 7, respectively. We can see that both models performs poorly, achieving the accuracy of only 68.6% for the knowledge component, similar to the accuracy achieved by the \u201copt-joint\u201d method.",
        "sentences": [
            "Table 2 shows model performance on the CR dataset.",
            "Our model again surpasses the base network and several other competitive neural methods by a large margin.",
            "Though falling behind AdaSent (row 7) which has a more specialized and complex architecture than standard convolutional networks, the proposed framework indeed is general enough to apply on top of it for further enhancement.",
            "To further evaluate the proposed mutual distillation framework for learning knowledge, we compare to an extensive set of other possible knowledge optimization approaches.",
            "Table 3 shows the results.",
            "In row 2, the \u201copt-joint\u201d method optimizes the regularized joint model of Eq.(2) directly in terms of both the neural network and knowledge parameters.",
            "Row 3, \u201copt-knwl-pipeline\u201d, is an approach that first optimizes the standalone knowledge component and then inserts it into the previous framework of (Hu et al. 2016) as a fixed constraint.",
            "Without interaction between the knowledge and neural network learning, the pipelined method yields inferior results.",
            "Finally, rows 4-5 display a method that adapts the knowledge component at each iteration by optimizing the joint model q in terms of the knowledge parameters.",
            "We report the accuracy of both the student network p (row 4) and the joint teacher network q (row 5), and compare with our method in row 6 and 7, respectively.",
            "We can see that both models performs poorly, achieving the accuracy of only 68.6% for the knowledge component, similar to the accuracy achieved by the \u201copt-joint\u201d method."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            0,
            0,
            0,
            0,
            0,
            0,
            0
        ],
        "header_mention": [
            null,
            [
                "CNN+REL+LEX",
                "Model"
            ],
            [
                "AdaSent (Zhao et al. 2015)"
            ],
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            null
        ],
        "n_sentence": 11.0,
        "table_id": "table_2",
        "paper_id": "D16-1173",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1174table_5",
        "description": "Table 5 shows the results on the word to sense dataset of the SemEval-2014 CLSS task, according to Pearson (r \u00d7 100) and Spearman (rho \u00d7 100) correlation scores and for the four strategies. As can be seen from the low overall performance, the task is a very challenging benchmark with many WordNet out-of-vocabulary or slang terms and rare usages. Despite this, DECONF provides consistent improvement over the comparison sense representation techniques according to both measures and for all the strategies. Across the four strategies, S2A proves to be the most effective for DECONF and the representations of Rothe and Schutze (2015). The representations of Chen et al. (2014) perform best with the S2W strat egy whereas those of Iacobacci et al. (2015) do not show a consistent trend with relatively low performance across the four strategies. Also, a comparison of our results across the S2W and S2A strategies reveals that a word\u2019s aggregated representation, i.e., the centroid of the representations of its senses, is more accurate than its original word representation.",
        "sentences": [
            "Table 5 shows the results on the word to sense dataset of the SemEval-2014 CLSS task, according to Pearson (r \u00d7 100) and Spearman (rho \u00d7 100) correlation scores and for the four strategies.",
            "As can be seen from the low overall performance, the task is a very challenging benchmark with many WordNet out-of-vocabulary or slang terms and rare usages.",
            "Despite this, DECONF provides consistent improvement over the comparison sense representation techniques according to both measures and for all the strategies.",
            "Across the four strategies, S2A proves to be the most effective for DECONF and the representations of Rothe and Schutze (2015).",
            "The representations of Chen et al. (2014) perform best with the S2W strat egy whereas those of Iacobacci et al. (2015) do not show a consistent trend with relatively low performance across the four strategies.",
            "Also, a comparison of our results across the S2W and S2A strategies reveals that a word\u2019s aggregated representation, i.e., the centroid of the representations of its senses, is more accurate than its original word representation."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "r",
                "rho"
            ],
            null,
            [
                "DECONF"
            ],
            [
                "S2A",
                "DECONF"
            ],
            [
                "Chen et al. (2014)*",
                "S2W",
                "Iacobacci et al. (2015)*"
            ],
            [
                "S2W",
                "S2A"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_5",
        "paper_id": "D16-1174",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1175table_1",
        "description": "Table 1 shows a number of features extracted from the aligned dependency trees in Figure 1 and highlights that adjectives and nouns do not share many features if only first order dependencies would be considered. However through the inclusion of inverse and higher order dependency paths we can observe that the second order features of the adjective align with the first order features of the noun. For composition, the adjective white needs to be offset by its inverse relation to clothes making it distributionally similar to a noun that has been modified by white. Offsetting can be seen as shifting the current viewpoint in the APT data structure and is necessary for aligning the feature spaces for composition (Weir et al., 2016). We are then in a position to compose the offset representation of white with the vector for clothes by the union or the intersection of their features.",
        "sentences": [
            "Table 1 shows a number of features extracted from the aligned dependency trees in Figure 1 and highlights that adjectives and nouns do not share many features if only first order dependencies would be considered.",
            "However through the inclusion of inverse and higher order dependency paths we can observe that the second order features of the adjective align with the first order features of the noun.",
            "For composition, the adjective white needs to be offset by its inverse relation to clothes making it distributionally similar to a noun that has been modified by white.",
            "Offsetting can be seen as shifting the current viewpoint in the APT data structure and is necessary for aligning the feature spaces for composition (Weir et al., 2016).",
            "We are then in a position to compose the offset representation of white with the vector for clothes by the union or the intersection of their features."
        ],
        "class_sentence": [
            1,
            2,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_1",
        "paper_id": "D16-1175",
        "valid": 0
    },
    {
        "table_id_paper": "D16-1175table_3",
        "description": "Table 3 highlights the effect of the SPPMI shift parameter k, while keeping the number of neighbours fixed at 30 and using the static top n neighbour retrieval function. For the APT model, a value of k = 40 performs best (except for SimLex-999, where smaller shifts give better results), with a performance drop-off for larger shifts. In our experiments we find that a shift of k = 1 results in top performance for the untyped vector space model. It appears that shifting the PPMI scores in the APT model has the effect of cleaning the vectors from noisy PPMI artefacts, which reinforces the predominant sense, while other senses get suppressed. Subsequently, this results in a cleaner neighbourhood around the word vector, dominated by a single sense. This explains why distributional inference slightly degrades performance for smaller values of k.",
        "sentences": [
            "Table 3 highlights the effect of the SPPMI shift parameter k, while keeping the number of neighbours fixed at 30 and using the static top n neighbour retrieval function.",
            "For the APT model, a value of k = 40 performs best (except for SimLex-999, where smaller shifts give better results), with a performance drop-off for larger shifts.",
            "In our experiments we find that a shift of k = 1 results in top performance for the untyped vector space model.",
            "It appears that shifting the PPMI scores in the APT model has the effect of cleaning the vectors from noisy PPMI artefacts, which reinforces the predominant sense, while other senses get suppressed.",
            "Subsequently, this results in a cleaner neighbourhood around the word vector, dominated by a single sense.",
            "This explains why distributional inference slightly degrades performance for smaller values of k."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "k = 40",
                "MEN",
                "SimLex-999",
                "WordSim-353 (rel)",
                "WordSim-353 (sub)"
            ],
            [
                "k = 1"
            ],
            null,
            null,
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_3",
        "paper_id": "D16-1175",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1175table_4",
        "description": "Table 4 shows that distributional inference successfully infers missing information for both model types, resulting in improved performance over models without the use of DI on all datasets. The improvements are typically larger for the APT model, suggesting that it is missing more distributional knowledge in its elementary representations than untyped models. The density window and static top n neighbour retrieval functions perform very similar, however the static approach is more consistent and never underperforms the baseline for either model type on any dataset. The WordNet based neighbour retrieval function performs particularly well on SimLex-999. This can be explained by the fact that antonyms, which frequently happen to be among the nearest neighbours in distributional vector spaces, are regarded as dissimilar in SimLex-999, whereas the WordNet neighbour retrieval function only returns synonyms. The results furthermore confirm the effect that untyped models perform better on datasets modelling relatedness, whereas typed models work better for substitutability tasks (Baroni and Lenci, 2011).",
        "sentences": [
            "Table 4 shows that distributional inference successfully infers missing information for both model types, resulting in improved performance over models without the use of DI on all datasets.",
            "The improvements are typically larger for the APT model, suggesting that it is missing more distributional knowledge in its elementary representations than untyped models.",
            "The density window and static top n neighbour retrieval functions perform very similar, however the static approach is more consistent and never underperforms the baseline for either model type on any dataset.",
            "The WordNet based neighbour retrieval function performs particularly well on SimLex-999.",
            "This can be explained by the fact that antonyms, which frequently happen to be among the nearest neighbours in distributional vector spaces, are regarded as dissimilar in SimLex-999, whereas the WordNet neighbour retrieval function only returns synonyms.",
            "The results furthermore confirm the effect that untyped models perform better on datasets modelling relatedness, whereas typed models work better for substitutability tasks (Baroni and Lenci, 2011)."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "APTs (k = 40)"
            ],
            [
                "Density Window",
                "Static Top n"
            ],
            [
                "WordNet",
                "SimLex-999"
            ],
            [
                "WordNet",
                "SimLex-999"
            ],
            [
                "Untyped VSM (k = 1)",
                "APTs (k = 40)"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_4",
        "paper_id": "D16-1175",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1175table_6",
        "description": "Table 6 shows that the static top n and density window neighbour retrieval functions perform very similar again. The density window retrieval function outperforms static top n for composition by intersection and vice versa for composition by union. The WordNet approach is competitive for composition by union, but underperfoms the other approaches for composition by intersection significantly. For further experiments we use the static top n approach as it is computationally cheap and easy to interpret due to the fixed number of neighbours. Table 6 also shows that while composition by intersection is significantly improved by distributional inference, composition by union does not appear to benefit from it.",
        "sentences": [
            "Table 6 shows that the static top n and density window neighbour retrieval functions perform very similar again.",
            "The density window retrieval function outperforms static top n for composition by intersection and vice versa for composition by union.",
            "The WordNet approach is competitive for composition by union, but underperfoms the other approaches for composition by intersection significantly.",
            "For further experiments we use the static top n approach as it is computationally cheap and easy to interpret due to the fixed number of neighbours.",
            "Table 6 also shows that while composition by intersection is significantly improved by distributional inference, composition by union does not appear to benefit from it."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            [
                "Density Window",
                "Static Top n"
            ],
            [
                "Density Window",
                "Static Top n"
            ],
            [
                "WordNet"
            ],
            null,
            [
                "intersection",
                "union"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_6",
        "paper_id": "D16-1175",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1175table_7",
        "description": "Table 7 shows that composition by intersection with distributional inference considerably improves upon the best results for APT models without distributional inference and for untyped count-based models, and is competitive with the state-of-the-art neural network based models of Hashimoto et al. (2014) and Wieting et al. (2015). Distributional inference also improves upon the performance of an untyped VSM where composition by pointwise multiplication is outperforming the models of Mitchell and Lapata (2010), and Blacoe and Lapata (2012). Table 7 furthermore shows that DI has a smaller effect on the APT model based on composition by union and the untyped model based on composition by pointwise addition. The reason, as pointed out in the discussion for Table 5, is that the composition function has no disambiguating effect and thus cannot eliminate unrelated neighbours introduced by distributional inference. An intersective composition function on the other hand is able to perform the disambiguation locally in any given phrasal context. This furthermore suggests that for the APT model it is not necessary to explicitly model different word senses in separate vectors, as composition by intersection is able to disambiguate any word in context individually. Unlike the models of Hashimoto et al. (2014) and Wieting et al. (2015), the elementary word representations, as well as the representations for composed phrases and the composition process in our models are fully interpretable2.",
        "sentences": [
            "Table 7 shows that composition by intersection with distributional inference considerably improves upon the best results for APT models without distributional inference and for untyped count-based models, and is competitive with the state-of-the-art neural network based models of Hashimoto et al. (2014) and Wieting et al. (2015).",
            "Distributional inference also improves upon the performance of an untyped VSM where composition by pointwise multiplication is outperforming the models of Mitchell and Lapata (2010), and Blacoe and Lapata (2012).",
            "Table 7 furthermore shows that DI has a smaller effect on the APT model based on composition by union and the untyped model based on composition by pointwise addition.",
            "The reason, as pointed out in the discussion for Table 5, is that the composition function has no disambiguating effect and thus cannot eliminate unrelated neighbours introduced by distributional inference.",
            "An intersective composition function on the other hand is able to perform the disambiguation locally in any given phrasal context.",
            "This furthermore suggests that for the APT model it is not necessary to explicitly model different word senses in separate vectors, as composition by intersection is able to disambiguate any word in context individually.",
            "Unlike the models of Hashimoto et al. (2014) and Wieting et al. (2015), the elementary word representations, as well as the representations for composed phrases and the composition process in our models are fully interpretable2."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "APT \u2013 union",
                "Untyped VSM \u2013 addition"
            ],
            null,
            null,
            null,
            null
        ],
        "n_sentence": 7.0,
        "table_id": "table_7",
        "paper_id": "D16-1175",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1179table_2",
        "description": "Results. Using a standard logistic regression classifier, we achieve an 11% improvement in accuracy over the baseline approach, as can be seen in Table 2. The rule-based approach was insufficient for be and have VPE, where logistic regression provides the largest improvements. Although we improve upon the baseline by 29%, the accuracy achieved for beVPE is still low, this occurs mainly because: (i) be is the most commonly used auxiliary, so the number of negative examples is high compared to the number of positive examples, and, (ii) the analysis of the some of the false positives showed that there may have been genuine cases of VPE that were missed by the annotators of the dataset (Bos and Spenader, 2011). For example, this sentence (in file wsj 2057) was missed by the annotators (trigger in bold, an tecedent italicized) \u201cSome people tend to ignore that a 50-point move is less in percentage terms than it was when the stock market was lower.\u201d, here it is clear that was is a trigger for VPE.",
        "sentences": [
            "Results.",
            "Using a standard logistic regression classifier, we achieve an 11% improvement in accuracy over the baseline approach, as can be seen in Table 2.",
            "The rule-based approach was insufficient for be and have VPE, where logistic regression provides the largest improvements.",
            "Although we improve upon the baseline by 29%, the accuracy achieved for beVPE is still low, this occurs mainly because: (i) be is the most commonly used auxiliary, so the number of negative examples is high compared to the number of positive examples, and, (ii) the analysis of the some of the false positives showed that there may have been genuine cases of VPE that were missed by the annotators of the dataset (Bos and Spenader, 2011).",
            "For example, this sentence (in file wsj 2057) was missed by the annotators (trigger in bold, an tecedent italicized) \u201cSome people tend to ignore that a 50-point move is less in percentage terms than it was when the stock market was lower.\u201d, here it is clear that was is a trigger for VPE."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "ALL",
                "Change"
            ],
            [
                "Be",
                "Have"
            ],
            [
                "Be"
            ],
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_2",
        "paper_id": "D16-1179",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1179table_3",
        "description": "In Table 3, we compare our results to those achieved by Liu et al. (2016) when using WSJ sets 0-14 for training and sets 20-24 for testing. We improve on their overall accuracy by over 11%, due to the 25% improvement in recall achieved by our method. Our results show that oversampling the positive examples in the dataset and incorporating linguistically motivated syntactic features provide substantial gains for VPE detection. Additionally, we consider every instance of the word to as a potential trigger, while they do not - this lowers their recall because they miss every gold-standard instance of toVPE. Thus, not only do we improve upon the stateof-the-art accuracy, but we also expand the scope of VPE-detection to include to-VPE without causing a significant decrease in accuracy.",
        "sentences": [
            "In Table 3, we compare our results to those achieved by Liu et al. (2016) when using WSJ sets 0-14 for training and sets 20-24 for testing.",
            "We improve on their overall accuracy by over 11%, due to the 25% improvement in recall achieved by our method.",
            "Our results show that oversampling the positive examples in the dataset and incorporating linguistically motivated syntactic features provide substantial gains for VPE detection.",
            "Additionally, we consider every instance of the word to as a potential trigger, while they do not - this lowers their recall because they miss every gold-standard instance of toVPE.",
            "Thus, not only do we improve upon the stateof-the-art accuracy, but we also expand the scope of VPE-detection to include to-VPE without causing a significant decrease in accuracy."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            [
                "Liu et al. (2016)",
                "This work"
            ],
            [
                "R",
                "This work"
            ],
            null,
            null,
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_3",
        "paper_id": "D16-1179",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1179table_6",
        "description": "Trigger Detection. In Table 6 we can see that the syntactic features were essential for obtaining the best results, as can be seen by the 8.3% improvement, from 73.4% to 81.7%, obtained from including these features. This shows that notions from theoretical linguistics can prove to be invaluable when approaching the problem of VPE detection and that extracting these features in related problems may improve performance.",
        "sentences": [
            "Trigger Detection.",
            "In Table 6 we can see that the syntactic features were essential for obtaining the best results, as can be seen by the 8.3% improvement, from 73.4% to 81.7%, obtained from including these features.",
            "This shows that notions from theoretical linguistics can prove to be invaluable when approaching the problem of VPE detection and that extracting these features in related problems may improve performance."
        ],
        "class_sentence": [
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Syntactic",
                "F1",
                "NONE"
            ],
            null
        ],
        "n_sentence": 3.0,
        "table_id": "table_6",
        "paper_id": "D16-1179",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1179table_7",
        "description": "Antecedent Identification. Table 7 presents the results from a feature ablation study on antecedent\r\nidentification. The most striking observation is that the alignment features do not add any significant improvement in the results. This is either because there simply is not an inherent parallelism between the trigger site and the antecedent site, or because the other features represent the parallelism adequately without necessitating the addition of the alignment features. The heuristic syntactic features provide a large (10%) accuracy improvement when included. These results show that a dependency-based alignment approach to feature extraction does not represent the parallelism between the trigger and antecedent as well as features based on the lexical and syntactic properties of the two.",
        "sentences": [
            "Antecedent Identification.",
            "Table 7 presents the results from a feature ablation study on antecedent\r\nidentification.",
            "The most striking observation is that the alignment features do not add any significant improvement in the results.",
            "This is either because there simply is not an inherent parallelism between the trigger site and the antecedent site, or because the other features represent the parallelism adequately without necessitating the addition of the alignment features.",
            "The heuristic syntactic features provide a large (10%) accuracy improvement when included.",
            "These results show that a dependency-based alignment approach to feature extraction does not represent the parallelism between the trigger and antecedent as well as features based on the lexical and syntactic properties of the two."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "Alignment",
                "Accuracy"
            ],
            null,
            [
                "Syntactic",
                "Accuracy"
            ],
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_7",
        "paper_id": "D16-1179",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1181table_1",
        "description": "Table 1 summarizes 1-best supertagging results. Our baseline BLSTM model without attention achieves the same level of accuracy as Lewis et al. (2016) and the baseline BLSTM model of Vaswani et al. (2016). Compared with the latter, our hidden state size is 50% smaller (256 vs. 512). For training and testing the local attention model (BLSTM-local), we used an attention window size of 5 (tuned on the dev set), and it gives an improvement of 0.94% over the BRNN supertagger (Xu et al., 2016), achieving an accuracy on par with the beam-search (size 12) model of Vaswani et al. (2016) that is enhanced with a language model. Despite being able to consider wider contexts than the local model, the global attention model (BLSTMglobal) did not show further gains, hence we used BLSTM-local for all parsing experiments below.",
        "sentences": [
            "Table 1 summarizes 1-best supertagging results.",
            "Our baseline BLSTM model without attention achieves the same level of accuracy as Lewis et al. (2016) and the baseline BLSTM model of Vaswani et al. (2016).",
            "Compared with the latter, our hidden state size is 50% smaller (256 vs. 512).",
            "For training and testing the local attention model (BLSTM-local), we used an attention window size of 5 (tuned on the dev set), and it gives an improvement of 0.94% over the BRNN supertagger (Xu et al., 2016), achieving an accuracy on par with the beam-search (size 12) model of Vaswani et al. (2016) that is enhanced with a language model.",
            "Despite being able to consider wider contexts than the local model, the global attention model (BLSTMglobal) did not show further gains, hence we used BLSTM-local for all parsing experiments below."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "BLSTM",
                "Lewis et al. (2016)",
                "Vaswani et al. (2016)"
            ],
            null,
            [
                "BLSTM-local",
                "Xu et al. (2016)",
                "Vaswani et al. (2016)"
            ],
            [
                "BLSTM-global",
                "BLSTM-local"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_1",
        "paper_id": "D16-1181",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1181table_4",
        "description": "The XF1 model. Table 4 also shows the results for the XF1 models (LSTM-XF1), which use all four types of embeddings. We used a beam size of 8, and a ? value of 0.06 for both training and testing (tuned on the dev set), and training took 12 epochs to converge (Fig. 4b), with an F1 of 87.45% on the dev set. Decoding the XF1 model with greedy inference only slightly decreased recall and F1, and this resulted in a highly accurate deterministic parser. On the test set, our XF1 greedy model gives 2.67% F1 improvement over the greedy model in Xu et al. (2016), and the beam-search XF1 model achieves an F1 improvement of 1.34% compared with the XF1 model of Xu et al. (2016).",
        "sentences": [
            "The XF1 model.",
            "Table 4 also shows the results for the XF1 models (LSTM-XF1), which use all four types of embeddings.",
            "We used a beam size of 8, and a ? value of 0.06 for both training and testing (tuned on the dev set), and training took 12 epochs to converge (Fig. 4b), with an F1 of 87.45% on the dev set.",
            "Decoding the XF1 model with greedy inference only slightly decreased recall and F1, and this resulted in a highly accurate deterministic parser.",
            "On the test set, our XF1 greedy model gives 2.67% F1 improvement over the greedy model in Xu et al. (2016), and the beam-search XF1 model achieves an F1 improvement of 1.34% compared with the XF1 model of Xu et al. (2016)."
        ],
        "class_sentence": [
            0,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "LSTM-XF1"
            ],
            [
                "Beam"
            ],
            [
                "LSTM-greedy",
                "LR",
                "LF"
            ],
            [
                "LSTM-greedy",
                "LF",
                "Xu et al. (2014)"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_4",
        "paper_id": "D16-1181",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1182table_1",
        "description": "The overall performances of all parsers are shown in Table 1. Note that the Tweebo Parser\u2019s performance is not trained on the PTB because it is a specialization of the Turbo Parser, designed to parse tweets. Table 1 shows that, for both training conditions, the parser that has the best robustness score in the ESL domain has also high robustness for the MT domain. This suggests that it might be possible to build robust parsers for multiple ungrammatical domains. The training conditions do matter \u2013 Malt performs better when trained from Tweebank than from the PTB. In contrast, Tweebank is not a good fit with the neural network parsers due to its small size. Moreover, SNN uses pre-trained word embeddings, and 60% of Tweebank tokens are missing. Next, let us compare parsers within each train/test configuration for their relative robustness. When trained on the PTB, all parsers are comparably robust on ESL data, while they exhibit more differences on the MT data, and, as expected, everyone\u2019s performance is much lower because MT errors are more diverse than ESL errors. We expected that by training on Tweebank, parsers will perform better on ESL data (and maybe even MT data), since Tweebank is arguably more similar to the test domains than the PTB, we also expected Tweebo to outperform others. The results are somewhat surprising. On the one hand, the highest parser score increased from 93.72% (Turbo trained on PTB) to 94.36% (Malt trained on Tweebank), but the two neural network parsers performed significantly worse, most likely due to the small training size of Tweebank. Interestingly, although SyntaxNet has the lowest score on ESL, it has the highest score on MT, showing promise in its robustness.",
        "sentences": [
            "The overall performances of all parsers are shown in Table 1.",
            "Note that the Tweebo Parser\u2019s performance is not trained on the PTB because it is a specialization of the Turbo Parser, designed to parse tweets.",
            "Table 1 shows that, for both training conditions, the parser that has the best robustness score in the ESL domain has also high robustness for the MT domain.",
            "This suggests that it might be possible to build robust parsers for multiple ungrammatical domains.",
            "The training conditions do matter \u2013 Malt performs better when trained from Tweebank than from the PTB.",
            "In contrast, Tweebank is not a good fit with the neural network parsers due to its small size.",
            "Moreover, SNN uses pre-trained word embeddings, and 60% of Tweebank tokens are missing.",
            "Next, let us compare parsers within each train/test configuration for their relative robustness.",
            "When trained on the PTB, all parsers are comparably robust on ESL data, while they exhibit more differences on the MT data, and, as expected, everyone\u2019s performance is much lower because MT errors are more diverse than ESL errors.",
            "We expected that by training on Tweebank, parsers will perform better on ESL data (and maybe even MT data), since Tweebank is arguably more similar to the test domains than the PTB, we also expected Tweebo to outperform others.",
            "The results are somewhat surprising.",
            "On the one hand, the highest parser score increased from 93.72% (Turbo trained on PTB) to 94.36% (Malt trained on Tweebank), but the two neural network parsers performed significantly worse, most likely due to the small training size of Tweebank.",
            "Interestingly, although SyntaxNet has the lowest score on ESL, it has the highest score on MT, showing promise in its robustness."
        ],
        "class_sentence": [
            1,
            2,
            1,
            2,
            1,
            2,
            2,
            2,
            1,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "Parser"
            ],
            null,
            [
                "Parser",
                "ESL",
                "MT",
                "Robustness F1"
            ],
            null,
            [
                "Malt",
                "Train on Tweebanktrain",
                "Train on PTB \u00a71-21"
            ],
            [
                "Train on Tweebanktrain"
            ],
            [
                "SNN"
            ],
            null,
            [
                "Parser",
                "ESL"
            ],
            [
                "Train on Tweebanktrain",
                "ESL",
                "Tweebo",
                "Train on PTB \u00a71-21"
            ],
            null,
            [
                "Turbo",
                "Train on PTB \u00a71-21",
                "Malt",
                "Train on Tweebanktrain"
            ],
            [
                "SyntaxNet",
                "ESL",
                "MT"
            ]
        ],
        "n_sentence": 13.0,
        "table_id": "table_1",
        "paper_id": "D16-1182",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1183table_2",
        "description": "Table 2 shows the test results using our best performing model (ensemble with syntax features). We compare our approach to the CKY parser of Artzi et al. ,(2015) and JAMR (Flanigan et al., 2014). We also list the results of Wang et al.,(2015b), who demonstrated the benefit of auxiliary analyzers and is the current state of the art. Our performance is comparable to the CKY parser of (Artzi et al., 2015), which we use to bootstrap our system. This demonstrates the ability of our parser to match the performance of a dynamic-programming parser, which executes significantly more operations per sentence.",
        "sentences": [
            "Table 2 shows the test results using our best performing model (ensemble with syntax features).",
            "We compare our approach to the CKY parser of Artzi et al. ,(2015) and JAMR (Flanigan et al., 2014).",
            "We also list the results of Wang et al.,(2015b), who demonstrated the benefit of auxiliary analyzers and is the current state of the art.",
            "Our performance is comparable to the CKY parser of (Artzi et al., 2015), which we use to bootstrap our system.",
            "This demonstrates the ability of our parser to match the performance of a dynamic-programming parser, which executes significantly more operations per sentence."
        ],
        "class_sentence": [
            1,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "CKY (Artzi et al. 2015)",
                "JAMR",
                "Shift Reduce"
            ],
            [
                "Wang et al. (2015a)"
            ],
            [
                "CKY (Artzi et al. 2015)",
                "Shift Reduce"
            ],
            [
                "Shift Reduce"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_2",
        "paper_id": "D16-1183",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1184table_6",
        "description": "Table 6 shows the results. We use 3 versions of QueryParser. The first two use random word embedding for initialization, and the first one does not use label refinement. From the results, it can be concluded that QueryParser consistently outperformed competitors on query parsing task. Pretrained word2vec embeddings improve performance by 3-5 percent, and the postprocess of label refinement also improves the performance by 1-2 percent. Table 6 also shows that conventional depencency parsers trained on sentence dataset relies much more on the syntactic signals in the input. While Stanford parser and MSTParser have similar performance to our parser on Func dataset, the performance drops significantly on All and NoFunc dataset, when the majority of input has no function words.",
        "sentences": [
            "Table 6 shows the results.",
            "We use 3 versions of QueryParser.",
            "The first two use random word embedding for initialization, and the first one does not use label refinement.",
            "From the results, it can be concluded that QueryParser consistently outperformed competitors on query parsing task.",
            "Pretrained word2vec embeddings improve performance by 3-5 percent, and the postprocess of label refinement also improves the performance by 1-2 percent.",
            "Table 6 also shows that conventional depencency parsers trained on sentence dataset relies much more on the syntactic signals in the input.",
            "While Stanford parser and MSTParser have similar performance to our parser on Func dataset, the performance drops significantly on All and NoFunc dataset, when the majority of input has no function words."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "QueryParser + label refinement",
                "QueryParser + word2vec",
                "QueryParser + label refinement + word2vec"
            ],
            [
                "QueryParser + label refinement",
                "QueryParser + word2vec",
                "QueryParser + label refinement + word2vec"
            ],
            [
                "QueryParser + label refinement",
                "QueryParser + word2vec",
                "QueryParser + label refinement + word2vec",
                "System"
            ],
            [
                "QueryParser + label refinement",
                "QueryParser + word2vec",
                "QueryParser + label refinement + word2vec"
            ],
            [
                "Stanford",
                "MSTParser",
                "LSTMParser"
            ],
            [
                "Stanford",
                "MSTParser",
                "Func (n=100)",
                "All (n=1000)",
                "NoFunc (n=900)"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_6",
        "paper_id": "D16-1184",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1185table_2",
        "description": "Table 2 shows our experimental results comparing TREM and baseline models using descriptions. In general, contingency-based methods (TREM, TREM w/o SR and TREM w/o CC) outperform summarization-based methods. Our contingency assumptions are verified as adding CC and SC both improve TREM with summarization component only. Moreover, the best result is achieved by the complete TREM model with both contingency factors. It suggests that these two factors, modeling word-level summarization and sentence-level reconstruction, are complementary. From the summarization-based methods, we can see that our TREM-Summ gets higher ROUGE scores than two ILP approaches. Additionally, we note that the performance of ILP-Ext is poor. This is because ILP-Ext tends to output short sentences, while ROUGE is a recall-oriented measurement.",
        "sentences": [
            "Table 2 shows our experimental results comparing TREM and baseline models using descriptions.",
            "In general, contingency-based methods (TREM, TREM w/o SR and TREM w/o CC) outperform summarization-based methods.",
            "Our contingency assumptions are verified as adding CC and SC both improve TREM with summarization component only.",
            "Moreover, the best result is achieved by the complete TREM model with both contingency factors.",
            "It suggests that these two factors, modeling word-level summarization and sentence-level reconstruction, are complementary.",
            "From the summarization-based methods, we can see that our TREM-Summ gets higher ROUGE scores than two ILP approaches.",
            "Additionally, we note that the performance of ILP-Ext is poor.",
            "This is because ILP-Ext tends to output short sentences, while ROUGE is a recall-oriented measurement."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Our approach TREM",
                "w/o SR",
                "w/o CC"
            ],
            null,
            [
                "Our approach TREM"
            ],
            null,
            [
                "Our approach TREM",
                "ILP-Ext (Banerjee et al. 2015)",
                "ILP-Abs (Banerjee et al. 2015)",
                "ROUGE-1",
                "ROUGE-2",
                "ROUGE-SU4"
            ],
            [
                "ILP-Ext (Banerjee et al. 2015)"
            ],
            null
        ],
        "n_sentence": 8.0,
        "table_id": "table_2",
        "paper_id": "D16-1185",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1187table_3",
        "description": "Table 3 reports the spam and nonspam review detection accuracy of our methods SMTL-LLR and MTLLR against all other baseline methods. In terms of 5% significance level, the differences between SMTL-LLR and the baseline methods are considered to be statistically significant. Under symmetric multi-task learning setting, our methods SMTL-LLR and MTL-LR outperform all other baselines for identifying spam reviews from nonspam ones. MTL-LR achieves the average accuracy of 85.2% across the three domains, which is 3.1% and 3.4% better than LR and SVM trained in the single task learning setting, and 1.2% higher than MTRL. Training with a large quantity of unlabeled review data in addition to labeled ones, SMTL-LLR improves the performance of MTL-LR, and achieves the best average accuracy of 87.2% across the domains, which is 3.2% better than that of MTRL, and is 4.3% better than TSVM, a semi-supervised single task learning model. PU gives the worst performance, because learning only with partially labeled positive review data (spam) and unlabeled data may not generalize as well as other methods.",
        "sentences": [
            "Table 3 reports the spam and nonspam review detection accuracy of our methods SMTL-LLR and MTLLR against all other baseline methods.",
            "In terms of 5% significance level, the differences between SMTL-LLR and the baseline methods are considered to be statistically significant.",
            "Under symmetric multi-task learning setting, our methods SMTL-LLR and MTL-LR outperform all other baselines for identifying spam reviews from nonspam ones.",
            "MTL-LR achieves the average accuracy of 85.2% across the three domains, which is 3.1% and 3.4% better than LR and SVM trained in the single task learning setting, and 1.2% higher than MTRL.",
            "Training with a large quantity of unlabeled review data in addition to labeled ones, SMTL-LLR improves the performance of MTL-LR, and achieves the best average accuracy of 87.2% across the domains, which is 3.2% better than that of MTRL, and is 4.3% better than TSVM, a semi-supervised single task learning model.",
            "PU gives the worst performance, because learning only with partially labeled positive review data (spam) and unlabeled data may not generalize as well as other methods."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "SMTL-LLR",
                "MTL-LR",
                "MTRL",
                "TSVM",
                "LR",
                "SVM",
                "PU"
            ],
            [
                "SMTL-LLR",
                "MTL-LR",
                "MTRL",
                "TSVM",
                "LR",
                "SVM",
                "PU"
            ],
            [
                "MTL-LR",
                "LR",
                "SVM",
                "MTRL",
                "Average"
            ],
            [
                "SMTL-LLR",
                "MTL-LR",
                "Average",
                "MTRL",
                "TSVM"
            ],
            [
                "PU"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_3",
        "paper_id": "D16-1187",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1194table_5",
        "description": "As Table 5 shows, the best performance of annotators is highlighted and regarded as the upper bound performance (UB) of the NLD task on our dataset. The state-of-the-art unsupervised PD system named STS (Islam and Inkpen, 2008), as well as the state-of-the-art supervised PD system named RAE (Socher et al., 2011), are utilized to generate the baselines of the NLD task. STS uses the similarity score of 0.5 as the threshold to evaluate their method in the PD task. RAE applies supervised learning to classify a pair as a true or false instance of paraphrasing. These approaches are utilized on our evaluation as baselines for the NLD task.",
        "sentences": [
            "As Table 5 shows, the best performance of annotators is highlighted and regarded as the upper bound performance (UB) of the NLD task on our dataset.",
            "The state-of-the-art unsupervised PD system named STS (Islam and Inkpen, 2008), as well as the state-of-the-art supervised PD system named RAE (Socher et al., 2011), are utilized to generate the baselines of the NLD task.",
            "STS uses the similarity score of 0.5 as the threshold to evaluate their method in the PD task.",
            "RAE applies supervised learning to classify a pair as a true or false instance of paraphrasing.",
            "These approaches are utilized on our evaluation as baselines for the NLD task."
        ],
        "class_sentence": [
            1,
            2,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_5",
        "paper_id": "D16-1194",
        "valid": 0
    },
    {
        "table_id_paper": "D16-1194table_6",
        "description": "To assess the importance of each feature utilized in the proposed framework, we performed a feature ablation study (Cohen and Howe, 1988) on N-gram, POS analysis, lexical analysis (GTM and WordNet), and Flickr, separately on the DStest dataset. The results are listed in Table 6. A series of cross-validation and Student\u2019s t-tests are applied after running NLDS, STS, RAE, and UB methods on the F-measure metric. The tests reveal that the performance of NLDS is significantly better than STS and RAE, no significant differences could be found between UB and NLDS. These results demonstrate that NLDS would represent an effective approach for NLD that is on pair with annotator judgement and overcomes state-of-the-art approaches for related tasks.",
        "sentences": [
            "To assess the importance of each feature utilized in the proposed framework, we performed a feature ablation study (Cohen and Howe, 1988) on N-gram, POS analysis, lexical analysis (GTM and WordNet), and Flickr, separately on the DStest dataset.",
            "The results are listed in Table 6.",
            "A series of cross-validation and Student\u2019s t-tests are applied after running NLDS, STS, RAE, and UB methods on the F-measure metric.",
            "The tests reveal that the performance of NLDS is significantly better than STS and RAE, no significant differences could be found between UB and NLDS.",
            "These results demonstrate that NLDS would represent an effective approach for NLD that is on pair with annotator judgement and overcomes state-of-the-art approaches for related tasks."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "Uni-gram",
                "Bi-gram",
                "Tri-gram",
                "POS",
                "Lexical",
                "Flickr"
            ],
            null,
            [
                "NLDS",
                "STS",
                "RAE",
                "UB"
            ],
            [
                "NLDS",
                "STS",
                "RAE",
                "UB"
            ],
            [
                "NLDS"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_6",
        "paper_id": "D16-1194",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1196table_3",
        "description": "Comparison of Translation Units: Table 3 compares the BLEU scores for various translation systems. The orthographic syllable level system is clearly better than all other systems. It significantly outperforms the character-level system (by 46% on an average). The system also outperforms two strong baselines which address data sparsity: (a) a word-level system with transliteration of OOV words (10% improvement), (b) a morph-level system with transliteration of OOV words (5% improvement). The OS-level representation is more beneficial when morphologically rich\r\nlanguages are involved in translation. Significantly, OS-level translation is also the best system for translation between languages of different language families. The Le-BLEU scores also show the same trend as BLEU scores, but we have not reported it due to space limits. There are a very small number of untranslated OSes, which we handled by simple mapping of untranslated characters from source to target script. This barely increased translation accuracy (0.02% increase in BLEU score).",
        "sentences": [
            "Table 3 compares the BLEU scores for various translation systems.",
            "The orthographic syllable level system is clearly better than all other systems.",
            "It significantly outperforms the character-level system (by 46% on an average).",
            "The system also outperforms two strong baselines which address data sparsity: (a) a word-level system with transliteration of OOV words (10% improvement), (b) a morph-level system with transliteration of OOV words (5% improvement).",
            "The OS-level representation is more beneficial when morphologically rich\r\nlanguages are involved in translation.",
            "Significantly, OS-level translation is also the best system for translation between languages of different language families.",
            "The Le-BLEU scores also show the same trend as BLEU scores, but we have not reported it due to space limits.",
            "There are a very small number of untranslated OSes, which we handled by simple mapping of untranslated characters from source to target script.",
            "This barely increased translation accuracy (0.02% increase in BLEU score)."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            null
        ],
        "n_sentence": 9.0,
        "table_id": "table_3",
        "paper_id": "D16-1196",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1200table_3",
        "description": "In Table 3 we compare the binary classification model (Our System Binary) against the alignment model (Our System Alignment) and show that the latter outperforms the former by a margin of 3.2 points in F-score, achieving a micro F1-score of 28.58 across the three test corpora, thus confirming the benefits of joint inference. The only corpus in which joint inference did not help was Stock which has on average shorter event chains per document (Minard et al., 2015) and thus renders joint anchoring less likely to be useful.",
        "sentences": [
            "In Table 3 we compare the binary classification model (Our System Binary) against the alignment model (Our System Alignment) and show that the latter outperforms the former by a margin of 3.2 points in F-score, achieving a micro F1-score of 28.58 across the three test corpora, thus confirming the benefits of joint inference.",
            "The only corpus in which joint inference did not help was Stock which has on average shorter event chains per document (Minard et al., 2015) and thus renders joint anchoring less likely to be useful."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "Our System Binary",
                "Our System Alignment"
            ],
            null
        ],
        "n_sentence": 2.0,
        "table_id": "table_3",
        "paper_id": "D16-1200",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1204table_1",
        "description": "Comparison of the proposed techniques in Table 1 shows that Deep Fusion performs well on both METEOR and BLEU, incorporating Glove embeddings substantially increases METEOR, and combining them both does best. Our final model is an ensemble (weighted average) of the Glove, and the two Glove+Deep Fusion models trained on the external and in-domain COCO (Lin et al., 2014) sentences. We note here that the state-of-the-art on this dataset is achieved by HRNE (Pan et al., 2015) (METEOR 33.1) which proposes a superior visual processing pipeline using attention to encode the video. Human ratings also correlate well with the METEOR scores, confirming that our methods give a modest improvement in descriptive quality. However, incorporating linguistic knowledge significantly improves the grammaticality of the results, making them more comprehensible to human users.",
        "sentences": [
            "Comparison of the proposed techniques in Table 1 shows that Deep Fusion performs well on both METEOR and BLEU, incorporating Glove embeddings substantially increases METEOR, and combining them both does best.",
            "Our final model is an ensemble (weighted average) of the Glove, and the two Glove+Deep Fusion models trained on the external and in-domain COCO (Lin et al., 2014) sentences.",
            "We note here that the state-of-the-art on this dataset is achieved by HRNE (Pan et al., 2015) (METEOR 33.1) which proposes a superior visual processing pipeline using attention to encode the video.",
            "Human ratings also correlate well with the METEOR scores, confirming that our methods give a modest improvement in descriptive quality.",
            "However, incorporating linguistic knowledge significantly improves the grammaticality of the results, making them more comprehensible to human users."
        ],
        "class_sentence": [
            1,
            2,
            2,
            2,
            2
        ],
        "header_mention": [
            [
                "Deep Fusion",
                "METEOR",
                "B-4",
                "Glove+Deep",
                "Ensemble"
            ],
            [
                "Ensemble"
            ],
            null,
            [
                "Ensemble"
            ],
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_1",
        "paper_id": "D16-1204",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1207table_2",
        "description": "Table 2 presents the results of the cross-domain experiment, whereby we train a model on MR and test on CR, and vice versa, to measure the robustness of the different regularization methods in a more real-world setting. Once again, we see that our regularization method is superior to word-level dropout and the baseline CNN, and the techniques combined do very well, consistent with our findings for synthetic noise.",
        "sentences": [
            "Table 2 presents the results of the cross-domain experiment, whereby we train a model on MR and test on CR, and vice versa, to measure the robustness of the different regularization methods in a more real-world setting.",
            "Once again, we see that our regularization method is superior to word-level dropout and the baseline CNN, and the techniques combined do very well, consistent with our findings for synthetic noise."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Robust Regularization (lambda) = 10^-3",
                "Robust Regularization (lambda) = 10^-2",
                "Robust Regularization (lambda) = 10^-1",
                "Robust Regularization (lambda) = 1",
                "Dropout + Robust beta = 0.5 lambda = 10^-2"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_2",
        "paper_id": "D16-1207",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1210table_2",
        "description": "Table 2 shows the results of word alignment evaluations, where none denotes that the model has no constraint. In KFTT and BTEC Corpus, itg achieved significant improvement against sym and none on IBM Model 4 (p ? 0.05). However, in the Hansard Corpus, itg shows no improvement against sym. This indicates that capturing structural coherence by itg yields a significant benefit to word alignment in a linguistically different language pair such as Ja-En. For example, some function words appear more than once in both a source and target sentence, and they are not symmetrically aligned with each other, especially in regards to the Ja-En language pair. Although the baseline methods tend to be unable to align such long-distance word pairs, the proposed method can correctly catch them because itg can determine the relation of long-distance words. We discuss more details about the effectiveness of the ITG constraint in Section 4.1.",
        "sentences": [
            "Table 2 shows the results of word alignment evaluations, where none denotes that the model has no constraint.",
            "In KFTT and BTEC Corpus, itg achieved significant improvement against sym and none on IBM Model 4 (p ? 0.05).",
            "However, in the Hansard Corpus, itg shows no improvement against sym.",
            "This indicates that capturing structural coherence by itg yields a significant benefit to word alignment in a linguistically different language pair such as Ja-En.",
            "For example, some function words appear more than once in both a source and target sentence, and they are not symmetrically aligned with each other, especially in regards to the Ja-En language pair.",
            "Although the baseline methods tend to be unable to align such long-distance word pairs, the proposed method can correctly catch them because itg can determine the relation of long-distance words.",
            "We discuss more details about the effectiveness of the ITG constraint in Section 4.1."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            2,
            2,
            0
        ],
        "header_mention": [
            null,
            [
                "HMM+itg",
                "KFTT Ja-En",
                "BTEC Ja-En",
                "IBM Model 4+none",
                "IBM Model 4+sym"
            ],
            [
                "Hansard Fr-En",
                "HMM+itg",
                "IBM Model 4+itg",
                "IBM Model 4+sym"
            ],
            [
                "HMM+itg"
            ],
            null,
            null,
            null
        ],
        "n_sentence": 7.0,
        "table_id": "table_2",
        "paper_id": "D16-1210",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1220table_2",
        "description": "We then evaluated the best configuration from the cross-fold validation (N \\ s) and the three feature sets B, N and B \u222a N on the held-out test data. The results of this experiment reported in Table 2 are similar to the cross-fold evaluation, and in this case the contribution of N features is even more accentuated. Indeed, the absolute F1 of N and B ? N is slightly higher on test data, while the f-measure of B decreases slightly. This might be explained by the low-dimensionality of N, which makes it less prone to overfitting the training data. On test data, N \\ s is not found to outperform N. Interestingly, N \\ s is the only configuration having higher recall than precision. As shown by the feature ablation experiments, one of the main reasons for the performance difference between N and B is the ability of the former to model domain information. This finding can be further confirmed by inspecting the cases where B misclassifies metaphors that are correctly detected by N. Among these, we can find several examples including words that belong to domains often used as a metaphor source, such as \u201cgrist\u201d (domain: \u201cgastronomy\u201d) in \u201cAll is grist that comes to the mill\u201d, or \u201chorse\u201d (domain: \u201canimals\u201d) in \u201cYou can take a horse to the water , but you can\u2019t make him drink\u201d.",
        "sentences": [
            "We then evaluated the best configuration from the cross-fold validation (N \\ s) and the three feature sets B, N and B \u222a N on the held-out test data.",
            "The results of this experiment reported in Table 2 are similar to the cross-fold evaluation, and in this case the contribution of N features is even more accentuated.",
            "Indeed, the absolute F1 of N and B ? N is slightly higher on test data, while the f-measure of B decreases slightly.",
            "This might be explained by the low-dimensionality of N, which makes it less prone to overfitting the training data.",
            "On test data, N \\ s is not found to outperform N.",
            "Interestingly, N \\ s is the only configuration having higher recall than precision.",
            "As shown by the feature ablation experiments, one of the main reasons for the performance difference between N and B is the ability of the former to model domain information.",
            "This finding can be further confirmed by inspecting the cases where B misclassifies metaphors that are correctly detected by N.",
            "Among these, we can find several examples including words that belong to domains often used as a metaphor source, such as \u201cgrist\u201d (domain: \u201cgastronomy\u201d) in \u201cAll is grist that comes to the mill\u201d, or \u201chorse\u201d (domain: \u201canimals\u201d) in \u201cYou can take a horse to the water , but you can\u2019t make him drink\u201d."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            1,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            [
                "B#",
                "N*",
                "N \\ s*",
                "B \u222a N*"
            ],
            null,
            [
                "F",
                "N*",
                "B \u222a N*"
            ],
            null,
            [
                "N \\ s*",
                "N*"
            ],
            [
                "R",
                "P",
                "N \\ s*"
            ],
            null,
            null,
            null
        ],
        "n_sentence": 9.0,
        "table_id": "table_2",
        "paper_id": "D16-1220",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1231table_3",
        "description": "Table 3 shows the empirical benchmark results. The dynamic model achieves the best results in all the metrics. The static model outperforms the baseline, but is inferior to the dynamic model. In addressee selection (ADR), the baseline model achieves around 55% in accuracy. This means that if you select the agents that spoke most recently as an addressee, the half of them are correct. Compared with the baseline, our proposed models achieve better results, which suggests that the models can select the correct addressees that spoke at more previous time steps. In particular, the dynamic model achieves 68% in accuracy, which is 7 point higher than the accuracy of static model. In response selection (RES), our models outperform the baseline. Compared with the static model,the dynamic model achieves around 0.5 point higher in accuracy.",
        "sentences": [
            "Table 3 shows the empirical benchmark results.",
            "The dynamic model achieves the best results in all the metrics.",
            "The static model outperforms the baseline, but is inferior to the dynamic model.",
            "In addressee selection (ADR), the baseline model achieves around 55% in accuracy.",
            "This means that if you select the agents that spoke most recently as an addressee, the half of them are correct.",
            "Compared with the baseline, our proposed models achieve better results, which suggests that the models can select the correct addressees that spoke at more previous time steps.",
            "In particular, the dynamic model achieves 68% in accuracy, which is 7 point higher than the accuracy of static model.",
            "In response selection (RES), our models outperform the baseline.",
            "Compared with the static model,the dynamic model achieves around 0.5 point higher in accuracy."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Dynamic",
                "ADR-RES",
                "ADR",
                "RES"
            ],
            [
                "Baseline",
                "Static",
                "Dynamic"
            ],
            [
                "ADR",
                "Baseline"
            ],
            null,
            [
                "Baseline"
            ],
            [
                "Dynamic",
                "ADR",
                "Static"
            ],
            [
                "RES",
                "Baseline",
                "Dynamic",
                "Static"
            ],
            [
                "Static",
                "Dynamic",
                "RES"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_3",
        "paper_id": "D16-1231",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1231table_4",
        "description": "To shed light on the relationship between the model performance and the number of agents in multi-party conversation, we investigate the effect of the number of agents participating in each context. Table 4 compares the performance of the models for different numbers of agents in a context. In addressee selection, the performance of all models gradually gets worse as the number of agents in the context increases. However, compared with the baseline, our proposed models suppress the performance degradation. In particular, the dynamic model predicts correct addressees most robustly. In response selection, unexpectedly, the performance of all the models gets better as the number of agents increases. Detailed investigation on the interaction between the number of agents and the response selection complexity is an interesting line of future work.",
        "sentences": [
            "To shed light on the relationship between the model performance and the number of agents in multi-party conversation, we investigate the effect of the number of agents participating in each context.",
            "Table 4 compares the performance of the models for different numbers of agents in a context.",
            "In addressee selection, the performance of all models gradually gets worse as the number of agents in the context increases.",
            "However, compared with the baseline, our proposed models suppress the performance degradation.",
            "In particular, the dynamic model predicts correct addressees most robustly.",
            "In response selection, unexpectedly, the performance of all the models gets better as the number of agents increases.",
            "Detailed investigation on the interaction between the number of agents and the response selection complexity is an interesting line of future work."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            1,
            0
        ],
        "header_mention": [
            null,
            null,
            [
                "Baseline",
                "Static",
                "Dynamic",
                "No. of Agents",
                "ADR"
            ],
            [
                "Baseline",
                "Static",
                "Dynamic"
            ],
            [
                "Dynamic"
            ],
            [
                "Baseline",
                "Static",
                "Dynamic",
                "No. of Agents",
                "RES"
            ],
            null
        ],
        "n_sentence": 7.0,
        "table_id": "table_4",
        "paper_id": "D16-1231",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1237table_2",
        "description": "By comparing the rows labeled DA and DA +DS in Table 2 (a) and Table 2 (b), we see that in both the headlines and the images datasets, adding sentence level information improves the untyped score, lifting the stricter typed score F1. On the headlines dataset, incorporating sentence-level information degrades both the untyped and typed alignment quality because we cross-validated on the typed score metric. The typed score metric is the combination of untyped alignment, untyped score and typed alignment. From the row DA + DS in Table 2(a), we observe that the typed score F1 is slightly behind that of rank 1 system while all other three metrics are significantly better, indicating that we need to improve our modeling of the intersection of the three aspects. However, this does not apply to images dataset where the improvement on the typed score F1 comes from the typed alignment. Further, we see that even our base model that only depends on the alignment data offers strong alignment F1 scores. This validates the utility of jointly modeling alignments and chunk similarities. Adding sentence data to this already strong system leads to performance that is comparable to or better than the state-of-the-art systems. Indeed, our final results would have been ranked first on the images task and a close second on the headlines task in the official standings.",
        "sentences": [
            "By comparing the rows labeled DA and DA +DS in Table 2 (a) and Table 2 (b), we see that in both the headlines and the images datasets, adding sentence level information improves the untyped score, lifting the stricter typed score F1.",
            "On the headlines dataset, incorporating sentence-level information degrades both the untyped and typed alignment quality because we cross-validated on the typed score metric.",
            "The typed score metric is the combination of untyped alignment, untyped score and typed alignment.",
            "From the row DA + DS in Table 2(a), we observe that the typed score F1 is slightly behind that of rank 1 system while all other three metrics are significantly better, indicating that we need to improve our modeling of the intersection of the three aspects.",
            "However, this does not apply to images dataset where the improvement on the typed score F1 comes from the typed alignment.",
            "Further, we see that even our base model that only depends on the alignment data offers strong alignment F1 scores.",
            "This validates the utility of jointly modeling alignments and chunk similarities.",
            "Adding sentence data to this already strong system leads to performance that is comparable to or better than the state-of-the-art systems.",
            "Indeed, our final results would have been ranked first on the images task and a close second on the headlines task in the official standings."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            2,
            1,
            2,
            0,
            0
        ],
        "header_mention": [
            [
                "DA",
                "DA +DS",
                "untyped",
                "typed"
            ],
            [
                "Headline results",
                "untyped",
                "typed"
            ],
            [
                "typed"
            ],
            [
                "DA +DS",
                "score",
                "Rank 1"
            ],
            null,
            [
                "DA",
                "DA +DS"
            ],
            null,
            null,
            null
        ],
        "n_sentence": 9.0,
        "table_id": "table_2",
        "paper_id": "D16-1237",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1238table_1",
        "description": "We first compare our parser with state-of-the-art neural transition-based dependency parsers on PTB and CTB. For English, we also compare with stateof-the-art graph-based dependency parsers. The results are shown in Table 1. It can be seen that the BiAtt-DP outperforms all other graph-based parsers on PTB. Compared with the transition-based parsers, it achieves better accuracy than Chen and Manning (2014), which uses a feed-forward neural network, and Dyer et al. (2015), which uses three stack LSTM networks. Compared with the integrated parsing and tagging models, the BiAtt-DP outperforms Bohnet and Nivre (2012) but has a small gap to Alberti et al. (2015).",
        "sentences": [
            "We first compare our parser with state-of-the-art neural transition-based dependency parsers on PTB and CTB.",
            "For English, we also compare with state-of-the-art graph-based dependency parsers.",
            "The results are shown in Table 1.",
            "It can be seen that the BiAtt-DP outperforms all other graph-based parsers on PTB.",
            "Compared with the transition-based parsers, it achieves better accuracy than Chen and Manning (2014), which uses a feed-forward neural network, and Dyer et al. (2015), which uses three stack LSTM networks.",
            "Compared with the integrated parsing and tagging models, the BiAtt-DP outperforms Bohnet and Nivre (2012) but has a small gap to Alberti et al. (2015)."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "BiAtt-DP",
                "Graph"
            ],
            [
                "Trans.",
                "C&M (2014)",
                "Dyer et al. (2015)"
            ],
            [
                "BiAtt-DP",
                "B&N (2012)",
                "Alberti et al. (2015)"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_1",
        "paper_id": "D16-1238",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1238table_3",
        "description": "It can be observed from Table 3 that the BiAttDP has highly competitive parsing accuracy as stateof-the-art parsers. Moreover, it achieves best UAS for 5 out of 12 languages. For the remaining seven languages, the UAS gaps between the BiAtt-DP and state-of-the-art parsers are within 1.0%, except Swedish. An arguably fair comparison for the BiAttDP is the MSTParser (McDonald and Pereira, 2006), since the BiAtt-DP replaces the scoring function for arcs but uses exactly the same search algorithm. Due to the space limit, we refer readers to (Lei et al., 2014) for results of the MSTParsers (also shown in Appendix B). The BiAtt-DP consistently outperforms both parser by up to 5% absolute UAS score. Finally, following (Pitler and McDonald, 2015), we also analyze the performance of the BiAtt-DP on both crossed and uncrossed arcs. Since the BiAttDP uses a graph-based non-projective parsing algorithm, it is interesting to evaluate the performance on crossed arcs, which result in the non-projectivity of the dependency tree. The last three columns of Table 3 show the recall of crossed arcs, that of uncrossed arcs, and the percentage of crossed arcs in the test set. Pitler and McDonald (2015) reported numbers on the same data for Dutch, German, Portuguese, and Slovene as in this paper. For these four languages, the BiAtt-DP achieves better UAS than that reported in (Pitler and McDonald, 2015). More importantly, we observe that the improvement on recall of crossed arcs (around 10\u201318% absolutely) is much more significant than that of uncrossed arcs (around 1\u20133% absolutely), which indicates the effectiveness of the BiAtt-DP in parsing languages with non-projective trees.",
        "sentences": [
            "It can be observed from Table 3 that the BiAttDP has highly competitive parsing accuracy as stateof-the-art parsers.",
            "Moreover, it achieves best UAS for 5 out of 12 languages.",
            "For the remaining seven languages, the UAS gaps between the BiAtt-DP and state-of-the-art parsers are within 1.0%, except Swedish.",
            "An arguably fair comparison for the BiAttDP is the MSTParser (McDonald and Pereira, 2006), since the BiAtt-DP replaces the scoring function for arcs but uses exactly the same search algorithm.",
            "Due to the space limit, we refer readers to (Lei et al., 2014) for results of the MSTParsers (also shown in Appendix B).",
            "The BiAtt-DP consistently outperforms both parser by up to 5% absolute UAS score.",
            "Finally, following (Pitler and McDonald, 2015), we also analyze the performance of the BiAtt-DP on both crossed and uncrossed arcs.",
            "Since the BiAttDP uses a graph-based non-projective parsing algorithm, it is interesting to evaluate the performance on crossed arcs, which result in the non-projectivity of the dependency tree.",
            "The last three columns of Table 3 show the recall of crossed arcs, that of uncrossed arcs, and the percentage of crossed arcs in the test set.",
            "Pitler and McDonald (2015) reported numbers on the same data for Dutch, German, Portuguese, and Slovene as in this paper.",
            "For these four languages, the BiAtt-DP achieves better UAS than that reported in (Pitler and McDonald, 2015).",
            "More importantly, we observe that the improvement on recall of crossed arcs (around 10\u201318% absolutely) is much more significant than that of uncrossed arcs (around 1\u20133% absolutely), which indicates the effectiveness of the BiAtt-DP in parsing languages with non-projective trees."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            0,
            1,
            2,
            2,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "BiAtt-DP"
            ],
            [
                "BiAtt-DP",
                "Language"
            ],
            [
                "BiAtt-DP",
                "Best Published",
                "Swedish"
            ],
            [
                "BiAtt-DP"
            ],
            null,
            [
                "BiAtt-DP",
                "RBGParser",
                "Best Published"
            ],
            [
                "BiAtt-DP"
            ],
            null,
            [
                "Crossed",
                "Uncrossed",
                "%Crossed"
            ],
            null,
            [
                "BiAtt-DP",
                "Dutch",
                "German",
                "Portuguese",
                "Slovene"
            ],
            [
                "BiAtt-DP",
                "Crossed",
                "Uncrossed"
            ]
        ],
        "n_sentence": 12.0,
        "table_id": "table_3",
        "paper_id": "D16-1238",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1242table_4",
        "description": "Out of the 523 problems, 417 are Japanese translations of the FraCaS problems. Table 4 shows a comparison between the performance of our system on this subset of the JSeM problems and the performance of the RTE system for English in Mineshima et al. (2015) on the corresponding problems in the FraCaS dataset. Mineshima et al. (2015) used system parses of the English C&C parser (Clark and Curran, 2007). The total accuracy of our system is comparable to that of Mineshima et al. (2015). Most errors we found are due to syntactic parse errors caused by the CCG parser, where no correct syntactic parses were found in n-best responses. Comparison between gold parses and system parses shows that correct syntactic disambiguation improves performance.",
        "sentences": [
            "Out of the 523 problems, 417 are Japanese translations of the FraCaS problems.",
            "Table 4 shows a comparison between the performance of our system on this subset of the JSeM problems and the performance of the RTE system for English in Mineshima et al. (2015) on the corresponding problems in the FraCaS dataset.",
            "Mineshima et al. (2015) used system parses of the English C&C parser (Clark and Curran, 2007).",
            "The total accuracy of our system is comparable to that of Mineshima et al. (2015).",
            "Most errors we found are due to syntactic parse errors caused by the CCG parser, where no correct syntactic parses were found in n-best responses.",
            "Comparison between gold parses and system parses shows that correct syntactic disambiguation improves performance."
        ],
        "class_sentence": [
            0,
            1,
            2,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "M15"
            ],
            [
                "Gold",
                "M15"
            ],
            null,
            [
                "System",
                "Gold"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_4",
        "paper_id": "D16-1242",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1243table_2",
        "description": "Results. The top section of Table 2 shows development set results comparing modeling effectiveness for atomic  and sequence model architectures and different features. The Fourier feature transformation generally improves on raw HSV vectors and discretized embeddings. The value of modeling descriptions as sequences can also be observed in these results, the LSTM models consistently outperform their atomic counterparts.",
        "sentences": [
            "Results.",
            "The top section of Table 2 shows development set results comparing modeling effectiveness for atomic  and sequence model architectures and different features.",
            "The Fourier feature transformation generally improves on raw HSV vectors and discretized embeddings.",
            "The value of modeling descriptions as sequences can also be observed in these results, the LSTM models consistently outperform their atomic counterparts."
        ],
        "class_sentence": [
            0,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "Fourier"
            ],
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "D16-1243",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1247table_4",
        "description": "The results are shown in Table 4. The Proposed method achieved significantly better automatic evaluation scores than the Baseline for all the language pairs except the BLEU score of En \u2192 Ja direction. Also, the decoding time is reduced by about 60% relative to that of the Baseline. Our tree-based model is better than the conventional models except Zh \u2192 Ja, where the accuracy of Chinese parsing for the input sentences has a bad effect.",
        "sentences": [
            "The results are shown in Table 4.",
            "The Proposed method achieved significantly better automatic evaluation scores than the Baseline for all the language pairs except the BLEU score of En \u2192 Ja direction.",
            "Also, the decoding time is reduced by about 60% relative to that of the Baseline.",
            "Our tree-based model is better than the conventional models except Zh \u2192 Ja, where the accuracy of Chinese parsing for the input sentences has a bad effect."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Proposed",
                "En \u2192 Ja",
                "BLEU",
                "Baseline",
                "Ja \u2192 En",
                "Ja \u2192 Zh",
                "Zh \u2192 Ja"
            ],
            [
                "Proposed",
                "Time",
                "Baseline"
            ],
            [
                "Proposed",
                "Zh \u2192 Ja",
                "PBSMT",
                "Hiero"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_4",
        "paper_id": "D16-1247",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1249table_1",
        "description": "Experimental results in Table 1 show some interesting results. First, with the same alignment, J joint optimization works best than other optimization strategies (lines 3 to 6). Unfortunately, breaking down the network into two separate parts (A and T) and optimizing them separately do not help (lines 3 to 5). We have to conduct joint optimization J in order to get a comparable or better result (lines 3, 5 and 6) over the baseline system. Second, when we change the training alignment seeds (Zh \u2192 En, GDFA, and MaxEnt) NMT model does not yield significant different results (lines 6 to 8). Third, the smoothed transformation (J + Gau.) gives some improvements over the simple transformation (the last two lines), and achieves the best result (1.2 better than LVNMT, and 0.3 better than Tree-to-string). In terms of BLEU scores, we conduct the statistical significance tests with the signtest of Collins et al. (2005), the results show that the improvements of our J + Gau. over LVNMT are significant on three test sets (p < 0.01). At last, the brevity penalty (BP) consistently gets better after we add the alignment cost to NMT objective. Our alignment objective adjusts the translation length to be more in line with the human references accordingly.",
        "sentences": [
            "Experimental results in Table 1 show some interesting results.",
            "First, with the same alignment, J joint optimization works best than other optimization strategies (lines 3 to 6).",
            "Unfortunately, breaking down the network into two separate parts (A and T) and optimizing them separately do not help (lines 3 to 5).",
            "We have to conduct joint optimization J in order to get a comparable or better result (lines 3, 5 and 6) over the baseline system.",
            "Second, when we change the training alignment seeds (Zh \u2192 En, GDFA, and MaxEnt) NMT model does not yield significant different results (lines 6 to 8).",
            "Third, the smoothed transformation (J + Gau.) gives some improvements over the simple transformation (the last two lines), and achieves the best result (1.2 better than LVNMT, and 0.3 better than Tree-to-string).",
            "In terms of BLEU scores, we conduct the statistical significance tests with the signtest of Collins et al. (2005), the results show that the improvements of our J + Gau. over LVNMT are significant on three test sets (p < 0.01).",
            "At last, the brevity penalty (BP) consistently gets better after we add the alignment cost to NMT objective.",
            "Our alignment objective adjusts the translation length to be more in line with the human references accordingly."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Zh \u2192 En"
            ],
            [
                "A \u2192 J",
                "A \u2192 T",
                "A \u2192 T \u2192 J"
            ],
            [
                "A \u2192 J",
                "A \u2192 T \u2192 J",
                "J",
                "Cov. LVNMT (Mi et al. 2016b)",
                "Tree-to-string"
            ],
            [
                "+Alignment",
                "J"
            ],
            [
                "J + Gau.",
                "MaxEnt",
                "Tree-to-string",
                "Cov. LVNMT (Mi et al. 2016b)"
            ],
            [
                "BLEU",
                "J + Gau.",
                "Cov. LVNMT (Mi et al. 2016b)"
            ],
            [
                "BP",
                "+Alignment"
            ],
            [
                "+Alignment"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_1",
        "paper_id": "D16-1249",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1250table_1",
        "description": "The rows in Table 1 show, respectively, the results for the original embeddings, the basic mapping proposed by Mikolov et al. (2013b) (cf. Section 2) and the addition of orthogonality constraint (cf. Section 2.1), with and without length normalization and, incrementally, mean centering. In all the cases, length normalization and mean centering were applied to all embeddings, even if missing from the dictionary. The results show that the orthogonality constraint is key to preserve monolingual performance, and it also improves bilingual performance by enforcing a relevant property (monolingual invariance) that the transformation to learn should intuitively have. The contribution of length normalization alone is marginal, but when followed by mean centering we obtain further improvements in bilingual performance without hurting monolingual performance.",
        "sentences": [
            "The rows in Table 1 show, respectively, the results for the original embeddings, the basic mapping proposed by Mikolov et al. (2013b) (cf. Section 2) and the addition of orthogonality constraint (cf. Section 2.1), with and without length normalization and, incrementally, mean centering.",
            "In all the cases, length normalization and mean centering were applied to all embeddings, even if missing from the dictionary.",
            "The results show that the orthogonality constraint is key to preserve monolingual performance, and it also improves bilingual performance by enforcing a relevant property (monolingual invariance) that the transformation to learn should intuitively have.",
            "The contribution of length normalization alone is marginal, but when followed by mean centering we obtain further improvements in bilingual performance without hurting monolingual performance."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "Original embeddings",
                "Unconstrained mapping",
                "Orthogonal mapping",
                "+ length normalization",
                "+ mean centering"
            ],
            null,
            [
                "Orthogonal mapping",
                "+ length normalization",
                "+ mean centering",
                "EN-IT",
                "EN AN."
            ],
            [
                "+ length normalization",
                "+ mean centering",
                "EN-IT",
                "EN AN."
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_1",
        "paper_id": "D16-1250",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1250table_2",
        "description": "Table 2 shows the results for our best performing configuration in comparison to previous work. As discussed before, (Mikolov et al., 2013b) and (Xing et al., 2015) were implemented as part of our framework, so they correspond to our uncostrained mapping with no preprocessing and orthogonal mapping  with length normalization, respectively.",
        "sentences": [
            "Table 2 shows the results for our best performing configuration in comparison to previous work.",
            "As discussed before, (Mikolov et al., 2013b) and (Xing et al., 2015) were implemented as part of our framework, so they correspond to our uncostrained mapping with no preprocessing and orthogonal mapping  with length normalization, respectively."
        ],
        "class_sentence": [
            1,
            2
        ],
        "header_mention": [
            [
                "Our method",
                "Xing et al. (2015)",
                "Original embeddings",
                "Mikolov et al. (2013b)",
                "Faruqui and Dyer (2014)"
            ],
            [
                "Mikolov et al. (2013b)",
                "Xing et al. (2015)"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_2",
        "paper_id": "D16-1250",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1253table_2",
        "description": "Table 2 shows the results of MT N combining our BiSynData (denoted as MT Nbi) on the PDTB. STN means we train MT N with only the main task. On the macro F1, MT Nbi gains an improvement of 4.17% over ST N. The improvement is significant under one-tailed t-test (p<0.05). A closer look into the results shows that MT Nbi performs better across all relations, on the precision, recall and F1 score, except a little drop on the recall of Cont. The reason for the recall drop of Cont is not clear. The greatest improvement is observed on Comp, up to 6.36% F1 score. The possible reason is that only while is ambiguous about Comp and T emp, while as, when and since are all ambiguous about T emp and Cont, among top 10 connectives in our BiSynData. Meanwhile the amount of labeled data for Comp is relatively small. Overall, using BiSynData under our multi-task model achieves significant improvements on the English DRRimp. We believe the reasons for the improvements are twofold: 1) the added synthetic English instances from our BiSynData can alleviate the meaning shift problem, and 2) a multi-task learning method is helpful for addressing the different word distribution problem between implicit and explicit data.",
        "sentences": [
            "Table 2 shows the results of MT N combining our BiSynData (denoted as MT Nbi) on the PDTB.",
            "STN means we train MT N with only the main task.",
            "On the macro F1, MT Nbi gains an improvement of 4.17% over ST N.",
            "The improvement is significant under one-tailed t-test (p<0.05).",
            "A closer look into the results shows that MT Nbi performs better across all relations, on the precision, recall and F1 score, except a little drop on the recall of Cont.",
            "The reason for the recall drop of Cont is not clear.",
            "The greatest improvement is observed on Comp, up to 6.36% F1 score.",
            "The possible reason is that only while is ambiguous about Comp and T emp, while as, when and since are all ambiguous about T emp and Cont, among top 10 connectives in our BiSynData.",
            "Meanwhile the amount of labeled data for Comp is relatively small.",
            "Overall, using BiSynData under our multi-task model achieves significant improvements on the English DRRimp.",
            "We believe the reasons for the improvements are twofold: 1) the added synthetic English instances from our BiSynData can alleviate the meaning shift problem, and 2) a multi-task learning method is helpful for addressing the different word distribution problem between implicit and explicit data."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            1,
            2,
            1,
            2,
            2,
            1,
            2
        ],
        "header_mention": [
            [
                "MT Nbi"
            ],
            [
                "STN"
            ],
            [
                "F1",
                "MT Nbi"
            ],
            [
                "F1"
            ],
            [
                "MT Nbi",
                "P",
                "R",
                "F1",
                "Cont"
            ],
            [
                "Cont",
                "R"
            ],
            [
                "Comp",
                "F1"
            ],
            [
                "Comp",
                "Temp",
                "Cont"
            ],
            [
                "Comp"
            ],
            [
                "MT Nbi"
            ],
            null
        ],
        "n_sentence": 11.0,
        "table_id": "table_2",
        "paper_id": "D16-1253",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1255table_3",
        "description": "One proposed advantage of syntax in linearization models is that it can better capture long-distance relationships. Figure 1 shows results by sentence length and distortion, which is defined as the absolute difference between a token\u2019s index position in y? and y\u02c6, normalized by M. The LSTM model exhibits consistently better performance than existing syntax models across sentence lengths and generates fewer long-range distortions than the ZGEN model. Finally, Table 3 compares the syntactic fluency of the output. As a lightweight test, we parse the output with the Yara Parser (Rasooli and Tetreault, 2015) and compare the unlabeled attachment scores (UAS) to the trees produced by the syntactic system. We first align the gold head to each output token. (In cases where the alignment is not one-to-one, we randomly sample among the possibilities.) The models with no knowledge of syntax are able to recover a higher proportion of gold arcs.",
        "sentences": [
            "One proposed advantage of syntax in linearization models is that it can better capture long-distance relationships.",
            "Figure 1 shows results by sentence length and distortion, which is defined as the absolute difference between a token\u2019s index position in y? and y\u02c6, normalized by M. The LSTM model exhibits consistently better performance than existing syntax models across sentence lengths and generates fewer long-range distortions than the ZGEN model.",
            "Finally, Table 3 compares the syntactic fluency of the output.",
            "As a lightweight test, we parse the output with the Yara Parser (Rasooli and Tetreault, 2015) and compare the unlabeled attachment scores (UAS) to the trees produced by the syntactic system.",
            "We first align the gold head to each output token.",
            "The models with no knowledge of syntax are able to recover a higher proportion of gold arcs."
        ],
        "class_sentence": [
            0,
            2,
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null,
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_3",
        "paper_id": "D16-1255",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1260table_3",
        "description": "Relation prediction aims to predict relations given two entities. Evaluation results are shown in Table 3 on only YG15K due to limited space, where we report Hits@1 instead of Hits@10. Example prediction results for TransE and tTransE are compared in Table 4. For example, when testing (Billy Hughes,?,London,1862), it\u2019s easy for TransE to mix relations wasBornIn and diedIn because they act similarly for a person and a place. But known that (Billy Hughes, isAffiliatedTo, National Labor Party) happened in 1916, and tTransE have learnt temporal order that wasBornIn?isAffiliatedTo?diedIn, so the regularization term |r-born-T ? r-affiliated| is smaller than |r-died-T - r-aff iliated|, so correct answer wasBornIn ranks higher than diedIn.",
        "sentences": [
            "Relation prediction aims to predict relations given two entities.",
            "Evaluation results are shown in Table 3 on only YG15K due to limited space, where we report Hits@1 instead of Hits@10.",
            "Example prediction results for TransE and tTransE are compared in Table 4.",
            "For example, when testing (Billy Hughes,?,London,1862), it\u2019s easy for TransE to mix relations wasBornIn and diedIn because they act similarly for a person and a place.",
            "But known that (Billy Hughes, isAffiliatedTo, National Labor Party) happened in 1916, and tTransE have learnt temporal order that wasBornIn?isAffiliatedTo?diedIn, so the regularization term |r-born-T ? r-affiliated| is smaller than |r-died-T - r-aff iliated|, so correct answer wasBornIn ranks higher than diedIn."
        ],
        "class_sentence": [
            0,
            1,
            0,
            0,
            0
        ],
        "header_mention": [
            null,
            [
                "Hits@1 (%)"
            ],
            null,
            null,
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_3",
        "paper_id": "D16-1260",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1260table_5",
        "description": "Results. Table 5 reports the results on the test sets. The results indicate that time-aware embedding outperforms all the baselines consistently. Temporal order information may help to distinguish positive and negative triples as different head entities may have different temporally associated relations. If the temporal order is the same with most facts, the regularization term helps it get lower energies and vice versa.",
        "sentences": [
            "Results.",
            "Table 5 reports the results on the test sets.",
            "The results indicate that time-aware embedding outperforms all the baselines consistently.",
            "Temporal order information may help to distinguish positive and negative triples as different head entities may have different temporally associated relations.",
            "If the temporal order is the same with most facts, the regularization term helps it get lower energies and vice versa."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_5",
        "paper_id": "D16-1260",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1262table_4",
        "description": "Table 4 compares the different violation-based learning objectives, as discussed in Section 5. Our novel all-violation updates outperform the alternatives. We attribute this improvement to the robustness over poor search spaces, which the greedy update lacks, and the incentive to explore good parses early, which the max-violation update lacks. Learning curves in Figure 5 show that the all-violations update also converges more quickly.",
        "sentences": [
            "Table 4 compares the different violation-based learning objectives, as discussed in Section 5.",
            "Our novel all-violation updates outperform the alternatives.",
            "We attribute this improvement to the robustness over poor search spaces, which the greedy update lacks, and the incentive to explore good parses early, which the max-violation update lacks.",
            "Learning curves in Figure 5 show that the all-violations update also converges more quickly."
        ],
        "class_sentence": [
            1,
            1,
            0,
            0
        ],
        "header_mention": [
            null,
            [
                "All-violations",
                "Update"
            ],
            null,
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_4",
        "paper_id": "D16-1262",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1264table_5",
        "description": "Table 5 shows the performance of our models alongside human performance on the v1.0 of development and test sets. The logistic regression model significantly outperforms the baselines, but underperforms humans. We note that the model is able to select the sentence containing the answer correctly with 79.3% accuracy, hence, the bulk of the difficulty lies in finding the exact span within the sentence.",
        "sentences": [
            "Table 5 shows the performance of our models alongside human performance on the v1.0 of development and test sets.",
            "The logistic regression model significantly outperforms the baselines, but underperforms humans.",
            "We note that the model is able to select the sentence containing the answer correctly with 79.3% accuracy, hence, the bulk of the difficulty lies in finding the exact span within the sentence."
        ],
        "class_sentence": [
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Logistic Regression",
                "Random Guess",
                "Sliding Window",
                "Sliding Win. + Dist.",
                "Human"
            ],
            null
        ],
        "n_sentence": 3.0,
        "table_id": "table_5",
        "paper_id": "D16-1264",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1001table_3",
        "description": "Table 3 shows the performance on the test set for variations of our method and that of the human annotators. The last column shows the percentage of pairs where a root pair is reached to be aligned, called reachability. Our method is denoted as Proposed, while its variations include a method with only monotonic alignment (monotonic), without EM (w/o EM), and a method aligning only 1-best trees (1-best tree). The performance of the human annotators was assessed by considering one annotator as the test and the other two as the gold-standard, and then taking the averages, which is the same setting as our method. We regard this as the pseudo inter annotator agreement, since the conventional interannotator agreement is not directly applicable due to variations in aligned phrases. Our method significantly outperforms the others as it achieved the highest recall and precision for alignment quality. Our recall and precision reach 92% and 89% of those of humans, respectively. Non-compositional alignment is shown to contribute to alignment quality, while the feature enhanced EM is effective for both the alignment and parsing quality. Comparing our method and the one aligning only 1-best trees demonstrates that the alignment of parse forests largely contributes to the alignment quality. Although we confirmed that aligning larger forests slightly improved recall and precision, the improvements were statistically insignificant. The parsing quality was not much affected by phrase alignment, which is further investigated in the following. Finally, our method achieved 98% reachability, where 2% of unreachable cases were due to the beam search. While understanding that the reachability depends on experimental data, ours is notably higher than that of SCFG, reported as 9.1% in (Weese et al., 2014). These results show the ability of our method to accurately align paraphrases with divergent phrase correspondences.",
        "sentences": [
            "Table 3 shows the performance on the test set for variations of our method and that of the human annotators.",
            "The last column shows the percentage of pairs where a root pair is reached to be aligned, called reachability.",
            "Our method is denoted as Proposed, while its variations include a method with only monotonic alignment (monotonic), without EM (w/o EM), and a method aligning only 1-best trees (1-best tree).",
            "The performance of the human annotators was assessed by considering one annotator as the test and the other two as the gold-standard, and then taking the averages, which is the same setting as our method.",
            "We regard this as the pseudo inter annotator agreement, since the conventional interannotator agreement is not directly applicable due to variations in aligned phrases.",
            "Our method significantly outperforms the others as it achieved the highest recall and precision for alignment quality.",
            "Our recall and precision reach 92% and 89% of those of humans, respectively.",
            "Non-compositional alignment is shown to contribute to alignment quality, while the feature enhanced EM is effective for both the alignment and parsing quality.",
            "Comparing our method and the one aligning only 1-best trees demonstrates that the alignment of parse forests largely contributes to the alignment quality.",
            "Although we confirmed that aligning larger forests slightly improved recall and precision, the improvements were statistically insignificant.",
            "The parsing quality was not much affected by phrase alignment, which is further investigated in the following.",
            "Finally, our method achieved 98% reachability, where 2% of unreachable cases were due to the beam search.",
            "While understanding that the reachability depends on experimental data, ours is notably higher than that of SCFG, reported as 9.1% in (Weese et al., 2014).",
            "These results show the ability of our method to accurately align paraphrases with divergent phrase correspondences."
        ],
        "class_sentence": [
            1,
            2,
            1,
            2,
            2,
            1,
            1,
            2,
            1,
            1,
            2,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "%"
            ],
            [
                "Proposed",
                "Monotonic",
                "w/o EM",
                "1-best tree"
            ],
            [
                "Human"
            ],
            null,
            [
                "Proposed",
                "Recall",
                "Prec."
            ],
            [
                "Recall",
                "Prec.",
                "Human"
            ],
            [
                "UAS"
            ],
            [
                "Proposed",
                "1-best tree",
                "UAS"
            ],
            [
                "1-best tree",
                "Recall",
                "Prec."
            ],
            null,
            [
                "Proposed",
                "%"
            ],
            [
                "Proposed",
                "%"
            ],
            [
                "Proposed"
            ]
        ],
        "n_sentence": 14.0,
        "table_id": "table_3",
        "paper_id": "D17-1001",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1004table_4",
        "description": "Table 4 summarizes our results. We observe that all neural models achieve higher F1 scores than the logistic regression and patterns systems, which demonstrates the effectiveness of neural models for relation extraction. Although positional embeddings help increase the F1 by around 2% over the plain CNN model, a simple (2-layer) LSTM model performs surprisingly better than CNN and dependency-based models. Lastly, our proposed position-aware mechanism is very effective and achieves an F1 score of 65.4%, with an absolute increase of 3.9% over the best baseline neural model (LSTM) and 7.9% over the baseline logistic regression system. We also run an ensemble of our position-aware attention model which takes majority votes from 5 runs with random initializations and it further pushes the F1 score up by 1.6%.",
        "sentences": [
            "Table 4 summarizes our results.",
            "We observe that all neural models achieve higher F1 scores than the logistic regression and patterns systems, which demonstrates the effectiveness of neural models for relation extraction.",
            "Although positional embeddings help increase the F1 by around 2% over the plain CNN model, a simple (2-layer) LSTM model performs surprisingly better than CNN and dependency-based models.",
            "Lastly, our proposed position-aware mechanism is very effective and achieves an F1 score of 65.4%, with an absolute increase of 3.9% over the best baseline neural model (LSTM) and 7.9% over the baseline logistic regression system.",
            "We also run an ensemble of our position-aware attention model which takes majority votes from 5 runs with random initializations and it further pushes the F1 score up by 1.6%."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Neural",
                "F1",
                "LR + Patterns"
            ],
            [
                "CNN-PE",
                "F1",
                "CNN",
                "LSTM",
                "SDP-LSTM"
            ],
            [
                "Our model",
                "F1",
                "LSTM",
                "LR"
            ],
            [
                "Ensemble",
                "Our model",
                "F1"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_4",
        "paper_id": "D17-1004",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1004table_5",
        "description": "Table 5 presents our results. We find that: (1) by only training our logistic regression model on TACRED (in contrast to on the 2 million bootstrapped examples used in the 2015 Stanford system) and combining it with patterns, we obtain a higher hop-0 F1 score than the 2015 Stanford system, and a similar hop-all F1; (2) our proposed position-aware attention model substantially outperforms the 2015 Stanford system on all hop-0, hop-1 and hop-all F1 scores. Combining it with the patterns, we achieve a hop-all F1 of 26.7%, an absolute improvement of 4.5% over the previous state-of-the-art result.",
        "sentences": [
            "Table 5 presents our results.",
            "We find that: (1) by only training our logistic regression model on TACRED (in contrast to on the 2 million bootstrapped examples used in the 2015 Stanford system) and combining it with patterns, we obtain a higher hop-0 F1 score than the 2015 Stanford system, and a similar hop-all F1; (2) our proposed position-aware attention model substantially outperforms the 2015 Stanford system on all hop-0, hop-1 and hop-all F1 scores.",
            "Combining it with the patterns, we achieve a hop-all F1 of 26.7%, an absolute improvement of 4.5% over the previous state-of-the-art result."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "LR trained on TACRED + Patterns",
                "Hop-all",
                "F1",
                "Our model",
                "Hop-0",
                "Hop-1"
            ],
            [
                "Our model + Patterns",
                "Hop-all",
                "F1"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_5",
        "paper_id": "D17-1004",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1006table_3",
        "description": "5.4 Final Results Table 3 shows the final results on the G&C 16 and C&J08 datasets, respectively. We compare the results of our final model with the following baselines:. \u2022 PMI is the co-occurrence based model of Chambers and Jurafsky (2008), who calculate event pair relations based on Pointwise Mutual Information (PMI), scoring each candidate event ec by the sum of PMI scores between the given events e0, e1, ..., en\u22121 and the candidate. \u2022 Bigram is the counting based model of Jans et al. (2012), calculating event pair relations based on skip bigram probabilities, trained using maximum likelihood estimation. \u2022 Event-Comp is the neural event relation model proposed by Granroth-Wilding and Clark (2016). They learn event representations by calculating pair-wise event scores using a Siamese network. \u2022 RNN is the method of Pichotta and Mooney (2016), who model event chains by directly using hc in Section 4.2 to predict the output, rather than taking them as features for event pair relation modeling. \u2022 MemNet is the proposed deep memory network model. Our reimplementation of PMI and Bigrams follows (Granroth-Wilding and Clark, 2016). It can be seen from the table that the statistical counting-based models PMI and Bigram significantly underperform the neural network models Event-Comp, RNN and MemNet, which is largely due to their sparsity and lack of semantic representation power. Under our event representation, Bigram does not outperform PMI significantly either, although considering the order of event pairs. This is likely due to sparsity of events when all arguments are considered. Direct comparison between Event-Comp and RNN shows that the event-pair model gives comparable results to the strong-order LSTM model. Although Granroth-Wilding and Clark (2016) and Pichotta and Mooney (2016) both compared with statistical baselines, they did not make direct comparisons between their methods, which represent two different approaches to the task. Our results show that they each have their unique advantages, which confirm our intuition in the introduction. By considering both pairwise relations and chain temporal orders, our method significantly outperform both Event-Comp and RNN (p \u2212 value < 0.01 using t-test), giving the best reported results on both datasets.",
        "sentences": [
            "5.4 Final Results Table 3 shows the final results on the G&C 16 and C&J08 datasets, respectively.",
            "We compare the results of our final model with the following baselines:.",
            "\u2022 PMI is the co-occurrence based model of Chambers and Jurafsky (2008), who calculate event pair relations based on Pointwise Mutual Information (PMI), scoring each candidate event ec by the sum of PMI scores between the given events e0, e1, ..., en\u22121 and the candidate.",
            "\u2022 Bigram is the counting based model of Jans et al. (2012), calculating event pair relations based on skip bigram probabilities, trained using maximum likelihood estimation.",
            "\u2022 Event-Comp is the neural event relation model proposed by Granroth-Wilding and Clark (2016). They learn event representations by calculating pair-wise event scores using a Siamese network.",
            "\u2022 RNN is the method of Pichotta and Mooney (2016), who model event chains by directly using hc in Section 4.2 to predict the output, rather than taking them as features for event pair relation modeling.",
            "\u2022 MemNet is the proposed deep memory network model.",
            "Our reimplementation of PMI and Bigrams follows (Granroth-Wilding and Clark, 2016).",
            "It can be seen from the table that the statistical counting-based models PMI and Bigram significantly underperform the neural network models Event-Comp, RNN and MemNet, which is largely due to their sparsity and lack of semantic representation power.",
            "Under our event representation, Bigram does not outperform PMI significantly either, although considering the order of event pairs.",
            "This is likely due to sparsity of events when all arguments are considered.",
            "Direct comparison between Event-Comp and RNN shows that the event-pair model gives comparable results to the strong-order LSTM model.",
            "Although Granroth-Wilding and Clark (2016) and Pichotta and Mooney (2016) both compared with statistical baselines, they did not make direct comparisons between their methods, which represent two different approaches to the task.",
            "Our results show that they each have their unique advantages, which confirm our intuition in the introduction.",
            "By considering both pairwise relations and chain temporal orders, our method significantly outperform both Event-Comp and RNN (p \u2212 value < 0.01 using t-test), giving the best reported results on both datasets."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            2,
            2,
            2,
            2,
            1,
            1,
            2,
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            [
                "G&C16",
                "C&J08"
            ],
            null,
            [
                "PMI"
            ],
            [
                "Bigram"
            ],
            [
                "Event-Comp"
            ],
            [
                "RNN"
            ],
            [
                "MemNet"
            ],
            [
                "PMI",
                "Bigram"
            ],
            [
                "PMI",
                "Bigram",
                "Event-Comp",
                "RNN",
                "MemNet"
            ],
            [
                "Bigram",
                "PMI"
            ],
            null,
            [
                "Event-Comp",
                "RNN",
                "MemNet"
            ],
            null,
            null,
            [
                "MemNet",
                "Event-Comp",
                "RNN"
            ]
        ],
        "n_sentence": 15.0,
        "table_id": "table_3",
        "paper_id": "D17-1006",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1214table_1",
        "description": "Before exploring unsupervised pre-training, we present summary results for SWEAR in a fully supervised setting, for comparison to previous work on the WikiReading task, namely that of Hewlett et al.(2016) and Choi et al.(2017), which we refer to as HE16 and CH17 in tables. Table 1 shows that SWEAR outperforms the best, results for various models reported in both publications, including the hierarchical models SoftAttend and Reinforce presented by Choi et al. (2017). Interestingly, SoftAttend computes an attention over sentence encodings, analogous to SWEAR\u2019s attention over overlapping window encodings, but it does so on the basis of less powerful encoders (BoW or convolution vs RNN), suggesting that the extra computation spent by the RNN provides a meaningful boost to performance. For further experiments, results, and discussion see Section 5.2.",
        "sentences": [
            "Before exploring unsupervised pre-training, we present summary results for SWEAR in a fully supervised setting, for comparison to previous work on the WikiReading task, namely that of Hewlett et al.(2016) and Choi et al.(2017), which we refer to as HE16 and CH17 in tables.",
            "Table 1 shows that SWEAR outperforms the best, results for various models reported in both publications, including the hierarchical models SoftAttend and Reinforce presented by Choi et al. (2017).",
            "Interestingly, SoftAttend computes an attention over sentence encodings, analogous to SWEAR\u2019s attention over overlapping window encodings, but it does so on the basis of less powerful encoders (BoW or convolution vs RNN), suggesting that the extra computation spent by the RNN provides a meaningful boost to performance.",
            "For further experiments, results, and discussion see Section 5.2."
        ],
        "class_sentence": [
            2,
            1,
            1,
            0
        ],
        "header_mention": [
            [
                "SWEAR",
                "SWEAR (w/ zeros)",
                "Placeholder seq2seq (HE16)",
                "SoftAttend (CH17)",
                "Reinforce (CH17)",
                "Placeholder seq2seq (CH17)"
            ],
            [
                "SWEAR",
                "SoftAttend (CH17)",
                "Reinforce (CH17)"
            ],
            [
                "SWEAR",
                "SoftAttend (CH17)",
                "Mean F1"
            ],
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_1",
        "paper_id": "D17-1214",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1214table_2",
        "description": "To quantify the effect of initializing the window encoder with the question state, we report results for two variants of SWEAR: In SWEAR the window encoder is initialized with the question encoding, while in SWEAR w/ zeros, the window encoder is initialized with zeros. In both cases the question encoding is used for attention over the window encodings. For SWEAR w/ zeros it is additionally concatenated with the document encoding and passed through a 2-layer fully connected neural network before the decoding step. Conditioning on the question increases Mean F1 by 0.4. Hewlett et al.(2016) grouped properties by answer distribution: Categorical properties have a small list of possible answers,  such as countries, Relational properties have an open set of answers, such as spouses or places of birth, and Date properties (a subset of relational properties) have date answers, such as date of birth. We reproduce this grouping in Table 2 to show that SWEAR improves performance for Relational and Date properties, demonstrating that it is better able to extract precise information from documents.",
        "sentences": [
            "To quantify the effect of initializing the window encoder with the question state, we report results for two variants of SWEAR: In SWEAR the window encoder is initialized with the question encoding, while in SWEAR w/ zeros, the window encoder is initialized with zeros.",
            "In both cases the question encoding is used for attention over the window encodings.",
            "For SWEAR w/ zeros it is additionally concatenated with the document encoding and passed through a 2-layer fully connected neural network before the decoding step.",
            "Conditioning on the question increases Mean F1 by 0.4.",
            "Hewlett et al.(2016) grouped properties by answer distribution: Categorical properties have a small list of possible answers,  such as countries, Relational properties have an open set of answers, such as spouses or places of birth, and Date properties (a subset of relational properties) have date answers, such as date of birth.",
            "We reproduce this grouping in Table 2 to show that SWEAR improves performance for Relational and Date properties, demonstrating that it is better able to extract precise information from documents."
        ],
        "class_sentence": [
            0,
            0,
            0,
            0,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            [
                "Categorical",
                "Relational",
                "Date",
                "HE16 Best"
            ],
            [
                "SWEAR",
                "HE16 Best",
                "Relational",
                "Date"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_2",
        "paper_id": "D17-1214",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1214table_4",
        "description": "Table 4 and 5 show the results of SWEAR and semi-supervised models with pretrained and fixed embeddings. Results show that SWEAR-SS always improves over SWEAR at small data sizes, with the difference become dramatic as the dataset becomes very small. VRAE pretraining yields the best performance. As training and testing datasets have different distributions in perproperty subsets, Mean F1 for supervised and semi-supervised models drops compared to uniform sampling. However, initialization with pretrained VRAE model leads to a substantial improvement on both subsamples. We further experimented by initializing the decoder (vs. only the encoder) with pretrained autoencoder weights but this resulted in a lower Mean F1.",
        "sentences": [
            "Table 4 and 5 show the results of SWEAR and semi-supervised models with pretrained and fixed embeddings.",
            "Results show that SWEAR-SS always improves over SWEAR at small data sizes, with the difference become dramatic as the dataset becomes very small.",
            "VRAE pretraining yields the best performance.",
            "As training and testing datasets have different distributions in perproperty subsets, Mean F1 for supervised and semi-supervised models drops compared to uniform sampling.",
            "However, initialization with pretrained VRAE model leads to a substantial improvement on both subsamples.",
            "We further experimented by initializing the decoder (vs. only the encoder) with pretrained autoencoder weights but this resulted in a lower Mean F1."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "SWEAR-SS (RAE)",
                "SWEAR-SS (VRAE)",
                "SWEAR"
            ],
            [
                "SWEAR-SS (VRAE)"
            ],
            [
                "SWEAR",
                "0.1%"
            ],
            [
                "SWEAR-SS (VRAE)",
                "1%",
                "0.5%"
            ],
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_4",
        "paper_id": "D17-1214",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1214table_6",
        "description": "Table 6 shows the results of semi-supervised reviewer models. When trained on 1% of the training data, SWEAR-MLR and the supervised SWEAR model perform similarly. Without using skip connections between embedding and hidden layers, the performance drops. The SWEARPR model further improves Mean F1 and outperforms the strongest SWEAR-SS model,  even without fine-tuning the weights initialized from the autoencoder.",
        "sentences": [
            "Table 6 shows the results of semi-supervised reviewer models.",
            "When trained on 1% of the training data, SWEAR-MLR and the supervised SWEAR model perform similarly.",
            "Without using skip connections between embedding and hidden layers, the performance drops.",
            "The SWEARPR model further improves Mean F1 and outperforms the strongest SWEAR-SS model,  even without fine-tuning the weights initialized from the autoencoder."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "SWEAR-MLR",
                "SWEAR-PR",
                "Mean F1"
            ],
            [
                "w/o skip connections",
                "Mean F1"
            ],
            [
                "SWEAR-PR",
                "Mean F1"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_6",
        "paper_id": "D17-1214",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1215table_5",
        "description": "Table  5  shows  the  results  of  evaluating  the four  main  models  on  adversarial  examples  generated by running either ADDSENT or ADDANY against  each  model. ADDSENT adversarial examples transfer between models quite effectively; in particular, they are harder than ADDONESENT examples, which implies that examples that fool one model are more likely to fool other models. The ADDANY adversarial examples exhibited more limited transferability between models.  For both  ADDSENT and  ADDANY,  examples  transferred slightly better between single and ensemble versions of the same model.",
        "sentences": [
            "Table  5  shows  the  results  of  evaluating  the four  main  models  on  adversarial  examples  generated by running either ADDSENT or ADDANY against  each  model.",
            "ADDSENT adversarial examples transfer between models quite effectively; in particular, they are harder than ADDONESENT examples, which implies that examples that fool one model are more likely to fool other models.",
            "The ADDANY adversarial examples exhibited more limited transferability between models.",
            " For both  ADDSENT and  ADDANY,  examples  transferred slightly better between single and ensemble versions of the same model."
        ],
        "class_sentence": [
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            [
                "ADDSENT",
                "ADDANY"
            ],
            [
                "ADDSENT"
            ],
            [
                "ADDANY"
            ],
            [
                "ML Single",
                "BiDAF Single",
                "ML Ens.",
                "BiDAF Ens."
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_5",
        "paper_id": "D17-1215",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1216table_4",
        "description": "Table 4 shows the results. From this table, we can see that :. 1) Our model outperforms all baselines significantly. Compared with baselines, the accuracy improvement on test set is at least 13.7%. This demonstrates the effectiveness of our model by mining and exploiting heteregenous knowledge. 2) The event narrative knowledge only is insufficient for commonsense machine comprehension. Compared with Narrative Event Chain Model, our model achieves a 16.3% accuracy improvement by considering richer commonsense knowledge, rather than only narrative event knowledge. 3) It is necessary to distinguish different kinds of commonsense relations for machine comprehension and commonsense reasoning.  Compared with DSSM and RNN, which model all relations  between  two  elements  using  a  single  semantic similarity score, our model achieves significant accuracy improvements by modeling, distinguishing and selecting different types of commonsense relations between  different kinds of elements.",
        "sentences": [
            "Table 4 shows the results.",
            "From this table, we can see that :.",
            "1) Our model outperforms all baselines significantly.",
            "Compared with baselines, the accuracy improvement on test set is at least 13.7%.",
            "This demonstrates the effectiveness of our model by mining and exploiting heteregenous knowledge.",
            "2) The event narrative knowledge only is insufficient for commonsense machine comprehension.",
            "Compared with Narrative Event Chain Model, our model achieves a 16.3% accuracy improvement by considering richer commonsense knowledge, rather than only narrative event knowledge.",
            "3) It is necessary to distinguish different kinds of commonsense relations for machine comprehension and commonsense reasoning.",
            " Compared with DSSM and RNN, which model all relations  between  two  elements  using  a  single  semantic similarity score, our model achieves significant accuracy improvements by modeling, distinguishing and selecting different types of commonsense relations between  different kinds of elements."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            2,
            2,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Our Model",
                "Accuracy"
            ],
            [
                "Our Model",
                "Accuracy"
            ],
            [
                "Our Model"
            ],
            [
                "Narrative Event Chain"
            ],
            [
                "Narrative Event Chain",
                "Accuracy"
            ],
            [
                "DSSM",
                "RNN Model"
            ],
            [
                "DSSM",
                "RNN Model",
                "Our Model",
                "Accuracy"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_4",
        "paper_id": "D17-1216",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1216table_5",
        "description": "The first group of experiments was conducted using only one kind of knowledge at a time in our model. Table 5 shows the results. We can see that using a single kind of knowledge is insufficient for commonsense machine comprehension: all single-knowledge settings cannot achieve competitive performance to the all-knowledge setting.",
        "sentences": [
            "The first group of experiments was conducted using only one kind of knowledge at a time in our model.",
            "Table 5 shows the results.",
            "We can see that using a single kind of knowledge is insufficient for commonsense machine comprehension: all single-knowledge settings cannot achieve competitive performance to the all-knowledge setting."
        ],
        "class_sentence": [
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "Our Model (All Knowledge)"
            ],
            null,
            [
                "Our Model (All Knowledge)",
                "Accuracy"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_5",
        "paper_id": "D17-1216",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1216table_7",
        "description": "Table 7 show the results. We can see that: 1) the minimum cost mechanism cannot achieve competitive performance, we believe this is because the selection of rules should not depend on the cost of them, and considering all valid inferences is critical for reasoning; 2) our attention mechanism can effectively model the inference rule selection possibility. Compared with the average cost mechanism, our method achieved a 6.36% accuracy improvement. This also verified the necessity of an effective inference rule probability model.",
        "sentences": [
            "Table 7 show the results.",
            "We can see that: 1) the minimum cost mechanism cannot achieve competitive performance, we believe this is because the selection of rules should not depend on the cost of them, and considering all valid inferences is critical for reasoning; 2) our attention mechanism can effectively model the inference rule selection possibility.",
            "Compared with the average cost mechanism, our method achieved a 6.36% accuracy improvement.",
            "This also verified the necessity of an effective inference rule probability model."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Minimum Cost Mechanism",
                "Accuracy",
                "Our Model (Attention Mechanism)"
            ],
            [
                "Average Cost Mechanism",
                "Accuracy"
            ],
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_7",
        "paper_id": "D17-1216",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1216table_8",
        "description": "Table 8 show the results. We can see that removing negation rules will significantly drop the system performance, which confirm the effectiveness of our proposed negation rules.",
        "sentences": [
            "Table 8 show the results.",
            "We can see that removing negation rules will significantly drop the system performance, which confirm the effectiveness of our proposed negation rules."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "-w/o Negation Rules",
                "Accuracy",
                "Our Model"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_8",
        "paper_id": "D17-1216",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1218table_3",
        "description": "For all six datasets, training on different sources resulted in a performance drop. Table 3 lists the results of the best feature-based (LR All features) and deep learning (CNN-rand) systems, as well as single feature groups (averages over all source domains, results for individual source domains can be found in the supplementary material to this article). We note the biggest performance drops on the datasets which performed best in the indomain setting (MT and PE). For the lowest scoring datasets, OC and WTP, the differences are only marginal when trained on a suitable dataset (VG and  OC,  respectively). The best feature-based approach outperforms the best deep learning approach in most scenarios. In particular, as opposed to the in-domain experiments, the difference of the Claim-F1 measure between the feature-based approaches and the deep learning approaches is striking. In the feature-based approaches, on average, a combination of all features yields the best results for both Macro-F1and Claim-F1. When comparing single features, lexical ones do the best job. Looking at the best overall system (LR withall features), the average test results when training on different source datasets are between 54% Macro-F1 resp. 23% Claim-F1 (both MT) and 58% (VG) resp. 34% (OC). Depending on the goal that should be achieved, training on VG (highest average Macro-F1) or OC (highest average Claim-F1) seems to be the best choice when the domain of test data is unknown (we analyze this finding inmore depth in\u00a46). MT clearly gives the best results as target domain, followed by PE and VG.",
        "sentences": [
            "For all six datasets, training on different sources resulted in a performance drop.",
            "Table 3 lists the results of the best feature-based (LR All features) and deep learning (CNN-rand) systems, as well as single feature groups (averages over all source domains, results for individual source domains can be found in the supplementary material to this article).",
            "We note the biggest performance drops on the datasets which performed best in the indomain setting (MT and PE).",
            "For the lowest scoring datasets, OC and WTP, the differences are only marginal when trained on a suitable dataset (VG and  OC,  respectively).",
            "The best feature-based approach outperforms the best deep learning approach in most scenarios.",
            "In particular, as opposed to the in-domain experiments, the difference of the Claim-F1 measure between the feature-based approaches and the deep learning approaches is striking.",
            "In the feature-based approaches, on average, a combination of all features yields the best results for both Macro-F1and Claim-F1.",
            "When comparing single features, lexical ones do the best job.",
            "Looking at the best overall system (LR withall features), the average test results when training on different source datasets are between 54% Macro-F1 resp. 23% Claim-F1 (both MT) and 58% (VG) resp. 34% (OC).",
            "Depending on the goal that should be achieved, training on VG (highest average Macro-F1) or OC (highest average Claim-F1) seems to be the best choice when the domain of test data is unknown (we analyze this finding inmore depth in\u00a46).",
            "MT clearly gives the best results as target domain, followed by PE and VG."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2,
            1,
            1,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "LR All Features",
                "CNN-rand",
                "Single feature groups (averages across all source domains)"
            ],
            [
                "MT",
                "PE",
                "CNN-rand",
                "LR All Features"
            ],
            [
                "Source/Sys.",
                "CNN-rand",
                "OC",
                "Target",
                "WTP",
                "LR All Features",
                "VG"
            ],
            null,
            [
                "Target",
                "MT",
                "Source/Sys."
            ],
            [
                "Target",
                "Average"
            ],
            null,
            [
                "Single feature groups (averages across all source domains)",
                "LR",
                "+Embeddings",
                "Target",
                "VG",
                "Source/Sys.",
                "OC"
            ],
            [
                "Target",
                "VG",
                "OC"
            ],
            [
                "Target",
                "MT",
                "PE",
                "VG"
            ]
        ],
        "n_sentence": 11.0,
        "table_id": "table_3",
        "paper_id": "D17-1218",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1219table_2",
        "description": "Table 2 shows that the QG system incorporating our best performing sentence extractor outperforms its LREG counterpart across metrics. Note that to calculate the score for the matching case, similar to our earlier work (Du et al., 2017), we adapt the image captioning evaluation scripts of Chen et al.(2015) since there can be several gold standard questions for a single input sentence.",
        "sentences": [
            "Table 2 shows that the QG system incorporating our best performing sentence extractor outperforms its LREG counterpart across metrics.",
            "Note that to calculate the score for the matching case, similar to our earlier work (Du et al., 2017), we adapt the image captioning evaluation scripts of Chen et al.(2015) since there can be several gold standard questions for a single input sentence."
        ],
        "class_sentence": [
            1,
            2
        ],
        "header_mention": [
            [
                "Ours + NQG"
            ],
            [
                "Ours + NQG"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_2",
        "paper_id": "D17-1219",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1224table_1",
        "description": "Firstly, we perform evaluation on the whole articles and Table 1 shows the comparison results. We can see that our approach outperforms all the baseline methods with respect to ROUGE-2 and ROUGE-SU4. The Submodular method achieves the highest ROUGE-1 score, but our approach also achieves very high ROUGE-1 score, which is very close to that of the Submodular method.",
        "sentences": [
            "Firstly, we perform evaluation on the whole articles and Table 1 shows the comparison results.",
            "We can see that our approach outperforms all the baseline methods with respect to ROUGE-2 and ROUGE-SU4.",
            "The Submodular method achieves the highest ROUGE-1 score, but our approach also achieves very high ROUGE-1 score, which is very close to that of the Submodular method."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Our Approach",
                "R-2",
                "R-SU4"
            ],
            [
                "Submodular",
                "R-1",
                "Our Approach"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_1",
        "paper_id": "D17-1224",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1224table_2",
        "description": "Table 2 shows the comparison results based on this evaluation protocol (two part evaluation I). Furthermore, we allow the first part in a reference article to match with the second part in a peer article, and vice versa. We allow one-to-one matching and find the optimal matching between the two sets of parts, which refers to the matching with the largest sum of the similarity values of the matched parts. We then compute and average the ROUGE scores of the matched parts.",
        "sentences": [
            "Table 2 shows the comparison results based on this evaluation protocol (two part evaluation I).",
            "Furthermore, we allow the first part in a reference article to match with the second part in a peer article, and vice versa.",
            "We allow one-to-one matching and find the optimal matching between the two sets of parts, which refers to the matching with the largest sum of the similarity values of the matched parts.",
            "We then compute and average the ROUGE scores of the matched parts."
        ],
        "class_sentence": [
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "R-1",
                "R-2",
                "R-SU4"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "D17-1224",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1224table_4",
        "description": "Table 4 shows the manual evaluation results. We can see that our proposed approach can produce news overview articles with better content coverage, readability and overall responsiveness than baseline methods. The quality of the news overview articles is generally acceptable by the human judges.",
        "sentences": [
            "Table 4 shows the manual evaluation results.",
            "We can see that our proposed approach can produce news overview articles with better content coverage, readability and overall responsiveness than baseline methods.",
            "The quality of the news overview articles is generally acceptable by the human judges."
        ],
        "class_sentence": [
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Our Approach",
                "Cov.",
                "Read.",
                "Overall",
                "Submodular"
            ],
            null
        ],
        "n_sentence": 3.0,
        "table_id": "table_4",
        "paper_id": "D17-1224",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1226table_1",
        "description": "Table 1 shows the comparisons of feature weights. We can see that within-document event linking mainly relies on the euclidean distance and cosine similarity scores calculated using event word features, with a reasonable amount of weight assigned to overlapped arguments\u2019 embedding as well. However, only very small weights were assigned to the similarity and distance scores calculated using context embeddings. In contrast, in the classifier trained with cross-doc coreferent event mention pairs, the highest weight was assigned to the cosine similarity score calculated using context embeddings of two event mentions. Additionally, both the cosine similarity score calculate dusing event word embeddings and the overlapped argument features were assigned high weights as well. The comparisons clearly demonstrate the significantly different nature of WD and CD event coreference.",
        "sentences": [
            "Table 1 shows the comparisons of feature weights.",
            "We can see that within-document event linking mainly relies on the euclidean distance and cosine similarity scores calculated using event word features, with a reasonable amount of weight assigned to overlapped arguments\u2019 embedding as well.",
            "However, only very small weights were assigned to the similarity and distance scores calculated using context embeddings.",
            "In contrast, in the classifier trained with cross-doc coreferent event mention pairs, the highest weight was assigned to the cosine similarity score calculated using context embeddings of two event mentions.",
            "Additionally, both the cosine similarity score calculate dusing event word embeddings and the overlapped argument features were assigned high weights as well.",
            "The comparisons clearly demonstrate the significantly different nature of WD and CD event coreference."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "WD",
                "Event Word Embedding: Euc",
                "Event Word Embedding: Cos"
            ],
            [
                "WD",
                "Context Embedding: Euc",
                "Context Embedding: Cos"
            ],
            [
                "CD",
                "Context Embedding: Cos"
            ],
            [
                "Event Word Embedding: Cos",
                "WD",
                "Argument Embedding"
            ],
            [
                "WD",
                "CD"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_1",
        "paper_id": "D17-1226",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1226table_3",
        "description": "Table 3 shows the comparison results for both within-document and cross-document event coreference resolution. In the first stage of iterative merging, using two distinct WD and CD classifiers for corresponding WD and CD merges yields clear improvements for both WD and CD event coreference resolution tasks, compared with using one common classifier for both types of merges. In addition, the second stage of iterative merging further improves both WD and CD event coreference resolution performance stably by leveraging second order event inter-dependencies. The improvements are consistent when measured using various coreference resolution evaluation metrics.",
        "sentences": [
            "Table 3 shows the comparison results for both within-document and cross-document event coreference resolution.",
            "In the first stage of iterative merging, using two distinct WD and CD classifiers for corresponding WD and CD merges yields clear improvements for both WD and CD event coreference resolution tasks, compared with using one common classifier for both types of merges.",
            "In addition, the second stage of iterative merging further improves both WD and CD event coreference resolution performance stably by leveraging second order event inter-dependencies.",
            "The improvements are consistent when measured using various coreference resolution evaluation metrics."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "Cross-Document Coreference Results",
                "Within-Document Coreference Result"
            ],
            [
                "WD & CD Classifiers",
                "Common Classifier (WD)",
                "Common Classifier (CD)"
            ],
            [
                "WD & CD Classifiers + 2nd Order Relations (Full Model)"
            ],
            [
                "R",
                "P",
                "F1"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "D17-1226",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1228table_3",
        "description": "We conducted mean opinion score (MOS) tests for overall quality assessment of generated responses with questionnaires described above. Table 3 shows the MOS results with standard error. It can be seen that all the systems based on selective sampling are significantly better than vanilla sampling baseline. When restricting output\u00e2\u20ac\u2122s style and/or topic, the MOS score results of most systems do not decline significantly except singer-songwriter, which attempts to generate lyrics-like outputs in response to political debate questions, resulting in uninterpretable strings. Table 3 also shows the likelihood of being labeled as JFK for different methods. It is encouraging that finetune based approaches have similar chances as the rank system which retrieves sentences directly from JFK corpus, and are significantly better than the selective sampling baseline.",
        "sentences": [
            "We conducted mean opinion score (MOS) tests for overall quality assessment of generated responses with questionnaires described above.",
            "Table 3 shows the MOS results with standard error.",
            "It can be seen that all the systems based on selective sampling are significantly better than vanilla sampling baseline.",
            "When restricting output\u00e2\u20ac\u2122s style and/or topic, the MOS score results of most systems do not decline significantly except singer-songwriter, which attempts to generate lyrics-like outputs in response to political debate questions, resulting in uninterpretable strings.",
            "Table 3 also shows the likelihood of being labeled as JFK for different methods.",
            "It is encouraging that finetune based approaches have similar chances as the rank system which retrieves sentences directly from JFK corpus, and are significantly better than the selective sampling baseline."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "Quality (MOS)"
            ],
            null,
            [
                "selective-sampling",
                "vanilla-sampling",
                "Quality (MOS)"
            ],
            [
                "Style",
                "Quality (MOS)",
                "singer-songwriter"
            ],
            [
                "multiply",
                "finetune",
                "finetune-cg-topic"
            ],
            [
                "finetune",
                "rank",
                "Quality (MOS)",
                "selective-sampling"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_3",
        "paper_id": "D17-1228",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1229table_1",
        "description": "Table 1 compares our results with those obtained by the baselines. Our two models, Uncertainty Propagation and Average Embedding, outperform all the baselines. Among these two models, Uncertainty Propagation, which is more analytically grounded, outperforms the Average Embedding model. Using the true current label during training seems to degrade performance compared to using the predicted label, which is expected, since the true label is not available during testing. The Scheduled Sampling method performs similarly to the predicted-label method for the MapTask corpus, and outperforms this method for the Switchboard corpus.",
        "sentences": [
            "Table 1 compares our results with those obtained by the baselines.",
            "Our two models, Uncertainty Propagation and Average Embedding, outperform all the baselines.",
            "Among these two models, Uncertainty Propagation, which is more analytically grounded, outperforms the Average Embedding model.",
            "Using the true current label during training seems to degrade performance compared to using the predicted label, which is expected, since the true label is not available during testing.",
            "The Scheduled Sampling method performs similarly to the predicted-label method for the MapTask corpus, and outperforms this method for the Switchboard corpus."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Average Embedding",
                "Uncertainty Propagation",
                "Accuracy"
            ],
            [
                "Uncertainty Propagation",
                "Accuracy",
                "Average Embedding"
            ],
            [
                "True current label",
                "Predicted current label"
            ],
            [
                "Scheduled Sampling",
                "Predicted current label",
                "MapTask",
                "Switchboard"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_1",
        "paper_id": "D17-1229",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1237table_1",
        "description": "Table 1 shows the performance on test data. For all types of users, the HRL-based agent yielded more robust dialogue policies outperforming the hand-crafted rule-based agents and flat RL-based agent measured on success rate. It also needed fewer turns per dialogue session to accomplish a task than the rule-based agents and flat RL agent. The results across all three types of simulated users suggest the following conclusions.",
        "sentences": [
            "Table 1 shows the performance on test data.",
            "For all types of users, the HRL-based agent yielded more robust dialogue policies outperforming the hand-crafted rule-based agents and flat RL-based agent measured on success rate.",
            "It also needed fewer turns per dialogue session to accomplish a task than the rule-based agents and flat RL agent.",
            "The results across all three types of simulated users suggest the following conclusions."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Type A",
                "Type B",
                "Type C",
                "HRL",
                "RL",
                "Rule",
                "Succ."
            ],
            [
                "HRL",
                "Rule",
                "RL",
                "Turn"
            ],
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_1",
        "paper_id": "D17-1237",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1240table_4",
        "description": "We observe that while in the SB bucket both the baseline and U U R perform equally well, for all the other buckets U U R massively outperforms the baseline. This implies that for the case where the likeliness of borrowing is the strongest, the baseline does as good as U U R. However, as one moves down the rank list, U U R turns out to be a considerably better predictor than the baseline. The overall macro and micro precision/recall as shown in table 4 further strengthens our observation that U U R is a better metric than the baseline.",
        "sentences": [
            "We observe that while in the SB bucket both the baseline and U U R perform equally well, for all the other buckets U U R massively outperforms the baseline.",
            "This implies that for the case where the likeliness of borrowing is the strongest, the baseline does as good as U U R.",
            "However, as one moves down the rank list, U U R turns out to be a considerably better predictor than the baseline.",
            "The overall macro and micro precision/recall as shown in table 4 further strengthens our observation that U U R is a better metric than the baseline."
        ],
        "class_sentence": [
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            [
                "Baseline",
                "UUR",
                "Macro prec./rec.",
                "Micro prec./rec."
            ],
            [
                "Baseline",
                "UUR"
            ],
            [
                "Baseline",
                "UUR"
            ],
            [
                "Macro prec./rec.",
                "Micro prec./rec.",
                "UUR",
                "Baseline"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_4",
        "paper_id": "D17-1240",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1240table_6",
        "description": "Table 6 shows the bucket-wise precision and recall for U U R and the baseline metrics with respect to two new ground truths. For the young population once again the number of words in each bucket for all the three sets is the same thus making the values of the precision and the recall same. In fact, the precision/recall for this ground truth is exactly same as in the case of the original ground truth.",
        "sentences": [
            "Table 6 shows the bucket-wise precision and recall for U U R and the baseline metrics with respect to two new ground truths.",
            "For the young population once again the number of words in each bucket for all the three sets is the same thus making the values of the precision and the recall same.",
            "In fact, the precision/recall for this ground truth is exactly same as in the case of the original ground truth."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "p",
                "r"
            ],
            [
                "Young-UUR",
                "Young-Baseline",
                "Elder-baseline",
                "SB",
                "p/r"
            ],
            [
                "p/r",
                "Young-UUR",
                "Young-Baseline",
                "SB"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_6",
        "paper_id": "D17-1240",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1244table_7",
        "description": "Table 7 presents results per dimension with the best overall combination of features (Verb + Personx + Persony + Personx Persony). All dimensions obtain overall F-measures between 0.65 and 0.83 (last column). Results per label are heavily biased towards the most frequent label per dimension (Figure2), although it is the case that the models we experiment with predict both 1 and -1 for all dimensions. As stated above,none  of  them  predict 0,  but  this  limitation  does not substantially penalize overall performance because of the low frequency of this label. The model obtains the same F-measures for1and-1with concurrent dimension (0.76), and the labels of this dimension are virtually distributed uniformly (46.4% vs. 47.1%, Figure 2). Similarly, F-measures for 1 and -1 with spatially near and active dimensions are similar (0.67 vs. 0.76 and 0.76  vs. 0.58), and the labels are distributed relatively evenly in our corpus (40.4% vs 51.4% and.58.4% vs. 35.3%). Finally, F-measures per label with other dimension  are  biased  towards  the  most  frequent  label. For example, only 15% of all pairs of people have an enduring relationship (Figure 2), and the F-measure for 1 with temporary dimension is much higher (0.91) that for -1 (0.16).",
        "sentences": [
            "Table 7 presents results per dimension with the best overall combination of features (Verb + Personx + Persony + Personx Persony).",
            "All dimensions obtain overall F-measures between 0.65 and 0.83 (last column).",
            "Results per label are heavily biased towards the most frequent label per dimension (Figure2), although it is the case that the models we experiment with predict both 1 and -1 for all dimensions.",
            "As stated above,none  of  them  predict 0,  but  this  limitation  does not substantially penalize overall performance because of the low frequency of this label.",
            "The model obtains the same F-measures for1and-1with concurrent dimension (0.76), and the labels of this dimension are virtually distributed uniformly (46.4% vs. 47.1%, Figure 2).",
            "Similarly, F-measures for 1 and -1 with spatially near and active dimensions are similar (0.67 vs. 0.76 and 0.76  vs. 0.58), and the labels are distributed relatively evenly in our corpus (40.4% vs 51.4% and.58.4% vs. 35.3%).",
            "Finally, F-measures per label with other dimension  are  biased  towards  the  most  frequent  label.",
            "For example, only 15% of all pairs of people have an enduring relationship (Figure 2), and the F-measure for 1 with temporary dimension is much higher (0.91) that for -1 (0.16)."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Dimension",
                "All",
                "F"
            ],
            [
                "1 (1st descriptor)",
                "0 (unknown)",
                "-1 (2nd descriptor)",
                "F"
            ],
            [
                "1 (1st descriptor)",
                "-1 (2nd descriptor)",
                "F"
            ],
            [
                "F",
                "1 (1st descriptor)",
                "-1 (2nd descriptor)",
                "Concurrent"
            ],
            [
                "F",
                "1 (1st descriptor)",
                "-1 (2nd descriptor)",
                "Spat. Near",
                "Active"
            ],
            [
                "F"
            ],
            [
                "F",
                "1 (1st descriptor)",
                "Temporary",
                "-1 (2nd descriptor)"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_7",
        "paper_id": "D17-1244",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1245table_4",
        "description": "We cast the argument detection task as a binary classification task, and we apply the supervised algorithms described in Section 2.1. Table 4 reports on the obtained results with the different configurations,. while Table 5 reports on the results obtained by the best configuration, i.e., LR + All features, per each category.",
        "sentences": [
            "We cast the argument detection task as a binary classification task, and we apply the supervised algorithms described in Section 2.1.",
            "Table 4 reports on the obtained results with the different configurations.",
            "Table 5 reports on the results obtained by the best configuration, i.e., LR + All features, per each category."
        ],
        "class_sentence": [
            2,
            1,
            0
        ],
        "header_mention": [
            null,
            [
                "LR+all features"
            ],
            null
        ],
        "n_sentence": 3.0,
        "table_id": "table_4",
        "paper_id": "D17-1245",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1245table_5",
        "description": "Table 4 reports on the obtained results with the different configurations,. while Table 5 reports on the results obtained by the best configuration, i.e., LR + All features, per each category.",
        "sentences": [
            "Table 4 reports on the obtained results with the different configurations.",
            "Table 5 reports on the results obtained by the best configuration, i.e., LR + All features, per each category."
        ],
        "class_sentence": [
            0,
            1
        ],
        "header_mention": [
            null,
            [
                "avg/total"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_5",
        "paper_id": "D17-1245",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1245table_6",
        "description": "To address the task of factual vs opinion arguments classification, we apply the supervised classification algorithms described in Section 2.1. Tweets from Grexit dataset are used as training set, and those from Brexit dataset as test set. Table 6 reports on the obtained results.",
        "sentences": [
            "To address the task of factual vs opinion arguments classification, we apply the supervised classification algorithms described in Section 2.1.",
            "Tweets from Grexit dataset are used as training set, and those from Brexit dataset as test set.",
            "Table 6 reports on the obtained results."
        ],
        "class_sentence": [
            1,
            2,
            1
        ],
        "header_mention": [
            [
                "RF+L",
                "LR+L"
            ],
            null,
            null
        ],
        "n_sentence": 3.0,
        "table_id": "table_6",
        "paper_id": "D17-1245",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1245table_8",
        "description": "Table 8 reports on the obtained results. As baseline, we use a method that considers all the NEs detected in the tweet as sources. Most of the errors of the algorithm are due to information sources not recognized as NEs (in particular, when the source is a Twitter user), or NEs that are linked to the wrong DBpedia page. However, in order to draw more interesting conclusions on the most suitable methods to address this task, we would need the increase the size of the dataset.",
        "sentences": [
            "Table 8 reports on the obtained results.",
            "As baseline, we use a method that considers all the NEs detected in the tweet as sources.",
            "Most of the errors of the algorithm are due to information sources not recognized as NEs (in particular, when the source is a Twitter user), or NEs that are linked to the wrong DBpedia page.",
            "However, in order to draw more interesting conclusions on the most suitable methods to address this task, we would need the increase the size of the dataset."
        ],
        "class_sentence": [
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "Baseline"
            ],
            [
                "Baseline",
                "Matching+heurist.",
                "F1"
            ],
            [
                "Matching+heurist.",
                "F1"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_8",
        "paper_id": "D17-1245",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1260table_2",
        "description": "Table 2 is the evaluation results of different ordered rules. The rule R4 can significantly boost the success rate (comparing line 2 with line 1),while the rule R1* can both boost the success rate and decrease the dialogue length (comparing line 3 with line 1). The combination of R4 and R1* takes respective advantages (comparing line 4 with line 1, line 2 and line 3). The performance of final order rules is comparable to the performance of optimized student policy.",
        "sentences": [
            "Table 2 is the evaluation results of different ordered rules.",
            "The rule R4 can significantly boost the success rate (comparing line 2 with line 1),while the rule R1* can both boost the success rate and decrease the dialogue length (comparing line 3 with line 1).",
            "The combination of R4 and R1* takes respective advantages (comparing line 4 with line 1, line 2 and line 3).",
            "The performance of final order rules is comparable to the performance of optimized student policy."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Success Rate",
                "#Turn"
            ],
            [
                "R1* R4 R2 R3",
                "R1 R2 R3",
                "R1 R4 R2 R3",
                "R1* R2 R3",
                "Reward"
            ],
            [
                "R1* R4 R2 R3"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "D17-1260",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1267table_3",
        "description": "Table 3 shows the results. LEXSTAT performs slightly better than the heuristic baseline, but both are limited by their inability to relate words that have non-identical definitions. In fact, only 21.4% of all gold cognate sets in the Algonquian dataset contain at least two words with the same definition, which establishes an upper bound on the number of found sets for systems that are designed to operate on word lists, rather than dictionaries. For example, most of the cognates in Figure 1 cannot be captured by such systems. Our system, SemaPhoR, finds approximately three times as many cognate sets as LEXSTAT, and over 75% of those sets are complete with respect to the gold annotation. In practical terms, our system is able to provide concrete evidence for the existence of most of the proto-words that have reflexes in the recorded languages, and identifies the majority of those reflexes in the process. The purity of the produced clusters indicates that there are many more hits than misses in the system output. In addition, the clusters can be sorted according to their confidence scores, in order to facilitate the analysis of the results by an expert linguist.",
        "sentences": [
            "Table 3 shows the results.",
            "LEXSTAT performs slightly better than the heuristic baseline, but both are limited by their inability to relate words that have non-identical definitions.",
            "In fact, only 21.4% of all gold cognate sets in the Algonquian dataset contain at least two words with the same definition, which establishes an upper bound on the number of found sets for systems that are designed to operate on word lists, rather than dictionaries.",
            "For example, most of the cognates in Figure 1 cannot be captured by such systems.",
            "Our system, SemaPhoR, finds approximately three times as many cognate sets as LEXSTAT, and over 75% of those sets are complete with respect to the gold annotation.",
            "In practical terms, our system is able to provide concrete evidence for the existence of most of the proto-words that have reflexes in the recorded languages, and identifies the majority of those reflexes in the process.",
            "The purity of the produced clusters indicates that there are many more hits than misses in the system output.",
            "In addition, the clusters can be sorted according to their confidence scores, in order to facilitate the analysis of the results by an expert linguist."
        ],
        "class_sentence": [
            1,
            1,
            2,
            0,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "LEXSTAT",
                "Heuristic Baseline",
                "Purity"
            ],
            null,
            null,
            [
                "SemaPhoR",
                "LEXSTAT",
                "Found Sets"
            ],
            [
                "SemaPhoR"
            ],
            [
                "Purity"
            ],
            null
        ],
        "n_sentence": 8.0,
        "table_id": "table_3",
        "paper_id": "D17-1267",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1269table_1",
        "description": "We show that our approach gives non-trivial scores across several languages, and when combined with orthogonal features from Wikipedia, improves on state-of-the-art scores. Table 1 compares a simple direct transfer baseline, the previous state-of-the-art in cross-lingual NER, and our proposed algorithm. For these languages, we beat the baseline by 25.4 points, and the state-of-the-art by 5.9 points. In addition, we found that translating from a language related to the target language gives a further boost. We conclude with a case study of a truly low-resource language, Uyghur, and show a good score, despite having almost no target language resources.",
        "sentences": [
            "We show that our approach gives non-trivial scores across several languages, and when combined with orthogonal features from Wikipedia, improves on state-of-the-art scores.",
            "Table 1 compares a simple direct transfer baseline, the previous state-of-the-art in cross-lingual NER, and our proposed algorithm.",
            "For these languages, we beat the baseline by 25.4 points, and the state-of-the-art by 5.9 points.",
            "In addition, we found that translating from a language related to the target language gives a further boost.",
            "We conclude with a case study of a truly low-resource language, Uyghur, and show a good score, despite having almost no target language resources."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "Baseline",
                "Previous SOA",
                "Cheap Translation"
            ],
            [
                "Cheap Translation",
                "Avg",
                "Baseline",
                "Previous SOA"
            ],
            null,
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_1",
        "paper_id": "D17-1269",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1274table_4",
        "description": "Table 4 shows the distribution of training data and the F-score of each single type. We can see that, for some slot types, such as per:dateofbirthandper:age, the entity types of their candidate fillers are easy to learn and differentiate from other slot types, and their indicative words are usually explicit, thus our approach can get high f-score with limited training data (less than 507 instances). In contrast, for some slots,such as org:location of headquarters, their clues are implicit and the entity types of candidate filler sare difficult to be inferred. Although the size of training data is larger (more than 1,433 instances), the f-score remains quite low. One possible solution is to incorporate fine-grained entity types from existing tools into the neural architecture.\".",
        "sentences": [
            "Table 4 shows the distribution of training data and the F-score of each single type.",
            "We can see that, for some slot types, such as per:dateofbirthandper:age, the entity types of their candidate fillers are easy to learn and differentiate from other slot types, and their indicative words are usually explicit, thus our approach can get high f-score with limited training data (less than 507 instances).",
            "In contrast, for some slots,such as org:location of headquarters, their clues are implicit and the entity types of candidate filler sare difficult to be inferred.",
            "Although the size of training data is larger (more than 1,433 instances), the f-score remains quite low.",
            "One possible solution is to incorporate fine-grained entity types from existing tools into the neural architecture.\"."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "Training Data Distribution (%)",
                "F1 (%)"
            ],
            [
                "date_of_birth",
                "age",
                "F1 (%)"
            ],
            [
                "state_of_headq.",
                "country_of_headq.",
                "city_of_headq.",
                "F1 (%)"
            ],
            [
                "state_of_headq.",
                "country_of_headq.",
                "city_of_headq.",
                "Training Data Distribution (%)",
                "F1 (%)"
            ],
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_4",
        "paper_id": "D17-1274",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1275table_2",
        "description": "Table 2 shows development set results on Darkode for each of the four systems for each metric described in Section 3. Our learning-based systems substantially outperform the baselines on the metrics they are optimized for. The post-level system underperforms the binary classifier on the token evaluation, but is superior at not only postlevel accuracy but also product type F1. This lends credence to our hypothesis that picking one product suffices to characterize a large fraction of posts. Comparing the automatic systems with human annotator performance we see a substantial gap. Note that our best annotator\u00e2\u20ac\u2122s token F1 was 89.8, and NP post accuracy was 100%; a careful, well-trained annotator can achieve very high performance, indicating a high skyline.",
        "sentences": [
            "Table 2 shows development set results on Darkode for each of the four systems for each metric described in Section 3.",
            "Our learning-based systems substantially outperform the baselines on the metrics they are optimized for.",
            "The post-level system underperforms the binary classifier on the token evaluation, but is superior at not only postlevel accuracy but also product type F1.",
            "This lends credence to our hypothesis that picking one product suffices to characterize a large fraction of posts.",
            "Comparing the automatic systems with human annotator performance we see a substantial gap.",
            "Note that our best annotator\u00e2\u20ac\u2122s token F1 was 89.8, and NP post accuracy was 100%; a careful, well-trained annotator can achieve very high performance, indicating a high skyline."
        ],
        "class_sentence": [
            1,
            2,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Post",
                "Acc.",
                "Binary",
                "Token",
                "F1"
            ],
            [
                "Products"
            ],
            [
                "Human*",
                "P"
            ],
            [
                "Binary",
                "Post",
                "F1",
                "Acc."
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_2",
        "paper_id": "D17-1275",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1275table_5",
        "description": "Table 5 confirms this intuition: it shows product out-of-vocabulary rates in each of the four forums relative to training on both Darkode and Hack Forums, along with recall of an NP-level system on both previously seen and OOV products. As expected, performance is substantially higher on in vocabulary  products. OOV rates of a Darkode-trained system are generally lower on new forums,indicating that that forum has better all-around product coverage. A system trained on Darkode is therefore in some sense more domain-general than one trained on Hack Forums.",
        "sentences": [
            "Table 5 confirms this intuition: it shows product out-of-vocabulary rates in each of the four forums relative to training on both Darkode and Hack Forums, along with recall of an NP-level system on both previously seen and OOV products.",
            "As expected, performance is substantially higher on in vocabulary  products.",
            "OOV rates of a Darkode-trained system are generally lower on new forums,indicating that that forum has better all-around product coverage.",
            "A system trained on Darkode is therefore in some sense more domain-general than one trained on Hack Forums."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "Binary (Darkode)",
                "Binary (HF)",
                "Rseen",
                "Roov"
            ],
            [
                "% OOV"
            ],
            [
                "% OOV",
                "Binary (Darkode)"
            ],
            [
                "Binary (Darkode)",
                "Binary (HF)"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_5",
        "paper_id": "D17-1275",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1276table_3",
        "description": "Table 3 shows the results of running the models with F1-score tuning on GENIA dataset. All models include Brown clustering features learned from PubMed abstracts. Besides the mention hypergraph baseline, we also make comparisons with the system of Finkel and Manning (2009) that can also support overlapping mentions. We see that the mention hypergraph model matches the performance of the constituency parser-based model of Finkel and Manning\r\n(2009), while our models based on mention separators yield significantly higher scores (p < 0.05) than all other baselines (except LCRF (multiple), which we will discuss shortly).",
        "sentences": [
            "Table 3 shows the results of running the models with F1-score tuning on GENIA dataset.",
            "All models include Brown clustering features learned from PubMed abstracts.",
            "Besides the mention hypergraph baseline, we also make comparisons with the system of Finkel and Manning (2009) that can also support overlapping mentions.",
            "We see that the mention hypergraph model matches the performance of the constituency parser-based model of Finkel and Manning\r\n(2009), while our models based on mention separators yield significantly higher scores (p < 0.05) than all other baselines (except LCRF (multiple), which we will discuss shortly)."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "F1"
            ],
            null,
            [
                "This work (STATE)",
                "This work (EDGE)",
                "Finkel and Manning (2009)"
            ],
            [
                "This work (STATE)",
                "This work (EDGE)",
                "LCRF (single)",
                "LCRF (multiple)",
                "Finkel and Manning (2009)",
                "Lu and Roth (2015)",
                "F1"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "D17-1276",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1279table_1",
        "description": "Table 1 reports the results of our neural sequence tagging model NN-CRF in both supervised and semi-supervised learning (ULM and graph-based), and compares them with the baselines and the state-of-the-art (best SemEval System (Augenstein et al., 2017)). Augenstein and S\u00c3\u00b8gaard (2017) use a multi-task learning strategy to improve the performance of supervised keyphrase classification, but they only report dev set performance on SemEval Task 10, we also include their result here and refer it as\r\nMULTITASK. We report results for both span identification (SemEval SubTask A) and span classification into TASK, PROCESS and MATERIAL (SemEval Subtask B). The results show that our neural sequence tagging models significantly outperforms the state of the art and both baselines. It confirms that our neural tagging model outperforms other nonneural and neural models for the SemEval ScienceIE challenge. It further shows that our system achieves significant boost from semi-supervised learning using unlabeled data.",
        "sentences": [
            "Table 1 reports the results of our neural sequence tagging model NN-CRF in both supervised and semi-supervised learning (ULM and graph-based), and compares them with the baselines and the state-of-the-art (best SemEval System (Augenstein et al., 2017)).",
            "Augenstein and S\u00c3\u00b8gaard (2017) use a multi-task learning strategy to improve the performance of supervised keyphrase classification, but they only report dev set performance on SemEval Task 10, we also include their result here and refer it as\r\nMULTITASK.",
            "We report results for both span identification (SemEval SubTask A) and span classification into TASK, PROCESS and MATERIAL (SemEval Subtask B).",
            "The results show that our neural sequence tagging models significantly outperforms the state of the art and both baselines.",
            "It confirms that our neural tagging model outperforms other nonneural and neural models for the SemEval ScienceIE challenge.",
            "It further shows that our system achieves significant boost from semi-supervised learning using unlabeled data."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "NN-CRF (supervised)",
                "NN-CRF (semi)"
            ],
            [
                "Classification (dev)",
                "MULTITASK"
            ],
            null,
            [
                "NN-CRF (semi)",
                "Identification"
            ],
            [
                "NN-CRF (semi)",
                "Best Non-Neural SemEval+",
                "Best Neural SemEval+",
                "Identification"
            ],
            [
                "NN-CRF (semi)"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_1",
        "paper_id": "D17-1279",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1284table_2",
        "description": "In Table 2 we present results for our models on ACE-2004. Our model outperforms the Wikifier and Vinculum systems that only use information from Wikipedia, and AIDA, by a significant margin, indicating its possible over-fitting to the CoNLL domain. Hence, it shows our model's ability to perform accurate linking across different datasets without using domain-specific information.",
        "sentences": [
            "In Table 2 we present results for our models on ACE-2004.",
            "Our model outperforms the Wikifier and Vinculum systems that only use information from Wikipedia, and AIDA, by a significant margin, indicating its possible over-fitting to the CoNLL domain.",
            "Hence, it shows our model's ability to perform accurate linking across different datasets without using domain-specific information."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Model C",
                "Model CDT",
                "Model CDTE",
                "F1",
                "Wikifier",
                "Vinculum",
                "AIDA"
            ],
            [
                "Model C",
                "Model CDT",
                "Model CDTE",
                "Accuracy"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_2",
        "paper_id": "D17-1284",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1285table_2",
        "description": "We also evaluate the relation extraction component (Sec. 5) on Cause-Effect subset of SemEval2010 dataset. Note our causality/correlation relation extraction component is not supposed to be a general purpose one, since our system only focuses on insight extraction of biomedical/health literature. We compare our relation extraction models against previous work on the Cause Effect subset of the data, Table 2 shows our relational similarity model, without the use of sparse features or external resources such as WordNet, outperforms recent state-of-the-art treeLSTM model (Miwa and Bansal, 2016). It also shows BiGRU model is reasonably competitive on this dataset, which is why we use it in our baseline system for comparison purpose.",
        "sentences": [
            "We also evaluate the relation extraction component (Sec. 5) on Cause-Effect subset of SemEval2010 dataset.",
            "Note our causality/correlation relation extraction component is not supposed to be a general purpose one, since our system only focuses on insight extraction of biomedical/health literature.",
            "We compare our relation extraction models against previous work on the Cause Effect subset of the data, Table 2 shows our relational similarity model, without the use of sparse features or external resources such as WordNet, outperforms recent state-of-the-art treeLSTM model (Miwa and Bansal, 2016).",
            "It also shows BiGRU model is reasonably competitive on this dataset, which is why we use it in our baseline system for comparison purpose."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Contextual similarity modeling",
                "Relational similarity modeling"
            ],
            [
                "Contextual similarity modeling",
                "Relational similarity modeling",
                "F1 Score",
                "Miwa and Bansal (2016)"
            ],
            [
                "BiGRU",
                "F1 Score"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "D17-1285",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1291table_2",
        "description": "Performance comparison. Table 2 shows performance of different outlier document detection methods. It can be observed that our method outperforms all the baselines in both data sets. In both data sets, VMF-Q can achieve a 45% to 135% increase from baselines in terms of recall by examining the top 1% outliers. Generally, performances of most methods are lower in the ARNET data set comparing to NYT, potentially because the relatively short document lengths and more technical terminologies in ARNET.",
        "sentences": [
            "Performance comparison.",
            "Table 2 shows performance of different outlier document detection methods.",
            "It can be observed that our method outperforms all the baselines in both data sets.",
            "In both data sets, VMF-Q can achieve a 45% to 135% increase from baselines in terms of recall by examining the top 1% outliers.",
            "Generally, performances of most methods are lower in the ARNET data set comparing to NYT, potentially because the relatively short document lengths and more technical terminologies in ARNET."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "VMF-SF",
                "VMF-E",
                "VMF-Q",
                "Dataset",
                "NYT",
                "ARNET"
            ],
            [
                "VMF-Q",
                "Dataset",
                "NYT",
                "ARNET"
            ],
            [
                "VMF-Q",
                "Dataset",
                "NYT",
                "ARNET"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_2",
        "paper_id": "D17-1291",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1292table_9",
        "description": "We also conduct a human assessment on the explanation chains produced by the two reasoners, asking people to choose more convincing explanation chains for each feature-target pair. Table 9 shows their relative preferences.",
        "sentences": [
            "We also conduct a human assessment on the explanation chains produced by the two reasoners, asking people to choose more convincing explanation chains for each feature-target pair.",
            "Table 9 shows their relative preferences."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "Reasoners",
                "Accuracy (%)"
            ],
            null
        ],
        "n_sentence": 2.0,
        "table_id": "table_9",
        "paper_id": "D17-1292",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1293table_2",
        "description": "Result Firstly we use all the data to learn word embeddings by models. Then the learned word vectors are utilized to calculate the similarity between threads and subtitles, and rank the subtitles. Table 2 reports the results of thread-subtitle matching. We can notice that there are some anomalies in P@3 and P@5 results. It may be for the reason of dataset. In the first MOOC (people and network), video subtitles contain relatively less words, and therefore it is hard to get effective representations. Overall, the proposed models can achieve better performance than baselines, and we highlight the Precision@1 results. Compared to HDV which also considers the streaming documents, our model is better at every task. This indicates our model can effectively capture the latent similarity.",
        "sentences": [
            "Result Firstly we use all the data to learn word embeddings by models.",
            "Then the learned word vectors are utilized to calculate the similarity between threads and subtitles, and rank the subtitles.",
            "Table 2 reports the results of thread-subtitle matching.",
            "We can notice that there are some anomalies in P@3 and P@5 results.",
            "It may be for the reason of dataset.",
            "In the first MOOC (people and network), video subtitles contain relatively less words, and therefore it is hard to get effective representations.",
            "Overall, the proposed models can achieve better performance than baselines, and we highlight the Precision@1 results.",
            "Compared to HDV which also considers the streaming documents, our model is better at every task.",
            "This indicates our model can effectively capture the latent similarity."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Word2Vec"
            ],
            null,
            [
                "P@3",
                "P@5"
            ],
            null,
            [
                "People and Network",
                "P@3",
                "P@5"
            ],
            [
                "CDR",
                "People and Network",
                "P@1"
            ],
            [
                "HDV",
                "CDR"
            ],
            null
        ],
        "n_sentence": 9.0,
        "table_id": "table_2",
        "paper_id": "D17-1293",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1294table_2",
        "description": "We performed ablation tests excluding one feature at a time from the coarse-grained classifier. The results of these tests are presented in Table 2 as precision, recall, and F1 scores for the positive class, i.e., the opt-out class. Using the F1 scores as the primary evaluation metric, it appears that all features help in classification. The unigram, topic distribution, nonterminal, and modal verb and optout phrase features contribute the most to performance. Including all the features results in an F1 score of 0.735. Ablation test without unigram features resulted in the lowest F1 score of 0.585, and by analyzing features with higher logistic regression weights, we found n-grams such as unsubscribe to have intuitively high weights. We also found the production rule \u201cS\u2192SBAR, VP\u201d to have a high weight, indicating that presence of subordinate clauses (SBARs) help in classification.",
        "sentences": [
            "We performed ablation tests excluding one feature at a time from the coarse-grained classifier.",
            "The results of these tests are presented in Table 2 as precision, recall, and F1 scores for the positive class, i.e., the opt-out class.",
            "Using the F1 scores as the primary evaluation metric, it appears that all features help in classification.",
            "The unigram, topic distribution, nonterminal, and modal verb and optout phrase features contribute the most to performance.",
            "Including all the features results in an F1 score of 0.735.",
            "Ablation test without unigram features resulted in the lowest F1 score of 0.585, and by analyzing features with higher logistic regression weights, we found n-grams such as unsubscribe to have intuitively high weights.",
            "We also found the production rule \u201cS\u2192SBAR, VP\u201d to have a high weight, indicating that presence of subordinate clauses (SBARs) help in classification."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Precision",
                "Recall",
                "F1",
                "All"
            ],
            [
                "F1",
                "All"
            ],
            [
                "All - Unigrams",
                "All - Topic Models",
                "All - Nonterminals",
                "F1"
            ],
            [
                "All",
                "F1"
            ],
            [
                "All - Unigrams",
                "F1"
            ],
            null
        ],
        "n_sentence": 7.0,
        "table_id": "table_2",
        "paper_id": "D17-1294",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1296table_4",
        "description": "We build two baseline systems using CRF and Bi-LSTM, respectively. The hand-crafted discrete features of CRF refer to those in Ferguson et al.(2015). For the Bi-LSTM model, the token embedding is the same with our transition-based method. Table 4 shows the result of our model on both the development and test sets. Beam search improves the F-score form 87.1% to 87.5%, which is consistent with the finding of Buckman et al.(2016) on the LSTM parser of (Dyer et al., 2015) (improvements by about 0.3 point). Scheduled sampling achieves the same improvements compared to beam-search. Because of high training speed, we conduct subsequent experiments based on scheduled sampling.",
        "sentences": [
            "We build two baseline systems using CRF and Bi-LSTM, respectively.",
            "The hand-crafted discrete features of CRF refer to those in Ferguson et al.(2015).",
            "For the Bi-LSTM model, the token embedding is the same with our transition-based method.",
            "Table 4 shows the result of our model on both the development and test sets.",
            "Beam search improves the F-score form 87.1% to 87.5%, which is consistent with the finding of Buckman et al.(2016) on the LSTM parser of (Dyer et al., 2015) (improvements by about 0.3 point).",
            "Scheduled sampling achieves the same improvements compared to beam-search.",
            "Because of high training speed, we conduct subsequent experiments based on scheduled sampling."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "CRF",
                "Bi-LSTM"
            ],
            null,
            [
                "Bi-LSTM"
            ],
            null,
            [
                "greedy + beam",
                "Test",
                "F1"
            ],
            [
                "greedy + scheduled",
                "Test",
                "F1"
            ],
            [
                "greedy + scheduled",
                "F1"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_4",
        "paper_id": "D17-1296",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1296table_6",
        "description": "As described in section 3.1, to directly compare with the transition-based parsing methods (Honnibal and Johnson, 2014; Wu et al., 2015), we only use MRG files, which are less than the DPS files. In fact, many methods, such as Qian and Liu (2013), have used all the DPS files as training data. We are curious about the performance of our system using all the DPS files. Following the experimental settings of Johnson and Charniak (2004), the corpus is split as follows: main training consisting of all sw[23]*.dps files, development training consisting of all sw4[5-9]*.dps files and test training consisting of all sw4[0-1]*.mrg files. Table 6 shows the result on the DPS files. The result of M3N\u2217 comes from our experiments with the toolkit4 released by Qian and Liu (2013), which use the same data set and pre-processing. Our model achieves a 88.1% F-score by using more training data, obtaining 0.6 point improvement compared with the system training on MRG files. The performance is far better than the sequence labeling methods that use DPS files for training.",
        "sentences": [
            "As described in section 3.1, to directly compare with the transition-based parsing methods (Honnibal and Johnson, 2014; Wu et al., 2015), we only use MRG files, which are less than the DPS files.",
            "In fact, many methods, such as Qian and Liu (2013), have used all the DPS files as training data.",
            "We are curious about the performance of our system using all the DPS files.",
            "Following the experimental settings of Johnson and Charniak (2004), the corpus is split as follows: main training consisting of all sw[23]*.dps files, development training consisting of all sw4[5-9]*.dps files and test training consisting of all sw4[0-1]*.mrg files.",
            "Table 6 shows the result on the DPS files.",
            "The result of M3N\u2217 comes from our experiments with the toolkit4 released by Qian and Liu (2013), which use the same data set and pre-processing.",
            "Our model achieves a 88.1% F-score by using more training data, obtaining 0.6 point improvement compared with the system training on MRG files.",
            "The performance is far better than the sequence labeling methods that use DPS files for training."
        ],
        "class_sentence": [
            2,
            2,
            1,
            2,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Our"
            ],
            null,
            null,
            [
                "M3N*"
            ],
            [
                "Our"
            ],
            [
                "Our"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_6",
        "paper_id": "D17-1296",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1296table_7",
        "description": "Table 7 shows the results of Chinese disfluency detection. Our model obtains a 2.4 point improvement compared with the baseline Bi-LSTM model and a 5.3 point compared with the baseline CRF model. The performance on Chinese is much lower than that on English. Apart from the smaller training set, the main reason is that the proportion of repair type disflueny is much higher.",
        "sentences": [
            "Table 7 shows the results of Chinese disfluency detection.",
            "Our model obtains a 2.4 point improvement compared with the baseline Bi-LSTM model and a 5.3 point compared with the baseline CRF model.",
            "The performance on Chinese is much lower than that on English.",
            "Apart from the smaller training set, the main reason is that the proportion of repair type disflueny is much higher."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "Our",
                "Test",
                "F1",
                "Bi-LSTM",
                "CRF"
            ],
            null,
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_7",
        "paper_id": "D17-1296",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1297table_6",
        "description": "Table 6 presents the performance for a subset of error types that are affected the most re-ranking CAMB16SMT on before and after the FCE test set. The error types are interpreted as follows: Missing error; Replace error; Unnecessary error. The largest improvement is observed in replacement errors referring to possessive nouns (R:NOUN:POSS) and verb agreement (R:VERB:SVA); and in unnecessary errors referring to adverbs (U:ADV), determiners (U:DET), pronouns (U:PRON), and verb tense (U:VERB:TENSE).",
        "sentences": [
            "Table 6 presents the performance for a subset of error types that are affected the most re-ranking CAMB16SMT on before and after the FCE test set.",
            "The error types are interpreted as follows: Missing error; Replace error; Unnecessary error.",
            "The largest improvement is observed in replacement errors referring to possessive nouns (R:NOUN:POSS) and verb agreement (R:VERB:SVA); and in unnecessary errors referring to adverbs (U:ADV), determiners (U:DET), pronouns (U:PRON), and verb tense (U:VERB:TENSE)."
        ],
        "class_sentence": [
            1,
            2,
            1
        ],
        "header_mention": [
            [
                "CAMB16SMT",
                "CAMB16SMT + LSTMcamb"
            ],
            null,
            [
                "R:NOUN:POSS",
                "R:VERB:SVA",
                "U:ADV",
                "U:DET",
                "U:VERB:TENSE",
                "R:CONTR",
                "CAMB16SMT + LSTMcamb",
                "F0.5"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_6",
        "paper_id": "D17-1297",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1298table_1",
        "description": "Table 1 shows the full set of experimental results on the AESW development and test data. The CHAR+BI+DOM model is stronger than the WORD+BI+DOM and CHARCNN+DOM models by 2.9 M2 (0.2 GLEU) and 3.3 M2 (0.3 GLEU), respectively. The sequence-to-sequence models were also more effective than the SMT models, as shown in Table 1. We find that training with target diffs is beneficial across all models, with an increase of about 5 M2 points for the WORD+BI model, for example. Adding +DOM information slightly improves effectiveness across models.",
        "sentences": [
            "Table 1 shows the full set of experimental results on the AESW development and test data.",
            "The CHAR+BI+DOM model is stronger than the WORD+BI+DOM and CHARCNN+DOM models by 2.9 M2 (0.2 GLEU) and 3.3 M2 (0.3 GLEU), respectively.",
            "The sequence-to-sequence models were also more effective than the SMT models, as shown in Table 1.",
            "We find that training with target diffs is beneficial across all models, with an increase of about 5 M2 points for the WORD+BI model, for example.",
            "Adding +DOM information slightly improves effectiveness across models."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "CHAR+BI+DOM",
                "WORD+BI+DOM",
                "CHARCNN+DOM",
                "Dev"
            ],
            null,
            [
                "WORD+BI-DIFFS",
                "WORD+BI",
                "Dev"
            ],
            [
                "CHAR+BI+DOM"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_1",
        "paper_id": "D17-1298",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1299table_1",
        "description": "Table 1 shows that all models based on the vector space achieve similar performance in terms of Spearman's \u03c1 (except SVM-W2V which yields lower performance). The baseline method based on unigram models was outperformed by 0.1+ point. So we select DENSIFIER-LSA as a representative for our FSMT system.",
        "sentences": [
            "Table 1 shows that all models based on the vector space achieve similar performance in terms of Spearman's \u03c1 (except SVM-W2V which yields lower performance).",
            "The baseline method based on unigram models was outperformed by 0.1+ point.",
            "So we select DENSIFIER-LSA as a representative for our FSMT system."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "SVM",
                "W2V"
            ],
            [
                "baseline"
            ],
            [
                "DENSIFIER",
                "LSA"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_1",
        "paper_id": "D17-1299",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1301table_1",
        "description": "Table 1 shows the translation performance in terms of BLEU score. Clearly, the proposed approaches significantly outperforms baseline in all cases.",
        "sentences": [
            "Table 1 shows the translation performance in terms of BLEU score.",
            "Clearly, the proposed approaches significantly outperforms baseline in all cases."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "+Initenc+dec+Gating Auxi"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_1",
        "paper_id": "D17-1301",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1309table_6",
        "description": "Table 6 shows results with or without using the predicted POS tags in the preorderer, as well as including the features used by the tagger in the reorderer directly and only training the downstream task. The preorderer that includes a separate network for POS tagging and then extracts features over the predicted tags is more accurate and smaller than the model that includes all the features that contribute to a POS tag in the reorderer directly. This pipeline processes 7k tokens/second when taking pretokenized text as input, with the POS tagger accounting for 23% of the computation time.",
        "sentences": [
            "Table 6 shows results with or without using the predicted POS tags in the preorderer, as well as including the features used by the tagger in the reorderer directly and only training the downstream task.",
            "The preorderer that includes a separate network for POS tagging and then extracts features over the predicted tags is more accurate and smaller than the model that includes all the features that contribute to a POS tag in the reorderer directly.",
            "This pipeline processes 7k tokens/second when taking pretokenized text as input, with the POS tagger accounting for 23% of the computation time."
        ],
        "class_sentence": [
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "Small FF + POS tags",
                "Small FF + Tagger input fts."
            ],
            [
                "Small FF + POS tags",
                "FRS"
            ],
            [
                "Small FF + POS tags"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_6",
        "paper_id": "D17-1309",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1317table_5",
        "description": "Table 5 summarizes the performance on the development set. We report macro averaged F1 score in all tables. The LSTM outperforms the other models when only using text as input; however the other two models improve substantially with adding LIWC features, particularly in the case of the multinomial naive Bayes model. In contrast, the LIWC features do not improve the neural model much, indicating that some of this lexical information is perhaps redundant to what the model was already learning from text.",
        "sentences": [
            "Table 5 summarizes the performance on the development set.",
            "We report macro averaged F1 score in all tables.",
            "The LSTM outperforms the other models when only using text as input; however the other two models improve substantially with adding LIWC features, particularly in the case of the multinomial naive Bayes model.",
            "In contrast, the LIWC features do not improve the neural model much, indicating that some of this lexical information is perhaps redundant to what the model was already learning from text."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "LSTM",
                "2-CLASS",
                "text",
                "+LIWC",
                "Naive Bayes"
            ],
            [
                "+LIWC",
                "text"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_5",
        "paper_id": "D17-1317",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1318table_1",
        "description": "As shown in Table 1, our system outperforms the other methods with an accuracy of 0.86 in the word-intrusion task with four key concepts in each cluster, while it decreases to 0.67 if we extend the evaluation to include eight key concepts.",
        "sentences": [
            "As shown in Table 1, our system outperforms the other methods with an accuracy of 0.86 in the word-intrusion task with four key concepts in each cluster, while it decreases to 0.67 if we extend the evaluation to include eight key concepts."
        ],
        "class_sentence": [
            1
        ],
        "header_mention": [
            [
                "Key concept Clusters",
                "Acc.@4",
                "Acc.@8"
            ]
        ],
        "n_sentence": 1.0,
        "table_id": "table_1",
        "paper_id": "D17-1318",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1323table_2",
        "description": "Our quantitative results are summarized in the Table 2. On the development set, the number of verbs whose bias exceed the original bias by over 5% decreases 30.5% (Viol.). Overall, we are able to significantly reduce bias amplification in vSRL by 52% on the development set (Amp. bias).",
        "sentences": [
            "Our quantitative results are summarized in the Table 2.",
            "On the development set, the number of verbs whose bias exceed the original bias by over 5% decreases 30.5% (Viol.).",
            "Overall, we are able to significantly reduce bias amplification in vSRL by 52% on the development set (Amp. bias)."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "CRF + RBA",
                "Viol."
            ],
            [
                "CRF + RBA",
                "Amp. bias"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_2",
        "paper_id": "D17-1323",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1001table_3",
        "description": "Effect of defenses. We report results for the main task accuracy and the representation privacy in Table 3 for the +DEMO setting and in Table 4 for the RAW setting. Recall that the privacy measure(Priv.) is computed by 1 - X where X is the average accuracy of the attacker on gender and age predictions. When this privacy metric is higher, it is more difficult to exploit the hidden representation of the network to recover information about x. The \u2018Standard\u2019 columns contain the accuracy and privacy of the base model described in Section 2. The next columns present the absolute variation in accuracy and privacy for the three defense methods presented in Section 3: Multidetasking, Adversarial Generation, and Declustering. We also report for each corpus the most frequent class baseline for the main task accuracy, and the privacy of the most frequent class baselines on private variables (i.e. the upper bound for privacy). The three modified training methods designed as defenses have a positive effect on privacy. Despite a model selection based on accuracy, they lead to an improvement in privacy on all datasets, except on the France subcorpus. In most cases,we observe only a small decrease in accuracy,or even an improvement at times (e.g. multidetasking on the Germany dataset, RAW setting), thus improving the tradeoff between the utility and the privacy of the text representations.",
        "sentences": [
            "Effect of defenses.",
            "We report results for the main task accuracy and the representation privacy in Table 3 for the +DEMO setting and in Table 4 for the RAW setting.",
            "Recall that the privacy measure(Priv.) is computed by 1 - X where X is the average accuracy of the attacker on gender and age predictions.",
            "When this privacy metric is higher, it is more difficult to exploit the hidden representation of the network to recover information about x.",
            "The \u2018Standard\u2019 columns contain the accuracy and privacy of the base model described in Section 2.",
            "The next columns present the absolute variation in accuracy and privacy for the three defense methods presented in Section 3: Multidetasking, Adversarial Generation, and Declustering.",
            "We also report for each corpus the most frequent class baseline for the main task accuracy, and the privacy of the most frequent class baselines on private variables (i.e. the upper bound for privacy).",
            "The three modified training methods designed as defenses have a positive effect on privacy.",
            "Despite a model selection based on accuracy, they lead to an improvement in privacy on all datasets, except on the France subcorpus.",
            "In most cases,we observe only a small decrease in accuracy,or even an improvement at times (e.g. multidetasking on the Germany dataset, RAW setting), thus improving the tradeoff between the utility and the privacy of the text representations."
        ],
        "class_sentence": [
            2,
            1,
            2,
            2,
            1,
            1,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Priv."
            ],
            [
                "Priv."
            ],
            [
                "Standard",
                "Main",
                "Priv."
            ],
            [
                "M-Detask.",
                "A-Gener.",
                "Decl. alpha = 0.1",
                "Main",
                "Priv."
            ],
            null,
            [
                "M-Detask.",
                "A-Gener.",
                "Decl. alpha = 0.1",
                "Priv."
            ],
            [
                "M-Detask.",
                "A-Gener.",
                "Decl. alpha = 0.1",
                "Priv.",
                "Germany",
                "Denmark",
                "UK",
                "US"
            ],
            [
                "M-Detask.",
                "A-Gener.",
                "Decl. alpha = 0.1",
                "Main"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_3",
        "paper_id": "D18-1001",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1001table_4",
        "description": "Effect of defenses. We report results for the main task accuracy and the representation privacy in Table 3 for the +DEMO setting and in Table 4 for the RAW setting. Recall that the privacy measure(Priv.) is computed by 1 - X where X is the average accuracy of the attacker on gender and age predictions. When this privacy metric is higher, it is more difficult to exploit the hidden representation of the network to recover information about x. The \u2018Standard\u2019 columns contain the accuracy and privacy of the base model described in Section 2. The next columns present the absolute variation in accuracy and privacy for the three defense methods presented in Section 3: Multidetasking, Adversarial Generation, and Declustering. We also report for each corpus the most frequent class baseline for the main task accuracy, and the privacy of the most frequent class baselines on private variables (i.e. the upper bound for privacy). The three modified training methods designed as defenses have a positive effect on privacy. Despite a model selection based on accuracy, they lead to an improvement in privacy on all datasets, except on the France subcorpus. In most cases,we observe only a small decrease in accuracy,or even an improvement at times (e.g. multidetasking on the Germany dataset, RAW setting), thus improving the tradeoff between the utility and the privacy of the text representations.",
        "sentences": [
            "Effect of defenses.",
            "We report results for the main task accuracy and the representation privacy in Table 3 for the +DEMO setting and in Table 4 for the RAW setting.",
            "Recall that the privacy measure(Priv.) is computed by 1 - X where X is the average accuracy of the attacker on gender and age predictions.",
            "When this privacy metric is higher, it is more difficult to exploit the hidden representation of the network to recover information about x.",
            "The \u2018Standard\u2019 columns contain the accuracy and privacy of the base model described in Section 2.",
            "The next columns present the absolute variation in accuracy and privacy for the three defense methods presented in Section 3: Multidetasking, Adversarial Generation, and Declustering.",
            "We also report for each corpus the most frequent class baseline for the main task accuracy, and the privacy of the most frequent class baselines on private variables (i.e. the upper bound for privacy).",
            "The three modified training methods designed as defenses have a positive effect on privacy.",
            "Despite a model selection based on accuracy, they lead to an improvement in privacy on all datasets, except on the France subcorpus.",
            "In most cases,we observe only a small decrease in accuracy,or even an improvement at times (e.g. multidetasking on the Germany dataset, RAW setting), thus improving the tradeoff between the utility and the privacy of the text representations."
        ],
        "class_sentence": [
            2,
            1,
            2,
            2,
            1,
            1,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Priv."
            ],
            [
                "Priv."
            ],
            [
                "Standard",
                "Main",
                "Priv."
            ],
            [
                "M-Detask.",
                "A-Gener.",
                "Decl. alpha = 0.1",
                "Main",
                "Priv."
            ],
            null,
            [
                "M-Detask.",
                "A-Gener.",
                "Decl. alpha = 0.1",
                "Priv."
            ],
            [
                "M-Detask.",
                "A-Gener.",
                "Decl. alpha = 0.1",
                "Priv.",
                "Germany",
                "Denmark",
                "UK",
                "US"
            ],
            [
                "M-Detask.",
                "A-Gener.",
                "Decl. alpha = 0.1",
                "Main"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_4",
        "paper_id": "D18-1001",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1002table_4",
        "description": "All methods are effective to some extent, Table 4 summarizes the results. Increasing the capacity of the adversarial network helped reduce the protected attribute\u00e2\u20ac\u2122s leakage, though different capacities work best on each setup. On the Sentiment/Race task, none of the higher dimensional adversaries worked better than the 300-dim one, on the PAN16 dataset it did. On PAN16/Gender the 8000-dim adversary performed best, and on PAN16/Age, the 500-dim one. Increasing the weight of the adversary through the lambda parameter also has a positive effect on the result (except on the Sentiment/Race pair). However, too large lambda values make training unstable, and require many more epochs for the main-task to stabilize around a satisfying accuracy. The adversarial ensemble method with 2 adversaries achieves 57.4% on Sentiment/Race, as opposed to 56.0% with a single one, but when using 5 different adversaries, we achieve 54.8%. On the PAN16 dataset larger ensembles are more effective.",
        "sentences": [
            "All methods are effective to some extent, Table 4 summarizes the results.",
            "Increasing the capacity of the adversarial network helped reduce the protected attribute\u00e2\u20ac\u2122s leakage, though different capacities work best on each setup.",
            "On the Sentiment/Race task, none of the higher dimensional adversaries worked better than the 300-dim one, on the PAN16 dataset it did.",
            "On PAN16/Gender the 8000-dim adversary performed best, and on PAN16/Age, the 500-dim one.",
            "Increasing the weight of the adversary through the lambda parameter also has a positive effect on the result (except on the Sentiment/Race pair).",
            "However, too large lambda values make training unstable, and require many more epochs for the main-task to stabilize around a satisfying accuracy.",
            "The adversarial ensemble method with 2 adversaries achieves 57.4% on Sentiment/Race, as opposed to 56.0% with a single one, but when using 5 different adversaries, we achieve 54.8%.",
            "On the PAN16 dataset larger ensembles are more effective."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Adv-Capacity"
            ],
            [
                "Adv-Capacity",
                "Standard Adversary",
                "Sentiment",
                "Race",
                "PAN16"
            ],
            [
                "PAN16",
                "Gender",
                "Age"
            ],
            [
                "lambda",
                "Standard Adversary",
                "PAN16"
            ],
            [
                "lambda"
            ],
            [
                "Ensemble",
                "Standard Adversary",
                "Sentiment",
                "Race"
            ],
            [
                "Ensemble",
                "PAN16"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_4",
        "paper_id": "D18-1002",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1006table_1",
        "description": "7.1 Comparison with Baselines. We compare our model (which make use of world knowledge) with the four baseline systems on the ProPara dataset. Table 1 shows the precision, recall, and F1 for all models on the the test partition. PROSTRUCT significantly outperforms the baselines, suggesting that world knowledge helps ProStruct avoid spurious predictions. This hypothesis is supported by the fact that the ProGlobal model has the highest recall and worst precision, indicating that it is over-generating state change predictions. Conversely, the ProLocal model has the highest precision, but its recall is much lower, likely because it makes predictions for individual sentences, and thus has no access to information in surrounding sentences that may suggest a state change is occurring.",
        "sentences": [
            "7.1 Comparison with Baselines.",
            "We compare our model (which make use of world knowledge) with the four baseline systems on the ProPara dataset.",
            "Table 1 shows the precision, recall, and F1 for all models on the the test partition.",
            "PROSTRUCT significantly outperforms the baselines, suggesting that world knowledge helps ProStruct avoid spurious predictions.",
            "This hypothesis is supported by the fact that the ProGlobal model has the highest recall and worst precision, indicating that it is over-generating state change predictions.",
            "Conversely, the ProLocal model has the highest precision, but its recall is much lower, likely because it makes predictions for individual sentences, and thus has no access to information in surrounding sentences that may suggest a state change is occurring."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "ProStruct",
                "ProLocal",
                "QRN",
                "EntNet",
                "ProGlobal"
            ],
            [
                "ProStruct",
                "ProLocal",
                "QRN",
                "EntNet",
                "ProGlobal",
                "Precision",
                "Recall",
                "F1"
            ],
            [
                "ProStruct",
                "ProLocal",
                "QRN",
                "EntNet",
                "ProGlobal"
            ],
            [
                "ProGlobal",
                "Recall",
                "Precision"
            ],
            [
                "ProLocal",
                "Precision",
                "Recall"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_1",
        "paper_id": "D18-1006",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1010table_2",
        "description": "Performance of passage retrieval. Table 2 compares our wikipage retriever with the one in (Thorne et al., 2018), which used a document retriever from DrQA (Chen et al., 2017). Our document retrieval module surpasses the competitor by a big margin in terms of the coverage of gold passages: 89.63% vs. 55.30% (k = 5 in all experiments). Its powerfulness should be attributed to:. (i) Entity mention detection in the claims. (ii) As wiki titles are entities, we have a bi-channel way to match the claim with the wiki page: one with the title, the other with the page body, as shown in Algorithm 1.",
        "sentences": [
            "Performance of passage retrieval.",
            "Table 2 compares our wikipage retriever with the one in (Thorne et al., 2018), which used a document retriever from DrQA (Chen et al., 2017).",
            "Our document retrieval module surpasses the competitor by a big margin in terms of the coverage of gold passages: 89.63% vs. 55.30% (k = 5 in all experiments).",
            "Its powerfulness should be attributed to:.",
            "(i) Entity mention detection in the claims.",
            "(ii) As wiki titles are entities, we have a bi-channel way to match the claim with the wiki page: one with the title, the other with the page body, as shown in Algorithm 1."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "ours",
                "(Thorne et al. 2018)"
            ],
            [
                "ours",
                "(Thorne et al. 2018)",
                "rate"
            ],
            null,
            null,
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_2",
        "paper_id": "D18-1010",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1010table_3",
        "description": "Performance on FEVER. Table 3 lists the performances of baselines and the TWOWINGOS variants on FEVER (dev&test). From the dev block, we observe that:. TWOWINGOS (from \"share-CNN\") surpasses prior systems in big margins. Overall,fine-grained schemes in each subtask contribute more than the coarse-grained counterparts;. In the three setups - \"pipeline\", \"diff-CNN\" and \"share-CNN\" - of coarse-coarse, \"pipeline\" gets better scores than (Thorne et al., 2018) in terms of evidence identification. \"Share-CNN\" has comparable F1 as \"diff-CNN\" while gaining a lot on NOSCOREEV (72.32 vs. 39.22) and SCOREEV (50.12 vs. 21.04). This clearly shows that the claim verification gains much knowledge transferred from the evidence identification module. Both \"diff-CNN\" and \"share-CNN\" perform better than \"pipeline\" (except for the slight inferiority at SCOREEV: 21.04 vs. 22.26). Two-channel fine-grained representations show more effective than the single-channel counterpart in claim verification (NOSCOREEV: 78.77 vs. 75.65, SCOREEV: 53.64 vs. 52.65). As we expected, evidence sentences should collaborate in inferring the truth value of the claims. Two-channel setup enables an evidence candidate aware of other candidates as well as the claim. In the last three rows of dev, there is no clear difference among their evidence identification scores. Recall that \"sent-wise\" is essentially an ensemble system over each (sentence, claim) entailment result. \"Coarse-grained\", instead, first sums up all sentence representation, then performs (\u00ce\u00a3(sentence), claim) reasoning. We can also treat this \"sum up\" as an ensemble. Their comparison shows that these two kinds of tricks do not make much difference. If we adopt \"two-channel fine-grained representation\" in claim verification, big improvements are observed in both NOSCOREEV (+7.42%) and SCOREEV (+3%). In the test block, our system (fine & fine (two)) beats the prior top system across all measurements by big margins F1: 47.15 vs. 17.47; SCOREEV: 54.33 vs. 31.87; NOSCOREEV: 75.99 vs. 50.91. In both dev and test blocks, we can observe that our evidence identification module consistently obtains balanced recall and precision.  In contrast, the pipeline system by Thorne et al. (2018) has much higher recall than precision (45.89 vs. 10.79). It is worth mentioning that the SCOREEV metric is highly influenced by the recall value, since SCOREEV is computed on the claim instances whose evidences are fully retrieved, regardless of the precision. So, ideally, a system can set all sentences as evidence, so that SCOREEV can be promoted to be equal to NOSCOREEV.",
        "sentences": [
            "Performance on FEVER.",
            "Table 3 lists the performances of baselines and the TWOWINGOS variants on FEVER (dev&test).",
            "From the dev block, we observe that:.",
            "TWOWINGOS (from \"share-CNN\") surpasses prior systems in big margins.",
            "Overall,fine-grained schemes in each subtask contribute more than the coarse-grained counterparts;.",
            "In the three setups - \"pipeline\", \"diff-CNN\" and \"share-CNN\" - of coarse-coarse, \"pipeline\" gets better scores than (Thorne et al., 2018) in terms of evidence identification.",
            "\"Share-CNN\" has comparable F1 as \"diff-CNN\" while gaining a lot on NOSCOREEV (72.32 vs. 39.22) and SCOREEV (50.12 vs. 21.04).",
            "This clearly shows that the claim verification gains much knowledge transferred from the evidence identification module.",
            "Both \"diff-CNN\" and \"share-CNN\" perform better than \"pipeline\" (except for the slight inferiority at SCOREEV: 21.04 vs. 22.26).",
            "Two-channel fine-grained representations show more effective than the single-channel counterpart in claim verification (NOSCOREEV: 78.77 vs. 75.65, SCOREEV: 53.64 vs. 52.65).",
            "As we expected, evidence sentences should collaborate in inferring the truth value of the claims.",
            "Two-channel setup enables an evidence candidate aware of other candidates as well as the claim.",
            "In the last three rows of dev, there is no clear difference among their evidence identification scores.",
            "Recall that \"sent-wise\" is essentially an ensemble system over each (sentence, claim) entailment result.",
            "\"Coarse-grained\", instead, first sums up all sentence representation, then performs (\u00ce\u00a3(sentence), claim) reasoning.",
            "We can also treat this \"sum up\" as an ensemble.",
            "Their comparison shows that these two kinds of tricks do not make much difference.",
            "If we adopt \"two-channel fine-grained representation\" in claim verification, big improvements are observed in both NOSCOREEV (+7.42%) and SCOREEV (+3%).",
            "In the test block, our system (fine & fine (two)) beats the prior top system across all measurements by big margins F1: 47.15 vs. 17.47; SCOREEV: 54.33 vs. 31.87; NOSCOREEV: 75.99 vs. 50.91.",
            "In both dev and test blocks, we can observe that our evidence identification module consistently obtains balanced recall and precision.",
            " In contrast, the pipeline system by Thorne et al. (2018) has much higher recall than precision (45.89 vs. 10.79).",
            "It is worth mentioning that the SCOREEV metric is highly influenced by the recall value, since SCOREEV is computed on the claim instances whose evidences are fully retrieved, regardless of the precision.",
            "So, ideally, a system can set all sentences as evidence, so that SCOREEV can be promoted to be equal to NOSCOREEV."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2,
            1,
            1,
            2,
            1,
            1,
            2,
            2,
            1,
            2,
            0,
            0,
            0,
            2,
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "TWOWINGOS",
                "MLP",
                "Decomp-Att"
            ],
            null,
            [
                "share-CNN",
                "MLP",
                "Decomp-Att"
            ],
            null,
            [
                "pipeline",
                "diff-CNN",
                "share-CNN",
                "(Thorne et al. 2018)",
                "evidence identification"
            ],
            [
                "share-CNN",
                "diff-CNN",
                "NOSCOREEV",
                "SCOREEV",
                "F1"
            ],
            [
                "claim verification",
                "evidence identification"
            ],
            [
                "pipeline",
                "diff-CNN",
                "share-CNN"
            ],
            [
                "coarse & fine (two)",
                "coarse & fine (single)",
                "claim verification",
                "NOSCOREEV",
                "SCOREEV"
            ],
            null,
            [
                "coarse & fine (two)",
                "fine & fine (two)"
            ],
            [
                "fine & sent-wise",
                "fine & coarse",
                "fine & fine (two)",
                "evidence identification"
            ],
            [
                "fine & sent-wise"
            ],
            null,
            null,
            null,
            [
                "claim verification",
                "SCOREEV",
                "NOSCOREEV"
            ],
            [
                "test",
                "(Thorne et al. 2018)",
                "TWOWINGOS",
                "fine & fine (two)",
                "F1",
                "SCOREEV",
                "NOSCOREEV"
            ],
            [
                "dev",
                "test",
                "evidence identification",
                "TWOWINGOS",
                "recall",
                "precision"
            ],
            [
                "(Thorne et al. 2018)",
                "recall",
                "precision"
            ],
            [
                "SCOREEV",
                "recall",
                "precision"
            ],
            [
                "SCOREEV",
                "NOSCOREEV"
            ]
        ],
        "n_sentence": 23.0,
        "table_id": "table_3",
        "paper_id": "D18-1010",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1013table_2",
        "description": "Table 2 shows the results on COCO. Among the directly comparable models, our model is arguably the best and outperforms the existing models except in terms of BLEU-4. Most encouragingly, our model is also competitive with Up-Down (Ander-son et al. 2018), which uses much larger dataset, Visual Genome (Krishna et al. 2017), with dense annotations to train the object detector, and directly optimizes CIDEr. Especially, our model outperforms the state-of-the-art substantially in SPICE and METEOR.",
        "sentences": [
            "Table 2 shows the results on COCO.",
            "Among the directly comparable models, our model is arguably the best and outperforms the existing models except in terms of BLEU-4.",
            "Most encouragingly, our model is also competitive with Up-Down (Ander-son et al. 2018), which uses much larger dataset, Visual Genome (Krishna et al. 2017), with dense annotations to train the object detector, and directly optimizes CIDEr.",
            "Especially, our model outperforms the state-of-the-art substantially in SPICE and METEOR."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            [
                "COCO"
            ],
            [
                "simNet",
                "HardAtt (Xu et al. 2015)",
                "ATT-FCN (You et al. 2016)",
                "SCA-CNN (Chen et al. 2017)",
                "LSTM-A (Yao et al. 2017)",
                "SCN-LSTM (Gan et al. 2017)",
                "Skeleton (Wang et al. 2017)",
                "AdaAtt (Lu et al. 2017)",
                "NBT (Lu et al. 2018)",
                "DRL (Ren et al. 2017b)",
                "TD-M-ATT (Chen et al. 2018)",
                "SCST (Rennie et al. 2017)",
                "BLEU-4"
            ],
            [
                "simNet",
                "Up-Down (Anderson et al. 2018)"
            ],
            [
                "simNet",
                "Up-Down (Anderson et al. 2018)",
                "SPICE",
                "METEOR"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "D18-1013",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1013table_6",
        "description": "A Supplementary Material. A.1 Results on COCO Evaluation Server. Table 6 shows the performance on the online COCO evaluation server. We put it in the appendix because the results are incomplete and the SPICE metric is not available for our submission, which correlates the best with human evaluation. The SPICE metrics are only available at the leaderboard on the COCO dataset website, which, unfortunately, has not been updated for more than a year. Our submission does not directly optimize CIDEr, use model ensemble, or use extra training data. The three techniques typically result in orthogonal improvements (Lu et al. 2017; Rennie et al. 2017; Anderson et al. 2018). Moreover, the SPICE results are missing, in which the proposed model has the most advantage. Nonetheless, our model is second only to Up-Down (Anderson et al. 2018) and surpasses almost all the other models in published work, especially when 40 references are considered.",
        "sentences": [
            "A Supplementary Material.",
            "A.1 Results on COCO Evaluation Server.",
            "Table 6 shows the performance on the online COCO evaluation server.",
            "We put it in the appendix because the results are incomplete and the SPICE metric is not available for our submission, which correlates the best with human evaluation.",
            "The SPICE metrics are only available at the leaderboard on the COCO dataset website, which, unfortunately, has not been updated for more than a year.",
            "Our submission does not directly optimize CIDEr, use model ensemble, or use extra training data.",
            "The three techniques typically result in orthogonal improvements (Lu et al. 2017; Rennie et al. 2017; Anderson et al. 2018).",
            "Moreover, the SPICE results are missing, in which the proposed model has the most advantage.",
            "Nonetheless, our model is second only to Up-Down (Anderson et al. 2018) and surpasses almost all the other models in published work, especially when 40 references are considered."
        ],
        "class_sentence": [
            2,
            2,
            1,
            2,
            0,
            2,
            0,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "COCO"
            ],
            null,
            null,
            null,
            null,
            [
                "simNet"
            ],
            [
                "simNet",
                "Up-Down (Anderson et al. 2018)",
                "c40",
                "HardAtt (Xu et al. 2015)",
                "ATT-FCN (You et al. 2016)",
                "SCA-CNN (Chen et al. 2017)",
                "LSTM-A (Yao et al. 2017)",
                "SCN-LSTM (Gan et al. 2017)",
                "AdaAtt (Lu et al. 2017)",
                "TD-M-ATT (Chen et al. 2018)",
                "SCST (Rennie et al. 2017)"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_6",
        "paper_id": "D18-1013",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1015table_1",
        "description": "4.3 Experimental Results and Analysis. 4.3.1 Comparisons with State-of-the-Arts. Experiments on DiDeMo. Table 1 illustrates the performance comparisons on the DiDeMo dataset. In addition to MCN, we also compare with the baseline Moment Frequency Prior (MFP) in (Hendricks et al., 2017), which selects segments corresponding to the positions of videos in the training dataset with most annotations. First, TGN with different features can significantly outperforms the \"prior baseline\" MFP, which retrieves segments corresponding to the most common start and end points in the dataset. Second, it can be observed that with the same visual features, specifically VGG16 and optical flow, TGN significantly outperforms MCN. And the performance of TGN with optical flow is better than that with VGG16. One possible reason is that the videos in DiDeMo are relatively short, which only contain a single event. In such a case, the action information plays a more critical role. This finding is also consistent with (Hendricks et al., 2017). By fusing the results obtained by VGG16 and optical flow together, the performance can be further boosted, as demonstrated by TGN-Fusion and MCN-Fusion. Third, MCN introduces the temporal endpoint feature (TEF) as prior knowledge, which indicates when a segment occurs in a video. With TEF, the performance of MCN can be significantly improved. However, it is still inferior to our proposed TGN.",
        "sentences": [
            "4.3 Experimental Results and Analysis.",
            "4.3.1 Comparisons with State-of-the-Arts.",
            "Experiments on DiDeMo.",
            "Table 1 illustrates the performance comparisons on the DiDeMo dataset.",
            "In addition to MCN, we also compare with the baseline Moment Frequency Prior (MFP) in (Hendricks et al., 2017), which selects segments corresponding to the positions of videos in the training dataset with most annotations.",
            "First, TGN with different features can significantly outperforms the \"prior baseline\" MFP, which retrieves segments corresponding to the most common start and end points in the dataset.",
            "Second, it can be observed that with the same visual features, specifically VGG16 and optical flow, TGN significantly outperforms MCN.",
            "And the performance of TGN with optical flow is better than that with VGG16.",
            "One possible reason is that the videos in DiDeMo are relatively short, which only contain a single event.",
            "In such a case, the action information plays a more critical role.",
            "This finding is also consistent with (Hendricks et al., 2017).",
            "By fusing the results obtained by VGG16 and optical flow together, the performance can be further boosted, as demonstrated by TGN-Fusion and MCN-Fusion.",
            "Third, MCN introduces the temporal endpoint feature (TEF) as prior knowledge, which indicates when a segment occurs in a video.",
            "With TEF, the performance of MCN can be significantly improved.",
            "However, it is still inferior to our proposed TGN."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            1,
            1,
            1,
            1,
            2,
            2,
            0,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            [
                "MFP",
                "MCN-VGG16",
                "MCN-Flow",
                "MCN-Fusion",
                "MCN-Fusion+TEF"
            ],
            [
                "MFP",
                "TGN-VGG16",
                "TGN-Flow",
                "TGN-Fusion"
            ],
            [
                "TGN-VGG16",
                "TGN-Flow",
                "MCN-VGG16",
                "MCN-Flow"
            ],
            [
                "TGN-Flow",
                "TGN-VGG16"
            ],
            null,
            null,
            null,
            [
                "TGN-Fusion",
                "MCN-Fusion"
            ],
            [
                "MCN-Fusion+TEF"
            ],
            [
                "MCN-Fusion+TEF"
            ],
            [
                "TGN-Fusion",
                "MCN-Fusion+TEF"
            ]
        ],
        "n_sentence": 15.0,
        "table_id": "table_1",
        "paper_id": "D18-1015",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1017table_2",
        "description": "4.3.1  Evaluation on WeiboNER. We compare our proposed model with the latest models on WeiboNER dataset. Table 2 shows the experimental results for named entities on the original WeiboNER dataset. In the first block of Table 2, we give the performance of the main model and baselines proposed by Peng and Dredze (2015). They propose a CRF-based model to jointly train the embeddings with NER task, which achieves better results than pipeline models. In addition, they consider the position of each character in a word to train character and position embeddings. In the second block of Table 2, we report the performance of the main model and baselines proposed by Peng and Dredze (2016). Aiming to incorporate word boundary information into the NER task, they propose an integrated model that can joint training CWS task, improving the F1 score from 46.20% to 48.41% as compared with pipeline model (Pipeline Seg.Repr.+NER). In the last block of Table 2, we give the experimental result of our proposed model (BiLSTM+CRF+adversarial+self-attention). We can observe that our proposed model significantly outperforms other models. Compared with the model proposed by Peng and Dredze (2016), our method gains 4.67% improvement in F1 score. Interestingly, WeiboNER dataset and MSR dataset are different domains. The WeiboNER dataset is social media domain, while the MSR dataset can be regard as news domain. The improvement of performance indicates that our proposed adversarial transfer learning framework may not only learn task-shared word boundary information from CWS task but also tackle the domain adaptation problem.",
        "sentences": [
            "4.3.1  Evaluation on WeiboNER.",
            "We compare our proposed model with the latest models on WeiboNER dataset.",
            "Table 2 shows the experimental results for named entities on the original WeiboNER dataset.",
            "In the first block of Table 2, we give the performance of the main model and baselines proposed by Peng and Dredze (2015).",
            "They propose a CRF-based model to jointly train the embeddings with NER task, which achieves better results than pipeline models.",
            "In addition, they consider the position of each character in a word to train character and position embeddings.",
            "In the second block of Table 2, we report the performance of the main model and baselines proposed by Peng and Dredze (2016).",
            "Aiming to incorporate word boundary information into the NER task, they propose an integrated model that can joint training CWS task, improving the F1 score from 46.20% to 48.41% as compared with pipeline model (Pipeline Seg.Repr.+NER).",
            "In the last block of Table 2, we give the experimental result of our proposed model (BiLSTM+CRF+adversarial+self-attention).",
            "We can observe that our proposed model significantly outperforms other models.",
            "Compared with the model proposed by Peng and Dredze (2016), our method gains 4.67% improvement in F1 score.",
            "Interestingly, WeiboNER dataset and MSR dataset are different domains.",
            "The WeiboNER dataset is social media domain, while the MSR dataset can be regard as news domain.",
            "The improvement of performance indicates that our proposed adversarial transfer learning framework may not only learn task-shared word boundary information from CWS task but also tackle the domain adaptation problem."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            2,
            2,
            1,
            1,
            1,
            1,
            1,
            2,
            0,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "CRF (Peng and Dredze 2015)",
                "CRF+word (Peng and Dredze 2015)",
                "CRF+character (Peng and Dredze 2015)",
                "CRF+character+position (Peng and Dredze 2015)",
                "Joint(cp) (main) (Peng and Dredze 2015)"
            ],
            [
                "CRF (Peng and Dredze 2015)"
            ],
            [
                "CRF+word (Peng and Dredze 2015)",
                "CRF+character (Peng and Dredze 2015)",
                "CRF+character+position (Peng and Dredze 2015)",
                "Joint(cp) (main) (Peng and Dredze 2015)"
            ],
            [
                "Pipeline Seg.Repr.+NER (Peng and Dredze 2016)",
                "Jointly Train Char.Emb (Peng and Dredze 2016)",
                "Jointly Train LSTM Hidden (Peng and Dredze 2016)",
                "Jointly Train LSTM+Emb (main) (Peng and Dredze 2016)"
            ],
            [
                "F1(%)",
                "Pipeline Seg.Repr.+NER (Peng and Dredze 2016)",
                "Jointly Train LSTM+Emb (main) (Peng and Dredze 2016)"
            ],
            [
                "BiLSTM+CRF+adversarial+self-attention"
            ],
            [
                "BiLSTM+CRF+adversarial+self-attention",
                "CRF (Peng and Dredze 2015)",
                "CRF+word (Peng and Dredze 2015)",
                "CRF+character (Peng and Dredze 2015)",
                "CRF+character+position (Peng and Dredze 2015)",
                "Joint(cp) (main) (Peng and Dredze 2015)",
                "Pipeline Seg.Repr.+NER (Peng and Dredze 2016)",
                "Jointly Train Char.Emb (Peng and Dredze 2016)",
                "Jointly Train LSTM Hidden (Peng and Dredze 2016)",
                "Jointly Train LSTM+Emb (main) (Peng and Dredze 2016)"
            ],
            [
                "BiLSTM+CRF+adversarial+self-attention",
                "Jointly Train LSTM+Emb (main) (Peng and Dredze 2016)",
                "F1(%)"
            ],
            null,
            null,
            null
        ],
        "n_sentence": 14.0,
        "table_id": "table_2",
        "paper_id": "D18-1017",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1017table_3",
        "description": "We also conduct an experiment on the updated WeiboNER dataset. Table 3 lists the performance of the latest models and our proposed model on the updated dataset. In the first block of Table 3, we report the performance of the latest models. The model proposed by Peng and Dredze (2015) achieves F1 score of 56.05% on overall performance. He and Sun (2017b) propose an unified model for Chinese NER task to exploit the data from out-of-domain corpus and in-domain unlabelled texts. The unified model improves the F1 score from 54.82% to 58.23% compared with the model proposed by He and Sun (2017a). In the second block of Table 3, we give the result of our proposed model. It can be observed that our proposed model achieves a very competitive performance. Compared with the latest model proposed by He and Sun (2017b), our model improves the F1 score from 58.23% to 58.70% on overall performance. The improvement demonstrates the effectiveness of our proposed model.",
        "sentences": [
            "We also conduct an experiment on the updated WeiboNER dataset.",
            "Table 3 lists the performance of the latest models and our proposed model on the updated dataset.",
            "In the first block of Table 3, we report the performance of the latest models.",
            "The model proposed by Peng and Dredze (2015) achieves F1 score of 56.05% on overall performance.",
            "He and Sun (2017b) propose an unified model for Chinese NER task to exploit the data from out-of-domain corpus and in-domain unlabelled texts.",
            "The unified model improves the F1 score from 54.82% to 58.23% compared with the model proposed by He and Sun (2017a).",
            "In the second block of Table 3, we give the result of our proposed model.",
            "It can be observed that our proposed model achieves a very competitive performance.",
            "Compared with the latest model proposed by He and Sun (2017b), our model improves the F1 score from 58.23% to 58.70% on overall performance.",
            "The improvement demonstrates the effectiveness of our proposed model."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2,
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Peng and Dredze (2015)",
                "Peng and Dredze (2016)",
                "He and Sun (2017a)",
                "He and Sun (2017b)",
                "BiLSTM+CRF+adv+self-attention"
            ],
            [
                "Peng and Dredze (2015)",
                "Peng and Dredze (2016)",
                "He and Sun (2017a)",
                "He and Sun (2017b)"
            ],
            [
                "Peng and Dredze (2015)",
                "Overall",
                "F1(%)"
            ],
            [
                "He and Sun (2017b)"
            ],
            [
                "He and Sun (2017b)",
                "He and Sun (2017a)",
                "Overall",
                "F1(%)"
            ],
            [
                "BiLSTM+CRF+adv+self-attention"
            ],
            [
                "BiLSTM+CRF+adv+self-attention"
            ],
            [
                "BiLSTM+CRF+adv+self-attention",
                "He and Sun (2017b)",
                "Overall",
                "F1(%)"
            ],
            [
                "BiLSTM+CRF+adv+self-attention"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_3",
        "paper_id": "D18-1017",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1017table_4",
        "description": "4.3.2 Evaluation on SighanNER. Table 4 lists the comparisons on SighanNER dataset. We observe that our proposed model achieves new state-of-the-art performance. In the first block, we give the performance of previous methods for Chinese NER task on SighanNER dataset. Chen et al. (2006) propose a character-based CRF model for Chinese NER task. Zhou et al. (2006) introduce a pipeline model, which first segments the text with character-level CRF model and then applies word-level CRF to tag. Luo and Yang (2016) first train a word segmenter and then use word segmentation as additional features for sequence tagging. Although the model achieves competitive performance, giving the F1 score of 89.21%, it suffers from the error propagation problem. In the second block, we report the result of our proposed model. Compared with the state-of-the-art model proposed by Luo and Yang (2016), our method improves the F1 score from 89.21% to 90.64% without any additional features, which demonstrates the effectiveness of our proposed model.",
        "sentences": [
            "4.3.2 Evaluation on SighanNER.",
            "Table 4 lists the comparisons on SighanNER dataset.",
            "We observe that our proposed model achieves new state-of-the-art performance.",
            "In the first block, we give the performance of previous methods for Chinese NER task on SighanNER dataset.",
            "Chen et al. (2006) propose a character-based CRF model for Chinese NER task.",
            "Zhou et al. (2006) introduce a pipeline model, which first segments the text with character-level CRF model and then applies word-level CRF to tag.",
            "Luo and Yang (2016) first train a word segmenter and then use word segmentation as additional features for sequence tagging.",
            "Although the model achieves competitive performance, giving the F1 score of 89.21%, it suffers from the error propagation problem.",
            "In the second block, we report the result of our proposed model.",
            "Compared with the state-of-the-art model proposed by Luo and Yang (2016), our method improves the F1 score from 89.21% to 90.64% without any additional features, which demonstrates the effectiveness of our proposed model."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2,
            2,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "BiLSTM+CRF+adversarial+self-attention"
            ],
            [
                "Chen et al. (2006)",
                "Zhou et al. (2006)",
                "Luo and Yang (2016)"
            ],
            [
                "Chen et al. (2006)"
            ],
            [
                "Zhou et al. (2006)"
            ],
            null,
            [
                "Luo and Yang (2016)",
                "F1(%)"
            ],
            [
                "BiLSTM+CRF+adversarial+self-attention"
            ],
            [
                "BiLSTM+CRF+adversarial+self-attention",
                "Luo and Yang (2016)",
                "F1(%)"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_4",
        "paper_id": "D18-1017",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1017table_5",
        "description": "Table 5 provides the experimental results of our proposed model and baseline as well as its simplified models on SighanNER dataset and WeiboNER dataset. The simplified models are described as follows:. BiLSTM+CRF: The model is used as strong baseline in our work, which is trained using Chinese NER training data. BiLSTM+CRF+transfer: We apply transfer learning to BiLSTM+CRF model without adversarial loss and self-attention mechanism. BiLSTM+CRF+adversarial: Compared with BiLSTM+CRF+transfer model, the BiLSTM+CRF+adversarial model incorporates adversarial training. BiLSTM+CRF+self-attention: The model integrates the self-attention mechanism based on BiLSTM+CRF model. From the experimental results of Table 5, we have following observations:. Effectiveness of transfer learning. BiLSTM+CRF+transfer improves F1 score from 89.13% to 89.89% as compared with BiLSTM+CRF on SighanNER dataset and achieves 1.08% improvement on WeiboNER dataset, which indicates the word boundary information from CWS is very effective for Chinese NER task. Effectiveness of adversarial training. By introducing adversarial training, BiLSTM+CRF+adversarial boosts the performance as compared with BiLSTM+CRF+transfer model, showing 0.15% and 0.36% improvement on SighanNER dataset and WeiboNER dataset, respectively. It proves that adversarial training can prevent specific features of CWS task from creeping into shared space. Effectiveness of self-attention mechanism. When compared with BiLSTM+CRF, the BiLSTM+CRF+self-attention significantly improves the performance on the two different datasets with the help of information learned from self-attention, which verifies that the self-attention mechanism is effective for Chinese NER task. We observe that our proposed adversarial transfer learning framework and self-attention lead to noticeable improvements over the baseline, improving F1 score from 51.01% to 53.08% on WeiboNER dataset and giving 1.51% improvement on SighanNER dataset.",
        "sentences": [
            "Table 5 provides the experimental results of our proposed model and baseline as well as its simplified models on SighanNER dataset and WeiboNER dataset.",
            "The simplified models are described as follows:.",
            "BiLSTM+CRF: The model is used as strong baseline in our work, which is trained using Chinese NER training data.",
            "BiLSTM+CRF+transfer: We apply transfer learning to BiLSTM+CRF model without adversarial loss and self-attention mechanism.",
            "BiLSTM+CRF+adversarial: Compared with BiLSTM+CRF+transfer model, the BiLSTM+CRF+adversarial model incorporates adversarial training.",
            "BiLSTM+CRF+self-attention: The model integrates the self-attention mechanism based on BiLSTM+CRF model.",
            "From the experimental results of Table 5, we have following observations:.",
            "Effectiveness of transfer learning.",
            "BiLSTM+CRF+transfer improves F1 score from 89.13% to 89.89% as compared with BiLSTM+CRF on SighanNER dataset and achieves 1.08% improvement on WeiboNER dataset, which indicates the word boundary information from CWS is very effective for Chinese NER task.",
            "Effectiveness of adversarial training.",
            "By introducing adversarial training, BiLSTM+CRF+adversarial boosts the performance as compared with BiLSTM+CRF+transfer model, showing 0.15% and 0.36% improvement on SighanNER dataset and WeiboNER dataset, respectively.",
            "It proves that adversarial training can prevent specific features of CWS task from creeping into shared space.",
            "Effectiveness of self-attention mechanism.",
            "When compared with BiLSTM+CRF, the BiLSTM+CRF+self-attention significantly improves the performance on the two different datasets with the help of information learned from self-attention, which verifies that the self-attention mechanism is effective for Chinese NER task.",
            "We observe that our proposed adversarial transfer learning framework and self-attention lead to noticeable improvements over the baseline, improving F1 score from 51.01% to 53.08% on WeiboNER dataset and giving 1.51% improvement on SighanNER dataset."
        ],
        "class_sentence": [
            1,
            2,
            2,
            2,
            2,
            2,
            1,
            2,
            1,
            2,
            1,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "BiLSTM+CRF",
                "BiLSTM+CRF+transfer",
                "BiLSTM+CRF+adversarial",
                "BiLSTM+CRF+self-attention",
                "BiLSTM+CRF+adversarial+self-attention",
                "SighanNER",
                "WeiboNER"
            ],
            null,
            [
                "BiLSTM+CRF"
            ],
            [
                "BiLSTM+CRF+transfer"
            ],
            [
                "BiLSTM+CRF+adversarial",
                "BiLSTM+CRF+transfer"
            ],
            [
                "BiLSTM+CRF+self-attention",
                "BiLSTM+CRF"
            ],
            null,
            null,
            [
                "BiLSTM+CRF+transfer",
                "BiLSTM+CRF",
                "SighanNER",
                "F1(%)",
                "WeiboNER"
            ],
            null,
            [
                "BiLSTM+CRF+adversarial",
                "BiLSTM+CRF+transfer",
                "SighanNER",
                "WeiboNER",
                "F1(%)"
            ],
            null,
            null,
            [
                "BiLSTM+CRF+self-attention",
                "BiLSTM+CRF",
                "SighanNER",
                "WeiboNER"
            ],
            [
                "BiLSTM+CRF+adversarial+self-attention",
                "BiLSTM+CRF",
                "WeiboNER",
                "SighanNER",
                "F1(%)"
            ]
        ],
        "n_sentence": 15.0,
        "table_id": "table_5",
        "paper_id": "D18-1017",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1020table_2",
        "description": "Table 2 shows our results on the UD datasets. The trends are broadly consistent with those of Table 1a and 1b. The best performing models use hierarchical structure in the latent variables. There are some differences across languages. For French, German, Indonesian and Russian, VSLG does not show improvement when using unlabeled data. This may be resolved with better tuning, since the model actually shows improvement on the dev set. Note that results reported by Zhang et al. (2017) and ours are not strictly comparable as their word embeddings were only pretrained on the UD training sets while ours were pretrained on Wikipedia. Nonetheless, they also mentioned that using embeddings pretrained on larger unlabeled data did not help. We include these results to show that our baselines are indeed strong compared to prior results reported in the literature.",
        "sentences": [
            "Table 2 shows our results on the UD datasets.",
            "The trends are broadly consistent with those of Table 1a and 1b.",
            "The best performing models use hierarchical structure in the latent variables.",
            "There are some differences across languages.",
            "For French, German, Indonesian and Russian, VSLG does not show improvement when using unlabeled data.",
            "This may be resolved with better tuning, since the model actually shows improvement on the dev set.",
            "Note that results reported by Zhang et al. (2017) and ours are not strictly comparable as their word embeddings were only pretrained on the UD training sets while ours were pretrained on Wikipedia.",
            "Nonetheless, they also mentioned that using embeddings pretrained on larger unlabeled data did not help.",
            "We include these results to show that our baselines are indeed strong compared to prior results reported in the literature."
        ],
        "class_sentence": [
            1,
            2,
            2,
            1,
            1,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "French",
                "German",
                "Indonesian",
                "Spanish",
                "Russian",
                "Croatian"
            ],
            [
                "French",
                "German",
                "Indonesian",
                "Russian",
                "VSL-G",
                "UL\u0394"
            ],
            [
                "VSL-GG-Flat",
                "VSL-GG-Hier"
            ],
            [
                "NCRF",
                "NCRF-AE",
                "VSL-G"
            ],
            null,
            [
                "BiGRU baseline"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_2",
        "paper_id": "D18-1020",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1020table_3",
        "description": "6.1 Effect of Position of Classification Loss. We investigate the effect of attaching the classifier to different latent variables. In particular, for the VSL-GG-Hier model, we compare the attachment of the classifier between z and y. See Figure 2. The results in Table 3 suggest that attaching the reconstruction and classification losses to the same latent variable (z) harms accuracy although attaching the classifier to z effectively gives the classifier an extra layer.",
        "sentences": [
            "6.1 Effect of Position of Classification Loss.",
            "We investigate the effect of attaching the classifier to different latent variables.",
            "In particular, for the VSL-GG-Hier model, we compare the attachment of the classifier between z and y.",
            "See Figure 2.",
            "The results in Table 3 suggest that attaching the reconstruction and classification losses to the same latent variable (z) harms accuracy although attaching the classifier to z effectively gives the classifier an extra layer."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            [
                "classifier on z",
                "acc."
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_3",
        "paper_id": "D18-1020",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1023table_4",
        "description": "For following experiments, we use MultiCluster and MultiCCA as baselines 14. Table 4 shows the results. We observe that both MultiCCA and CorrNet approaches are sensitive to the size of the bilingual lexicons. Our approach on the other hand can maintain high performance, even when the size of bilingual lexicons is reduced to 250. The performances of MultiCluster based on various sizes of bilingual dictionary are close because it jointly trains the embedding of multiple languages from scratch and by default takes advantage of identical strings among all the languages.",
        "sentences": [
            "For following experiments, we use MultiCluster and MultiCCA as baselines 14.",
            "Table 4 shows the results.",
            "We observe that both MultiCCA and CorrNet approaches are sensitive to the size of the bilingual lexicons.",
            "Our approach on the other hand can maintain high performance, even when the size of bilingual lexicons is reduced to 250.",
            "The performances of MultiCluster based on various sizes of bilingual dictionary are close because it jointly trains the embedding of multiple languages from scratch and by default takes advantage of identical strings among all the languages."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "multiCCA",
                "multiCluster"
            ],
            null,
            [
                "multiCCA",
                "CorrNet W"
            ],
            [
                "CorrNet W+N+C+L",
                "250"
            ],
            [
                "multiCluster",
                "40000",
                "10000",
                "2000",
                "1000",
                "500",
                "250"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_4",
        "paper_id": "D18-1023",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1023table_7",
        "description": "Cross-lingual direct transfer. In this setting, we train a name tagger on one or two languages using multilingual embeddings and test it on a new language without any annotated data. Table 7 shows the performance. For most testing languages, our approach achieves better performance than MultiCCA and MultiCluster. The closer that the languages are, such as Amharic and Tigrinya, the better performance is achieved.",
        "sentences": [
            "Cross-lingual direct transfer.",
            "In this setting, we train a name tagger on one or two languages using multilingual embeddings and test it on a new language without any annotated data.",
            "Table 7 shows the performance.",
            "For most testing languages, our approach achieves better performance than MultiCCA and MultiCluster.",
            "The closer that the languages are, such as Amharic and Tigrinya, the better performance is achieved."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Train",
                "Test"
            ],
            null,
            [
                "CorrNet",
                "W+N+C+L",
                "MultiCCA",
                "MultiCluster"
            ],
            [
                "CorrNet",
                "W+N+C+L",
                "Amh",
                "Tig"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_7",
        "paper_id": "D18-1023",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1027table_4",
        "description": "The results listed in Table 4 indicate several trends. First and foremost, in terms of model-wise comparisons, we observe that our proposed alterations of both VecMap and MUSE improve their quality in a consistent manner, across most metrics and data configurations. In Italian our proposed model shows an improvement across all configurations. However, in Spanish VecMap emerges as a highly competitive baseline, with our model only showing an improved performance when training data in this language abounds (in this specific case there is an increase from 17.2 to 19.5 points in the MRR metric). This suggests that the fact that the monolingual spaces are closer in our model is clearly beneficial when hybrid training data is given as input, opening up avenues for future work on weakly-supervised learning. Concerning the other baseline, MUSE, the contribution of our proposed model is consistent for both languages, again becoming more apparent in the Italian split and in a fully cross-lingual setting, where the improvement in MRR is almost 3 points (from 10.6 to 13.3). Finally, it is noteworthy that even in the setting where no training data from the target language is leveraged, all the systems based on cross-lingual embeddings outperform the best unsupervised baseline, which is a very encouraging result with regards to solving tasks for languages on which training data is not easily accessible or not directly available. Analysis. A manual exploration of the results obtained in cross-lingual hypernym discovery reveals a systematic pattern when comparing, for example, VecMap and our model. It was shown in Table 4 that the performance of our model gradually increased alongside the size of the training data in the target language until surpassing VecMap in the most informed configuration (i.e., EN+2k). Specifically, our model seems to show a higher presence of generic words in the output hypernyms, which may be explained by these being closer in the space. In fact, out of 1000 candidate hyponyms, our model correctly finds person 143 times, as compared to the 111 of VecMap, and this systematically occurs with generic types such as citizen or transport. Let us mention, however, that the considered baselines perform remarkably well in some cases. For example, the English-only VecMap configuration (EN), unlike ours, correctly discovered the following hypernyms for Francesc Maci`a (a Spanish politician and soldier): politician, ruler, leader and person. These were missing from the prediction of our model in all configurations until the most informed one (EN+2k).",
        "sentences": [
            "The results listed in Table 4 indicate several trends.",
            "First and foremost, in terms of model-wise comparisons, we observe that our proposed alterations of both VecMap and MUSE improve their quality in a consistent manner, across most metrics and data configurations.",
            "In Italian our proposed model shows an improvement across all configurations.",
            "However, in Spanish VecMap emerges as a highly competitive baseline, with our model only showing an improved performance when training data in this language abounds (in this specific case there is an increase from 17.2 to 19.5 points in the MRR metric).",
            "This suggests that the fact that the monolingual spaces are closer in our model is clearly beneficial when hybrid training data is given as input, opening up avenues for future work on weakly-supervised learning.",
            "Concerning the other baseline, MUSE, the contribution of our proposed model is consistent for both languages, again becoming more apparent in the Italian split and in a fully cross-lingual setting, where the improvement in MRR is almost 3 points (from 10.6 to 13.3).",
            "Finally, it is noteworthy that even in the setting where no training data from the target language is leveraged, all the systems based on cross-lingual embeddings outperform the best unsupervised baseline, which is a very encouraging result with regards to solving tasks for languages on which training data is not easily accessible or not directly available.",
            "Analysis.",
            "A manual exploration of the results obtained in cross-lingual hypernym discovery reveals a systematic pattern when comparing, for example, VecMap and our model.",
            "It was shown in Table 4 that the performance of our model gradually increased alongside the size of the training data in the target language until surpassing VecMap in the most informed configuration (i.e., EN+2k).",
            "Specifically, our model seems to show a higher presence of generic words in the output hypernyms, which may be explained by these being closer in the space.",
            "In fact, out of 1000 candidate hyponyms, our model correctly finds person 143 times, as compared to the 111 of VecMap, and this systematically occurs with generic types such as citizen or transport.",
            "Let us mention, however, that the considered baselines perform remarkably well in some cases.",
            "For example, the English-only VecMap configuration (EN), unlike ours, correctly discovered the following hypernyms for Francesc Maci`a (a Spanish politician and soldier): politician, ruler, leader and person.",
            "These were missing from the prediction of our model in all configurations until the most informed one (EN+2k)."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            1,
            1,
            2,
            2,
            1,
            2,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "VecMap",
                "VecMap\u03bc",
                "MUSE",
                "MUSE\u03bc"
            ],
            [
                "VecMap\u03bc",
                "MUSE\u03bc",
                "EN",
                "EN+500",
                "EN+1k",
                "EN+2k",
                "Spanish"
            ],
            [
                "Spanish",
                "VecMap",
                "VecMap\u03bc",
                "MUSE\u03bc",
                "EN+2k",
                "MUSE",
                "MRR"
            ],
            null,
            [
                "MUSE",
                "MUSE\u03bc",
                "Spanish",
                "Italian",
                "MRR"
            ],
            [
                "BestUns",
                "VecMap",
                "VecMap\u03bc",
                "MUSE",
                "MUSE\u03bc"
            ],
            null,
            [
                "VecMap",
                "VecMap\u03bc"
            ],
            [
                "VecMap\u03bc",
                "VecMap",
                "EN",
                "EN+500",
                "EN+1k",
                "EN+2k"
            ],
            [
                "VecMap\u03bc"
            ],
            [
                "VecMap\u03bc",
                "VecMap"
            ],
            [
                "VecMap"
            ],
            [
                "VecMap",
                "VecMap\u03bc",
                "EN",
                "Spanish"
            ],
            [
                "VecMap",
                "VecMap\u03bc",
                "EN",
                "EN+500",
                "EN+1k",
                "EN+2k",
                "Spanish"
            ]
        ],
        "n_sentence": 15.0,
        "table_id": "table_4",
        "paper_id": "D18-1027",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1033table_1",
        "description": "Table 1 exhibits the evaluation results of crosslingual lexical sememe prediction with different seed lexicon sizes in {1000, 2000, 4000, 6000}. From the table, we can clearly see that:. (1) Our two models perform much better compared with BiLex in all the seed lexicon size settings. It indicates that incorporating sememe information into word embeddings can effectively improve the performance of predicting sememes for target words. The reason is that both of our models make words with similar sememe annotations have similar embeddings, and as a result, we can recommend better sememes for target words according to its related source words. (2) CLSP-SE model achieves better results than CLSP-WR model. The reason is that by representing sememes in a latent semantic space, CLSP-SE model can further capture the relatedness between sememes as well as the relatedness between words and sememes, which is helpful for modeling the representations of those words with similar sememes.",
        "sentences": [
            "Table 1 exhibits the evaluation results of crosslingual lexical sememe prediction with different seed lexicon sizes in {1000, 2000, 4000, 6000}.",
            "From the table, we can clearly see that:.",
            "(1) Our two models perform much better compared with BiLex in all the seed lexicon size settings.",
            "It indicates that incorporating sememe information into word embeddings can effectively improve the performance of predicting sememes for target words.",
            "The reason is that both of our models make words with similar sememe annotations have similar embeddings, and as a result, we can recommend better sememes for target words according to its related source words.",
            "(2) CLSP-SE model achieves better results than CLSP-WR model.",
            "The reason is that by representing sememes in a latent semantic space, CLSP-SE model can further capture the relatedness between sememes as well as the relatedness between words and sememes, which is helpful for modeling the representations of those words with similar sememes."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            2,
            1,
            2
        ],
        "header_mention": [
            [
                "Seed Lexicon"
            ],
            null,
            [
                "CLSP-WR",
                "CLSP-SE",
                "BiLex",
                "1000",
                "2000",
                "4000",
                "6000"
            ],
            null,
            [
                "CLSP-WR",
                "CLSP-SE"
            ],
            [
                "CLSP-SE",
                "CLSP-WR"
            ],
            [
                "CLSP-SE"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_1",
        "paper_id": "D18-1033",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1036table_1",
        "description": "5 Results on CH-EN Translation. 5.1 Our methods vs. Baseline. Table 1 reports the main translation results of CHEN translation. We first compare Baseline+MEM with Baseline. As shown in row 1 and row 5 in Table 1, Baseline+MEM can improve over Baseline on all test datasets, and the average improvement is 1.37 BLEU points. The results show that our method could significantly outperform the baseline model. 5.2 Results on Sub-words,2, We also test the proposed method when the translation unit is sub-word. The baseline and our method using sub-word as translation unit are respectively denoted by Baseline(subword) and Baseline(sub-word)+MEM. The results are shown in row 4 and row 7. From the results, Baseline(sub-word)+MEM outperforms Baseline(sub-word) by 1.01 BLEU points, indicating that adopting sub-words as translation units still faces the problem of troublesome tokens, and our method could alleviate this problem. 5.3 Our Method vs. Method Using Translation Lexicon. We also compare our method with Arthur et al. (2016)\u2019s method which incorporates a translation lexicon into NMT. Here, the comparison is conducted in two ways based on whether the baseline neural model is fixed or retrained. Fixed Baseline. Comparing Arthur(test) (row 2 in Table 1) and Baseline+MEM (row 5 in Table 1), we can see that our proposed method can surpass Arthur(test) with 1.05 BLEU points. As there are three differences between our methods and Arthur(test), we take the following experiments to evaluate the effect of each difference. Retrained Baseline. In the second comparison, we allow the baseline model to be retrained by Arthur's method (Arthur(train+test)). We then implement our method using Arthur(train+test) as baseline (denoted by Arthur(train+test)+MEM). Comparing the results of these two methods in Table 1 (line 3 and 6), our method is still effective on the retrained model. The average gains are 0.92 BLEU points.",
        "sentences": [
            "5 Results on CH-EN Translation.",
            "5.1 Our methods vs. Baseline.",
            "Table 1 reports the main translation results of CHEN translation.",
            "We first compare Baseline+MEM with Baseline.",
            "As shown in row 1 and row 5 in Table 1, Baseline+MEM can improve over Baseline on all test datasets, and the average improvement is 1.37 BLEU points.",
            "The results show that our method could significantly outperform the baseline model.",
            "5.2 Results on Sub-words,2, We also test the proposed method when the translation unit is sub-word.",
            "The baseline and our method using sub-word as translation unit are respectively denoted by Baseline(subword) and Baseline(sub-word)+MEM.",
            "The results are shown in row 4 and row 7.",
            "From the results, Baseline(sub-word)+MEM outperforms Baseline(sub-word) by 1.01 BLEU points, indicating that adopting sub-words as translation units still faces the problem of troublesome tokens, and our method could alleviate this problem.",
            "5.3 Our Method vs. Method Using Translation Lexicon.",
            "We also compare our method with Arthur et al. (2016)\u2019s method which incorporates a translation lexicon into NMT.",
            "Here, the comparison is conducted in two ways based on whether the baseline neural model is fixed or retrained.",
            "Fixed Baseline.",
            "Comparing Arthur(test) (row 2 in Table 1) and Baseline+MEM (row 5 in Table 1), we can see that our proposed method can surpass Arthur(test) with 1.05 BLEU points.",
            "As there are three differences between our methods and Arthur(test), we take the following experiments to evaluate the effect of each difference.",
            "Retrained Baseline.",
            "In the second comparison, we allow the baseline model to be retrained by Arthur's method (Arthur(train+test)).",
            "We then implement our method using Arthur(train+test) as baseline (denoted by Arthur(train+test)+MEM).",
            "Comparing the results of these two methods in Table 1 (line 3 and 6), our method is still effective on the retrained model.",
            "The average gains are 0.92 BLEU points."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1,
            1,
            2,
            2,
            1,
            1,
            2,
            2,
            2,
            2,
            1,
            2,
            2,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "Baseline",
                "Baseline+MEM"
            ],
            [
                "Baseline",
                "Baseline+MEM",
                "03",
                "04",
                "05",
                "06",
                "08",
                "\u25b3"
            ],
            [
                "Baseline",
                "Baseline+MEM"
            ],
            null,
            [
                "Baseline(sub-word)",
                "Baseline(sub-word)+MEM"
            ],
            [
                "Baseline(sub-word)",
                "Baseline(sub-word)+MEM"
            ],
            [
                "Baseline(sub-word)",
                "Baseline(sub-word)+MEM",
                "\u25b3"
            ],
            null,
            null,
            null,
            null,
            [
                "Arthur(test)",
                "Baseline+MEM",
                "Avg."
            ],
            [
                "Arthur(test)",
                "Baseline+MEM"
            ],
            null,
            [
                "Arthur(train+test)"
            ],
            [
                "Arthur(train+test)+MEM"
            ],
            [
                "Arthur(train+test)",
                "Arthur(train+test)+MEM"
            ],
            [
                "Arthur(train+test)+MEM",
                "Arthur(train+test)",
                "\u25b3"
            ]
        ],
        "n_sentence": 21.0,
        "table_id": "table_1",
        "paper_id": "D18-1036",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1037table_4",
        "description": "3.3 Results. Table 3 and Table 4 has the BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010) scores. In our IWSLT2017 tests, both Seq2DRNN and Seq2DRNN+SynC produce better results than the Seq2Seq baseline model in terms of BLEU scores, while Seq2DRNN+SynC also produces better RIBES scores indicating better reordering of phrases in the output. The Seq2DRNN+SynC model performs better than the Seq2DRNN model. Both Seq2Seq and Seq2DRNN+SynC are able to produce results with lower perplexities than the baseline Seq2Seq model on the test data. In our News Commentary v8 tests, the same relative performance from Seq2DRNN(SynC) can be observed. The Seq2DRNN+SynC model is also able to out-perform the Str2Tree model proposed by Aharoni and Goldberg (2017) and NMT+RNNG by Eriguchi et al. (2017) in most cases. Note that Eriguchi et al. (2017) used dependency information instead of constituency information as presented in our work and Aharoni and Goldberg (2017)\u0081fs work.",
        "sentences": [
            "3.3 Results.",
            "Table 3 and Table 4 has the BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010) scores.",
            "In our IWSLT2017 tests, both Seq2DRNN and Seq2DRNN+SynC produce better results than the Seq2Seq baseline model in terms of BLEU scores, while Seq2DRNN+SynC also produces better RIBES scores indicating better reordering of phrases in the output.",
            "The Seq2DRNN+SynC model performs better than the Seq2DRNN model.",
            "Both Seq2Seq and Seq2DRNN+SynC are able to produce results with lower perplexities than the baseline Seq2Seq model on the test data.",
            "In our News Commentary v8 tests, the same relative performance from Seq2DRNN(SynC) can be observed.",
            "The Seq2DRNN+SynC model is also able to out-perform the Str2Tree model proposed by Aharoni and Goldberg (2017) and NMT+RNNG by Eriguchi et al. (2017) in most cases.",
            "Note that Eriguchi et al. (2017) used dependency information instead of constituency information as presented in our work and Aharoni and Goldberg (2017)\u0081fs work."
        ],
        "class_sentence": [
            2,
            1,
            0,
            0,
            0,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "BLEU",
                "RIBES"
            ],
            null,
            null,
            null,
            [
                "Seq2DRNN+SynC"
            ],
            [
                "Seq2DRNN+SynC",
                "Str2Tree",
                "NMT+RNNG"
            ],
            [
                "NMT+RNNG",
                "Str2Tree"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_4",
        "paper_id": "D18-1037",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1045table_2",
        "description": "We report the perplexity of our language model on all versions of the source data in Table 2. The results show that beam outputs receive higher probability by the language model compared to sampling, beam+noise and\nreal source sentences. This indicates that beam search outputs are not as rich as sampling outputs or beam+noise. This lack of variability probably explains in part why back-translations from pure beam search provide a weaker training signal than alternatives.",
        "sentences": [
            "We report the perplexity of our language model on all versions of the source data in Table 2.",
            "The results show that beam outputs receive higher probability by the language model compared to sampling, beam+noise and\nreal source sentences.",
            "This indicates that beam search outputs are not as rich as sampling outputs or beam+noise.",
            "This lack of variability probably explains in part why back-translations from pure beam search provide a weaker training signal than alternatives."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "beam",
                "sampling",
                "beam+noise",
                "human data"
            ],
            [
                "beam",
                "sampling",
                "beam+noise"
            ],
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "D18-1045",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1045table_6",
        "description": "We upsample the bitext with a rate of 16 so that we observe every bitext sentence 16 times more often than each monolingual sentence. This results in a new state of the art of 35 BLEU on newstest2014 by using only WMT benchmark data. For comparison, DeepL, a commercial translation engine relying on high quality bilingual training data, achieves 33.3 tokenized BLEU. Table 6 summarizes our results and compares to other work in the literature. This shows that back-translation with sampling can result in high-quality translation models based on benchmark data only.",
        "sentences": [
            "We upsample the bitext with a rate of 16 so that we observe every bitext sentence 16 times more often than each monolingual sentence.",
            "This results in a new state of the art of 35 BLEU on newstest2014 by using only WMT benchmark data.",
            "For comparison, DeepL, a commercial translation engine relying on high quality bilingual training data, achieves 33.3 tokenized BLEU.",
            "Table 6 summarizes our results and compares to other work in the literature.",
            "This shows that back-translation with sampling can result in high-quality translation models based on benchmark data only."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Our result"
            ],
            [
                "DeepL"
            ],
            [
                "Our result"
            ],
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_6",
        "paper_id": "D18-1045",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1046table_2",
        "description": "6.1 Full Supervision Setting. We compare Seq2Seq(HMA) with previous approaches when provided all available supervision, to see how it fares under standard evaluation. Results in the unconstrained inference (U) setting (Table 2 top 5 rows) shows Seq2Seq(HMA),denoted by \u201cOurs\u201d, outperforms previous approaches on Hindi, Kannada, and Bengali, with almost 3-4% gains. I. Improvements over the Seq2Seq with Attention (Seq2Seq w/ Att) model demonstrate the benefit of imposing the monotonicity constraint in the generation model. On Tamil and Hebrew, Seq2Seq(HMA) is at par with the best approaches, with negligible gap (\u223c0.3) in scores. Overall, we see that Seq2Seq(HMA) can achieve better (and sometimes competitive) scores than state-of-the-art approaches in full supervision settings. When comparing approaches which use constrained inference (Table 2, rows 6 and 7), we see that using dictionary-constrained inference (as in Ours(DC)) is more effective than using a entitylinking model for re-ranking (RPI-ISI + EL). 6.2 Low-Resource Setting. In Table 2 (rows under \u201cLow-Resource Setting\u201d), we evaluate different models in a low-resource setting when provided only 500 name pairs as supervision. Results are averaged over 5 different random sub-samples of 500 examples. The results clearly demonstrate that all generation models suffer a drop in performance when provided limited training data. Note that models like Seq2Seq with Attention suffer a larger drop than those which enforce monotonicity, suggesting that incorporating monotonicity into the inference step in the low-resource setting is essential. After bootstrapping our weak generation model using Algorithm 1, the performance improves substantially (last row in Table 2). On almost all languages, the generation model improves by at least 6%, with performance for Hindi and Bengali improving by more than 10%. Bootstrapping results for the languages are within 2-4% of the best model trained with all available supervision.\".",
        "sentences": [
            "6.1 Full Supervision Setting.",
            "We compare Seq2Seq(HMA) with previous approaches when provided all available supervision, to see how it fares under standard evaluation.",
            "Results in the unconstrained inference (U) setting (Table 2 top 5 rows) shows Seq2Seq(HMA),denoted by \u201cOurs\u201d, outperforms previous approaches on Hindi, Kannada, and Bengali, with almost 3-4% gains. I.",
            "Improvements over the Seq2Seq with Attention (Seq2Seq w/ Att) model demonstrate the benefit of imposing the monotonicity constraint in the generation model.",
            "On Tamil and Hebrew, Seq2Seq(HMA) is at par with the best approaches, with negligible gap (\u223c0.3) in scores.",
            "Overall, we see that Seq2Seq(HMA) can achieve better (and sometimes competitive) scores than state-of-the-art approaches in full supervision settings.",
            "When comparing approaches which use constrained inference (Table 2, rows 6 and 7), we see that using dictionary-constrained inference (as in Ours(DC)) is more effective than using a entitylinking model for re-ranking (RPI-ISI + EL).",
            "6.2 Low-Resource Setting.",
            "In Table 2 (rows under \u201cLow-Resource Setting\u201d), we evaluate different models in a low-resource setting when provided only 500 name pairs as supervision.",
            "Results are averaged over 5 different random sub-samples of 500 examples.",
            "The results clearly demonstrate that all generation models suffer a drop in performance when provided limited training data.",
            "Note that models like Seq2Seq with Attention suffer a larger drop than those which enforce monotonicity, suggesting that incorporating monotonicity into the inference step in the low-resource setting is essential.",
            "After bootstrapping our weak generation model using Algorithm 1, the performance improves substantially (last row in Table 2).",
            "On almost all languages, the generation model improves by at least 6%, with performance for Hindi and Bengali improving by more than 10%.",
            "Bootstrapping results for the languages are within 2-4% of the best model trained with all available supervision.\"."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1,
            1,
            1,
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Full Supervision Setting (5-10k examples)",
                "Ours(U)"
            ],
            [
                "Seq2Seq w/ Att (U)",
                "P&R (U)",
                "DirecTL+ (U)",
                "RPI-ISI (U)",
                "Ours(U)"
            ],
            [
                "Seq2Seq w/ Att (U)"
            ],
            [
                "Ours(U)",
                "ta",
                "he",
                "Seq2Seq w/ Att (U)",
                "P&R (U)",
                "DirecTL+ (U)",
                "RPI-ISI (U)"
            ],
            [
                "Ours(U)",
                "Seq2Seq w/ Att (U)",
                "P&R (U)",
                "DirecTL+ (U)",
                "RPI-ISI (U)"
            ],
            [
                "Approaches Using Constrained Inference",
                "RPI-ISI + EL",
                "Ours(DC)"
            ],
            null,
            [
                "Low-Resource Setting (500 examples)"
            ],
            [
                "Low-Resource Setting (500 examples)",
                "Avg."
            ],
            [
                "Seq2Seq w/ Att (U)",
                "P&R (U)",
                "DirecTL+ (U)",
                "Ours(U)",
                "Ours(U) + Boot."
            ],
            [
                "Seq2Seq w/ Att (U)",
                "Low-Resource Setting (500 examples)",
                "Full Supervision Setting (5-10k examples)"
            ],
            [
                "Ours(U) + Boot."
            ],
            [
                "hi",
                "kn",
                "bn",
                "ta",
                "he",
                "Ours(U) + Boot.",
                "Ours(U)"
            ],
            [
                "Ours(U) + Boot."
            ]
        ],
        "n_sentence": 15.0,
        "table_id": "table_2",
        "paper_id": "D18-1046",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1046table_3",
        "description": "To quantify the effect of this, we annotate native and foreign names in the test split of the four Indian languages, and evaluate performance for both categories. Table 3 shows that our model performs significantly better on native names for all the languages. A possible reason for is that the source scripts were designed for writing native names (e.g., Tamil script lacks separate {ta, da, tha, dha} characters because the Tamil language does not distinguish these sounds). Furthermore, foreign names have a wide variety of origins with their own conventions as discussed in \u0e22\u0e077.1. The performance gap is proportionally greatest for Tamil, likely due to its script.",
        "sentences": [
            "To quantify the effect of this, we annotate native and foreign names in the test split of the four Indian languages, and evaluate performance for both categories.",
            "Table 3 shows that our model performs significantly better on native names for all the languages.",
            "A possible reason for is that the source scripts were designed for writing native names (e.g., Tamil script lacks separate {ta, da, tha, dha} characters because the Tamil language does not distinguish these sounds).",
            "Furthermore, foreign names have a wide variety of origins with their own conventions as discussed in \u0e22\u0e077.1.",
            "The performance gap is proportionally greatest for Tamil, likely due to its script."
        ],
        "class_sentence": [
            2,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            [
                "Native",
                "Foreign",
                "Hindi",
                "Bengali",
                "Kannada",
                "Tamil"
            ],
            [
                "Native",
                "Hindi",
                "Bengali",
                "Kannada",
                "Tamil"
            ],
            [
                "Tamil"
            ],
            null,
            [
                "Tamil"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_3",
        "paper_id": "D18-1046",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1047table_3",
        "description": "Table 3 shows that NORMA-Linear outperforms (Artetxe et al., 2018a) by over 10 points on the RARE words dataset. On the regular MUSE dictionary, (Artetxe et al., 2018a) is ahead by about 5 points. On RARE, (Lazaridou et al., 2015) is behind NORMA-Linear by 9 points, whereas on the MUSE dictionary performance of (Lazaridou et al., 2015) and NORMA-Linear is about the same.",
        "sentences": [
            "Table 3 shows that NORMA-Linear outperforms (Artetxe et al., 2018a) by over 10 points on the RARE words dataset.",
            "On the regular MUSE dictionary, (Artetxe et al., 2018a) is ahead by about 5 points.",
            "On RARE, (Lazaridou et al., 2015) is behind NORMA-Linear by 9 points, whereas on the MUSE dictionary performance of (Lazaridou et al., 2015) and NORMA-Linear is about the same."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "NORMA-Linear",
                "Artetxe et al . 2018",
                "RARE"
            ],
            [
                "Artetxe et al . 2018",
                "NORMA-Linear",
                "MUSE"
            ],
            [
                "Lazaridou et al 2015",
                "NORMA-Linear",
                "RARE",
                "MUSE"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_3",
        "paper_id": "D18-1047",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1049table_3",
        "description": "As shown in Table 3, using the same data, our approach achieves significant improvements over the original Transformer model (Vaswani et al. 2017) (p < 0.01). The gain on the concatenated test set (i.e., \u201cAll\u201d) is 1.96 BLEU points. It also outperforms the cache-based method (Kuang et al. 2017) adapted for Transformer significantly (p < 0.01), which also uses the two-step training strategy.",
        "sentences": [
            "As shown in Table 3, using the same data, our approach achieves significant improvements over the original Transformer model (Vaswani et al. 2017) (p < 0.01).",
            "The gain on the concatenated test set (i.e., \u201cAll\u201d) is 1.96 BLEU points.",
            "It also outperforms the cache-based method (Kuang et al. 2017) adapted for Transformer significantly (p < 0.01), which also uses the two-step training strategy."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "this work",
                "(Vaswani et al. 2017)"
            ],
            [
                "this work",
                "(Vaswani et al. 2017)",
                "All"
            ],
            [
                "this work",
                "(Kuang et al. 2017)*"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_3",
        "paper_id": "D18-1049",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1049table_5",
        "description": "Table 5 shows the results of subjective evaluation. Three human evaluators generally made consistent judgements. On average, around 19% of Transformer\u00e2\u20ac\u2122s translations are better than that of our model, 51% are equal, and 31% are worse. This evaluation confirms that exploiting document-level context helps to improve translation quality.",
        "sentences": [
            "Table 5 shows the results of subjective evaluation.",
            "Three human evaluators generally made consistent judgements.",
            "On average, around 19% of Transformer\u00e2\u20ac\u2122s translations are better than that of our model, 51% are equal, and 31% are worse.",
            "This evaluation confirms that exploiting document-level context helps to improve translation quality."
        ],
        "class_sentence": [
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Human 1",
                "Human 2",
                "Human 3"
            ],
            [
                "Overall",
                ">",
                "=",
                "<"
            ],
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_5",
        "paper_id": "D18-1049",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1052table_1",
        "description": "Results. Table 1 shows the results for the PIQA baselines (top) and the unconstrained state of the art (bottom). First, the TF-IDF model performs poorly, which signifies the limitations of traditional document retrieval models for the task. Second, we note that the addition of self-attention makes a significant impact on results, improving F1 by 2.6%. Next, we see that adding ELMo gives 3.7% and 2.9% improvement on F1 for LSTM and LSTM+SA models, respectively. Lastly, the best PIQA baseline model is 11.7% higher than the first (unconstrained) baseline model (Rajpurkar et al., 2016) and 26.6% lower than the state of the art (Yu et al., 2018). This gives us a reasonable starting point of the new task and a significant gap to close for future work.",
        "sentences": [
            "Results.",
            "Table 1 shows the results for the PIQA baselines (top) and the unconstrained state of the art (bottom).",
            "First, the TF-IDF model performs poorly, which signifies the limitations of traditional document retrieval models for the task.",
            "Second, we note that the addition of self-attention makes a significant impact on results, improving F1 by 2.6%.",
            "Next, we see that adding ELMo gives 3.7% and 2.9% improvement on F1 for LSTM and LSTM+SA models, respectively.",
            "Lastly, the best PIQA baseline model is 11.7% higher than the first (unconstrained) baseline model (Rajpurkar et al., 2016) and 26.6% lower than the state of the art (Yu et al., 2018).",
            "This gives us a reasonable starting point of the new task and a significant gap to close for future work."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "PI",
                "None"
            ],
            [
                "TF-IDF"
            ],
            [
                "LSTM",
                "LSTM+SA",
                "F1 (%)"
            ],
            [
                "LSTM",
                "LSTM+SA",
                "LSTM+ELMo",
                "LSTM+SA+ELMo",
                "F1 (%)"
            ],
            [
                "LSTM+SA+ELMo",
                "Rajpurkar et al. (2016)",
                "Yu et al. (2018)",
                "F1 (%)"
            ],
            null
        ],
        "n_sentence": 7.0,
        "table_id": "table_1",
        "paper_id": "D18-1052",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1057table_1",
        "description": "4.2 Intrinsic Evalution. Table 1 shows the evaluation results of word similarity tasks and word analogy tasks. Word similarity is measured as the Spearman\u2019s rank correlation \u03c1 between human-judged similarity and cosine distance of word vectors. In word analogy task, the questions are answered over the whole vocabulary through 3CosMul (Levy and Goldberg, 2014a). In addition to GloVe and Swivel, the evaluations of SGNS are also reported for reference. We train SGNS with the word2vec tool, using symmetric context window of five words to the left and five words to the right, and 5 negative samples. As can be seen from the table, the context overlap information enhanced word embeddings perform better in most word similarity tasks and get higher analogy accuracy in semantic aspect at the cost of syntactic score. The improved semantics performance, to a certain extent, reflects second order co-occurrence relations are more semantic.",
        "sentences": [
            "4.2 Intrinsic Evalution.",
            "Table 1 shows the evaluation results of word similarity tasks and word analogy tasks.",
            "Word similarity is measured as the Spearman\u2019s rank correlation \u03c1 between human-judged similarity and cosine distance of word vectors.",
            "In word analogy task, the questions are answered over the whole vocabulary through 3CosMul (Levy and Goldberg, 2014a).",
            "In addition to GloVe and Swivel, the evaluations of SGNS are also reported for reference.",
            "We train SGNS with the word2vec tool, using symmetric context window of five words to the left and five words to the right, and 5 negative samples.",
            "As can be seen from the table, the context overlap information enhanced word embeddings perform better in most word similarity tasks and get higher analogy accuracy in semantic aspect at the cost of syntactic score.",
            "The improved semantics performance, to a certain extent, reflects second order co-occurrence relations are more semantic."
        ],
        "class_sentence": [
            2,
            1,
            2,
            2,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            [
                "GloVe",
                "Swivel",
                "SGNS"
            ],
            [
                "SGNS"
            ],
            [
                "Swivel + CO",
                "WS353",
                "SL999",
                "SCWS",
                "RW",
                "MEN",
                "MT771",
                "Analogy"
            ],
            null
        ],
        "n_sentence": 8.0,
        "table_id": "table_1",
        "paper_id": "D18-1057",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1060table_5",
        "description": "Table 5 reports the breakdown of performance by POS tags. Not surprisingly, tags with more data are easier to classify. Adposition is the easiest to identify as metaphorical and is also the most frequently metaphorical class (28%). On the other hand, particles are challenging to identify, since they are often associated with multi-word expressions, such as \u201cput down the disturbances\u201d.",
        "sentences": [
            "Table 5 reports the breakdown of performance by POS tags.",
            "Not surprisingly, tags with more data are easier to classify.",
            "Adposition is the easiest to identify as metaphorical and is also the most frequently metaphorical class (28%).",
            "On the other hand, particles are challenging to identify, since they are often associated with multi-word expressions, such as \u201cput down the disturbances\u201d."
        ],
        "class_sentence": [
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            [
                "POS"
            ],
            null,
            [
                "ADP"
            ],
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_5",
        "paper_id": "D18-1060",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1060table_6",
        "description": "Verb Classification Results. Table 6 shows performance on the verb classification task for three datasets (MOH-X , TroFi and VUA). Our models achieve strong performance on all datasets, outperforming existing models on the MOH-X and VUA datasets. On the MOH-X dataset, the CLS model outperforms the SEQ model, likely due to the simpler overall sentence structure and the fact that the target verbs are the only words annotated for metaphoricity. For the VUA dataset, where we have annotations for all words in a sentence, the SEQ model significantly outperforms the CLS model. This result shows that predicting metaphor labels of context words helps to predict the target verb. We hypothesize that Koper et al. (2017) outperforms our models on the TroFi dataset for a similar reason:. their work uses concreteness labels, which highly correlate to metaphor labels of neighboring words in the sentence. Also, their best model uses the verb lemma as a feature, which itself provides a strong clue in the dataset of 50 verbs (see lexical baseline).",
        "sentences": [
            "Verb Classification Results.",
            "Table 6 shows performance on the verb classification task for three datasets (MOH-X , TroFi and VUA).",
            "Our models achieve strong performance on all datasets, outperforming existing models on the MOH-X and VUA datasets.",
            "On the MOH-X dataset, the CLS model outperforms the SEQ model, likely due to the simpler overall sentence structure and the fact that the target verbs are the only words annotated for metaphoricity.",
            "For the VUA dataset, where we have annotations for all words in a sentence, the SEQ model significantly outperforms the CLS model.",
            "This result shows that predicting metaphor labels of context words helps to predict the target verb.",
            "We hypothesize that Koper et al. (2017) outperforms our models on the TroFi dataset for a similar reason:.",
            "their work uses concreteness labels, which highly correlate to metaphor labels of neighboring words in the sentence.",
            "Also, their best model uses the verb lemma as a feature, which itself provides a strong clue in the dataset of 50 verbs (see lexical baseline)."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            2,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "MOH-X (10 fold)",
                "TroFi (10 fold)",
                "VUA - Test"
            ],
            [
                "CLS",
                "SEQ",
                "Lexical Baseline",
                "Klebanov (2016)",
                "Rei (2017)",
                "Koper (2017)",
                "Wu (2018) ensemble",
                "MOH-X (10 fold)",
                "TroFi (10 fold)",
                "VUA - Test"
            ],
            [
                "MOH-X (10 fold)",
                "CLS",
                "SEQ"
            ],
            [
                "VUA - Test",
                "CLS",
                "SEQ"
            ],
            null,
            [
                "Koper (2017)",
                "TroFi (10 fold)",
                "CLS",
                "SEQ"
            ],
            [
                "Koper (2017)"
            ],
            [
                "Koper (2017)"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_6",
        "paper_id": "D18-1060",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1062table_2",
        "description": "Table 2 summarizes the performance of baseline models and our approach. The results of baseline models are cited from Zhang et al. (2017). As we can see from the table, our model could achieve superior performance compared with other baseline models.",
        "sentences": [
            "Table 2 summarizes the performance of baseline models and our approach.",
            "The results of baseline models are cited from Zhang et al. (2017).",
            "As we can see from the table, our model could achieve superior performance compared with other baseline models."
        ],
        "class_sentence": [
            1,
            2,
            1
        ],
        "header_mention": [
            [
                "MonoGiza w/ emb.",
                "TM",
                "IA",
                "Zhang et al. (2017)",
                "Ours"
            ],
            [
                "MonoGiza w/ emb.",
                "TM",
                "IA",
                "Zhang et al. (2017)"
            ],
            [
                "Ours",
                "MonoGiza w/ emb.",
                "TM",
                "IA",
                "Zhang et al. (2017)"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_2",
        "paper_id": "D18-1062",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1067table_3",
        "description": "Table 3 shows the performance of several deep learning models trained on either SST or TSA datasets and evaluated on the OPT dataset. Note that the Dev set was used for model selection. As can be seen from the table, the models trained on the sentiment datasets perform poorly on the optimism/pessimism dataset. For example, there is a drop in performance from 80.19% to 67.60% when training on TSA (with an even larger decrease when we train on SST). The SST/TSA sentiment classifiers are trained to predict the sentiment as negative, neutral, or positive. To calculate the accuracy in Table 3, an optimistic tweet predicted as positive by the sentiment classifier counts as a correct prediction, whereas an optimistic tweet predicted as either neutral or negative by the sentiment classifier counts as an incorrect prediction (similarly for pessimistic tweets). This analysis is done at tweet level for the threshold of 0.",
        "sentences": [
            "Table 3 shows the performance of several deep learning models trained on either SST or TSA datasets and evaluated on the OPT dataset.",
            "Note that the Dev set was used for model selection.",
            "As can be seen from the table, the models trained on the sentiment datasets perform poorly on the optimism/pessimism dataset.",
            "For example, there is a drop in performance from 80.19% to 67.60% when training on TSA (with an even larger decrease when we train on SST).",
            "The SST/TSA sentiment classifiers are trained to predict the sentiment as negative, neutral, or positive.",
            "To calculate the accuracy in Table 3, an optimistic tweet predicted as positive by the sentiment classifier counts as a correct prediction, whereas an optimistic tweet predicted as either neutral or negative by the sentiment classifier counts as an incorrect prediction (similarly for pessimistic tweets).",
            "This analysis is done at tweet level for the threshold of 0."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            [
                "LSTM",
                "BiLSTM",
                "CNN",
                "RNN(char)",
                "SST",
                "TSA"
            ],
            [
                "Dev"
            ],
            [
                "LSTM",
                "BiLSTM",
                "CNN",
                "RNN(char)"
            ],
            [
                "GRUStack",
                "CNN",
                "Acc%",
                "TSA",
                "SST"
            ],
            [
                "SST",
                "TSA"
            ],
            null,
            null
        ],
        "n_sentence": 7.0,
        "table_id": "table_3",
        "paper_id": "D18-1067",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1071table_1",
        "description": "Table 1 depicts the human annotations (t-test: p < 0.05 for C and L, p < 0.01 for E). Overall, E-SCBA outperforms S2S-AW on all three metrics, where the compound information plays a positive role in the comprehensive promotion. However, in Surprise and Angry, the grades of Consistency and Logic are not satisfactory, since the data for them are much less than others (Surprise (1.2%) and Angry (0.7%)). Besides, the score of Emotion in Surprise has a big difference from others. We think the reason is that the characteristic of Surprise overlaps with other categories that have much more data, such as Happy, which interferes with the learning efficiency of the approach in Surprise. Meanwhile, it is harder for annotators to determine which one is the right emotion.",
        "sentences": [
            "Table 1 depicts the human annotations (t-test: p < 0.05 for C and L, p < 0.01 for E).",
            "Overall, E-SCBA outperforms S2S-AW on all three metrics, where the compound information plays a positive role in the comprehensive promotion.",
            "However, in Surprise and Angry, the grades of Consistency and Logic are not satisfactory, since the data for them are much less than others (Surprise (1.2%) and Angry (0.7%)).",
            "Besides, the score of Emotion in Surprise has a big difference from others.",
            "We think the reason is that the characteristic of Surprise overlaps with other categories that have much more data, such as Happy, which interferes with the learning efficiency of the approach in Surprise.",
            "Meanwhile, it is harder for annotators to determine which one is the right emotion."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "Overall",
                "E-SCBA",
                "S2S-AW",
                "C",
                "L",
                "E"
            ],
            [
                "Surprise",
                "Angry",
                "E-SCBA",
                "S2S",
                "S2S-AW",
                "C",
                "L"
            ],
            [
                "Surprise",
                "E",
                "E-SCBA",
                "S2S",
                "S2S-AW"
            ],
            [
                "Surprise"
            ],
            [
                "E"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_1",
        "paper_id": "D18-1071",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1074table_3",
        "description": "The results of our experiments are summarized in the Table 3. Findings indicate that our proposed approach leads to a small performance boost after using the topic embeddings. Thus, our simple feature augmentation approach has the potential to make classifiers more robust. In addition, the contextual information (\u201cCt\u201d) is quite useful to identify the patients\u2019 current intentions, and the sequential information through time stages has strong indications of human intentions. Significance Analysis. We conducted significance analysis to compare Xiao2016 and our proposed method. Because Xiao2016 only used content and context inputs, in this analysis, we train our method with the same inputs (Co+Ct). We followed the method of bootstrap samples (Berg-Kirkpatrick et al., 2012) to create 50 pairs of training and test datasets with replacement, where we keep the sizes the same in the Table 2. We keep the same experimental steps and use the parameters that achieved the best performances in the Table 3 to train the models.",
        "sentences": [
            "The results of our experiments are summarized in the Table 3.",
            "Findings indicate that our proposed approach leads to a small performance boost after using the topic embeddings.",
            "Thus, our simple feature augmentation approach has the potential to make classifiers more robust.",
            "In addition, the contextual information (\u201cCt\u201d) is quite useful to identify the patients\u2019 current intentions, and the sequential information through time stages has strong indications of human intentions.",
            "Significance Analysis.",
            "We conducted significance analysis to compare Xiao2016 and our proposed method.",
            "Because Xiao2016 only used content and context inputs, in this analysis, we train our method with the same inputs (Co+Ct).",
            "We followed the method of bootstrap samples (Berg-Kirkpatrick et al., 2012) to create 50 pairs of training and test datasets with replacement, where we keep the sizes the same in the Table 2.",
            "We keep the same experimental steps and use the parameters that achieved the best performances in the Table 3 to train the models."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            2,
            2,
            2,
            0,
            1
        ],
        "header_mention": [
            null,
            [
                "Proposed model"
            ],
            [
                "Proposed model"
            ],
            [
                "Co+Ct",
                "Co+Ct+T"
            ],
            null,
            [
                "Proposed model"
            ],
            [
                "Proposed model"
            ],
            null,
            [
                "Proposed model"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_3",
        "paper_id": "D18-1074",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1075table_3",
        "description": "Table 3 shows the results of human evaluation. The inter-annotator agreement is satisfactory considering the difficulty of human evaluation. The Pearson\u00e2\u20ac\u2122s correlation coefficient is 0.69 on coherence and 0.57 on fluency, with p < 0.0001. First, it is clear that the AEM model outperforms the Seq2Seq model with a large margin, which proves the effectiveness of the AEM model on. Second, it is interesting to note that with the attention mechanism, the coherence is decreased slightly in the Seq2Seq model but increased significantly in the AEM model. It suggests that the utterance-level dependency greatly benefits the learning of word-level dependency. Therefore, it is expected that the AEM+Attention model achieves the best G-score.",
        "sentences": [
            "Table 3 shows the results of human evaluation.",
            "The inter-annotator agreement is satisfactory considering the difficulty of human evaluation.",
            "The Pearson\u00e2\u20ac\u2122s correlation coefficient is 0.69 on coherence and 0.57 on fluency, with p < 0.0001.",
            "First, it is clear that the AEM model outperforms the Seq2Seq model with a large margin, which proves the effectiveness of the AEM model on.",
            "Second, it is interesting to note that with the attention mechanism, the coherence is decreased slightly in the Seq2Seq model but increased significantly in the AEM model.",
            "It suggests that the utterance-level dependency greatly benefits the learning of word-level dependency.",
            "Therefore, it is expected that the AEM+Attention model achieves the best G-score."
        ],
        "class_sentence": [
            1,
            2,
            2,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "AEM",
                "Seq2Seq"
            ],
            [
                "Seq2Seq+Attention",
                "AEM+Attention",
                "Coherence"
            ],
            null,
            [
                "AEM+Attention",
                "G-Score"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_3",
        "paper_id": "D18-1075",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1078table_2",
        "description": "3.2 Results. Table 2 shows the accuracy of all w2v configurations. Representing an argument using its more verbose several-sentences-long content outperforms using its short single-sentence title. On the speech side, considering each sentence separately is preferable to using the entire speech. We compared the results of the best w2v-based configuration (arg-sentence), to the performance of the skip-thought auto-encoder. In this setting, encoding individual speech sentences and an argument, the accuracy of skip-thought was 60.2%. The highest scoring method, w2v arg-sentence, reaches, then, a rather modest accuracy of 64.6%. One weakness of this method, revealed through analysis of its false positive predictions, is its tendency to prefer longer sentences. It is nevertheless substantially superior to the trivial all-yes baseline, as well as its all-no counterpart.",
        "sentences": [
            "3.2 Results.",
            "Table 2 shows the accuracy of all w2v configurations.",
            "Representing an argument using its more verbose several-sentences-long content outperforms using its short single-sentence title.",
            "On the speech side, considering each sentence separately is preferable to using the entire speech.",
            "We compared the results of the best w2v-based configuration (arg-sentence), to the performance of the skip-thought auto-encoder.",
            "In this setting, encoding individual speech sentences and an argument, the accuracy of skip-thought was 60.2%.",
            "The highest scoring method, w2v arg-sentence, reaches, then, a rather modest accuracy of 64.6%.",
            "One weakness of this method, revealed through analysis of its false positive predictions, is its tendency to prefer longer sentences.",
            "It is nevertheless substantially superior to the trivial all-yes baseline, as well as its all-no counterpart."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            1,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "w2v title-speech",
                "w2v arg-speech",
                "w2v title-sentence",
                "w2v arg-sentence"
            ],
            [
                "w2v arg-sentence",
                "w2v title-sentence",
                "Accuracy (%)"
            ],
            null,
            [
                "w2v arg-sentence",
                "ST arg-sentence"
            ],
            [
                "ST arg-sentence",
                "Accuracy (%)"
            ],
            [
                "w2v arg-sentence",
                "Accuracy (%)"
            ],
            [
                "w2v arg-sentence"
            ],
            [
                "w2v arg-sentence",
                "all-yes"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_2",
        "paper_id": "D18-1078",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1084table_1",
        "description": "Results. Table 1 shows the main experimental results. Our baseline cross-entropy captioning model gets similar scores to the original flat model. When the repetition penalty is applied to a model trained with cross-entropy, we see a large improvement on CIDEr and a minor improvement on other metrics. When combining the repetition penalty with SCST, we see a dramatic improvement across all metrics, and particularly on CIDEr. Interestingly, SCST only works when its baseline reward model is strong;. for this reason the combination of the repetition penalty and SCST is particularly effective.",
        "sentences": [
            "Results.",
            "Table 1 shows the main experimental results.",
            "Our baseline cross-entropy captioning model gets similar scores to the original flat model.",
            "When the repetition penalty is applied to a model trained with cross-entropy, we see a large improvement on CIDEr and a minor improvement on other metrics.",
            "When combining the repetition penalty with SCST, we see a dramatic improvement across all metrics, and particularly on CIDEr.",
            "Interestingly, SCST only works when its baseline reward model is strong;.",
            "for this reason the combination of the repetition penalty and SCST is particularly effective."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "Ours (XE training w/o rep. penalty)",
                "Krause et al. (Flat)"
            ],
            [
                "Ours (XE training w/ rep. penalty)",
                "Ours (SCST training w/ rep. penalty)",
                "CIDEr",
                "METEOR",
                "BLEU-1",
                "BLEU-2",
                "BLEU-3",
                "BLEU-4"
            ],
            [
                "Ours (XE training w/ rep. penalty)",
                "CIDEr",
                "METEOR",
                "BLEU-1",
                "BLEU-2",
                "BLEU-3",
                "BLEU-4"
            ],
            [
                "Ours (SCST training w/ rep. penalty)"
            ],
            [
                "Ours (SCST training w/ rep. penalty)"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_1",
        "paper_id": "D18-1084",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1085table_1",
        "description": "We evaluate ROUGE-G, against the top metrics (C S IIITH3, DemokritosGR1, Catolicasc1) among the 23 metrics participated in TAC AESOP 2011, ROUGE, and the most recent related work (ROUGE-WE) (Table 1). Overall results support our proposal to consider semantics besides surface with ROUGE. We analyze the correlation results reported in Table 1 in the following. ROUGE-G-2 achieves the best correlation with Pyramid, regarding all correlation metrics. Moreover, every ROUGE-G variant outperforms its corresponding ROUGE and ROUGE-WE variants, regardless of the correlation metric used. However, the only exception is ROUGE-SU4, which correlates slightly better with Pyramid when measuring with Pearson correlation. One possible reason is that Pyramid measures content similarity between peer and model summaries, while the variants of ROUGE-G favor semantics behind the content for measuring similarities. Since some of the semantics attached to the skipped words are lost in the construction of skip-bigrams, ROUGE-SU4 shows a better correlation comparing to ROUGE-G-SU4. For Responsiveness, ROUGE-G-SU4 achieves the best correlation when measuring with Pearson. We also observe that ROUGE-G-2 obtains the best correlation with Responsiveness while measuring with the Spearman and Kendall rank correlations. The reason is that semantic interpretation of bigrams is easier, and that of contiguous bigrams is much more precise. We also see that every variant of ROUGE-G outperforms its corresponding ROUGE and ROUGE-WE variants. The readability score is based on grammaticality, structure, and coherence. Although our main goal is not to improve the readability, ROUGE-G-SU4 and ROUGE-G-2 are observed to correlate very well with this metric when measured with the Pearson and Spearman/Kendall rank correlations, respectively. Besides, every variant of ROUGE-G represents the best correlation results comparing to its corresponding variants of ROUGE and ROUGE-WE for all correlation metrics.",
        "sentences": [
            "We evaluate ROUGE-G, against the top metrics (C S IIITH3, DemokritosGR1, Catolicasc1) among the 23 metrics participated in TAC AESOP 2011, ROUGE, and the most recent related work (ROUGE-WE) (Table 1).",
            "Overall results support our proposal to consider semantics besides surface with ROUGE.",
            "We analyze the correlation results reported in Table 1 in the following.",
            "ROUGE-G-2 achieves the best correlation with Pyramid, regarding all correlation metrics.",
            "Moreover, every ROUGE-G variant outperforms its corresponding ROUGE and ROUGE-WE variants, regardless of the correlation metric used.",
            "However, the only exception is ROUGE-SU4, which correlates slightly better with Pyramid when measuring with Pearson correlation.",
            "One possible reason is that Pyramid measures content similarity between peer and model summaries, while the variants of ROUGE-G favor semantics behind the content for measuring similarities.",
            "Since some of the semantics attached to the skipped words are lost in the construction of skip-bigrams, ROUGE-SU4 shows a better correlation comparing to ROUGE-G-SU4.",
            "For Responsiveness, ROUGE-G-SU4 achieves the best correlation when measuring with Pearson.",
            "We also observe that ROUGE-G-2 obtains the best correlation with Responsiveness while measuring with the Spearman and Kendall rank correlations.",
            "The reason is that semantic interpretation of bigrams is easier, and that of contiguous bigrams is much more precise.",
            "We also see that every variant of ROUGE-G outperforms its corresponding ROUGE and ROUGE-WE variants.",
            "The readability score is based on grammaticality, structure, and coherence.",
            "Although our main goal is not to improve the readability, ROUGE-G-SU4 and ROUGE-G-2 are observed to correlate very well with this metric when measured with the Pearson and Spearman/Kendall rank correlations, respectively.",
            "Besides, every variant of ROUGE-G represents the best correlation results comparing to its corresponding variants of ROUGE and ROUGE-WE for all correlation metrics."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1,
            2,
            2,
            1,
            1,
            2,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "ROUGE-G-1",
                "ROUGE-G-2",
                "ROUGE-G-SU4",
                "C S IIITH3",
                "DemokritosGR1",
                "Catolicasc1",
                "ROUGE-1",
                "ROUGE-2",
                "ROUGE-SU4",
                "ROUGE-WE-1",
                "ROUGE-WE-2",
                "ROUGE-WE-SU4"
            ],
            [
                "ROUGE-G-1",
                "ROUGE-G-2",
                "ROUGE-G-SU4",
                "ROUGE-1",
                "ROUGE-2",
                "ROUGE-SU4",
                "ROUGE-WE-1",
                "ROUGE-WE-2",
                "ROUGE-WE-SU4"
            ],
            null,
            [
                "ROUGE-G-2",
                "Pyramid",
                "Pearson",
                "Spearman",
                "Kendall"
            ],
            [
                "ROUGE-G-1",
                "ROUGE-G-2",
                "ROUGE-G-SU4",
                "ROUGE-1",
                "ROUGE-2",
                "ROUGE-SU4",
                "ROUGE-WE-1",
                "ROUGE-WE-2",
                "ROUGE-WE-SU4"
            ],
            [
                "ROUGE-SU4",
                "ROUGE-G-SU4",
                "Pyramid",
                "Pearson"
            ],
            [
                "Pyramid",
                "ROUGE-G-SU4"
            ],
            [
                "ROUGE-SU4",
                "ROUGE-G-SU4"
            ],
            [
                "ROUGE-G-SU4",
                "Responsiveness",
                "Pearson"
            ],
            [
                "ROUGE-G-2",
                "Responsiveness",
                "Spearman",
                "Kendall"
            ],
            null,
            [
                "Responsiveness",
                "ROUGE-G-1",
                "ROUGE-G-2",
                "ROUGE-G-SU4",
                "ROUGE-1",
                "ROUGE-2",
                "ROUGE-SU4",
                "ROUGE-WE-1",
                "ROUGE-WE-2",
                "ROUGE-WE-SU4"
            ],
            [
                "Readability"
            ],
            [
                "Readability",
                "ROUGE-G-2",
                "ROUGE-G-SU4",
                "Pearson",
                "Spearman",
                "Kendall"
            ],
            [
                "Readability",
                "ROUGE-G-1",
                "ROUGE-G-2",
                "ROUGE-G-SU4",
                "ROUGE-1",
                "ROUGE-2",
                "ROUGE-SU4",
                "ROUGE-WE-1",
                "ROUGE-WE-2",
                "ROUGE-WE-SU4"
            ]
        ],
        "n_sentence": 15.0,
        "table_id": "table_1",
        "paper_id": "D18-1085",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1086table_1",
        "description": "AMR-to-Text baseline comparison. We compare our baseline model (described in \u0e22\u0e073.2) against previous works in AMR-to-text using the data from the recent SemEval-2016 Task 8 (May, 2016, LDC2015E86). Table 1 reports BLEU scores comparing our model against previous works. Here, we see that our model achieves a BLEU score comparable with the state-of-the-art, and thus we argue that it is sufficient to be used in our subsequent experiments with guidance.",
        "sentences": [
            "AMR-to-Text baseline comparison.",
            "We compare our baseline model (described in \u0e22\u0e073.2) against previous works in AMR-to-text using the data from the recent SemEval-2016 Task 8 (May, 2016, LDC2015E86).",
            "Table 1 reports BLEU scores comparing our model against previous works.",
            "Here, we see that our model achieves a BLEU score comparable with the state-of-the-art, and thus we argue that it is sufficient to be used in our subsequent experiments with guidance."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "NeuralAMR (Konstas et al. 2017)",
                "TSP (Song et al. 2016)",
                "TreeToStr (Flanigan et al. 2016)"
            ],
            [
                "Our model (unguided NLG)",
                "NeuralAMR (Konstas et al. 2017)",
                "TSP (Song et al. 2016)",
                "TreeToStr (Flanigan et al. 2016)",
                "BLEU"
            ],
            [
                "Our model (unguided NLG)",
                "NeuralAMR (Konstas et al. 2017)",
                "TSP (Song et al. 2016)",
                "TreeToStr (Flanigan et al. 2016)",
                "BLEU"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_1",
        "paper_id": "D18-1086",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1086table_2",
        "description": "Guided NLG for AMR-to-Text. In this experiment we apply our guided NLG mechanism described in \u00a73.3 to our baseline seq2seq model. To isolate the effects of guidance we skip the actual summarization process and proceed to directly generating the summary text from the gold standard summary AMR graphs from the Proxy Report section. To determine the hyper-parameters, we perform a grid search using the dev dataset, where we found the best combination of \u03c8, \u03b8 and k are 0.95, 2.5 and 15 respectively. We have two different settings for this experiment: the oracle and non-oracle settings. In the oracle setting, we directly use the gold standard summary text as the guidance for our model. The intuition is that in this setting, our model knows precisely which words should appear in the summary text, thus providing an upper bound for the performance of our guided NLG approach. In the non-oracle setting, we use the mechanism described in \u00a73.3. We also compare them against the baseline (unguided) model from \u00a73.2. Table 2 reports performance for all models. The difference between the guided and the unguided model is 16.2 points in BLEU and 9.9 points in ROUGE-2, while there is room for improvement as evidenced by the difference between the oracle and non-oracle result.",
        "sentences": [
            "Guided NLG for AMR-to-Text.",
            "In this experiment we apply our guided NLG mechanism described in \u00a73.3 to our baseline seq2seq model.",
            "To isolate the effects of guidance we skip the actual summarization process and proceed to directly generating the summary text from the gold standard summary AMR graphs from the Proxy Report section.",
            "To determine the hyper-parameters, we perform a grid search using the dev dataset, where we found the best combination of \u03c8, \u03b8 and k are 0.95, 2.5 and 15 respectively.",
            "We have two different settings for this experiment: the oracle and non-oracle settings.",
            "In the oracle setting, we directly use the gold standard summary text as the guidance for our model.",
            "The intuition is that in this setting, our model knows precisely which words should appear in the summary text, thus providing an upper bound for the performance of our guided NLG approach.",
            "In the non-oracle setting, we use the mechanism described in \u00a73.3.",
            "We also compare them against the baseline (unguided) model from \u00a73.2.",
            "Table 2 reports performance for all models.",
            "The difference between the guided and the unguided model is 16.2 points in BLEU and 9.9 points in ROUGE-2, while there is room for improvement as evidenced by the difference between the oracle and non-oracle result."
        ],
        "class_sentence": [
            2,
            2,
            2,
            0,
            2,
            2,
            2,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Guided NLG"
            ],
            null,
            null,
            [
                "Guided NLG (Oracle)",
                "Guided NLG"
            ],
            [
                "Guided NLG (Oracle)"
            ],
            [
                "Guided NLG (Oracle)"
            ],
            [
                "Guided NLG"
            ],
            [
                "Guided NLG"
            ],
            [
                "Guided NLG (Oracle)",
                "Guided NLG",
                "Unguided NLG"
            ],
            [
                "Guided NLG",
                "Unguided NLG",
                "BLEU",
                "R-2",
                "Guided NLG (Oracle)"
            ]
        ],
        "n_sentence": 11.0,
        "table_id": "table_2",
        "paper_id": "D18-1086",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1110table_3",
        "description": "Table 3 shows the results of our model on the second type of adversarial examples, i.e., the paraphrased ATIS development set. We also report the result of our model on the original ATIS development set. We can see that (1) no matter which feature our model uses, the performance degrades at least 2.5% on the paraphrased dataset;. (2) the model that only uses word order features achieves the worst robustness to the paraphrased queries while the dependency feature seems more robust than other two features. (3) simultaneously utilizing three syntactic features could greatly enhance the robustness of our model. These results again demonstrate that our model could benefit from incorporating more aspects of syntactic information.",
        "sentences": [
            "Table 3 shows the results of our model on the second type of adversarial examples, i.e., the paraphrased ATIS development set.",
            "We also report the result of our model on the original ATIS development set.",
            "We can see that (1) no matter which feature our model uses, the performance degrades at least 2.5% on the paraphrased dataset;.",
            "(2) the model that only uses word order features achieves the worst robustness to the paraphrased queries while the dependency feature seems more robust than other two features.",
            "(3) simultaneously utilizing three syntactic features could greatly enhance the robustness of our model.",
            "These results again demonstrate that our model could benefit from incorporating more aspects of syntactic information."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "Accpara"
            ],
            [
                "Accori"
            ],
            [
                "Word Order + Dep + Cons",
                "Accpara",
                "Diff."
            ],
            [
                "Word Order"
            ],
            [
                "Word Order + Dep + Cons"
            ],
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_3",
        "paper_id": "D18-1110",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1111table_2",
        "description": "5.1 Results. Table 2 shows that RECODE outperforms the baselines in both BLEU and accuracy, providing evidence for the effectiveness of incorporating retrieval methods into tree-based approaches. We ran statistical significance tests for RECODE and YN17, using bootstrap resampling with N = 10,000. For the BLEU scores of both datasets, p < 0.001. For the exact match accuracy, p < 0.001 for Django dataset, but for Hearthstone, p > 0.3, showing that the retrieval-based model is on par with YN17. It is worth noting, though, that HS consists of long and complex code, and that generating exact matches is very difficult, making exact match accuracy a less reliable metric. We also compare RECODE with Rabinovich et al. (2017)\u00e2\u20ac\u2122s Abstract Syntax Networks with supervision (ASN+SUPATT) which is the state-of-the-art system for HS. RECODE exceeds ASN without extra supervision though ASN+SUPATT has a slightly better result. However, ASN+SUPATT is trained with supervisedattention extracted through heuristic exact wordmatches while our attention is unsupervised.",
        "sentences": [
            "5.1 Results.",
            "Table 2 shows that RECODE outperforms the baselines in both BLEU and accuracy, providing evidence for the effectiveness of incorporating retrieval methods into tree-based approaches.",
            "We ran statistical significance tests for RECODE and YN17, using bootstrap resampling with N = 10,000.",
            "For the BLEU scores of both datasets, p < 0.001.",
            "For the exact match accuracy, p < 0.001 for Django dataset, but for Hearthstone, p > 0.3, showing that the retrieval-based model is on par with YN17.",
            "It is worth noting, though, that HS consists of long and complex code, and that generating exact matches is very difficult, making exact match accuracy a less reliable metric.",
            "We also compare RECODE with Rabinovich et al. (2017)\u00e2\u20ac\u2122s Abstract Syntax Networks with supervision (ASN+SUPATT) which is the state-of-the-art system for HS.",
            "RECODE exceeds ASN without extra supervision though ASN+SUPATT has a slightly better result.",
            "However, ASN+SUPATT is trained with supervisedattention extracted through heuristic exact wordmatches while our attention is unsupervised."
        ],
        "class_sentence": [
            2,
            1,
            2,
            2,
            2,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "RECODE",
                "SEQ2SEQ",
                "YN17",
                "ASN",
                "ASN + SUPATT",
                "Acc",
                "BLEU"
            ],
            [
                "RECODE",
                "YN17"
            ],
            [
                "BLEU",
                "HS",
                "Django"
            ],
            [
                "Django",
                "HS",
                "YN17"
            ],
            [
                "HS"
            ],
            [
                "RECODE",
                "ASN + SUPATT",
                "HS"
            ],
            [
                "RECODE",
                "ASN",
                "ASN + SUPATT"
            ],
            [
                "ASN + SUPATT",
                "RECODE"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_2",
        "paper_id": "D18-1111",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1112table_1",
        "description": "Results and Discussion. Table 1 summarizes the results of our models and baselines. Although the template-based method achieves decent BLEU scores, its grammaticality score is substantially worse than other baselines. We can see that on both two datasets, our Graph2Seq models perform significantly better than the Seq2Seq and Tree2Seq baselines. One possible reason is that in our graph encoder, the node embedding retains the information of neighbor nodes within K hops. However, in the tree encoder, the node embedding only aggregates the information of descendants while losing the knowledge of ancestors. The pooling-based graph embedding is found to be more useful than the node-based graph embedding because Graph2Seq-NGE adds a nonexistent node into the graph, which introduces the noisy information in calculating the embeddings of other nodes. We also conducted an experiment that treats the SQL query graph as an undirected graph and found the performance degrades. By manually analyzing the cases in which the Graph2Seq model performs better than Seq2Seq, we find the Graph2Seq model is better at interpreting two classes of queries:.",
        "sentences": [
            "Results and Discussion.",
            "Table 1 summarizes the results of our models and baselines.",
            "Although the template-based method achieves decent BLEU scores, its grammaticality score is substantially worse than other baselines.",
            "We can see that on both two datasets, our Graph2Seq models perform significantly better than the Seq2Seq and Tree2Seq baselines.",
            "One possible reason is that in our graph encoder, the node embedding retains the information of neighbor nodes within K hops.",
            "However, in the tree encoder, the node embedding only aggregates the information of descendants while losing the knowledge of ancestors.",
            "The pooling-based graph embedding is found to be more useful than the node-based graph embedding because Graph2Seq-NGE adds a nonexistent node into the graph, which introduces the noisy information in calculating the embeddings of other nodes.",
            "We also conducted an experiment that treats the SQL query graph as an undirected graph and found the performance degrades.",
            "By manually analyzing the cases in which the Graph2Seq model performs better than Seq2Seq, we find the Graph2Seq model is better at interpreting two classes of queries:."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Template",
                "Seq2Seq",
                "Seq2Seq + Copy",
                "Tree2Seq",
                "Graph2Seq-PGE",
                "Graph2Seq-NGE",
                "(Iyer et al. 2016)"
            ],
            [
                "Template",
                "BLEU-4",
                "Grammar.",
                "Seq2Seq",
                "Seq2Seq + Copy",
                "Tree2Seq"
            ],
            [
                "Graph2Seq-PGE",
                "Graph2Seq-NGE",
                "Seq2Seq",
                "Tree2Seq"
            ],
            [
                "Graph2Seq-PGE",
                "Graph2Seq-NGE"
            ],
            null,
            [
                "Graph2Seq-NGE"
            ],
            [
                "Graph2Seq-NGE"
            ],
            [
                "Graph2Seq-PGE",
                "Graph2Seq-NGE",
                "Seq2Seq"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_1",
        "paper_id": "D18-1112",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1114table_6",
        "description": "Table 6 shows binary classification F1 on SPR1. All new results outperforming prior work (CRF) in bold.",
        "sentences": [
            "Table 6 shows binary classification F1 on SPR1.",
            "All new results outperforming prior work (CRF) in bold."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "SPR1",
                "MT:SPR1",
                "SPR1+2",
                "CRF"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_6",
        "paper_id": "D18-1114",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1124table_1",
        "description": "The main results are reported in Table 1. Our neural transition-based model achieves the best results in ACE datasets and comparable results in GENIA dataset in terms of F1 measure. We hypothesize that the performance gain of our model compared with other methods is largely due to improved performance on the portions of nested mentions in our datasets. To verify this, we design an experiment to evaluate how well a system can recognize nested mentions. Handling Nested Mentions. The idea is that we split the test data into two portions: sentences with and without nested mentions. The results of GENIA are listed in Table 2. We can observe that the margin of improvement is more significant in the portion of nested mentions, revealing our model\u00e2\u20ac\u2122s effectiveness in handling nested mentions. This observation helps explain why our model achieves greater improvement in ACE than in GENIA in Table 1 since the former has much more nested structures than the latter. Moreover, Ju et al. (2018) performs better when it comes to non-nested mentions possibly due to the CRF they used, which globally normalizes each stacked layer. Decoding Speed. Note that Lu and Roth (2015) and Muis and Lu (2017) also feature linear-time complexity, but with a greater constant factor. To compare the decoding speed, we re-implemented their model with the same platform (PyTorch) and run them on the same machine (CPU: Intel i5 2.7GHz). Our model turns out to be around 3-5 times faster than theirs, showing its scalability. Ablation Study. To evaluate the contribution of neural components including pre-trained embeddings, the characterlevel LSTM and dropout layers, we test the performances of ablated models. The results are listed in Table 1. From the performance gap, we can conclude that these components contribute significantly to the effectiveness of our model in all three datasets.",
        "sentences": [
            "The main results are reported in Table 1.",
            "Our neural transition-based model achieves the best results in ACE datasets and comparable results in GENIA dataset in terms of F1 measure.",
            "We hypothesize that the performance gain of our model compared with other methods is largely due to improved performance on the portions of nested mentions in our datasets.",
            "To verify this, we design an experiment to evaluate how well a system can recognize nested mentions.",
            "Handling Nested Mentions.",
            "The idea is that we split the test data into two portions: sentences with and without nested mentions.",
            "The results of GENIA are listed in Table 2.",
            "We can observe that the margin of improvement is more significant in the portion of nested mentions, revealing our model\u00e2\u20ac\u2122s effectiveness in handling nested mentions.",
            "This observation helps explain why our model achieves greater improvement in ACE than in GENIA in Table 1 since the former has much more nested structures than the latter.",
            "Moreover, Ju et al. (2018) performs better when it comes to non-nested mentions possibly due to the CRF they used, which globally normalizes each stacked layer.",
            "Decoding Speed.",
            "Note that Lu and Roth (2015) and Muis and Lu (2017) also feature linear-time complexity, but with a greater constant factor.",
            "To compare the decoding speed, we re-implemented their model with the same platform (PyTorch) and run them on the same machine (CPU: Intel i5 2.7GHz).",
            "Our model turns out to be around 3-5 times faster than theirs, showing its scalability.",
            "Ablation Study.",
            "To evaluate the contribution of neural components including pre-trained embeddings, the characterlevel LSTM and dropout layers, we test the performances of ablated models.",
            "The results are listed in Table 1.",
            "From the performance gap, we can conclude that these components contribute significantly to the effectiveness of our model in all three datasets."
        ],
        "class_sentence": [
            1,
            1,
            2,
            0,
            2,
            0,
            0,
            0,
            1,
            0,
            2,
            2,
            2,
            1,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Ours",
                "ACE04",
                "ACE05",
                "GENIA"
            ],
            [
                "Ours"
            ],
            null,
            null,
            null,
            null,
            null,
            [
                "Ours",
                "ACE04",
                "ACE05",
                "GENIA"
            ],
            null,
            null,
            [
                "Lu and Roth (2015)",
                "Muis and Lu (2017)"
            ],
            [
                "Lu and Roth (2015)",
                "Muis and Lu (2017)"
            ],
            [
                "Ours",
                "Lu and Roth (2015)",
                "Muis and Lu (2017)",
                "w/s"
            ],
            null,
            [
                "Ours",
                "- char-level LSTM",
                "- pre-trained embeddings",
                "- dropout layer"
            ],
            null,
            [
                "Ours",
                "- char-level LSTM",
                "- pre-trained embeddings",
                "- dropout layer",
                "ACE04",
                "ACE05",
                "GENIA"
            ]
        ],
        "n_sentence": 18.0,
        "table_id": "table_1",
        "paper_id": "D18-1124",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1126table_1",
        "description": "Results. The model set forth in this section is the basis for the remaining models in this paper;. we call it the GRU model as that is the only context encoding mechanism it uses. As shown in Table 1, this GRU model gets a score of 73.4 on the WikilinksNED development set. Results. In Table 1, we see that our model with attention (GRU+ATTN) outperforms our basic GRU model by around 1% absolute. It also outperforms the roughly similar model of Eshel et al. (2017) on the test set:. this gain is due to a combination of factors including the improved training procedure and some small modeling changes. However, our attention scheme is not without its shortcomings, as we now discuss. Table 1 shows the impact of incorporating character CNNs (GRU+ATTN+CNN). Surprisingly, these have a mild negative impact on performance. One possible explanation of this is that it causes the model to split its attention between semantically important and lexically similar context terms. Table 1 shows the results of stacking these features on top of our model with attention (GRU+ATTN+FEATS). We see our highest development set performance and correspondingly high test performance from this model. This indicates that character-level information is useful for disambiguation, but character CNNs as we incorporated them are not able to distill it as effectively as sparse features can. Our model augmented with these sparse features achieves state-of-the-art results on the test set.",
        "sentences": [
            "Results.",
            "The model set forth in this section is the basis for the remaining models in this paper;.",
            "we call it the GRU model as that is the only context encoding mechanism it uses.",
            "As shown in Table 1, this GRU model gets a score of 73.4 on the WikilinksNED development set.",
            "Results.",
            "In Table 1, we see that our model with attention (GRU+ATTN) outperforms our basic GRU model by around 1% absolute.",
            "It also outperforms the roughly similar model of Eshel et al. (2017) on the test set:.",
            "this gain is due to a combination of factors including the improved training procedure and some small modeling changes.",
            "However, our attention scheme is not without its shortcomings, as we now discuss.",
            "Table 1 shows the impact of incorporating character CNNs (GRU+ATTN+CNN).",
            "Surprisingly, these have a mild negative impact on performance.",
            "One possible explanation of this is that it causes the model to split its attention between semantically important and lexically similar context terms.",
            "Table 1 shows the results of stacking these features on top of our model with attention (GRU+ATTN+FEATS).",
            "We see our highest development set performance and correspondingly high test performance from this model.",
            "This indicates that character-level information is useful for disambiguation, but character CNNs as we incorporated them are not able to distill it as effectively as sparse features can.",
            "Our model augmented with these sparse features achieves state-of-the-art results on the test set."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            2,
            1,
            1,
            2,
            2,
            1,
            1,
            2,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "GRU"
            ],
            [
                "GRU"
            ],
            [
                "GRU",
                "Accuracy on Dev (%)"
            ],
            null,
            [
                "GRU",
                "GRU+ATTN",
                "Accuracy on Dev (%)"
            ],
            [
                "GRU+ATTN",
                "Eshel et al. (2017)",
                "Accuracy on Test (%)"
            ],
            null,
            [
                "GRU+ATTN"
            ],
            [
                "GRU+ATTN+CNN"
            ],
            [
                "GRU+ATTN+CNN",
                "Accuracy on Dev (%)"
            ],
            [
                "GRU+ATTN+CNN"
            ],
            [
                "GRU+ATTN+FEATS"
            ],
            [
                "GRU+ATTN+FEATS",
                "Accuracy on Dev (%)",
                "Accuracy on Test (%)"
            ],
            [
                "GRU+ATTN+FEATS",
                "GRU+ATTN+CNN"
            ],
            [
                "GRU+ATTN+FEATS",
                "GRU+ATTN+CNN",
                "Accuracy on Test (%)"
            ]
        ],
        "n_sentence": 16.0,
        "table_id": "table_1",
        "paper_id": "D18-1126",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1130table_2",
        "description": "Results and Discussion. Table 2 shows the full results of our best models on the LAMBADA and CBT-NE datasets, and compares them to recent, best-performing results in the literature. For both tasks the inclusion of either entity features or multi-task objectives leads to large statistically significant increases in validation and test score, according to the McNemar test (\u03b1 = 0.05) with continuity correction (Dietterich, 1998). Without features, AttSum + L2 achieves the best test results, whereas with features AttSum-Feat + L1 performs best on CBTNE. The results on LAMBADA indicate that entity tracking is a very important overlooked aspect of the task. Interestingly, with features included, AttSum-Feat + L2 appears to hurt test performance on LAMBADA and leaves CBTNE performance essentially unchanged, amounting to a negative result for L2. On the other hand, the effect of AttSum-Feat + L1 is pronounced on CBT-NE, and while our simple models do not increase the state-of-the-art test performance on CBT-NE, they outperform \u201cattentionover-attention\u201d in addition to reranking (Cui et al. 2016), and is outperformed only by architectures supporting \u201cmultiple-hop\u201d inference over the document (Dhingra et al. 2016). Our best model on CBT-NE test set, AttSum-Feat + L1, is very close to the current state-of-the-art result. On the validation sets for both LAMBADA and CBT-NE, the improvements from adding features to AttSum + Li are statistically significant (for full results refer to our supplementary material). On LAMBADA, the L1 multi-tasked model is a 3.5-point increase on the state of the art.",
        "sentences": [
            "Results and Discussion.",
            "Table 2 shows the full results of our best models on the LAMBADA and CBT-NE datasets, and compares them to recent, best-performing results in the literature.",
            "For both tasks the inclusion of either entity features or multi-task objectives leads to large statistically significant increases in validation and test score, according to the McNemar test (\u03b1 = 0.05) with continuity correction (Dietterich, 1998).",
            "Without features, AttSum + L2 achieves the best test results, whereas with features AttSum-Feat + L1 performs best on CBTNE.",
            "The results on LAMBADA indicate that entity tracking is a very important overlooked aspect of the task.",
            "Interestingly, with features included, AttSum-Feat + L2 appears to hurt test performance on LAMBADA and leaves CBTNE performance essentially unchanged, amounting to a negative result for L2.",
            "On the other hand, the effect of AttSum-Feat + L1 is pronounced on CBT-NE, and while our simple models do not increase the state-of-the-art test performance on CBT-NE, they outperform \u201cattentionover-attention\u201d in addition to reranking (Cui et al. 2016), and is outperformed only by architectures supporting \u201cmultiple-hop\u201d inference over the document (Dhingra et al. 2016).",
            "Our best model on CBT-NE test set, AttSum-Feat + L1, is very close to the current state-of-the-art result.",
            "On the validation sets for both LAMBADA and CBT-NE, the improvements from adding features to AttSum + Li are statistically significant (for full results refer to our supplementary material).",
            "On LAMBADA, the L1 multi-tasked model is a 3.5-point increase on the state of the art."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "AttSum",
                "AttSum + L1",
                "AttSum + L2",
                "AttSum-Feat",
                "AttSum-Feat + L1",
                "AttSum-Feat + L2",
                "LAMBADA",
                "CBT-NE",
                "GA Reader (Chu et al. 2017)",
                "MAGE (48) (Dhingra et al. 2017)",
                "MAGE (64) (Dhingra et al. 2017)",
                "GA + C-GRU (Dhingra et al. 2018)",
                "EpiReader (Trischler et al. 2016)",
                "DIM Reader (Liu et al. 2017)",
                "AoA (Cui et al. 2016)",
                "AoA + Reranker (Cui et al. 2016)"
            ],
            null,
            [
                "AttSum + L2",
                "Test",
                "AttSum-Feat + L1",
                "CBT-NE"
            ],
            [
                "LAMBADA"
            ],
            [
                "AttSum-Feat + L2",
                "Test",
                "LAMBADA",
                "CBT-NE"
            ],
            [
                "AttSum-Feat + L1",
                "CBT-NE",
                "AttSum",
                "Test",
                "GA Reader (Dhingra et al. 2016)",
                "AoA + Reranker (Cui et al. 2016)"
            ],
            [
                "AttSum-Feat + L1",
                "CBT-NE",
                "Test",
                "GA Reader (Dhingra et al. 2016)"
            ],
            [
                "AttSum-Feat + L1",
                "AttSum-Feat + L2",
                "Val",
                "LAMBADA",
                "CBT-NE"
            ],
            [
                "AttSum + L1",
                "AttSum-Feat + L1",
                "GA Reader (Chu et al. 2017)",
                "MAGE (48) (Dhingra et al. 2017)",
                "MAGE (64) (Dhingra et al. 2017)",
                "GA + C-GRU (Dhingra et al. 2018)"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_2",
        "paper_id": "D18-1130",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1130table_3",
        "description": "Table 3 considers the performance of the different models based on a segmentation of the data. Here we consider examples where:. (1) Entity if the answer is a named entity;. (2) Speaker if the answer is a named entity and the speaker of quote;. (3) Quote if the answer is found within a quoted speech. Note that Speaker and Quote categories, while mutually exclusive, are subsets of the overall Entity category. We see that both the additional features and multi-task objectives independently result in a clear improvement in all categories, but that the gains are particularly pronounced for named entities and specifically for Speaker and Quote examples. Here we see sizable increases in performance, particularly in the Speaker category. We see larger increases in the more dialog heavy LAMBADA task.",
        "sentences": [
            "Table 3 considers the performance of the different models based on a segmentation of the data.",
            "Here we consider examples where:.",
            "(1) Entity if the answer is a named entity;.",
            "(2) Speaker if the answer is a named entity and the speaker of quote;.",
            "(3) Quote if the answer is found within a quoted speech.",
            "Note that Speaker and Quote categories, while mutually exclusive, are subsets of the overall Entity category.",
            "We see that both the additional features and multi-task objectives independently result in a clear improvement in all categories, but that the gains are particularly pronounced for named entities and specifically for Speaker and Quote examples.",
            "Here we see sizable increases in performance, particularly in the Speaker category.",
            "We see larger increases in the more dialog heavy LAMBADA task."
        ],
        "class_sentence": [
            1,
            2,
            2,
            2,
            2,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "AttSum",
                "AttSum + L1",
                "AttSum + L2",
                "AttSum-Feat",
                "AttSum-Feat + L1",
                "AttSum-Feat + L2",
                "All",
                "Entity",
                "Speaker",
                "Quote",
                "LAMBADA",
                "CBT-NE"
            ],
            null,
            [
                "Entity"
            ],
            [
                "Speaker"
            ],
            [
                "Quote"
            ],
            [
                "Entity",
                "Speaker",
                "Quote"
            ],
            [
                "AttSum",
                "AttSum + L1",
                "AttSum + L2",
                "AttSum-Feat",
                "AttSum-Feat + L1",
                "AttSum-Feat + L2",
                "All",
                "Speaker",
                "Quote"
            ],
            [
                "Speaker"
            ],
            [
                "LAMBADA",
                "Speaker"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_3",
        "paper_id": "D18-1130",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1138table_2",
        "description": "Table 2 shows the evaluation results. Our model has obvious advantage over the baseline systems in content preservation, and also performs well in other aspects.",
        "sentences": [
            "Table 2 shows the evaluation results.",
            "Our model has obvious advantage over the baseline systems in content preservation, and also performs well in other aspects."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "SMAE",
                "CAE",
                "MAE",
                "Content"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_2",
        "paper_id": "D18-1138",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1139table_1",
        "description": "Aspect polarity. Table 1 shows the results of our experiments, as well as the results of our strong baselines. Note that the majority class baseline already provides good results. This is due to highly unbalanced data;. the aspect category \u201cAllgemein\u201d (\u201cgeneral\u201d), e.g., constitutes 61.5% of the cases. This imbalance makes the task even more challenging. Over all architectures, we observe a comparable or better performance when using fasttext embeddings instead of word2vec or glove. This backs our hypothesis that subword features are important for processing the morphologically rich German language. Leaving everything else unchanged, we can furthermore see an increase in performance for all settings, when switching from the pipeline to an end-to-end approach. The best performance (marked in bold) is achieved by a combination of CNN and FastText embeddings, which outperforms the highly adapted winning system of the shared task.",
        "sentences": [
            "Aspect polarity.",
            "Table 1 shows the results of our experiments, as well as the results of our strong baselines.",
            "Note that the majority class baseline already provides good results.",
            "This is due to highly unbalanced data;.",
            "the aspect category \u201cAllgemein\u201d (\u201cgeneral\u201d), e.g., constitutes 61.5% of the cases.",
            "This imbalance makes the task even more challenging.",
            "Over all architectures, we observe a comparable or better performance when using fasttext embeddings instead of word2vec or glove.",
            "This backs our hypothesis that subword features are important for processing the morphologically rich German language.",
            "Leaving everything else unchanged, we can furthermore see an increase in performance for all settings, when switching from the pipeline to an end-to-end approach.",
            "The best performance (marked in bold) is achieved by a combination of CNN and FastText embeddings, which outperforms the highly adapted winning system of the shared task."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            2,
            2,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Pipeline LSTM + word2vec",
                "End-to-end LSTM + word2vec",
                "Pipeline CNN + word2vec",
                "End-to-end CNN + word2vec",
                "Pipeline LSTM + glove",
                "End-to-end LSTM + glove",
                "Pipeline CNN + glove",
                "End-to-end CNN + glove",
                "Pipeline LSTM + fasttext",
                "End-to-end LSTM + fasttext",
                "Pipeline CNN + fasttext",
                "End-to-end CNN + fasttext",
                "majority class baseline",
                "GermEval baseline",
                "GermEval best submission"
            ],
            [
                "majority class baseline"
            ],
            null,
            null,
            null,
            [
                "Pipeline LSTM + fasttext",
                "End-to-end LSTM + fasttext",
                "Pipeline CNN + fasttext",
                "End-to-end CNN + fasttext",
                "Pipeline LSTM + word2vec",
                "End-to-end LSTM + word2vec",
                "Pipeline CNN + word2vec",
                "End-to-end CNN + word2vec",
                "Pipeline LSTM + glove",
                "End-to-end LSTM + glove",
                "Pipeline CNN + glove",
                "End-to-end CNN + glove"
            ],
            null,
            [
                "Pipeline LSTM + word2vec",
                "End-to-end LSTM + word2vec",
                "Pipeline CNN + word2vec",
                "End-to-end CNN + word2vec",
                "Pipeline LSTM + glove",
                "End-to-end LSTM + glove",
                "Pipeline CNN + glove",
                "End-to-end CNN + glove",
                "Pipeline LSTM + fasttext",
                "End-to-end LSTM + fasttext",
                "Pipeline CNN + fasttext",
                "End-to-end CNN + fasttext"
            ],
            [
                "End-to-end CNN + fasttext",
                "GermEval baseline"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_1",
        "paper_id": "D18-1139",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1139table_2",
        "description": "Aspect category only. Even though our architectures are designed for the task of joint prediction of aspect category and polarity, we can also evaluate them on the detection of aspect categories only. Table 2 shows the results for this task. First of all, we can see that the SVM-based GermEval baseline model has very decent performance as it is practically on par with the best submission for the synchronic and even outperforms the best submission on the diachronic test set. It is therefore well-suited to serve as input to the pipeline LSTM model we compare with in our main task. Comparing our architectures, we see again that fasttext embeddings always lead to equal or better performance. And even though we do not directly optimize our models for this task only, our best model (CNN+fasttext) outperforms all baselines, as well as the GermEval winning system.",
        "sentences": [
            "Aspect category only.",
            "Even though our architectures are designed for the task of joint prediction of aspect category and polarity, we can also evaluate them on the detection of aspect categories only.",
            "Table 2 shows the results for this task.",
            "First of all, we can see that the SVM-based GermEval baseline model has very decent performance as it is practically on par with the best submission for the synchronic and even outperforms the best submission on the diachronic test set.",
            "It is therefore well-suited to serve as input to the pipeline LSTM model we compare with in our main task.",
            "Comparing our architectures, we see again that fasttext embeddings always lead to equal or better performance.",
            "And even though we do not directly optimize our models for this task only, our best model (CNN+fasttext) outperforms all baselines, as well as the GermEval winning system."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "GermEval baseline",
                "GermEval best submission",
                "synchronic test set",
                "diachronic test set"
            ],
            [
                "GermEval baseline"
            ],
            [
                "End-to-end LSTM + fasttext",
                "End-to-end CNN + fasttext"
            ],
            [
                "End-to-end CNN + fasttext",
                "majority class baseline",
                "GermEval baseline",
                "GermEval best submission"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_2",
        "paper_id": "D18-1139",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1146table_2",
        "description": "Table 2 shows the average F1 scores of different models versus sommeliers\u2019 performance. Likewise, the sommeliers\u2019 performance measures represent a conservative(ly higher) estimate since scores lower than 60% in section 1 were removed. We find the HSLDA model, especially of monovarietals, outperforms sommeliers by 6.3%, as measured by F1.",
        "sentences": [
            "Table 2 shows the average F1 scores of different models versus sommeliers\u2019 performance.",
            "Likewise, the sommeliers\u2019 performance measures represent a conservative(ly higher) estimate since scores lower than 60% in section 1 were removed.",
            "We find the HSLDA model, especially of monovarietals, outperforms sommeliers by 6.3%, as measured by F1."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "HSLDA1 Monovarietal",
                "HSLDA2 Blend",
                "HSLDA3 Balanced",
                "Sommeliers",
                "F1 Scores"
            ],
            [
                "Sommeliers"
            ],
            [
                "HSLDA1 Monovarietal",
                "Sommeliers",
                "Testing Set"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_2",
        "paper_id": "D18-1146",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1147table_3",
        "description": "Table 3 shows the results of this comparison. As can be seen from the table, ConvLexLSTM achieves the best results consistently throughout all experiments in terms of all compared measures. This ablation experiment confirms our intuition that all components are contributing to the final emotion detection. For example, removing the seven lexicon features from ConvLexLSTM, which yields ConvLSTM, results in a drop in F1-score by 5.8% on joy in B-DS, and by 3.9% on sadness in B-DS. Still, ConvLSTM is the second performing model in terms of F1-score. These results show that our model can be successfully applied in a health domain even in the absence of health lexicons, which are often expensive to obtain. Not surprisingly, the SVM with the sevenlexicon based features (denoted as Seven-Lexicon) performs the worst among the compared models, suggesting that capturing the semantic information from text via deep neural networks improves emotion detection. Second, we compare ConvLexLSTM with three baselines: C-ConvLSTM (i.e., a character-level CNN-LSTM) (Kim et al., 2016), SWAT (Katz et al., 2007) (i.e., an emotion detection model from SemEval-2007), and EmoSVM (i.e., an SVM with a set of handcrafted features: unigrams, bigrams, POS tags, the word-emotions association lexicon by Mohammad (2012), the WordNet-Affect lexicon by Strapparava et al. (2004), and the output of the Stanford sentiment tool by Socher et al. (2013). Table 3 shows the results of this comparison as well. As can be seen, ConvLexLSTM outperforms all three baselines on both datasets, and more importantly, the character-level CNN-LSTM by Kim et al. (2016) (i.e., the C-ConvLSTM model). This result confirms our belief that applying word embedding vectors, which are trained directly on data from OHCs yields improvement in performance over character-level models. It is worth mentioning that all deep neural networks, ConvLexLSTM, ConvLSTM, CNN, LSTM, and C-ConvLSTM, that capture high-level semantic features perform better than the traditional models on emotion detection. The lexiconbased features act as a complement (for the highlevel semantic features) by looking into exact words in the text to generate appropriate features in ConvLexLSTM for emotion detection. With a paired T-test, the improvements of ConvLexLSTM over the compared models for F1-score are statistically significant for p-values < 0.05.",
        "sentences": [
            "Table 3 shows the results of this comparison.",
            "As can be seen from the table, ConvLexLSTM achieves the best results consistently throughout all experiments in terms of all compared measures.",
            "This ablation experiment confirms our intuition that all components are contributing to the final emotion detection.",
            "For example, removing the seven lexicon features from ConvLexLSTM, which yields ConvLSTM, results in a drop in F1-score by 5.8% on joy in B-DS, and by 3.9% on sadness in B-DS.",
            "Still, ConvLSTM is the second performing model in terms of F1-score.",
            "These results show that our model can be successfully applied in a health domain even in the absence of health lexicons, which are often expensive to obtain.",
            "Not surprisingly, the SVM with the sevenlexicon based features (denoted as Seven-Lexicon) performs the worst among the compared models, suggesting that capturing the semantic information from text via deep neural networks improves emotion detection.",
            "Second, we compare ConvLexLSTM with three baselines: C-ConvLSTM (i.e., a character-level CNN-LSTM) (Kim et al., 2016), SWAT (Katz et al., 2007) (i.e., an emotion detection model from SemEval-2007), and EmoSVM (i.e., an SVM with a set of handcrafted features: unigrams, bigrams, POS tags, the word-emotions association lexicon by Mohammad (2012), the WordNet-Affect lexicon by Strapparava et al. (2004), and the output of the Stanford sentiment tool by Socher et al. (2013).",
            "Table 3 shows the results of this comparison as well.",
            "As can be seen, ConvLexLSTM outperforms all three baselines on both datasets, and more importantly, the character-level CNN-LSTM by Kim et al. (2016) (i.e., the C-ConvLSTM model).",
            "This result confirms our belief that applying word embedding vectors, which are trained directly on data from OHCs yields improvement in performance over character-level models.",
            "It is worth mentioning that all deep neural networks, ConvLexLSTM, ConvLSTM, CNN, LSTM, and C-ConvLSTM, that capture high-level semantic features perform better than the traditional models on emotion detection.",
            "The lexiconbased features act as a complement (for the highlevel semantic features) by looking into exact words in the text to generate appropriate features in ConvLexLSTM for emotion detection.",
            "With a paired T-test, the improvements of ConvLexLSTM over the compared models for F1-score are statistically significant for p-values < 0.05."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            1,
            2,
            1,
            2,
            1,
            1,
            2,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "ConvLexLSTM",
                "B-DS",
                "L-DS",
                "Pr",
                "Re",
                "F1"
            ],
            null,
            [
                "ConvLexLSTM",
                "ConvLSTM",
                "B-DS",
                "F1",
                "Joy",
                "Sad"
            ],
            [
                "ConvLSTM",
                "F1"
            ],
            [
                "ConvLexLSTM"
            ],
            [
                "Seven-Lexicon"
            ],
            [
                "ConvLexLSTM",
                "C-ConvLSTM",
                "SWAT",
                "EmoSVM"
            ],
            null,
            [
                "C-ConvLSTM",
                "SWAT",
                "EmoSVM",
                "B-DS",
                "L-DS"
            ],
            [
                "Seven-Lexicon"
            ],
            [
                "ConvLexLSTM",
                "ConvLSTM",
                "CNN",
                "LSTM",
                "C-ConvLSTM",
                "SWAT",
                "EmoSVM"
            ],
            [
                "ConvLexLSTM"
            ],
            [
                "ConvLexLSTM",
                "F1"
            ]
        ],
        "n_sentence": 14.0,
        "table_id": "table_3",
        "paper_id": "D18-1147",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1148table_4",
        "description": "In Table 4 we remove the 30+ tweet requirement from the \u201cTweet to County\u201d and \u201cCounty\u201d methods and compare against the \u201cUser to County\u201d method (with the 30+ tweet requirement). Again we see the \u201cUser to County\u201d method outperforms all others in spite of the fact that the \u201cUser to County\u201d approach uses less data than both \u201call\u201d approaches, which contains 108 million more tweets.",
        "sentences": [
            "In Table 4 we remove the 30+ tweet requirement from the \u201cTweet to County\u201d and \u201cCounty\u201d methods and compare against the \u201cUser to County\u201d method (with the 30+ tweet requirement).",
            "Again we see the \u201cUser to County\u201d method outperforms all others in spite of the fact that the \u201cUser to County\u201d approach uses less data than both \u201call\u201d approaches, which contains 108 million more tweets."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "User to County",
                "Tweet to County (all)",
                "County (all)"
            ],
            [
                "User to County",
                "Tweet to County (all)",
                "County (all)"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_4",
        "paper_id": "D18-1148",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1148table_5",
        "description": "1% data. In Table 5 we repeat the above experiment on a 1% Twitter sample. Here we see that the \u201cUser to County\u201d method outperforms both the \u201cTweet to County\u201d and \u201cCounty\u201d methods (with all three tasks using the same number of tweets). When we compare the \u201cUser to County\u201d and \u201cCounty (all)\u201d methods we see the \u201cUser to county\u201d outperforming on two out of four tasks (Income and Life Satisfaction). Again, we note that the \u201cUser to County\u201d is using less data than the \u201cCounty (all)\u201d. While, across the board, the performance increase is not as substantial as in the 10% results, we see comparable performance between \u201cUser to County\u201d and \u201cCounty (all)\u201d methods despite the difference in the number of tweets.",
        "sentences": [
            "1% data.",
            "In Table 5 we repeat the above experiment on a 1% Twitter sample.",
            "Here we see that the \u201cUser to County\u201d method outperforms both the \u201cTweet to County\u201d and \u201cCounty\u201d methods (with all three tasks using the same number of tweets).",
            "When we compare the \u201cUser to County\u201d and \u201cCounty (all)\u201d methods we see the \u201cUser to county\u201d outperforming on two out of four tasks (Income and Life Satisfaction).",
            "Again, we note that the \u201cUser to County\u201d is using less data than the \u201cCounty (all)\u201d.",
            "While, across the board, the performance increase is not as substantial as in the 10% results, we see comparable performance between \u201cUser to County\u201d and \u201cCounty (all)\u201d methods despite the difference in the number of tweets."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "User to County",
                "Tweet to County",
                "County"
            ],
            [
                "User to County",
                "County (all)",
                "Income",
                "Life Satis."
            ],
            [
                "User to County",
                "County (all)"
            ],
            [
                "User to County",
                "County (all)"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_5",
        "paper_id": "D18-1148",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1152table_3",
        "description": "Results. Following Collins et al. (2017) and Melis et al. (2018), we compare models controlling for parameter budget. Table 3 summarizes language modeling perplexities on PTB test set. The middle block compares all models with two layers and 10M trainable parameters. RRNN(B) and RRNN(C) achieve roughly the same performance; interpolating both unigram and bigram features, RRNN(F) outperforms others by more than 2.9 test perplexity. For the three-layer and 24M setting (the bottom block), we observe similar trends, except that RRNN(C) slightly underperforms RRNN(B). Here RRNN(F) outperforms others by more than 2.1 perplexity. Using a max-plus semiring, RRNN(B)m+ underperforms RRNN(B) under both settings. Possible reasons could be the suboptimal design choice for computing input representations in the former (\u0e22\u0e075.2). Finally, most compared models outperform the LSTM baselines, whose numbers are taken from Lei et al. (2017b).",
        "sentences": [
            "Results.",
            "Following Collins et al. (2017) and Melis et al. (2018), we compare models controlling for parameter budget.",
            "Table 3 summarizes language modeling perplexities on PTB test set.",
            "The middle block compares all models with two layers and 10M trainable parameters.",
            "RRNN(B) and RRNN(C) achieve roughly the same performance; interpolating both unigram and bigram features, RRNN(F) outperforms others by more than 2.9 test perplexity.",
            "For the three-layer and 24M setting (the bottom block), we observe similar trends, except that RRNN(C) slightly underperforms RRNN(B).",
            "Here RRNN(F) outperforms others by more than 2.1 perplexity.",
            "Using a max-plus semiring, RRNN(B)m+ underperforms RRNN(B) under both settings.",
            "Possible reasons could be the suboptimal design choice for computing input representations in the former (\u0e22\u0e075.2).",
            "Finally, most compared models outperform the LSTM baselines, whose numbers are taken from Lei et al. (2017b)."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "# Params."
            ],
            null,
            [
                "RRNN(B)",
                "RRNN(B)m+",
                "RRNN(C)",
                "RRNN(F)",
                "l",
                "2",
                "# Params.",
                "10M"
            ],
            [
                "RRNN(B)",
                "RRNN(C)",
                "RRNN(F)",
                "RRNN(B)m+",
                "Test",
                "l",
                "2",
                "# Params.",
                "10M"
            ],
            [
                "RRNN(B)",
                "RRNN(B)m+",
                "RRNN(C)",
                "RRNN(F)",
                "l",
                "3",
                "# Params.",
                "24M"
            ],
            [
                "RRNN(F)",
                "RRNN(B)",
                "RRNN(B)m+",
                "RRNN(C)",
                "l",
                "3",
                "# Params.",
                "24M"
            ],
            [
                "RRNN(B)m+",
                "RRNN(B)",
                "l",
                "2",
                "3",
                "# Params.",
                "10M",
                "24M"
            ],
            null,
            [
                "RRNN(B)",
                "RRNN(B)m+",
                "RRNN(C)",
                "RRNN(F)",
                "LSTM"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_3",
        "paper_id": "D18-1152",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1156table_1",
        "description": "Table 1 shows the overall performance comparing to the above state-of-the-art methods with golden-standard entities. From the table, we can see that our JMEE framework achieves the best F1 scores for both trigger classification and argumentrelated subtasks among all the compared methods. There is a significant gain with the trigger classification and argument role labeling performances, which is 2% higher over the best-reported models. These results demonstrate the effectivenesses of our method to incorporate with the graph convolution and syntactic shortcut arcs.",
        "sentences": [
            "Table 1 shows the overall performance comparing to the above state-of-the-art methods with golden-standard entities.",
            "From the table, we can see that our JMEE framework achieves the best F1 scores for both trigger classification and argumentrelated subtasks among all the compared methods.",
            "There is a significant gain with the trigger classification and argument role labeling performances, which is 2% higher over the best-reported models.",
            "These results demonstrate the effectivenesses of our method to incorporate with the graph convolution and syntactic shortcut arcs."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "JMEE",
                "Cross-Event",
                "JointBeam",
                "DMCNN",
                "PSL",
                "JRNN",
                "dbRNN"
            ],
            [
                "JMEE",
                "F1",
                "Trigger Classification (%)",
                "Argument Identification (%)",
                "Argument Role (%)",
                "Cross-Event",
                "JointBeam",
                "DMCNN",
                "PSL",
                "JRNN",
                "dbRNN"
            ],
            [
                "JMEE",
                "Trigger Classification (%)",
                "Argument Role (%)",
                "dbRNN"
            ],
            [
                "JMEE"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_1",
        "paper_id": "D18-1156",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1158table_2",
        "description": "Table 2 shows the results. And we have the the following observations:. 1) Compared with LSTM+Softmax, LSTM-based collective ED methods (LSTM+CRF, LSTM+TLSTM, LSTM+HTLSTM, LSTM+HTLSTM+Bias) achieves a better performance. Surprisingly, the LSTM+HTLSTM+Bias yields a 14.9% improvement on the sentence contains multiple events over the LSTM+Softmax. It proves neural tagging schema is effective for ED task especially for the sentences contain multiple events. 2) The LSTM+TLSTM achieve better performances than LSTM+CRF. And the LSTM+HTLSTM achieve better performances than LSTM+TLSTM. The results prove the effectiveness of the TLSTM layer and HTLSTM layer. 3) Compared with LSTM+HTLSTM, the LSTM+HTLSTM+Bias gains a 0.9% improvement on all sentence. It demonstrates the effectiveness of our proposed bias objective function.",
        "sentences": [
            "Table 2 shows the results.",
            "And we have the the following observations:.",
            "1) Compared with LSTM+Softmax, LSTM-based collective ED methods (LSTM+CRF, LSTM+TLSTM, LSTM+HTLSTM, LSTM+HTLSTM+Bias) achieves a better performance.",
            "Surprisingly, the LSTM+HTLSTM+Bias yields a 14.9% improvement on the sentence contains multiple events over the LSTM+Softmax.",
            "It proves neural tagging schema is effective for ED task especially for the sentences contain multiple events.",
            "2) The LSTM+TLSTM achieve better performances than LSTM+CRF.",
            "And the LSTM+HTLSTM achieve better performances than LSTM+TLSTM.",
            "The results prove the effectiveness of the TLSTM layer and HTLSTM layer.",
            "3) Compared with LSTM+HTLSTM, the LSTM+HTLSTM+Bias gains a 0.9% improvement on all sentence.",
            "It demonstrates the effectiveness of our proposed bias objective function."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            2,
            1,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "LSTM+Softmax",
                "LSTM+CRF",
                "LSTM+TLSTM",
                "LSTM+HTLSTM",
                "LSTM+HTLSTM+Bias"
            ],
            [
                "LSTM+HTLSTM+Bias",
                "LSTM+Softmax",
                "1/N"
            ],
            [
                "LSTM+HTLSTM+Bias"
            ],
            [
                "LSTM+TLSTM",
                "LSTM+CRF"
            ],
            [
                "LSTM+HTLSTM",
                "LSTM+TLSTM"
            ],
            [
                "LSTM+HTLSTM",
                "LSTM+TLSTM"
            ],
            [
                "LSTM+HTLSTM",
                "LSTM+HTLSTM+Bias",
                "all"
            ],
            [
                "LSTM+HTLSTM"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_2",
        "paper_id": "D18-1158",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1159table_5",
        "description": "Table 5 presents the results for different combinations of valency relation subsets. We find that PP-attachment decisions are generally harder to make, compared with core and functional relations. Including them during training distracts other parsing objectives (compare Core + PP with only analyzing Core in \u0e22\u0e076)). However, they do permit improvements on precision for PP attachment by 3.30, especially with our proposed joint decoding. This demonstrates the usage of our algorithm outside the traditional notions of valency, it can be a general method for training parsers to focus on specific subsets of syntactic relations.",
        "sentences": [
            "Table 5 presents the results for different combinations of valency relation subsets.",
            "We find that PP-attachment decisions are generally harder to make, compared with core and functional relations.",
            "Including them during training distracts other parsing objectives (compare Core + PP with only analyzing Core in \u0e22\u0e076)).",
            "However, they do permit improvements on precision for PP attachment by 3.30, especially with our proposed joint decoding.",
            "This demonstrates the usage of our algorithm outside the traditional notions of valency, it can be a general method for training parsers to focus on specific subsets of syntactic relations."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            [
                "UAS",
                "LAS",
                "Core P",
                "Core R",
                "Core F",
                "Func. P",
                "Func. R",
                "Func. F",
                "PP P",
                "PP R",
                "PP F"
            ],
            [
                "PP P",
                "PP R",
                "PP F",
                "Core P",
                "Core R",
                "Core F",
                "Func. P",
                "Func. R",
                "Func. F"
            ],
            null,
            [
                "PP P",
                "Baseline",
                "PP MTL + Joint Decoding",
                "Core + PP MTL + Joint Decoding",
                "Core + Func. + PP MTL + Joint Decoding"
            ],
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_5",
        "paper_id": "D18-1159",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1167table_5",
        "description": "Human Evaluation on Usefulness of Video and Subtitle in Dataset:. To gain a better understandng of the roles of videos and subtitles in the our dataset, we perform a human study, asking different groups of workers to complete the QA task in settings while observing different sources (subsets) of information:. \u2022 Question only. \u2022 Video and Question. \u2022 Subtitle and Question. \u2022 Video, Subtitle, and Question. We made sure the workers that have written the questions did not participate in this study and that workers see only one of the above settings for answering each question. Human accuracy on our test set under these 4 settings are reported in Table 5. As expected, compared to human accuracy based only on question-answer pairs (Q), adding videos (V+Q), or subtitles (S+Q) significantly improves human performance. Adding both videos and subtitles (V+S+Q) brings the accuracy to 89.41%. This indicates that in order to answer the questions correctly, both visual and textual understanding are essential. We also observe that workers obtain 31.84% accuracy given questionanswer pairs only, which is higher than random guessing (20%). We ascribe this to people\u2019s prior knowledge about the shows. Note, timestamp annotations are not provided in these experiments.",
        "sentences": [
            "Human Evaluation on Usefulness of Video and Subtitle in Dataset:.",
            "To gain a better understandng of the roles of videos and subtitles in the our dataset, we perform a human study, asking different groups of workers to complete the QA task in settings while observing different sources (subsets) of information:.",
            "\u2022 Question only.",
            "\u2022 Video and Question.",
            "\u2022 Subtitle and Question.",
            "\u2022 Video, Subtitle, and Question.",
            "We made sure the workers that have written the questions did not participate in this study and that workers see only one of the above settings for answering each question.",
            "Human accuracy on our test set under these 4 settings are reported in Table 5.",
            "As expected, compared to human accuracy based only on question-answer pairs (Q), adding videos (V+Q), or subtitles (S+Q) significantly improves human performance.",
            "Adding both videos and subtitles (V+S+Q) brings the accuracy to 89.41%.",
            "This indicates that in order to answer the questions correctly, both visual and textual understanding are essential.",
            "We also observe that workers obtain 31.84% accuracy given questionanswer pairs only, which is higher than random guessing (20%).",
            "We ascribe this to people\u2019s prior knowledge about the shows.",
            "Note, timestamp annotations are not provided in these experiments."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            1,
            1,
            1,
            2,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "Question"
            ],
            [
                "Video and Question"
            ],
            [
                "Subtitle and Question"
            ],
            [
                "Video Subtitle and Question"
            ],
            null,
            null,
            [
                "Question",
                "Video and Question",
                "Subtitle and Question",
                "Human accuracy on test."
            ],
            [
                "Video Subtitle and Question",
                "Human accuracy on test."
            ],
            [
                "Video Subtitle and Question"
            ],
            [
                "Question",
                "Human accuracy on test."
            ],
            null,
            null
        ],
        "n_sentence": 14.0,
        "table_id": "table_5",
        "paper_id": "D18-1167",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1168table_6",
        "description": "Results: TEMPO - HL. Table 6 compares performance on TEMPO - HL. We compare our best-performing model from training on the TEMPOTL (strongly supervised MLLC and conTEF) to prior work (MCN and TALL) and to MLLC with global and before/after context. Performance on TEMPO-HL is considerably lower than TEMPOTL suggesting that TEMPO-HL is harder than TEMPO-TL. On TEMPO - HL, we observe similar trends as on TEMPO-TL. When considering all sentence types, MLLC has the best performance across all metrics. In particular, our model has the strongest performance for all sentence types considering the mIoU metric. In addition to performing better on temporal words, our model also performs better on the original DiDeMo dataset. As was seen in TEMPO-TL, including before/after context performs better than our model trained with global context for both \u201cbefore\u201d and \u201cafter\u201d words. The final row of Table 6 shows an upper bound in which the ground truth context is used at test time instead of the latent context. We note that results improve for \u201cbefore\u201d, \u201cafter\u201d, and \u201cthen\u201d, suggesting that learning to better localize context will improve results for these sentence types.",
        "sentences": [
            "Results: TEMPO - HL.",
            "Table 6 compares performance on TEMPO - HL.",
            "We compare our best-performing model from training on the TEMPOTL (strongly supervised MLLC and conTEF) to prior work (MCN and TALL) and to MLLC with global and before/after context.",
            "Performance on TEMPO-HL is considerably lower than TEMPOTL suggesting that TEMPO-HL is harder than TEMPO-TL.",
            "On TEMPO - HL, we observe similar trends as on TEMPO-TL.",
            "When considering all sentence types, MLLC has the best performance across all metrics.",
            "In particular, our model has the strongest performance for all sentence types considering the mIoU metric.",
            "In addition to performing better on temporal words, our model also performs better on the original DiDeMo dataset.",
            "As was seen in TEMPO-TL, including before/after context performs better than our model trained with global context for both \u201cbefore\u201d and \u201cafter\u201d words.",
            "The final row of Table 6 shows an upper bound in which the ground truth context is used at test time instead of the latent context.",
            "We note that results improve for \u201cbefore\u201d, \u201cafter\u201d, and \u201cthen\u201d, suggesting that learning to better localize context will improve results for these sentence types."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "MLLC (Ours)",
                "MCN",
                "TALL + TEF",
                "MLLC - Global",
                "MLLC - B/A"
            ],
            [
                "TEMPO - Human Language (HL)"
            ],
            [
                "TEMPO - Human Language (HL)"
            ],
            [
                "MLLC - Global",
                "MLLC - B/A",
                "MLLC (Ours)"
            ],
            [
                "MLLC (Ours)",
                "mIoU"
            ],
            [
                "MLLC (Ours)",
                "DiDeMo"
            ],
            null,
            [
                "MLLC (Ours) Context Sup. Test"
            ],
            [
                "MLLC (Ours) Context Sup. Test",
                "Before",
                "After",
                "Then"
            ]
        ],
        "n_sentence": 11.0,
        "table_id": "table_6",
        "paper_id": "D18-1168",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1173table_2",
        "description": "Named Entity Recognition. Table 2 shows the results of domain specific named entity recognition. Used for pre-training embeddings, Mem2Vec achieves higher F1-score than all the baselines. It first surpasses CRE and DAREP that only bring slight improvements over Word2Vec. CRE and DAREP are both methods which relies on words with cooccurance patterns in source and target domain as the pivots for cross-domain transfer. This indicates the advantage of Mem2Vec over the traditional word frequency based methods in fast mapping cases where word cooccurrence pattern is not clear.",
        "sentences": [
            "Named Entity Recognition.",
            "Table 2 shows the results of domain specific named entity recognition.",
            "Used for pre-training embeddings, Mem2Vec achieves higher F1-score than all the baselines.",
            "It first surpasses CRE and DAREP that only bring slight improvements over Word2Vec.",
            "CRE and DAREP are both methods which relies on words with cooccurance patterns in source and target domain as the pivots for cross-domain transfer.",
            "This indicates the advantage of Mem2Vec over the traditional word frequency based methods in fast mapping cases where word cooccurrence pattern is not clear."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "Mem2Vec",
                "F1",
                "Word2Vec",
                "GloVe",
                "N2V",
                "SUM",
                "DAREP",
                "CRE"
            ],
            [
                "DAREP",
                "CRE",
                "Word2Vec"
            ],
            [
                "DAREP",
                "CRE"
            ],
            [
                "Mem2Vec"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_2",
        "paper_id": "D18-1173",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1176table_2",
        "description": "5.2 Results. Table 2 shows a similar pattern as we observed the naive baseline outperforms the with NLI:. the naive baseline outperforms the single-embedding encoders;. the DME methods outperform the naive baseline, with the contextualized version appearing to work best. Finally, we experiment with replacing \u00cf\u2020 in Eq. 1 and 2 with a sigmoid gate instead of a softmax, and observe improved performance on this task, outperforming the comparable models listed in the table. These results further strengthen the point that having multiple different embeddings helps, and that we can learn to combine those different embeddings efficiently, in interpretable ways.",
        "sentences": [
            "5.2 Results.",
            "Table 2 shows a similar pattern as we observed the naive baseline outperforms the with NLI:.",
            "the naive baseline outperforms the single-embedding encoders;.",
            "the DME methods outperform the naive baseline, with the contextualized version appearing to work best.",
            "Finally, we experiment with replacing \u00cf\u2020 in Eq. 1 and 2 with a sigmoid gate instead of a softmax, and observe improved performance on this task, outperforming the comparable models listed in the table.",
            "These results further strengthen the point that having multiple different embeddings helps, and that we can learn to combine those different embeddings efficiently, in interpretable ways."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Naive baseline (5.4M)"
            ],
            [
                "Naive baseline (5.4M)",
                "GloVe BiLSTM-Max (4.1M)",
                "FastText BiLSTM-Max (4.1M)"
            ],
            [
                "DME (4.1M)",
                "Naive baseline (5.4M)"
            ],
            [
                "CDME*-Sigmoid (4.6M)",
                "CDME*-Softmax (4.6M)",
                "Const. Tree LSTM (Tai et al. 2015)",
                "DMN (Kumar et al. 2016)",
                "DCG (Looks et al. 2017)",
                "NSE (Munkhdalai and Yu 2017)",
                "GloVe BiLSTM-Max (4.1M)",
                "FastText BiLSTM-Max (4.1M)",
                "Naive baseline (5.4M)",
                "Unweighted DME (4.1M)",
                "DME (4.1M)",
                "CDME (4.1M)"
            ],
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_2",
        "paper_id": "D18-1176",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1176table_3",
        "description": "6.2 Results. Table 3 shows the results, comparing against VSE++. First, note that the ImageNet-only embeddings don\u2019t work as well as the FastText ones, which is most likely due to poorer coverage. We observe that DME outperforms naive and FastText-only, and outperforms VSE++ by a large margin. These findings confirm the intuition that knowing what things look like (i.e., having a wordlevel visual representation) improves performance in visual retrieval tasks (i.e., where we need to find relevant images for phrases or sentences) \u2014 something that sounds obvious but has not really been explored before, to our knowledge. This showcases DME\u2019s usefulness for fusing embeddings in multi-modal tasks.",
        "sentences": [
            "6.2 Results.",
            "Table 3 shows the results, comparing against VSE++.",
            "First, note that the ImageNet-only embeddings don\u2019t work as well as the FastText ones, which is most likely due to poorer coverage.",
            "We observe that DME outperforms naive and FastText-only, and outperforms VSE++ by a large margin.",
            "These findings confirm the intuition that knowing what things look like (i.e., having a wordlevel visual representation) improves performance in visual retrieval tasks (i.e., where we need to find relevant images for phrases or sentences) \u2014 something that sounds obvious but has not really been explored before, to our knowledge.",
            "This showcases DME\u2019s usefulness for fusing embeddings in multi-modal tasks."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "VSE++"
            ],
            [
                "FastText (15M)",
                "ImageNet (29M)"
            ],
            [
                "DME (15M)",
                "FastText (15M)",
                "Naive (32M)"
            ],
            null,
            [
                "DME (15M)"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_3",
        "paper_id": "D18-1176",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1177table_2",
        "description": "4.2.1 Main results. The results across different datasets are shown in Table 2. We perform evaluations under two settings: by considering (i) word similarity between visual words only and (ii) between all words (column 100% in Table 2). For the models CNN, VAE and their concatenation with SGNS embeddings, the latter setting is not applicable. The two last rows correspond to the multimodal embeddings inferred from our model. In particular, PIXIE+ (resp. PIXIE\u2295) represents the multimodal embeddings built using Eq. (10) (resp. Eq. (11)). Overall, we note that PIXIE\u2295 offers the best performance in almost all situations. This provides strong empirical support for the proposed model. Below, we discuss the above results in more depth to better understand them and characterize the circumstances in which our model performs better. How relevant is our formulation?. Except PIXIE and V-SGNS, most of the multimodal competing methods rely on independently pre-computed linguistic embeddings. As Table 2 shows, PIXIE and V-SGNS are often the best performing multimodal models, which provides empirical evidence that accounting for perceptual information while learning word embeddings from text is beneficial. Moreover, the superior performance of PIXIE\u2295 over V-SGNS suggests that our model does a better job at combining perception and language to learn word representations. Joint learning is beneficial. PIXIE\u2295 outperforms VAE\u2295SGNS in almost all cases, which demonstrates the importance of joint learning. Where does our approach perform better?. On datasets that focus on semantic/taxonomic similarity, our approach dominates all other methods. On datasets focusing on general relatedness, our approach obtains mixed results. While dominating other approaches on MEN, it tends to perform worst than SGNS on MTurk and REL (under the 100% setting). One possible explanation is that general relatedness tends to focus more on \u201cextrapolating\u201d  from one word to another word (such as SWAN is related to LAKE), while our approach better models more concrete relationships (such as SWAN is related to GOOSE). The low performance of CNN and VAE confirms this hypothesis. On the VISSIM dataset focusing on visual similarity, both CNN\u2295SGNS and VAE\u2295SGNS perform the best, strongly suggesting that visual and linguistic data are complementary. Our approach comes very close to these two methods. Note that our learning objective is to jointly explain visual features and word-context co-occurrences.",
        "sentences": [
            "4.2.1 Main results.",
            "The results across different datasets are shown in Table 2.",
            "We perform evaluations under two settings: by considering (i) word similarity between visual words only and (ii) between all words (column 100% in Table 2).",
            "For the models CNN, VAE and their concatenation with SGNS embeddings, the latter setting is not applicable.",
            "The two last rows correspond to the multimodal embeddings inferred from our model.",
            "In particular, PIXIE+ (resp. PIXIE\u2295) represents the multimodal embeddings built using Eq. (10) (resp. Eq. (11)).",
            "Overall, we note that PIXIE\u2295 offers the best performance in almost all situations.",
            "This provides strong empirical support for the proposed model.",
            "Below, we discuss the above results in more depth to better understand them and characterize the circumstances in which our model performs better.",
            "How relevant is our formulation?.",
            "Except PIXIE and V-SGNS, most of the multimodal competing methods rely on independently pre-computed linguistic embeddings.",
            "As Table 2 shows, PIXIE and V-SGNS are often the best performing multimodal models, which provides empirical evidence that accounting for perceptual information while learning word embeddings from text is beneficial.",
            "Moreover, the superior performance of PIXIE\u2295 over V-SGNS suggests that our model does a better job at combining perception and language to learn word representations.",
            "Joint learning is beneficial.",
            "PIXIE\u2295 outperforms VAE\u2295SGNS in almost all cases, which demonstrates the importance of joint learning.",
            "Where does our approach perform better?.",
            "On datasets that focus on semantic/taxonomic similarity, our approach dominates all other methods.",
            "On datasets focusing on general relatedness, our approach obtains mixed results.",
            "While dominating other approaches on MEN, it tends to perform worst than SGNS on MTurk and REL (under the 100% setting).",
            "One possible explanation is that general relatedness tends to focus more on \u201cextrapolating\u201d  from one word to another word (such as SWAN is related to LAKE), while our approach better models more concrete relationships (such as SWAN is related to GOOSE).",
            "The low performance of CNN and VAE confirms this hypothesis.",
            "On the VISSIM dataset focusing on visual similarity, both CNN\u2295SGNS and VAE\u2295SGNS perform the best, strongly suggesting that visual and linguistic data are complementary.",
            "Our approach comes very close to these two methods.",
            "Note that our learning objective is to jointly explain visual features and word-context co-occurrences."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            1,
            2,
            1,
            2,
            2,
            2,
            2,
            1,
            1,
            2,
            1,
            2,
            1,
            1,
            1,
            2,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "SEMSIM",
                "SimLex",
                "SIM",
                "EN-RG",
                "EN-MC",
                "MEN",
                "REL",
                "MTurk",
                "VISSIM",
                "WORDSIM"
            ],
            [
                "98%",
                "39%",
                "44%",
                "72%",
                "73%",
                "54%",
                "53%",
                "26%",
                "100%"
            ],
            [
                "CNN",
                "VAE",
                "CNN\u2295SGNS",
                "VAE\u2295SGNS"
            ],
            [
                "PIXIE+",
                "PIXIE\u2295"
            ],
            [
                "PIXIE+",
                "PIXIE\u2295"
            ],
            [
                "PIXIE\u2295"
            ],
            [
                "PIXIE\u2295"
            ],
            null,
            null,
            [
                "CNN\u2295SGNS",
                "VAE\u2295SGNS",
                "IV-SGNS(LINEAR)",
                "IV-SGNS(NONLINEAR)"
            ],
            [
                "PIXIE+",
                "PIXIE\u2295",
                "V-SGNS"
            ],
            [
                "PIXIE\u2295",
                "V-SGNS"
            ],
            null,
            [
                "PIXIE\u2295",
                "VAE\u2295SGNS",
                "SEMSIM",
                "SimLex",
                "SIM",
                "EN-RG",
                "EN-MC",
                "MEN",
                "REL",
                "VISSIM",
                "WORDSIM"
            ],
            null,
            [
                "Semantic/taxonomic similarity",
                "PIXIE+",
                "PIXIE\u2295"
            ],
            [
                "General relatedness",
                "PIXIE+",
                "PIXIE\u2295"
            ],
            [
                "MEN",
                "PIXIE+",
                "PIXIE\u2295",
                "REL",
                "MTurk",
                "SGNS"
            ],
            [
                "General relatedness"
            ],
            [
                "CNN",
                "VAE",
                "General relatedness"
            ],
            [
                "Visual similarity",
                "VISSIM",
                "CNN\u2295SGNS",
                "VAE\u2295SGNS"
            ],
            [
                "PIXIE+",
                "PIXIE\u2295",
                "CNN\u2295SGNS",
                "VAE\u2295SGNS"
            ],
            null
        ],
        "n_sentence": 24.0,
        "table_id": "table_2",
        "paper_id": "D18-1177",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1177table_6",
        "description": "Results. Table 6 summarizes the results. The evaluation metrics are accuracies at top-K (K=1, 5, or 10) retrieved sentences or images. Our model consistently outperforms SGNS and other competing multimodal methods, which provides additional support for the benefits of our approach.",
        "sentences": [
            "Results.",
            "Table 6 summarizes the results.",
            "The evaluation metrics are accuracies at top-K (K=1, 5, or 10) retrieved sentences or images.",
            "Our model consistently outperforms SGNS and other competing multimodal methods, which provides additional support for the benefits of our approach."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "K=1",
                "K=5",
                "K=10"
            ],
            [
                "PIXIE+",
                "PIXIE\u2295",
                "SGNS",
                "V-SGNS",
                "IV-SGNS (LINEAR)"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_6",
        "paper_id": "D18-1177",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1182table_3",
        "description": "5.2 Word Embeddings Solvers. Table 3 shows the results of embedding-based solvers on the Anomia and crowdsourced datasets, using several different pre-trained embedding maps. We find the best performance on ANOMIACOMMON and ANOMIAPROPER using the word2vec vectors trained on 100 billion tokens from the Google News corpus. Based on this tuning, we fixed the value of K to 5, and repeated the experiments from Section 5 (see the first row in Table 3). ELMo sense vectors clearly outperform all previous baselines on all Odd-Man-Out datasets. This improved performance can be attributed both to ELMo\u00e2\u20ac\u2122s better ability to capture context, as well as to the finer sense representation, as opposed to the single representation per word in most of the other baselines.",
        "sentences": [
            "5.2 Word Embeddings Solvers.",
            "Table 3 shows the results of embedding-based solvers on the Anomia and crowdsourced datasets, using several different pre-trained embedding maps.",
            "We find the best performance on ANOMIACOMMON and ANOMIAPROPER using the word2vec vectors trained on 100 billion tokens from the Google News corpus.",
            "Based on this tuning, we fixed the value of K to 5, and repeated the experiments from Section 5 (see the first row in Table 3).",
            "ELMo sense vectors clearly outperform all previous baselines on all Odd-Man-Out datasets.",
            "This improved performance can be attributed both to ELMo\u00e2\u20ac\u2122s better ability to capture context, as well as to the finer sense representation, as opposed to the single representation per word in most of the other baselines."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "w2v.googlenews",
                "glove.commoncrawl2",
                "glove.commoncrawl1",
                "glove.wikipedia",
                "Neelakantan",
                "w2v.freebase",
                "WordNet"
            ],
            [
                "w2v.googlenews",
                "100B",
                "AnomiaCommon",
                "AnomiaProper"
            ],
            [
                "ELMo clusters (K = 5)"
            ],
            [
                "ELMo clusters (K = 5)",
                "w2v.googlenews",
                "glove.commoncrawl2",
                "glove.commoncrawl1",
                "glove.wikipedia",
                "Neelakantan",
                "w2v.freebase",
                "WordNet"
            ],
            [
                "ELMo clusters (K = 5)"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_3",
        "paper_id": "D18-1182",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1183table_3",
        "description": "5.2.2 Results. Table 3 shows the performance of the systems. The results are reported with the precision (P), recall (R), and their harmonic mean F1 score (F1). The two feature based methods perform differently. Baseline1 performs poorly. The reason may be that the classification depends on the performance of component extraction, while even our best component extractor performs far from perfect, which brings error propagation. In addition, classifying with component related features only ignores much context, which further decreases the performance. Baseline2 considers context windows and outperforms baseline1 largely. This confirms our intuition that context information implies the semantic of simile expression. Furthermore, we have other observations:. (1) neural network based approaches largely outperform feature-based classifiers;. (2) multitask learning approaches outperform every single task approach and other baselines. Both the component extraction and the language modeling task contribute for simile sentence classification. Component extraction improves the precision and language modeling improves both the precision and the recall. Combining them together can achieve the best performance. The improvement of F1 score can reach to 3.3% compared with the best single task model.",
        "sentences": [
            "5.2.2 Results.",
            "Table 3 shows the performance of the systems.",
            "The results are reported with the precision (P), recall (R), and their harmonic mean F1 score (F1).",
            "The two feature based methods perform differently.",
            "Baseline1 performs poorly.",
            "The reason may be that the classification depends on the performance of component extraction, while even our best component extractor performs far from perfect, which brings error propagation.",
            "In addition, classifying with component related features only ignores much context, which further decreases the performance.",
            "Baseline2 considers context windows and outperforms baseline1 largely.",
            "This confirms our intuition that context information implies the semantic of simile expression.",
            "Furthermore, we have other observations:.",
            "(1) neural network based approaches largely outperform feature-based classifiers;.",
            "(2) multitask learning approaches outperform every single task approach and other baselines.",
            "Both the component extraction and the language modeling task contribute for simile sentence classification.",
            "Component extraction improves the precision and language modeling improves both the precision and the recall.",
            "Combining them together can achieve the best performance.",
            "The improvement of F1 score can reach to 3.3% compared with the best single task model."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            2,
            2,
            1,
            2,
            1,
            1,
            1,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "P",
                "R",
                "F1"
            ],
            [
                "Baseline1",
                "Baseline2"
            ],
            [
                "Baseline1"
            ],
            [
                "Baseline1"
            ],
            [
                "Baseline1"
            ],
            [
                "Baseline2",
                "Baseline1"
            ],
            null,
            null,
            [
                "Baseline1",
                "Baseline2",
                "Singletask (SC)",
                "Multitask (SC+CE)",
                "Multitask (SC+LM)",
                "Multitask (SC+CE+LM)"
            ],
            [
                "Multitask (SC+CE)",
                "Multitask (SC+LM)",
                "Singletask (SC)",
                "Baseline1",
                "Baseline2"
            ],
            [
                "Multitask (SC+CE)",
                "Multitask (SC+LM)"
            ],
            [
                "Multitask (SC+CE)",
                "Multitask (SC+LM)",
                "P",
                "R"
            ],
            [
                "Multitask (SC+CE+LM)"
            ],
            [
                "Multitask (SC+CE+LM)",
                "Singletask (SC)",
                "F1"
            ]
        ],
        "n_sentence": 16.0,
        "table_id": "table_3",
        "paper_id": "D18-1183",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1183table_4",
        "description": "Table 4 shows the results of various systems and settings on two test sets. The first dataset consists of all manually labeled simile sentences in the test set and the second dataset is the whole test set. We want to compare how component extraction systems work when they know whether a sentence contains a simile or not. We report and discuss the results from the following aspects. The effect of simile sentence classification. First, we can compare the results in the middle column and the rightmost column in Table 4. It is clear that the component extraction systems work much better when they know whether a sentence contains a simile or not. Second, we can see that both pipelines (the feature-based and the neural network based) achieve a better performance compared with extracting components directly using either the CRF model or the neural single task model. Third, Multitask(CE+SC) doesn\u2019t bring significant improvements compared with the single task neural model. These observations indicate that simile sentence classification is suitable to be a pre-processing for simile component classification. It is necessary to further study how to use high level predictions (sentence classification) to learn better representations for consistently improving local predictions (simile component extraction). Rule based, feature-based and neural models. We can see that even on gold simile sentences, the rule based method doesn\u2019t work well. The poor performance of the rule based approach is due to the following reasons. First, the rule-based method is difficult to deal with complex sentence structures. It often fails when there are multiple subordinate clauses. Second, the comparator \u201c\u50cf\u201d in Chinese has multiple syntactic roles, sometimes is used as a verb, sometimes is used as a preposition. Third, the accuracy of Chinese dependency parser still has room to be improved. The CRF method performs significantly better, because it considers more contextual signals. Our neural single task model achieves large improvements on both datasets. This verifies the effectiveness of the end-to-end approach. Neural models can see a long range of context and learn features automatically. The word embeddings learned on external resources implicitly have semantic domain information, which is not only useful for generalization but also important for figurative language processing. The effect of language modeling. Surprisingly, using language modeling as an auxiliary task is very useful, especially when dealing with noisy sentences. It gains a 1.3% F1 improvement on the gold simile sentences due to the improvement on the precision and a 3% F1 improvement on the whole test set due to a large improvement on the recall. Generally, language modeling may help learn better task specific representations, especially when data size is limited (Rei, 2017). Another reason may be that language modeling aims to make local predictions, the same as simile component extraction. As shown in Table 4, the optimized pipeline performs better than the strongest multitask learning setting. However, in all settings, the precision scores are lower compared with the recall scores. This indicates that compared with identifying surface patterns, distinguishing metaphorical from literal meanings is much harder and more external knowledge should be incorporated.",
        "sentences": [
            "Table 4 shows the results of various systems and settings on two test sets.",
            "The first dataset consists of all manually labeled simile sentences in the test set and the second dataset is the whole test set.",
            "We want to compare how component extraction systems work when they know whether a sentence contains a simile or not.",
            "We report and discuss the results from the following aspects.",
            "The effect of simile sentence classification.",
            "First, we can compare the results in the middle column and the rightmost column in Table 4.",
            "It is clear that the component extraction systems work much better when they know whether a sentence contains a simile or not.",
            "Second, we can see that both pipelines (the feature-based and the neural network based) achieve a better performance compared with extracting components directly using either the CRF model or the neural single task model.",
            "Third, Multitask(CE+SC) doesn\u2019t bring significant improvements compared with the single task neural model.",
            "These observations indicate that simile sentence classification is suitable to be a pre-processing for simile component classification.",
            "It is necessary to further study how to use high level predictions (sentence classification) to learn better representations for consistently improving local predictions (simile component extraction).",
            "Rule based, feature-based and neural models.",
            "We can see that even on gold simile sentences, the rule based method doesn\u2019t work well.",
            "The poor performance of the rule based approach is due to the following reasons.",
            "First, the rule-based method is difficult to deal with complex sentence structures.",
            "It often fails when there are multiple subordinate clauses.",
            "Second, the comparator \u201c\u50cf\u201d in Chinese has multiple syntactic roles, sometimes is used as a verb, sometimes is used as a preposition.",
            "Third, the accuracy of Chinese dependency parser still has room to be improved.",
            "The CRF method performs significantly better, because it considers more contextual signals.",
            "Our neural single task model achieves large improvements on both datasets.",
            "This verifies the effectiveness of the end-to-end approach.",
            "Neural models can see a long range of context and learn features automatically.",
            "The word embeddings learned on external resources implicitly have semantic domain information, which is not only useful for generalization but also important for figurative language processing.",
            "The effect of language modeling.",
            "Surprisingly, using language modeling as an auxiliary task is very useful, especially when dealing with noisy sentences.",
            "It gains a 1.3% F1 improvement on the gold simile sentences due to the improvement on the precision and a 3% F1 improvement on the whole test set due to a large improvement on the recall.",
            "Generally, language modeling may help learn better task specific representations, especially when data size is limited (Rei, 2017).",
            "Another reason may be that language modeling aims to make local predictions, the same as simile component extraction.",
            "As shown in Table 4, the optimized pipeline performs better than the strongest multitask learning setting.",
            "However, in all settings, the precision scores are lower compared with the recall scores.",
            "This indicates that compared with identifying surface patterns, distinguishing metaphorical from literal meanings is much harder and more external knowledge should be incorporated."
        ],
        "class_sentence": [
            1,
            1,
            2,
            0,
            2,
            1,
            2,
            1,
            1,
            2,
            2,
            2,
            1,
            2,
            2,
            2,
            2,
            2,
            1,
            1,
            2,
            2,
            2,
            2,
            2,
            1,
            2,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "Rule based",
                "CRF",
                "Singletask (CE)",
                "RandomForest \u2192 CRF",
                "SingleSC \u2192 SingleCE",
                "Multitask (CE+SC)",
                "Multitask (CE+LM)",
                "Multitask (CE+SC+LM)",
                "Optimized pipeline",
                "Gold simile sentences",
                "Whole test set"
            ],
            [
                "Gold simile sentences",
                "Whole test set"
            ],
            [
                "Rule based",
                "CRF",
                "Singletask (CE)",
                "RandomForest \u2192 CRF",
                "SingleSC \u2192 SingleCE",
                "Multitask (CE+SC)",
                "Multitask (CE+LM)",
                "Multitask (CE+SC+LM)",
                "Optimized pipeline"
            ],
            null,
            null,
            [
                "Gold simile sentences",
                "Whole test set"
            ],
            [
                "Gold simile sentences",
                "Whole test set"
            ],
            [
                "CRF",
                "Singletask (CE)",
                "RandomForest \u2192 CRF",
                "SingleSC \u2192 SingleCE"
            ],
            [
                "Singletask (CE)",
                "Multitask (CE+SC)",
                "Multitask (CE+LM)",
                "Multitask (CE+SC+LM)"
            ],
            null,
            null,
            null,
            [
                "Gold simile sentences",
                "Rule based"
            ],
            [
                "Rule based"
            ],
            [
                "Rule based"
            ],
            [
                "Rule based"
            ],
            [
                "Rule based"
            ],
            [
                "Rule based"
            ],
            [
                "CRF"
            ],
            [
                "Singletask (CE)",
                "Gold simile sentences",
                "Whole test set"
            ],
            null,
            null,
            null,
            null,
            [
                "Multitask (CE+LM)"
            ],
            [
                "Multitask (CE+LM)",
                "Singletask (CE)",
                "F1",
                "P",
                "R",
                "Gold simile sentences",
                "Whole test set"
            ],
            [
                "Multitask (CE+LM)"
            ],
            [
                "Multitask (CE+LM)"
            ],
            [
                "Optimized pipeline",
                "Multitask (CE+LM)"
            ],
            [
                "Singletask (CE)",
                "RandomForest \u2192 CRF",
                "SingleSC \u2192 SingleCE",
                "Multitask (CE+SC)",
                "Multitask (CE+LM)",
                "Multitask (CE+SC+LM)",
                "Optimized pipeline",
                "P",
                "R"
            ],
            null
        ],
        "n_sentence": 31.0,
        "table_id": "table_4",
        "paper_id": "D18-1183",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1184table_1",
        "description": "Experimental results are listed in Table 1. We measure performance by the mean average precision (MAP) and mean reciprocal rank (MRR) using the standard TREC evaluation script. In the first block of Table 1, we compare our model and variants thereof against several baselines. The first baseline is the Word-level Decomposable Attention model strengthened with a bidirectional LSTM for obtaining a contextualized representation for each word. The second baseline is a Simple Span Alignment model; we use an MLP layer over the LSTM outputs to calculate the unnormalized scores and replace the inside-outside algorithm with a simple softmax function to obtain the probability distribution over all candidate spans. We also introduce a pipelined baseline where we extract constituents from trees parsed by the CoreNLP (Manning et al. 2014) constituency parser, and use the Simple Span Alignment model to only align these constituents. As shown in Table 1, we use two variants of the Structured Alignment model, since the structure of the question and the answer sentence may be different;the first model shares parameters across the question and the answer for computing the structures, while the second one uses separate parameters. We view the sentence selection task as a binary classification problem and the final ranking is based on the predicted probability of the sentence containing the correct answer (positive label). We apply dropout to the output of the BiLSTM with dropout ratio set to 0.2. All parameters (including word embeddings) are updated with AdaGrad (Duchi et al. 2011), and the learning rate is set to 0.05. Table 1 (second block) also reports the performance of various comparison systems and stateof-the-art models. As can be seen, on both MAP and MRR metrics, structured alignment models perform better than the decomposable attention model, showing that structural bias is helpful for matching a question to the correct answer sentence. We also observe that using separate parameters achieves higher scores on both metrics. The simple span alignment model obtains results similar to the decomposable attention model, suggesting that the shallow softmax distribution is ineffective for capturing structural information and may even introduce redundant noise. The pipelined model with an external parser also slightly improves upon the baseline, but still cannot outperform the end-to-end trained structured alignment model which achieves results comparable with several strong baselines with fewer parameters. As mentioned earlier, our model could be used as a plug-in component for other more complex models, and may boost their performance by modeling the latent structures. At the same time, the structured alignment can provide better interpretability\nfor sentence matching tasks, which is a defect of\nmost neural models.",
        "sentences": [
            "Experimental results are listed in Table 1.",
            "We measure performance by the mean average precision (MAP) and mean reciprocal rank (MRR) using the standard TREC evaluation script.",
            "In the first block of Table 1, we compare our model and variants thereof against several baselines.",
            "The first baseline is the Word-level Decomposable Attention model strengthened with a bidirectional LSTM for obtaining a contextualized representation for each word.",
            "The second baseline is a Simple Span Alignment model; we use an MLP layer over the LSTM outputs to calculate the unnormalized scores and replace the inside-outside algorithm with a simple softmax function to obtain the probability distribution over all candidate spans.",
            "We also introduce a pipelined baseline where we extract constituents from trees parsed by the CoreNLP (Manning et al. 2014) constituency parser, and use the Simple Span Alignment model to only align these constituents.",
            "As shown in Table 1, we use two variants of the Structured Alignment model, since the structure of the question and the answer sentence may be different;the first model shares parameters across the question and the answer for computing the structures, while the second one uses separate parameters.",
            "We view the sentence selection task as a binary classification problem and the final ranking is based on the predicted probability of the sentence containing the correct answer (positive label).",
            "We apply dropout to the output of the BiLSTM with dropout ratio set to 0.2.",
            "All parameters (including word embeddings) are updated with AdaGrad (Duchi et al. 2011), and the learning rate is set to 0.05.",
            "Table 1 (second block) also reports the performance of various comparison systems and stateof-the-art models.",
            "As can be seen, on both MAP and MRR metrics, structured alignment models perform better than the decomposable attention model, showing that structural bias is helpful for matching a question to the correct answer sentence.",
            "We also observe that using separate parameters achieves higher scores on both metrics.",
            "The simple span alignment model obtains results similar to the decomposable attention model, suggesting that the shallow softmax distribution is ineffective for capturing structural information and may even introduce redundant noise.",
            "The pipelined model with an external parser also slightly improves upon the baseline, but still cannot outperform the end-to-end trained structured alignment model which achieves results comparable with several strong baselines with fewer parameters.",
            "As mentioned earlier, our model could be used as a plug-in component for other more complex models, and may boost their performance by modeling the latent structures.",
            "At the same time, the structured alignment can provide better interpretability\nfor sentence matching tasks, which is a defect of\nmost neural models."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            2,
            2,
            1,
            2,
            2,
            2,
            1,
            1,
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "MAP",
                "MRR"
            ],
            [
                "Word-level Attention",
                "Simple Span Alignment",
                "Simple Span Alignment + External Parser",
                "Structured Alignment (Shared Parameters)",
                "Structured Alignment (Separated Parameters)"
            ],
            [
                "Word-level Attention"
            ],
            [
                "Simple Span Alignment"
            ],
            [
                "Simple Span Alignment + External Parser"
            ],
            [
                "Structured Alignment (Shared Parameters)",
                "Structured Alignment (Separated Parameters)"
            ],
            [
                "Structured Alignment (Shared Parameters)",
                "Structured Alignment (Separated Parameters)"
            ],
            [
                "Structured Alignment (Shared Parameters)",
                "Structured Alignment (Separated Parameters)"
            ],
            [
                "Structured Alignment (Shared Parameters)",
                "Structured Alignment (Separated Parameters)"
            ],
            [
                "QA-LSTM (Tan et al. 2016b)",
                "Attentive Pooling Network (Santos et al. 2016)",
                "Pairwise Word Interaction (He and Lin 2016)",
                "Lexical Decomposition and Composition (Wang et al. 2016)",
                "Noise-Contrastive Estimation (Rao et al. 2016)",
                "BiMPM (Wang et al. 2017b)"
            ],
            [
                "MAP",
                "MRR",
                "Structured Alignment (Shared Parameters)",
                "Structured Alignment (Separated Parameters)"
            ],
            [
                "Structured Alignment (Separated Parameters)"
            ],
            [
                "Word-level Attention",
                "Simple Span Alignment"
            ],
            [
                "Simple Span Alignment + External Parser",
                "Word-level Attention",
                "Structured Alignment (Shared Parameters)"
            ],
            [
                "Structured Alignment (Shared Parameters)",
                "Structured Alignment (Separated Parameters)"
            ],
            [
                "Structured Alignment (Shared Parameters)",
                "Structured Alignment (Separated Parameters)"
            ]
        ],
        "n_sentence": 17.0,
        "table_id": "table_1",
        "paper_id": "D18-1184",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1185table_2",
        "description": "Table 2 reports our results on the MultiNLI and SciTail datasets. On MultiNLI, CAFE significantly outperforms ESIM, a strong state-of-the-art model on both settings. We also outperform the ESIM + Read model (Weissenborn, 2017). An ensemble of CAFE models achieve competitive result on the MultiNLI dataset. On SciTail, our proposed CAFE model achieves state-of-the-art performance. The performance gain over strong baselines such as DecompAtt and ESIM are \u2248 10% \u2212 13% in terms of accuracy. CAFE also outperforms DGEM, which uses a graph-based attention for improved performance, by a significant margin of 5%. As such, empirical results demonstrate the effectiveness of our proposed CAFE model on the challenging SciTail dataset.",
        "sentences": [
            "Table 2 reports our results on the MultiNLI and SciTail datasets.",
            "On MultiNLI, CAFE significantly outperforms ESIM, a strong state-of-the-art model on both settings.",
            "We also outperform the ESIM + Read model (Weissenborn, 2017).",
            "An ensemble of CAFE models achieve competitive result on the MultiNLI dataset.",
            "On SciTail, our proposed CAFE model achieves state-of-the-art performance.",
            "The performance gain over strong baselines such as DecompAtt and ESIM are \u2248 10% \u2212 13% in terms of accuracy.",
            "CAFE also outperforms DGEM, which uses a graph-based attention for improved performance, by a significant margin of 5%.",
            "As such, empirical results demonstrate the effectiveness of our proposed CAFE model on the challenging SciTail dataset."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "MultiNLI",
                "SciTail"
            ],
            [
                "MultiNLI",
                "CAFE",
                "ESIM\u2020",
                "ESIM#\u266d"
            ],
            [
                "MultiNLI",
                "CAFE",
                "ESIM + Read\u2020"
            ],
            [
                "CAFE Ensemble",
                "MultiNLI"
            ],
            [
                "CAFE",
                "SciTail"
            ],
            [
                "CAFE",
                "SciTail",
                "DecompAtt# -",
                "ESIM#\u266d"
            ],
            [
                "SciTail",
                "CAFE",
                "DGEM + Edge#"
            ],
            [
                "CAFE",
                "SciTail"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_2",
        "paper_id": "D18-1185",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1186table_2",
        "description": "SNLI. Table 2 shows the results of different models on the train set and test set of SNLI. The first row gives a baseline model with handcrafted features presented by Bowman et al. (2015). All the other models are attention-based neural networks. Wang and Jiang (2016) exploits the long short-term memory (LSTM) for NLI. Parikh et al. (2016) uses attention to decompose the problem into subproblems that can be solved separately. Chen et al. (2017a) incorporates the chain LSTM and tree LSTM jointly. Zhiguo Wang (2017) proposes a bilateral multi-perspective matching for NLI. In Table 2, the second block gives the single models. As we can see, our proposed model CIN achieves 88.0% in accuracy on SNLI test set. Compared to the previous work, CIN obtains competitive performance. To further improve the performance of NLI systems, researchers have built ensemble models. Ensemble systems obtained the best performance on SNLI. Our ensemble model obtains 89.1% in accuracy and outperforms the current state-of-the-art model. Overall, single model of CIN performs competitively well and outperforms the previous models on ensemble scenarios for the natural language inference task.",
        "sentences": [
            "SNLI.",
            "Table 2 shows the results of different models on the train set and test set of SNLI.",
            "The first row gives a baseline model with handcrafted features presented by Bowman et al. (2015).",
            "All the other models are attention-based neural networks.",
            "Wang and Jiang (2016) exploits the long short-term memory (LSTM) for NLI.",
            "Parikh et al. (2016) uses attention to decompose the problem into subproblems that can be solved separately.",
            "Chen et al. (2017a) incorporates the chain LSTM and tree LSTM jointly.",
            "Zhiguo Wang (2017) proposes a bilateral multi-perspective matching for NLI.",
            "In Table 2, the second block gives the single models.",
            "As we can see, our proposed model CIN achieves 88.0% in accuracy on SNLI test set.",
            "Compared to the previous work, CIN obtains competitive performance.",
            "To further improve the performance of NLI systems, researchers have built ensemble models.",
            "Ensemble systems obtained the best performance on SNLI.",
            "Our ensemble model obtains 89.1% in accuracy and outperforms the current state-of-the-art model.",
            "Overall, single model of CIN performs competitively well and outperforms the previous models on ensemble scenarios for the natural language inference task."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            2,
            2,
            2,
            2,
            1,
            1,
            1,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Handcrafted features (Bowman et al. 2015)",
                "LSTM with attention (Rocktaschel et al. 2015)",
                "Match-LSTM (Wang and Jiang 2016)",
                "Decomposable attention model (Parikh et al. 2016)",
                "BiMPM (Zhiguo Wang 2017)",
                "NTI-SLSTM-LSTM (Munkhdalai and Yu 2017)",
                "Re-read LSTM (Sha et al. 2016)",
                "DIIN (Gong et al. 2017)",
                "ESIM (Chen et al. 2017a)",
                "CIN",
                "ESIM (Chen et al. 2017a) (Ensemble)",
                "BiMPM (Zhiguo Wang 2017) (Ensemble)",
                "DIIN (Gong et al. 2017) (Ensemble)",
                "CIN (Ensemble)",
                "Train",
                "Test"
            ],
            [
                "Handcrafted features (Bowman et al. 2015)"
            ],
            [
                "LSTM with attention (Rocktaschel et al. 2015)",
                "Match-LSTM (Wang and Jiang 2016)",
                "Decomposable attention model (Parikh et al. 2016)",
                "BiMPM (Zhiguo Wang 2017)",
                "NTI-SLSTM-LSTM (Munkhdalai and Yu 2017)",
                "Re-read LSTM (Sha et al. 2016)",
                "DIIN (Gong et al. 2017)",
                "ESIM (Chen et al. 2017a)",
                "CIN",
                "ESIM (Chen et al. 2017a) (Ensemble)",
                "BiMPM (Zhiguo Wang 2017) (Ensemble)",
                "DIIN (Gong et al. 2017) (Ensemble)",
                "CIN (Ensemble)"
            ],
            [
                "Match-LSTM (Wang and Jiang 2016)"
            ],
            [
                "Decomposable attention model (Parikh et al. 2016)"
            ],
            [
                "ESIM (Chen et al. 2017a) (Ensemble)"
            ],
            [
                "BiMPM (Zhiguo Wang 2017)"
            ],
            [
                "LSTM with attention (Rocktaschel et al. 2015)",
                "Match-LSTM (Wang and Jiang 2016)",
                "Decomposable attention model (Parikh et al. 2016)",
                "BiMPM (Zhiguo Wang 2017)",
                "NTI-SLSTM-LSTM (Munkhdalai and Yu 2017)",
                "Re-read LSTM (Sha et al. 2016)",
                "DIIN (Gong et al. 2017)",
                "ESIM (Chen et al. 2017a)",
                "CIN"
            ],
            [
                "CIN",
                "Test"
            ],
            [
                "CIN"
            ],
            [
                "ESIM (Chen et al. 2017a) (Ensemble)",
                "BiMPM (Zhiguo Wang 2017) (Ensemble)",
                "DIIN (Gong et al. 2017) (Ensemble)",
                "CIN (Ensemble)"
            ],
            [
                "ESIM (Chen et al. 2017a) (Ensemble)",
                "BiMPM (Zhiguo Wang 2017) (Ensemble)",
                "DIIN (Gong et al. 2017) (Ensemble)",
                "CIN (Ensemble)"
            ],
            [
                "CIN (Ensemble)",
                "Test"
            ],
            [
                "CIN"
            ]
        ],
        "n_sentence": 15.0,
        "table_id": "table_2",
        "paper_id": "D18-1186",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1186table_3",
        "description": "MultiNLI. Table 3 shows the performance of different models on MultiNLI. The original aim of this dataset is to evaluate the quality of sentence representations. Recently this dataset is also used to evaluate the interaction model involving attention mechanism. The first line of Table 3 gives a baseline model without interaction. The second block of Table 3 gives the attention-based models. The proposed model, CIN, achieves the accuracies of 77.0% and 77.6% on the match and mismatch test sets respectively. The results show that our model outperforms the other models.",
        "sentences": [
            "MultiNLI.",
            "Table 3 shows the performance of different models on MultiNLI.",
            "The original aim of this dataset is to evaluate the quality of sentence representations.",
            "Recently this dataset is also used to evaluate the interaction model involving attention mechanism.",
            "The first line of Table 3 gives a baseline model without interaction.",
            "The second block of Table 3 gives the attention-based models.",
            "The proposed model, CIN, achieves the accuracies of 77.0% and 77.6% on the match and mismatch test sets respectively.",
            "The results show that our model outperforms the other models."
        ],
        "class_sentence": [
            2,
            1,
            2,
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "BiLSTM (Williams et al. 2017)",
                "InnerAtt (Balazs et al. 2017)",
                "ESIM (Chen et al. 2017a)",
                "Gated-Att BiLSTM (Chen et al. 2017b)",
                "CIN"
            ],
            null,
            null,
            [
                "BiLSTM (Williams et al. 2017)"
            ],
            [
                "InnerAtt (Balazs et al. 2017)",
                "ESIM (Chen et al. 2017a)",
                "Gated-Att BiLSTM (Chen et al. 2017b)"
            ],
            [
                "CIN",
                "Match",
                "Mismatch"
            ],
            [
                "CIN",
                "BiLSTM (Williams et al. 2017)",
                "InnerAtt (Balazs et al. 2017)",
                "ESIM (Chen et al. 2017a)",
                "Gated-Att BiLSTM (Chen et al. 2017b)"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_3",
        "paper_id": "D18-1186",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1186table_4",
        "description": "Quora. Table 4 shows the performance of different models on the Quora test set. The baselines on Table 4 are all implemented in Zhiguo Wang (2017). The Siamese-CNN model and SiameseLSTM model encode sentences with CNN and LSTM respectively, and then predict the relationship between them based on the cosine similarity. Multi-Perspective-CNN and MultiPerspective-LSTM are transformed from SiameseCNN and Siamese-LSTM respectively by replacing the cosine similarity calculation layer with their multi-perspective cosine matching function. The L.D.C is a general compare-aggregate framework that performs word-level matching followed by a aggregation of convolution neural networks. As we can see, our model outperforms the base lines and achieve 88.62% in the test sets of Quora\ncorpus.",
        "sentences": [
            "Quora.",
            "Table 4 shows the performance of different models on the Quora test set.",
            "The baselines on Table 4 are all implemented in Zhiguo Wang (2017).",
            "The Siamese-CNN model and SiameseLSTM model encode sentences with CNN and LSTM respectively, and then predict the relationship between them based on the cosine similarity.",
            "Multi-Perspective-CNN and MultiPerspective-LSTM are transformed from SiameseCNN and Siamese-LSTM respectively by replacing the cosine similarity calculation layer with their multi-perspective cosine matching function.",
            "The L.D.C is a general compare-aggregate framework that performs word-level matching followed by a aggregation of convolution neural networks.",
            "As we can see, our model outperforms the base lines and achieve 88.62% in the test sets of Quora\ncorpus."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "Siamese-CNN",
                "Multi-Perspective CNN",
                "Siamese-LSTM",
                "Multi-Perspective-LSTM",
                "L.D.C",
                "BiMPM (Zhiguo Wang 2017)",
                "CIN",
                "Test"
            ],
            [
                "Siamese-CNN",
                "Multi-Perspective CNN",
                "Siamese-LSTM",
                "Multi-Perspective-LSTM",
                "L.D.C",
                "BiMPM (Zhiguo Wang 2017)"
            ],
            [
                "Siamese-CNN",
                "Siamese-LSTM"
            ],
            [
                "Multi-Perspective CNN",
                "Multi-Perspective-LSTM"
            ],
            [
                "L.D.C"
            ],
            [
                "CIN",
                "Test",
                "Siamese-CNN",
                "Multi-Perspective CNN",
                "Siamese-LSTM",
                "Multi-Perspective-LSTM",
                "L.D.C",
                "BiMPM (Zhiguo Wang 2017)"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_4",
        "paper_id": "D18-1186",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1194table_1",
        "description": "6.3 Results. Table 1 reports the experimental results on the test set. Results on the in-domain test set are similar and shown in Appendix D. In Table 1, S metric (defined in Section 4) measures the similarity between predicted and reference graph representations. Based on the optimal variable mapping provided by the S metric, we are able to evaluate our model and the variants in different aspects: BLEUINST measures the BLEU score of all matched instance edges; MAESPR measures the mean absolute error of SPR property scores of all matched argument edges; and MAEFACT measures the mean absolute error of factuality scores of all matched attribute edges. Overall, our proposed model outperforms the variants in every aspect. Variants (a) and (b) use simple heuristics to solve coreference, and achieve reasonable results: they both employ sequence-tosequence models to predict graph representations, which can be considered a replica of state-ofthe-art approaches for structured prediction (Choe and Charniak, 2016; Barzdins and Gosko, 2016; Peng et al., 2017). Compared to our model which employs the coreference annotating mechanism, these two variants suffer notable loss in the precision of S metric. As a result, their performance drops on the other metrics. Variant (c) only uses the encoder-side information for token representation, resulting in significant loss in MAESPR and MAEFACT. In the pipeline approach, each component is trained independently. During test, residual errors from each component are propagated through the pipeline. As expected, it shows a significant performance drop.",
        "sentences": [
            "6.3 Results.",
            "Table 1 reports the experimental results on the test set.",
            "Results on the in-domain test set are similar and shown in Appendix D.",
            "In Table 1, S metric (defined in Section 4) measures the similarity between predicted and reference graph representations.",
            "Based on the optimal variable mapping provided by the S metric, we are able to evaluate our model and the variants in different aspects: BLEUINST measures the BLEU score of all matched instance edges; MAESPR measures the mean absolute error of SPR property scores of all matched argument edges; and MAEFACT measures the mean absolute error of factuality scores of all matched attribute edges.",
            "Overall, our proposed model outperforms the variants in every aspect.",
            "Variants (a) and (b) use simple heuristics to solve coreference, and achieve reasonable results: they both employ sequence-tosequence models to predict graph representations, which can be considered a replica of state-ofthe-art approaches for structured prediction (Choe and Charniak, 2016; Barzdins and Gosko, 2016; Peng et al., 2017).",
            "Compared to our model which employs the coreference annotating mechanism, these two variants suffer notable loss in the precision of S metric.",
            "As a result, their performance drops on the other metrics.",
            "Variant (c) only uses the encoder-side information for token representation, resulting in significant loss in MAESPR and MAEFACT.",
            "In the pipeline approach, each component is trained independently.",
            "During test, residual errors from each component are propagated through the pipeline.",
            "As expected, it shows a significant performance drop."
        ],
        "class_sentence": [
            2,
            1,
            2,
            2,
            2,
            1,
            2,
            1,
            1,
            2,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "S metric"
            ],
            [
                "BLEU INST",
                "MAE SPR",
                "MAE FACT"
            ],
            [
                "Our model",
                "Variant (a)",
                "Variant (b)",
                "Variant (c)",
                "S metric",
                "BLEU INST",
                "MAE SPR",
                "MAE FACT"
            ],
            [
                "Variant (a)",
                "Variant (b)"
            ],
            [
                "Our model",
                "Variant (a)",
                "Variant (b)",
                "S metric",
                "Precision"
            ],
            [
                "Variant (a)",
                "Variant (b)",
                "F1"
            ],
            [
                "MAE SPR",
                "MAE FACT"
            ],
            [
                "Pipeline"
            ],
            [
                "Pipeline"
            ],
            [
                "Pipeline"
            ]
        ],
        "n_sentence": 13.0,
        "table_id": "table_1",
        "paper_id": "D18-1194",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1199table_4",
        "description": "Table 4 shows the performances of the new models, which are all worse than our full models MinV +infGibbs and MinV +infEM. This highlights the advantage of integrating distributional semantic information and local features into one single learning procedure. Without the informed prior (encoded by the soft labels), the Gibbs sampling and EM algorithms only seek to maximize the probability of the observed data, and may fail to learn the underlying usage structure. The model MinV +NN is not as competitive as our full models. It is too sensitive to the selected instances. Even though the training examples are instances that MinV is the most confident about, there are still mislabelled instances. These \u201dnoisy training examples\u201d would lead the NN classifier to make unreliable predictions.",
        "sentences": [
            "Table 4 shows the performances of the new models, which are all worse than our full models MinV +infGibbs and MinV +infEM.",
            "This highlights the advantage of integrating distributional semantic information and local features into one single learning procedure.",
            "Without the informed prior (encoded by the soft labels), the Gibbs sampling and EM algorithms only seek to maximize the probability of the observed data, and may fail to learn the underlying usage structure.",
            "The model MinV +NN is not as competitive as our full models.",
            "It is too sensitive to the selected instances.",
            "Even though the training examples are instances that MinV is the most confident about, there are still mislabelled instances.",
            "These \u201dnoisy training examples\u201d would lead the NN classifier to make unreliable predictions."
        ],
        "class_sentence": [
            1,
            2,
            2,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            [
                "Gibbs",
                "EM",
                "MinV+NN"
            ],
            null,
            [
                "Gibbs",
                "EM"
            ],
            [
                "MinV+NN"
            ],
            [
                "MinV+NN"
            ],
            [
                "MinV+NN"
            ],
            [
                "MinV+NN"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_4",
        "paper_id": "D18-1199",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1201table_1",
        "description": "5 Results. Table 1 presents the results on the development set. Lines 1-3 depict the results for local models using averaged FastText embedding initialization, showing that the best performance in terms of MRR and top-rank hits is achieved by TRANSE. Mean Rank does not align with the other metrics;. this is an interpretable tradeoff, as both BILIN and DISTMULT have an inherent preference for correlated synset embeddings, giving a stronger fallback for cases where the relation embedding is completely off, but allowing less freedom for separating strong cases from correlated false positives, compared to a translational objective. Effect of global score. There is a clear advantage to re-ranking the top local candidates using the score signal from the M3GM model (line 4). These results are further improved when the graph score is weighted against the association component per relation (line 5). We obtain similar improvements when re-ranking the predictions from DISTMULT and BILIN.",
        "sentences": [
            "5 Results.",
            "Table 1 presents the results on the development set.",
            "Lines 1-3 depict the results for local models using averaged FastText embedding initialization, showing that the best performance in terms of MRR and top-rank hits is achieved by TRANSE.",
            "Mean Rank does not align with the other metrics;.",
            "this is an interpretable tradeoff, as both BILIN and DISTMULT have an inherent preference for correlated synset embeddings, giving a stronger fallback for cases where the relation embedding is completely off, but allowing less freedom for separating strong cases from correlated false positives, compared to a translational objective.",
            "Effect of global score.",
            "There is a clear advantage to re-ranking the top local candidates using the score signal from the M3GM model (line 4).",
            "These results are further improved when the graph score is weighted against the association component per relation (line 5).",
            "We obtain similar improvements when re-ranking the predictions from DISTMULT and BILIN."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "DISTMULT",
                "BILIN",
                "TRANSE",
                "MRR",
                "H@10",
                "H@1"
            ],
            [
                "MR",
                "MRR",
                "H@10",
                "H@1"
            ],
            [
                "DISTMULT",
                "BILIN"
            ],
            null,
            [
                "M3GM"
            ],
            [
                "M3GM\u03b1r"
            ],
            [
                "DISTMULT",
                "BILIN"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_1",
        "paper_id": "D18-1201",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1201table_2",
        "description": "Table 2 shows that our main results transfer onto the test set, with even a slightly larger margin. This could be the result of the greater edge density of the combined training and dev graphs, which enhance the global coherence of the graph structure captured by M3GM features. To support this theory, we tested the M3GM model trained on only the training set, and its test set performance was roughly one point worse on all metrics, as compared with the model trained on the training+dev data.",
        "sentences": [
            "Table 2 shows that our main results transfer onto the test set, with even a slightly larger margin.",
            "This could be the result of the greater edge density of the combined training and dev graphs, which enhance the global coherence of the graph structure captured by M3GM features.",
            "To support this theory, we tested the M3GM model trained on only the training set, and its test set performance was roughly one point worse on all metrics, as compared with the model trained on the training+dev data."
        ],
        "class_sentence": [
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "M3GM\u03b1r"
            ],
            [
                "M3GM\u03b1r"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_2",
        "paper_id": "D18-1201",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1205table_5",
        "description": "Results in Table 5 show that our simple extractive method OurExtractive significantly outperforms state-of-the-art neural extractive baselines, which demonstrates the effectiveness of the information selection component in our model. Moreover, OurExtractive significantly outperforms the two comparison systems which remove different components of our model one by one. The results show that both the gated global information filtering and distant supervision training are effective for improving information selection in document summarization. Our proposed method effectively combines the strengths of extractive methods and abstractive methods into a unified framework.",
        "sentences": [
            "Results in Table 5 show that our simple extractive method OurExtractive significantly outperforms state-of-the-art neural extractive baselines, which demonstrates the effectiveness of the information selection component in our model.",
            "Moreover, OurExtractive significantly outperforms the two comparison systems which remove different components of our model one by one.",
            "The results show that both the gated global information filtering and distant supervision training are effective for improving information selection in document summarization.",
            "Our proposed method effectively combines the strengths of extractive methods and abstractive methods into a unified framework."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            [
                "OurExtractive",
                "SummaRuNNer-abs",
                "SummaRuNNer"
            ],
            [
                "OurExtractive",
                "\u2013 distS",
                "\u2013 distS&gateF"
            ],
            [
                "\u2013 distS",
                "\u2013 distS&gateF"
            ],
            [
                "OurExtractive"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_5",
        "paper_id": "D18-1205",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1206table_1",
        "description": "Table 1 compares XSum with the CNN, DailyMail, and NY Times benchmarks. As can be seen, XSum contains a substantial number of training instances, similar to DailyMail; documents and summaries in XSum are shorter in relation to other datasets but the vocabulary size is sufficiently large, comparable to CNN.",
        "sentences": [
            "Table 1 compares XSum with the CNN, DailyMail, and NY Times benchmarks.",
            "As can be seen, XSum contains a substantial number of training instances, similar to DailyMail; documents and summaries in XSum are shorter in relation to other datasets but the vocabulary size is sufficiently large, comparable to CNN."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "CNN",
                "DailyMail",
                "NY Times",
                "XSum"
            ],
            [
                "XSum",
                "# docs",
                "avg. summary length",
                "vocabulary size",
                "DailyMail",
                "CNN"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_1",
        "paper_id": "D18-1206",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1206table_2",
        "description": "Table 2 provides empirical analysis supporting our claim that XSum is less biased toward extractive methods compared to other summarization datasets. We report the percentage of novel n-grams in the target gold summaries that do not appear in their source documents. There are 36% novel unigrams in the XSum reference summaries compared to 17% in CNN, 17% in DailyMail, and 23% in NY Times. This indicates that XSum summaries are more abstractive. The proportion of novel constructions grows for larger n-grams across datasets, however, it is much steeper in XSum whose summaries exhibit approximately 83% novel bigrams, 96% novel trigrams, and 98% novel 4-grams (comparison datasets display around 47?55% new bigrams, 58?72% new trigrams, and 63?80% novel 4-grams). We further evaluated two extractive methods on these datasets. LEAD is often used as a strong lower bound for news summarization (Nenkova, 2005) and creates a summary by selecting the first few sentences or words in the document. We extracted the first 3 sentences for CNN documents and the first 4 sentences for DailyMail (Narayan et al., 2018b). Following previous work (Durrett et al., 2016; Paulus et al., 2018), we obtained LEAD summaries based on the first 100 words for NY Times documents. For XSum, we selected the first sentence in the document (excluding the oneline summary) to generate the LEAD. Our second method, EXT-ORACLE, can be viewed as an upper bound for extractive models (Nallapati et al., 2017; Narayan et al., 2018b). It creates an oracle summary by selecting the best possible set of sentences in the document that gives the highest ROUGE (Lin and Hovy, 2003) with respect to the gold summary. For XSum, we simply selected the single-best sentence in the document as summary. Table 2 reports the performance of the two extractive methods using ROUGE-1 (R1), ROUGE2 (R2), and ROUGE-L (RL) with the gold summaries as reference. The LEAD baseline performs extremely well on CNN, DailyMail and NY Times confirming that they are biased towards extractive methods. EXT-ORACLE further shows that improved sentence selection would bring further performance gains to extractive approaches. Abstractive systems trained on these datasets often have a hard time beating the LEAD, let alone EXTORACLE, or display a low degree of novelty in their summaries (See et al., 2017; Tan and Wan, 2017; Paulus et al., 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018). Interestingly, LEAD and EXT-ORACLE perform poorly on XSum underlying the fact that it is less biased towards extractive methods.",
        "sentences": [
            "Table 2 provides empirical analysis supporting our claim that XSum is less biased toward extractive methods compared to other summarization datasets.",
            "We report the percentage of novel n-grams in the target gold summaries that do not appear in their source documents.",
            "There are 36% novel unigrams in the XSum reference summaries compared to 17% in CNN, 17% in DailyMail, and 23% in NY Times.",
            "This indicates that XSum summaries are more abstractive.",
            "The proportion of novel constructions grows for larger n-grams across datasets, however, it is much steeper in XSum whose summaries exhibit approximately 83% novel bigrams, 96% novel trigrams, and 98% novel 4-grams (comparison datasets display around 47?55% new bigrams, 58?72% new trigrams, and 63?80% novel 4-grams).",
            "We further evaluated two extractive methods on these datasets.",
            "LEAD is often used as a strong lower bound for news summarization (Nenkova, 2005) and creates a summary by selecting the first few sentences or words in the document.",
            "We extracted the first 3 sentences for CNN documents and the first 4 sentences for DailyMail (Narayan et al., 2018b).",
            "Following previous work (Durrett et al., 2016; Paulus et al., 2018), we obtained LEAD summaries based on the first 100 words for NY Times documents.",
            "For XSum, we selected the first sentence in the document (excluding the oneline summary) to generate the LEAD.",
            "Our second method, EXT-ORACLE, can be viewed as an upper bound for extractive models (Nallapati et al., 2017; Narayan et al., 2018b).",
            "It creates an oracle summary by selecting the best possible set of sentences in the document that gives the highest ROUGE (Lin and Hovy, 2003) with respect to the gold summary.",
            "For XSum, we simply selected the single-best sentence in the document as summary.",
            "Table 2 reports the performance of the two extractive methods using ROUGE-1 (R1), ROUGE2 (R2), and ROUGE-L (RL) with the gold summaries as reference.",
            "The LEAD baseline performs extremely well on CNN, DailyMail and NY Times confirming that they are biased towards extractive methods.",
            "EXT-ORACLE further shows that improved sentence selection would bring further performance gains to extractive approaches.",
            "Abstractive systems trained on these datasets often have a hard time beating the LEAD, let alone EXTORACLE, or display a low degree of novelty in their summaries (See et al., 2017; Tan and Wan, 2017; Paulus et al., 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018).",
            "Interestingly, LEAD and EXT-ORACLE perform poorly on XSum underlying the fact that it is less biased towards extractive methods."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1,
            2,
            2,
            0,
            2,
            2,
            2,
            2,
            1,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            [
                "CNN",
                "DailyMail",
                "NY Times",
                "XSum"
            ],
            [
                "% of novel n-grams in gold summary"
            ],
            [
                "CNN",
                "DailyMail",
                "NY Times",
                "XSum",
                "unigrams"
            ],
            [
                "XSum"
            ],
            [
                "XSum",
                "bigrams",
                "trigrams",
                "4-grams"
            ],
            [
                "LEAD",
                "EXT-ORACLE"
            ],
            [
                "LEAD"
            ],
            [
                "LEAD",
                "CNN",
                "DailyMail"
            ],
            null,
            [
                "XSum",
                "LEAD"
            ],
            [
                "EXT-ORACLE"
            ],
            [
                "EXT-ORACLE"
            ],
            [
                "XSum",
                "EXT-ORACLE"
            ],
            [
                "LEAD",
                "EXT-ORACLE",
                "R1",
                "R2",
                "RL"
            ],
            [
                "LEAD",
                "CNN",
                "DailyMail",
                "NY Times"
            ],
            [
                "EXT-ORACLE"
            ],
            [
                "LEAD",
                "EXT-ORACLE"
            ],
            [
                "XSum",
                "LEAD",
                "EXT-ORACLE"
            ]
        ],
        "n_sentence": 18.0,
        "table_id": "table_2",
        "paper_id": "D18-1206",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1206table_4",
        "description": "Automatic Evaluation. We report results using automatic metrics in Table 4. We evaluated summarization quality using F1 ROUGE (Lin and Hovy, 2003). Unigram and bigram overlap (ROUGE-1 and ROUGE-2) are a proxy for assessing informativeness and the longest common subsequence (ROUGE-L) represents fluency. On the XSum dataset, SEQ2SEQ outperforms the LEAD and RANDOM baselines by a large margin. PTGEN, a SEQ2SEQ model with a \u201ccopying\u201d mechanism outperforms EXT-ORACLE, a \u201cperfect\u201d extractive system on ROUGE-2 and ROUGE-L. This is in sharp contrast to the performance of these models on CNN/DailyMail (See et al., 2017) and Newsroom datasets (Grusky et al., 2018), where they fail to outperform the LEAD. The result provides further evidence that XSum is a good testbed for abstractive summarization. PTGEN-COVG, the best performing abstractive system on the CNN/DailyMail datasets, does not do well. We believe that the coverage mechanism is more useful when generating multi-line summaries and is basically redundant for extreme summarization. CONVS2S, the convolutional variant of SEQ2SEQ, significantly outperforms all RNN-based abstractive systems. We hypothesize that its superior performance stems from the ability to better represent document content (i.e., by capturing long-range dependencies). Table 4 shows several variants of T-CONVS2S including an encoder network enriched with information about how topical a word is on its own (enct') or in the document (enc(t',tD)). We also experimented with various decoders by conditioning every prediction on the topic of the document, basically encouraging the summary to be in the same theme as the document (dectD) or letting the decoder decide the theme of the summary. Interestingly, all four T-CONVS2S variants outperform CONVS2S. T-CONVS2S performs best when both encoder and decoder are constrained by the document topic (enc(t',tD),dectD).",
        "sentences": [
            "Automatic Evaluation.",
            "We report results using automatic metrics in Table 4.",
            "We evaluated summarization quality using F1 ROUGE (Lin and Hovy, 2003).",
            "Unigram and bigram overlap (ROUGE-1 and ROUGE-2) are a proxy for assessing informativeness and the longest common subsequence (ROUGE-L) represents fluency.",
            "On the XSum dataset, SEQ2SEQ outperforms the LEAD and RANDOM baselines by a large margin.",
            "PTGEN, a SEQ2SEQ model with a \u201ccopying\u201d mechanism outperforms EXT-ORACLE, a \u201cperfect\u201d extractive system on ROUGE-2 and ROUGE-L.",
            "This is in sharp contrast to the performance of these models on CNN/DailyMail (See et al., 2017) and Newsroom datasets (Grusky et al., 2018), where they fail to outperform the LEAD.",
            "The result provides further evidence that XSum is a good testbed for abstractive summarization.",
            "PTGEN-COVG, the best performing abstractive system on the CNN/DailyMail datasets, does not do well.",
            "We believe that the coverage mechanism is more useful when generating multi-line summaries and is basically redundant for extreme summarization.",
            "CONVS2S, the convolutional variant of SEQ2SEQ, significantly outperforms all RNN-based abstractive systems.",
            "We hypothesize that its superior performance stems from the ability to better represent document content (i.e., by capturing long-range dependencies).",
            "Table 4 shows several variants of T-CONVS2S including an encoder network enriched with information about how topical a word is on its own (enct') or in the document (enc(t',tD)).",
            "We also experimented with various decoders by conditioning every prediction on the topic of the document, basically encouraging the summary to be in the same theme as the document (dectD) or letting the decoder decide the theme of the summary.",
            "Interestingly, all four T-CONVS2S variants outperform CONVS2S.",
            "T-CONVS2S performs best when both encoder and decoder are constrained by the document topic (enc(t',tD),dectD)."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            1,
            1,
            2,
            2,
            1,
            2,
            1,
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "R1",
                "R2",
                "RL"
            ],
            [
                "R1",
                "R2",
                "RL"
            ],
            [
                "SEQ2SEQ",
                "Random",
                "LEAD"
            ],
            [
                "PTGEN",
                "EXT-ORACLE",
                "R2",
                "RL"
            ],
            [
                "PTGEN",
                "EXT-ORACLE",
                "LEAD"
            ],
            null,
            [
                "PTGEN-COVG"
            ],
            [
                "PTGEN-COVG"
            ],
            [
                "CONVS2S",
                "SEQ2SEQ",
                "PTGEN",
                "PTGEN-COVG"
            ],
            [
                "CONVS2S"
            ],
            [
                "T-CONVS2SS (enct)",
                "T-CONVS2S (enc(t tD))"
            ],
            [
                "T-CONVS2S (enct dectD)",
                "T-CONVS2S (enc(t tD) dectD)"
            ],
            [
                "CONVS2S",
                "T-CONVS2SS (enct)",
                "T-CONVS2S (enct dectD)",
                "T-CONVS2S (enc(t tD))",
                "T-CONVS2S (enc(t tD) dectD)"
            ],
            [
                "T-CONVS2S (enc(t tD) dectD)"
            ]
        ],
        "n_sentence": 16.0,
        "table_id": "table_4",
        "paper_id": "D18-1206",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1208table_3",
        "description": "Word Embedding Learning. Given that learning a sentence encoder (averaging has no learned parameters) does not yield significant improvement, it is natural to consider whether learning word embeddings is also necessary. In Table 3 we compare the performance of different extractors using the averaging encoder, when the word embeddings are held fixed or learned during training. In both cases, word embeddings are initialized with GloVe embeddings trained on a combination of Gigaword and Wikipedia. When learning embeddings, words occurring fewer than three times in the training data are mapped to an unknown token (with learned embedding). In all but one case, fixed embeddings are as good or better than the learned embeddings. Thisis a somewhat surprising finding on the CNN/DM data since it is reasonably large, and learning embeddings should give the models more flexibility to identify important word features. This suggests that we cannot extract much generalizable learning signal from the content other than what is already present from initialization. Even on PubMed, where the language is quite different from the news/Wikipedia articles the GloVe embeddings were trained on, learning leads to significantly worse results.",
        "sentences": [
            "Word Embedding Learning.",
            "Given that learning a sentence encoder (averaging has no learned parameters) does not yield significant improvement, it is natural to consider whether learning word embeddings is also necessary.",
            "In Table 3 we compare the performance of different extractors using the averaging encoder, when the word embeddings are held fixed or learned during training.",
            "In both cases, word embeddings are initialized with GloVe embeddings trained on a combination of Gigaword and Wikipedia.",
            "When learning embeddings, words occurring fewer than three times in the training data are mapped to an unknown token (with learned embedding).",
            "In all but one case, fixed embeddings are as good or better than the learned embeddings.",
            "Thisis a somewhat surprising finding on the CNN/DM data since it is reasonably large, and learning embeddings should give the models more flexibility to identify important word features.",
            "This suggests that we cannot extract much generalizable learning signal from the content other than what is already present from initialization.",
            "Even on PubMed, where the language is quite different from the news/Wikipedia articles the GloVe embeddings were trained on, learning leads to significantly worse results."
        ],
        "class_sentence": [
            2,
            2,
            1,
            2,
            0,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "Seq2Seq",
                "C&L",
                "Summa",
                "Runner",
                "Learn",
                "Fixed"
            ],
            [
                "Learn",
                "Fixed"
            ],
            null,
            [
                "Fixed"
            ],
            null,
            null,
            [
                " PubMed"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_3",
        "paper_id": "D18-1208",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1208table_5",
        "description": "Table 5 shows the results of the shuffling experiments. The news domains and PubMed suffer a significant drop in performance when the document order is shuffled. By comparison, there is no significant difference between the shuffled and inorder models on the Reddit domain, and shuffling actually improves performance on AMI. This suggest that position is being learned by the models in the news/journal article domain even when the model has no explicit position features, and that this feature is more important than either content or function words.",
        "sentences": [
            "Table 5 shows the results of the shuffling experiments.",
            "The news domains and PubMed suffer a significant drop in performance when the document order is shuffled.",
            "By comparison, there is no significant difference between the shuffled and inorder models on the Reddit domain, and shuffling actually improves performance on AMI.",
            "This suggest that position is being learned by the models in the news/journal article domain even when the model has no explicit position features, and that this feature is more important than either content or function words."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Shuffled",
                "CNN/DM",
                "NYT",
                "PubMed"
            ],
            [
                "In-Order",
                "Shuffled",
                "Reddit",
                "AMI"
            ],
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_5",
        "paper_id": "D18-1208",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1215table_2",
        "description": "old in all cases (an instance is classified as positive if the normalized probability score is at least 0.5). For each system, sample precision was estimated by sampling 100 positive extractions and manually determining the proportion of correct extractions by an author knowledgeable about this domain. Absolute recall is estimated by multiplying sample precision with the number of positive extractions. Table 2 shows the results. DPL substantially outperformed Peng et al. (2017), improving sample precision by ten absolute points and raising absolute recall by 25%. Combining disparate indirect supervision strategies is key to this performance gain, as evident from the ablation results. While distant supervision remained the most potent source of indirect supervision, data programming and joint inference each contributed significantly. Replacing out-of-domain (Wikipedia) word embedding with in-domain (PubMed) word embedding (Pyysalo et al., 2013) also led to a small gain.",
        "sentences": [
            "old in all cases (an instance is classified as positive if the normalized probability score is at least 0.5).",
            "For each system, sample precision was estimated by sampling 100 positive extractions and manually determining the proportion of correct extractions by an author knowledgeable about this domain.",
            "Absolute recall is estimated by multiplying sample precision with the number of positive extractions.",
            "Table 2 shows the results.",
            "DPL substantially outperformed Peng et al. (2017), improving sample precision by ten absolute points and raising absolute recall by 25%.",
            "Combining disparate indirect supervision strategies is key to this performance gain, as evident from the ablation results.",
            "While distant supervision remained the most potent source of indirect supervision, data programming and joint inference each contributed significantly.",
            "Replacing out-of-domain (Wikipedia) word embedding with in-domain (PubMed) word embedding (Pyysalo et al., 2013) also led to a small gain."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            [
                "DPL + EMB",
                "Peng 2017",
                "Prec.",
                "Abs. Rec."
            ],
            [
                "DPL",
                "DPL -DS",
                "DPL -DP",
                "DPL -DP (ENTITY)",
                "DPL -JI"
            ],
            [
                "DPL -DS",
                "DPL -DP",
                "DPL -DP (ENTITY)",
                "DPL -JI"
            ],
            [
                "DPL + EMB"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_2",
        "paper_id": "D18-1215",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1215table_5",
        "description": "Experiment results. For evaluation, we annotated a larger set of sample gene-mention candidates and then subsampled a balanced test set of 550 instances (half are true gene mentions, half not). These instances were excluded from training and development. Table 5 compares system performance on this test set. The string-matching baseline has a very low precision, as gene mentions are highly ambiguous, which explains why Peng et al. (2017) resorted to heavy filtering. By combining indirect supervision strategies, DPL improved precision by over 50 absolute points, while retaining a reasonably high recall (86%). All indirect supervision strategies contributed significantly, as the ablation tests show.",
        "sentences": [
            "Experiment results.",
            "For evaluation, we annotated a larger set of sample gene-mention candidates and then subsampled a balanced test set of 550 instances (half are true gene mentions, half not).",
            "These instances were excluded from training and development.",
            "Table 5 compares system performance on this test set.",
            "The string-matching baseline has a very low precision, as gene mentions are highly ambiguous, which explains why Peng et al. (2017) resorted to heavy filtering.",
            "By combining indirect supervision strategies, DPL improved precision by over 50 absolute points, while retaining a reasonably high recall (86%).",
            "All indirect supervision strategies contributed significantly, as the ablation tests show."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "String Match",
                "DS",
                "DS + DP",
                "DS + DP + JI"
            ],
            [
                "String Match",
                "Prec."
            ],
            [
                "DS",
                "DS + DP",
                "DS + DP + JI",
                "Rec."
            ],
            [
                "DS",
                "DS + DP",
                "DS + DP + JI"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_5",
        "paper_id": "D18-1215",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1218table_5",
        "description": "In Table 5 we present the results obtained on simulated data from the CONLL-2012 test set. The results follow a similar trend to those observed using actual annotations: a much better quality of the chains produced using the mention pairs inferred by our MPA model, across all the simulated scenarios. Furthermore, the MV baseline achieves better chains compared to the Stanford system in 3 out of 4 simulation settings, again showcasing the potential of crowdsourced annotations.",
        "sentences": [
            "In Table 5 we present the results obtained on simulated data from the CONLL-2012 test set.",
            "The results follow a similar trend to those observed using actual annotations: a much better quality of the chains produced using the mention pairs inferred by our MPA model, across all the simulated scenarios.",
            "Furthermore, the MV baseline achieves better chains compared to the Stanford system in 3 out of 4 simulation settings, again showcasing the potential of crowdsourced annotations."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "MPA",
                "Synthetic Uniform",
                "PD-inspired Uniform",
                "Synthetic Sparse",
                "PD-inspired Sparse"
            ],
            [
                "MV",
                "Stanford",
                "Synthetic Uniform",
                "PD-inspired Uniform",
                "PD-inspired Sparse"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_5",
        "paper_id": "D18-1218",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1219table_6",
        "description": "Table 6 lists the best results of the two models for bridging anaphora resolution from Hou et al. (2013b). pairwise model III is a pairwise mentionentity model based on various semantic, syntactic and lexical features. MLN model II is a joint inference framework based on Markov logic networks (Domingos and Lowd, 2009). It models that semantically or syntactically related anaphors are likely to share the same antecedent and achieves an accuracy of 41.32% on the ISNotes corpus. The results for GloVe GigaWiki14 and GloVe Giga are similar on two settings (using NP head vs. using NP head + modifiers). For embeddings PP, the result on using NP head + modifiers (31.67%) is worse than the result on using NP head (33.03%). However, if we apply embeddings PP to a bridging anaphor\u2019s head and modifiers, and only apply embeddings PP to the head noun of an antecedent candidate, we get an accuracy of 34.53%. Although the differences are not significant, it confirms that the information from the modifiers of the antecedent candidates in embeddings PP hurts the performance. This corresponds to our observations in the previous section that the representations for words without the suffix \u201cPP\u201d in embeddings PP are not as good as in embeddings bridging due to less training instances. Finally, our method based on embeddings bridging achieves an accuracy of 39.52%, which is competitive to the best result (41.32%) reported in Hou et al. (2013b). There is no significant difference between NP head + modifiers based on embeddings bridging and MLN model II (randomization test with p < 0.01).",
        "sentences": [
            "Table 6 lists the best results of the two models for bridging anaphora resolution from Hou et al. (2013b).",
            "pairwise model III is a pairwise mentionentity model based on various semantic, syntactic and lexical features.",
            "MLN model II is a joint inference framework based on Markov logic networks (Domingos and Lowd, 2009).",
            "It models that semantically or syntactically related anaphors are likely to share the same antecedent and achieves an accuracy of 41.32% on the ISNotes corpus.",
            "The results for GloVe GigaWiki14 and GloVe Giga are similar on two settings (using NP head vs. using NP head + modifiers).",
            "For embeddings PP, the result on using NP head + modifiers (31.67%) is worse than the result on using NP head (33.03%).",
            "However, if we apply embeddings PP to a bridging anaphor\u2019s head and modifiers, and only apply embeddings PP to the head noun of an antecedent candidate, we get an accuracy of 34.53%.",
            "Although the differences are not significant, it confirms that the information from the modifiers of the antecedent candidates in embeddings PP hurts the performance.",
            "This corresponds to our observations in the previous section that the representations for words without the suffix \u201cPP\u201d in embeddings PP are not as good as in embeddings bridging due to less training instances.",
            "Finally, our method based on embeddings bridging achieves an accuracy of 39.52%, which is competitive to the best result (41.32%) reported in Hou et al. (2013b).",
            "There is no significant difference between NP head + modifiers based on embeddings bridging and MLN model II (randomization test with p < 0.01)."
        ],
        "class_sentence": [
            1,
            2,
            2,
            1,
            1,
            1,
            2,
            2,
            2,
            1,
            2
        ],
        "header_mention": [
            [
                "models from Hou et al. (2013b)",
                "pairwise model III",
                "MLN model II"
            ],
            [
                "pairwise model III"
            ],
            [
                "MLN model II"
            ],
            [
                "pairwise model III",
                "MLN model II",
                "acc"
            ],
            [
                "GloVe GigaWiki14",
                "GloVe Giga"
            ],
            [
                "embeddings PP",
                "acc"
            ],
            [
                "embeddings PP",
                "acc"
            ],
            [
                "embeddings PP"
            ],
            [
                "embeddings PP",
                "embeddings bridging"
            ],
            [
                "embeddings bridging",
                "MLN model II",
                "acc"
            ],
            [
                "embeddings bridging",
                "MLN model II"
            ]
        ],
        "n_sentence": 11.0,
        "table_id": "table_6",
        "paper_id": "D18-1219",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1219table_8",
        "description": "5.6 Combining NP Head + Modifiers with MLN II. For bridging anaphora resolution, Hou (2018) integrates a much simpler deterministic approach by combining an NP head with its noun modifiers (appearing before the head) based on embeddings PP into the MLN II system (Hou et al., 2013b). Similarly, we add a constraint on top of MLN II using our deterministic approach (NP head + modifiers) based on embeddings bridging. Table 8 lists the results of different systems for bridging anaphora resolution in ISNotes. It shows that combining our deterministic approach (NP Head + modifiers) with MLN II slightly improves the result compared to Hou (2018). Although combining NP Head + modifiers with MLN II achieves significant improvement over NP Head + modifiers, we think the latter has its own value. Our deterministic algorithm is simpler and more efficient compared to MLN model II + embeddings bridging, which contains many complicated features and might be hard to migrate to other bridging corpora. Moreover, our algorithm is \u201cunsupervised\u201d and requires no training when applied to other English bridging corpora.",
        "sentences": [
            "5.6 Combining NP Head + Modifiers with MLN II.",
            "For bridging anaphora resolution, Hou (2018) integrates a much simpler deterministic approach by combining an NP head with its noun modifiers (appearing before the head) based on embeddings PP into the MLN II system (Hou et al., 2013b).",
            "Similarly, we add a constraint on top of MLN II using our deterministic approach (NP head + modifiers) based on embeddings bridging.",
            "Table 8 lists the results of different systems for bridging anaphora resolution in ISNotes.",
            "It shows that combining our deterministic approach (NP Head + modifiers) with MLN II slightly improves the result compared to Hou (2018).",
            "Although combining NP Head + modifiers with MLN II achieves significant improvement over NP Head + modifiers, we think the latter has its own value.",
            "Our deterministic algorithm is simpler and more efficient compared to MLN model II + embeddings bridging, which contains many complicated features and might be hard to migrate to other bridging corpora.",
            "Moreover, our algorithm is \u201cunsupervised\u201d and requires no training when applied to other English bridging corpora."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "Models from Hou et al. (2013b)",
                "MLN model II + embeddings PP (NP head + noun pre-modifiers)"
            ],
            [
                "MLN model II + embeddings PP (NP head + noun pre-modifiers)",
                "MLN model II + embeddings bridging (NP head + modifiers)"
            ],
            [
                "Schulte im Walde (1998)",
                "Poesio et al. (2004)",
                "pairwise model III",
                "MLN model II",
                "MLN model II + embeddings PP (NP head + noun pre-modifiers)",
                "embeddings bridging (NP head + modifiers)",
                "MLN model II + embeddings bridging (NP head + modifiers)"
            ],
            [
                "MLN model II + embeddings PP (NP head + noun pre-modifiers)",
                "MLN model II + embeddings bridging (NP head + modifiers)",
                "acc"
            ],
            [
                "MLN model II + embeddings bridging (NP head + modifiers)",
                "embeddings bridging (NP head + modifiers)",
                "acc"
            ],
            [
                "embeddings bridging (NP head + modifiers)",
                "MLN model II + embeddings bridging (NP head + modifiers)"
            ],
            [
                "embeddings bridging (NP head + modifiers)"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_8",
        "paper_id": "D18-1219",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1219table_9",
        "description": "Table 9 lists the results of bridging anaphora resolution in the BASHI and ARRAU corpora, respectively. On the test set of the ARRAU (RST) corpus, Rosiger (2018b) proposed a modified rule-based system based on Hou et al. (2014)\u00e2\u20ac\u2122s work and reported an accuracy of 39.8% for bridging anaphora resolution. And our algorithm achieves an accuracy of 32.39% using only embeddings bridging. Overall, the reasonable performance on these two corpora demonstrates thatembeddings bridging is a general word representation resource for bridging.",
        "sentences": [
            "Table 9 lists the results of bridging anaphora resolution in the BASHI and ARRAU corpora, respectively.",
            "On the test set of the ARRAU (RST) corpus, Rosiger (2018b) proposed a modified rule-based system based on Hou et al. (2014)\u00e2\u20ac\u2122s work and reported an accuracy of 39.8% for bridging anaphora resolution.",
            "And our algorithm achieves an accuracy of 32.39% using only embeddings bridging.",
            "Overall, the reasonable performance on these two corpora demonstrates thatembeddings bridging is a general word representation resource for bridging."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "BASHI",
                "ARRAU (RST Train)",
                "ARRAU (RST Test)"
            ],
            [
                "ARRAU (RST Train)",
                "ARRAU (RST Test)"
            ],
            [
                "ARRAU (RST Test)"
            ],
            [
                "BASHI",
                "ARRAU (RST Train)",
                "ARRAU (RST Test)"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_9",
        "paper_id": "D18-1219",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1221table_2",
        "description": "Table 2 shows the results, based on a MS-LSTM setup similar to that of \u0e22\u0e074.1. Note that the MSLSTM achieves 0.95-0.96 top-10 accuracy for the seen evaluation, significantly higher not only than the best model of Hill et al. (2016), but also higher than OneLook, a commercial system with access to more than 1000 dictionaries. It also presents considerably higher performance in the unseen evaluation. We are not aware of any other models with higher performance on the specific task.",
        "sentences": [
            "Table 2 shows the results, based on a MS-LSTM setup similar to that of \u0e22\u0e074.1.",
            "Note that the MSLSTM achieves 0.95-0.96 top-10 accuracy for the seen evaluation, significantly higher not only than the best model of Hill et al. (2016), but also higher than OneLook, a commercial system with access to more than 1000 dictionaries.",
            "It also presents considerably higher performance in the unseen evaluation.",
            "We are not aware of any other models with higher performance on the specific task."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "MS-LSTM +TF vectors",
                "MS-LSTM +TF vectors + anchors"
            ],
            [
                "MS-LSTM +TF vectors",
                "MS-LSTM +TF vectors + anchors",
                "Acc-10",
                "Seen (500 WordNet definitions)",
                "OneLook (Hill et al. 2016)",
                "RNN cosine (Hill et al. 2016)"
            ],
            [
                "MS-LSTM +TF vectors",
                "MS-LSTM +TF vectors + anchors",
                "Unseen (500 WordNet definitions)"
            ],
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "D18-1221",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1221table_3",
        "description": "In Table 3 we report results for two evaluation settings. In Evaluation 1, we provide a comparison with the method of Yang et al. (2015) who include textual features in graph embeddings based on matrix factorisation, and two topic models used as baselines in their paper. Using the same classification algorithm (a linear SVM) and training ratio (0.50) with them, we present state-of-the-art results for vectors of 150 dimensions, prepared by a graph extended with 1422 textual features. We set \u03bb = 0.5 by tuning on a dev set of 677 randomly selected entries from the training data. In Evaluation 2, using the same linear SVM classifier and \u03bb as before, we reduce the training ratio to 0.05 in order to make our task comparable to the experiments reported by Velickovic et al. (2018) for a number of deep learning models: specifically, the graph attention network (GAT) of Veli\u010dkovic et al. (2018), the graph convolutional network (GCN) of Kipf and Welling (2017), and the Planetoid model of Yang et al. (2016). Again, our simple setting presents results within the state of the art range, comparable to (or better than) those of much more sophisticated models that have been specifically designed for the task of node classification. We consider this as a strong indication for the effectiveness of the textually enhanced vectors as representations of KB entities.",
        "sentences": [
            "In Table 3 we report results for two evaluation settings.",
            "In Evaluation 1, we provide a comparison with the method of Yang et al. (2015) who include textual features in graph embeddings based on matrix factorisation, and two topic models used as baselines in their paper.",
            "Using the same classification algorithm (a linear SVM) and training ratio (0.50) with them, we present state-of-the-art results for vectors of 150 dimensions, prepared by a graph extended with 1422 textual features.",
            "We set \u03bb = 0.5 by tuning on a dev set of 677 randomly selected entries from the training data.",
            "In Evaluation 2, using the same linear SVM classifier and \u03bb as before, we reduce the training ratio to 0.05 in order to make our task comparable to the experiments reported by Velickovic et al. (2018) for a number of deep learning models: specifically, the graph attention network (GAT) of Veli\u010dkovic et al. (2018), the graph convolutional network (GCN) of Kipf and Welling (2017), and the Planetoid model of Yang et al. (2016).",
            "Again, our simple setting presents results within the state of the art range, comparable to (or better than) those of much more sophisticated models that have been specifically designed for the task of node classification.",
            "We consider this as a strong indication for the effectiveness of the textually enhanced vectors as representations of KB entities."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "Evaluation 1 (training ratio=0.50)",
                "Evaluation 2 (training ratio=0.05)"
            ],
            [
                "PLSA (Hofmann 1999)",
                "NetPLSA (Mei et al. 2008)",
                "TADW (Yang et al. 2015)"
            ],
            [
                "Linear SVM + DeepWalk vectors",
                "Linear SVM + TF vectors"
            ],
            [
                "Linear SVM + TF vectors"
            ],
            [
                "Evaluation 2 (training ratio=0.05)",
                "Planetoid (Yang et al. 2016)",
                "GCN (Kipf and Welling 2017)",
                "GAT (Veli\u010dkovic et al. 2018)"
            ],
            [
                "Linear SVM + DeepWalk vectors",
                "Linear SVM + TF vectors",
                "Planetoid (Yang et al. 2016)",
                "GCN (Kipf and Welling 2017)",
                "GAT (Veli\u010dkovic et al. 2018)"
            ],
            [
                "Linear SVM + TF vectors"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_3",
        "paper_id": "D18-1221",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1222table_3",
        "description": "Our datasets have three kinds of triples. Hence, we do experiments on them respectively. Experimental results for relational triples, instanceOf triples, and subClassOf triples are shown in Table 2, Table 3, and Table 4 respectively. In Table 3 and Table 4, a rising arrow means performance of this model have a promotion from YAGO39K to M-YAGO39K and a down arrow means a drop. From Table 2, we can learn that: (1) TransC outperforms all previous work in relational triple classification. (2) The \u201cbern\u201d sampling trick works better than \u201cunif\u201d in TransC. From Table 3 and Table 4, we can conclude that: (1) On YAGO39K, some compared models perform better than TransC in instanceOf triple classification. This is because that instanceOf has most triples (53.5%) among all relations in YAGO39K. This relation is trained superabundant times and nearly achieves the best performance, which has an adverse effect on other triples. TransC can find a balance between them and all triples achieve a good performance. (2) On YAGO39K, TransC outperforms other models in subClassOf triple classification. As shown in Table 1, subClassOf triples are much less than instanceOf triples. Hence, other models can not achieve the best performance under the bad influence of instanceOf triples. (3) On M-YAGO39K, TransC outperforms previous work in both instanceOf triple classification and subClassOf triple classification, which indicates that TransC can handle the transitivity of isA relations much better than other models. (4) After comparing experimental results in YAGO39K and M-YAGO39K, we can find that most previous work\u2019s performance suffers a big drop in instanceOf triple classification and a small drop in subClassOf triple classification. This shows that previous work can not deal with instanceOf-subClassOf transitivity well. (5) In TransC, nearly all performances have a significant promotion from YAGO39K to MYAGO39K. Both instanceOf-subClassOf transitivity and subClassOf-subClassOf transitivity are solved well in TransC.",
        "sentences": [
            "Our datasets have three kinds of triples.",
            "Hence, we do experiments on them respectively.",
            "Experimental results for relational triples, instanceOf triples, and subClassOf triples are shown in Table 2, Table 3, and Table 4 respectively.",
            "In Table 3 and Table 4, a rising arrow means performance of this model have a promotion from YAGO39K to M-YAGO39K and a down arrow means a drop.",
            "From Table 2, we can learn that: (1) TransC outperforms all previous work in relational triple classification. (2) The \u201cbern\u201d sampling trick works better than \u201cunif\u201d in TransC.",
            "From Table 3 and Table 4, we can conclude that: (1) On YAGO39K, some compared models perform better than TransC in instanceOf triple classification.",
            "This is because that instanceOf has most triples (53.5%) among all relations in YAGO39K.",
            "This relation is trained superabundant times and nearly achieves the best performance, which has an adverse effect on other triples.",
            "TransC can find a balance between them and all triples achieve a good performance.",
            "(2) On YAGO39K, TransC outperforms other models in subClassOf triple classification.",
            "As shown in Table 1, subClassOf triples are much less than instanceOf triples.",
            "Hence, other models can not achieve the best performance under the bad influence of instanceOf triples.",
            "(3) On M-YAGO39K, TransC outperforms previous work in both instanceOf triple classification and subClassOf triple classification, which indicates that TransC can handle the transitivity of isA relations much better than other models.",
            "(4) After comparing experimental results in YAGO39K and M-YAGO39K, we can find that most previous work\u2019s performance suffers a big drop in instanceOf triple classification and a small drop in subClassOf triple classification.",
            "This shows that previous work can not deal with instanceOf-subClassOf transitivity well.",
            "(5) In TransC, nearly all performances have a significant promotion from YAGO39K to MYAGO39K.",
            "Both instanceOf-subClassOf transitivity and subClassOf-subClassOf transitivity are solved well in TransC."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            2,
            1,
            2,
            2,
            1,
            2,
            2,
            2,
            1,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "YAGO39K",
                "M-YAGO39K"
            ],
            [
                "TransC (unif)",
                "TransC (bern)"
            ],
            [
                "YAGO39K",
                "TransE",
                "TransH",
                "TransR",
                "TransD",
                "HolE",
                "DistMult",
                "ComplEx",
                "TransC (unif)",
                "TransC (bern)"
            ],
            null,
            null,
            [
                "TransC (unif)",
                "TransC (bern)"
            ],
            [
                "YAGO39K",
                "TransC (unif)",
                "TransC (bern)"
            ],
            null,
            null,
            [
                "M-YAGO39K",
                "TransC (unif)",
                "TransC (bern)"
            ],
            null,
            null,
            [
                "TransC (unif)",
                "TransC (bern)",
                "YAGO39K",
                "M-YAGO39K"
            ],
            [
                "TransC (unif)",
                "TransC (bern)"
            ]
        ],
        "n_sentence": 17.0,
        "table_id": "table_3",
        "paper_id": "D18-1222",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1225table_4",
        "description": "Temporal scoping of facts:. We report the rank of correct time instance of the triple. If the triple scope is an interval of time, we consider the lowest rank that corresponds to the time within that interval. The ranks are reported in table 4 for both the datasets. The results depict the effectiveness of TDNS.",
        "sentences": [
            "Temporal scoping of facts.",
            "We report the rank of correct time instance of the triple.",
            "If the triple scope is an interval of time, we consider the lowest rank that corresponds to the time within that interval.",
            "The ranks are reported in table 4 for both the datasets.",
            "The results depict the effectiveness of TDNS."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "YAGO11K",
                "Wikidata12k"
            ],
            [
                "TDNS (Equation 2)"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_4",
        "paper_id": "D18-1225",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1227table_2",
        "description": "5.3 Comparison Results. Table 2 shows the entity linking performance of different methods on Yelp-EL. Here, all three types of features described in Section 4 are fed into LinkYelp. Within the compared methods, LinkYelp performs substantially better. This shows that methods carefully designed for traditional entity linking problems may not work so well when applied to entity linking within a social media platform, and this new problem we propose is worth studying differently from the traditional entity linking problem. The accuracy of DirectLink means that many mentions (about 67%) in Yelp-EL simply refer to the corresponding reviewed businesses.\". However, this does not mean that our problem is less challenging than traditional entity linking, since simply using the popularity measure of entities can achieve an accuracy of about 82% in the latter task (Pan et al., 2015).",
        "sentences": [
            "5.3 Comparison Results.",
            "Table 2 shows the entity linking performance of different methods on Yelp-EL.",
            "Here, all three types of features described in Section 4 are fed into LinkYelp.",
            "Within the compared methods, LinkYelp performs substantially better.",
            "This shows that methods carefully designed for traditional entity linking problems may not work so well when applied to entity linking within a social media platform, and this new problem we propose is worth studying differently from the traditional entity linking problem.",
            "The accuracy of DirectLink means that many mentions (about 67%) in Yelp-EL simply refer to the corresponding reviewed businesses.\".",
            "However, this does not mean that our problem is less challenging than traditional entity linking, since simply using the popularity measure of entities can achieve an accuracy of about 82% in the latter task (Pan et al., 2015)."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "DirectLink",
                "ELT",
                "SSRegu",
                "LinkYelp"
            ],
            null,
            [
                "LinkYelp",
                "Accuracy (mean\u00b1std)",
                "DirectLink",
                "ELT",
                "SSRegu"
            ],
            [
                "DirectLink",
                "ELT",
                "SSRegu"
            ],
            [
                "DirectLink"
            ],
            null
        ],
        "n_sentence": 7.0,
        "table_id": "table_2",
        "paper_id": "D18-1227",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1230table_2",
        "description": "5.3 NER Performance Comparison. We present F1, precision, and recall scores on all datasets in Table 2 and Table 3. From both tables, one can find the AutoNER achieves the best performance when there is no extra human effort. Fuzzy-LSTM-CRF does have some improvements over the Dictionary Match, but it is always worse than AutoNER. Even though SwellShark is designed for the biomedical domain and utilizes much more expert effort, AutoNER outperforms it in almost all cases. The only outlier happens on the NCBIdisease dataset when the entity span matcher in SwellShark is carefully tuned by experts for many special cases. It is worth mentioning that AutoNER beats Distant-LSTM-CRF, which is the previous stateof-the-art distantly supervised model on the LaptopReview dataset. Moreover, AutoNER\u00e2\u20ac\u2122s performance is competitive to the supervised benchmarks. For example, on the BC5CDR dataset, its F1 score is only 2.16% away from the supervised benchmark.",
        "sentences": [
            "5.3 NER Performance Comparison.",
            "We present F1, precision, and recall scores on all datasets in Table 2 and Table 3.",
            "From both tables, one can find the AutoNER achieves the best performance when there is no extra human effort.",
            "Fuzzy-LSTM-CRF does have some improvements over the Dictionary Match, but it is always worse than AutoNER.",
            "Even though SwellShark is designed for the biomedical domain and utilizes much more expert effort, AutoNER outperforms it in almost all cases.",
            "The only outlier happens on the NCBIdisease dataset when the entity span matcher in SwellShark is carefully tuned by experts for many special cases.",
            "It is worth mentioning that AutoNER beats Distant-LSTM-CRF, which is the previous stateof-the-art distantly supervised model on the LaptopReview dataset.",
            "Moreover, AutoNER\u00e2\u20ac\u2122s performance is competitive to the supervised benchmarks.",
            "For example, on the BC5CDR dataset, its F1 score is only 2.16% away from the supervised benchmark."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Pre",
                "Rec",
                "F1",
                "BC5CDR",
                "NCBI-Disease"
            ],
            [
                "AutoNER",
                "Human Effort other than Dictionary",
                "None"
            ],
            [
                "Fuzzy-LSTM-CRF",
                "AutoNER",
                "Dictionary Match"
            ],
            [
                "AutoNER",
                "SwellShark"
            ],
            [
                "SwellShark",
                "Regex Design + Special Case Tuning",
                "NCBI-Disease"
            ],
            [
                "AutoNER"
            ],
            [
                "AutoNER",
                "Supervised Benchmark"
            ],
            [
                "AutoNER",
                "Supervised Benchmark",
                "BC5CDR",
                "F1"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_2",
        "paper_id": "D18-1230",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1231table_3",
        "description": "4.2 Coarse Entity Typing. In Table 3 we study entity typing for the coarse types on three datasets. We focus on three types that are shared among the datasets: PER, LOC, ORG. In coarse-entity typing, the best available systems are heavily supervised. In this evaluation, we use gold mention spans; i.e., we force the decoding algorithm of the supervised systems to select the best of the three classes for each gold mention. As expected, the supervised systems have strong in-domain performance. However, they suffer a significant drop when evaluated in a different domain. Our system, while not trained on any supervised data, achieves better or comparable performance compared to other supervised baselines in the out-of-domain evaluations.",
        "sentences": [
            "4.2 Coarse Entity Typing.",
            "In Table 3 we study entity typing for the coarse types on three datasets.",
            "We focus on three types that are shared among the datasets: PER, LOC, ORG.",
            "In coarse-entity typing, the best available systems are heavily supervised.",
            "In this evaluation, we use gold mention spans; i.e., we force the decoding algorithm of the supervised systems to select the best of the three classes for each gold mention.",
            "As expected, the supervised systems have strong in-domain performance.",
            "However, they suffer a significant drop when evaluated in a different domain.",
            "Our system, while not trained on any supervised data, achieves better or comparable performance compared to other supervised baselines in the out-of-domain evaluations."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "OntoNotes",
                "CoNLL",
                "MUC"
            ],
            [
                "PER",
                "LOC",
                "ORG"
            ],
            [
                "COGCOMPNLP"
            ],
            [
                "COGCOMPNLP"
            ],
            [
                "COGCOMPNLP",
                "OntoNotes",
                "CoNLL"
            ],
            [
                "COGCOMPNLP",
                "OntoNotes",
                "CoNLL",
                "MUC"
            ],
            [
                "ZOE (ours)",
                "Trained on",
                "\u00d7",
                "COGCOMPNLP"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_3",
        "paper_id": "D18-1231",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1231table_6",
        "description": "4.5 Ablation Study. We carry out ablation studies that quantify the contribution of surface information (\u0e22\u0e073.3) and context information (\u0e22\u0e073.2). As Table 6 shows, both factors are crucial and complementary for the system. However, the contextual information seems to have a bigger role overall. We complement our qualitative analysis with the quantitative share of each component. In 69.3%, 54.6%, and 69.7% of mentions, our system uses the context information (and ignores the surface), in FIGER, BBN, and OntoNotesfine datasets, respectively, underscoring the importance of contextual information.",
        "sentences": [
            "4.5 Ablation Study.",
            "We carry out ablation studies that quantify the contribution of surface information (\u0e22\u0e073.3) and context information (\u0e22\u0e073.2).",
            "As Table 6 shows, both factors are crucial and complementary for the system.",
            "However, the contextual information seems to have a bigger role overall.",
            "We complement our qualitative analysis with the quantitative share of each component.",
            "In 69.3%, 54.6%, and 69.7% of mentions, our system uses the context information (and ignores the surface), in FIGER, BBN, and OntoNotesfine datasets, respectively, underscoring the importance of contextual information."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "ZOE (ours)",
                "no surface-based concepts",
                "no context-based concepts"
            ],
            [
                "no context-based concepts"
            ],
            null,
            [
                "ZOE (ours)",
                "no surface-based concepts",
                "no context-based concepts",
                "FIGER",
                "BBN",
                "OntoNotesfine"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_6",
        "paper_id": "D18-1231",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1233table_3",
        "description": "Results. Our results, shown in Table 3 indicate that systems that return contiguous spans from the rule text perform better according to our BLEU metric. We speculate that the logical forms in the data are challenging for existing models to extract and manipulate, which may suggest why the explicit rule-based system performed best. We further note that only the rule-based and NMT-Copy models are capable of generating genuine questions rather than spans or sentences.",
        "sentences": [
            "Results.",
            "Our results, shown in Table 3 indicate that systems that return contiguous spans from the rule text perform better according to our BLEU metric.",
            "We speculate that the logical forms in the data are challenging for existing models to extract and manipulate, which may suggest why the explicit rule-based system performed best.",
            "We further note that only the rule-based and NMT-Copy models are capable of generating genuine questions rather than spans or sentences."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "BiDAF",
                "Rule-based",
                "BLEU-1",
                "BLEU-2",
                "BLEU-3",
                "BLEU-4"
            ],
            [
                "Rule-based"
            ],
            [
                "NMT-Copy",
                "Rule-based"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "D18-1233",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1233table_4",
        "description": "Results. Table 4 shows the result of our baseline models on the entailment corpus of ShARC test set. Results show poor performance especially for the macro accuracy metric of both simple baselines and neural state-of-the-art entailment models. This performance highlights the challenges that the scenario interpretation task of ShARC presents, many of which are discussed in Section 4.2.2.",
        "sentences": [
            "Results.",
            "Table 4 shows the result of our baseline models on the entailment corpus of ShARC test set.",
            "Results show poor performance especially for the macro accuracy metric of both simple baselines and neural state-of-the-art entailment models.",
            "This performance highlights the challenges that the scenario interpretation task of ShARC presents, many of which are discussed in Section 4.2.2."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Random",
                "Surface LR",
                "DAM (SNLI)",
                "DAM (ShARC)"
            ],
            [
                "Random",
                "Surface LR",
                "DAM (SNLI)",
                "DAM (ShARC)",
                "Macro Acc."
            ],
            [
                "Random",
                "Surface LR",
                "DAM (SNLI)",
                "DAM (ShARC)",
                "Macro Acc."
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_4",
        "paper_id": "D18-1233",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1235table_2",
        "description": "4.3.2 Different loss functions with multi-answer. Table 2 shows the experimental results with three different multi-answer loss functions introduced in Section 3.5.1. All of them offer improvement over the single-answer baseline, which shows the effectiveness of utilizing multiple answers. The average loss performs better than the min loss, which suggests that forcing the model to predict all possible answers is better than asking it to just find the easiest one. Not surprisingly, by taking into account the quality of different answer spans, the weighted average loss outperforms the average loss and achieves the best result among the three. All later experiments are conducted based on the weighted average loss.",
        "sentences": [
            "4.3.2 Different loss functions with multi-answer.",
            "Table 2 shows the experimental results with three different multi-answer loss functions introduced in Section 3.5.1.",
            "All of them offer improvement over the single-answer baseline, which shows the effectiveness of utilizing multiple answers.",
            "The average loss performs better than the min loss, which suggests that forcing the model to predict all possible answers is better than asking it to just find the easiest one.",
            "Not surprisingly, by taking into account the quality of different answer spans, the weighted average loss outperforms the average loss and achieves the best result among the three.",
            "All later experiments are conducted based on the weighted average loss."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Lmin",
                "Lavg",
                "Lwavg"
            ],
            [
                "ROUGE-L",
                "single answer",
                "Lmin",
                "Lavg",
                "Lwavg"
            ],
            [
                "ROUGE-L",
                "Lmin",
                "Lavg"
            ],
            [
                "ROUGE-L",
                "Lmin",
                "Lavg",
                "Lwavg"
            ],
            [
                "Lwavg"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_2",
        "paper_id": "D18-1235",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1235table_4",
        "description": "4.3.4 Comparison with State-of-the-art. Table 4 shows the performance of our model and other state-of-the-art models on the DuReader test set. First, we compare our passage extraction method with the paragraph ranking model from Wang et al. (2018b). Based on the same BiDAF model described in Section 3.4, our method (PE+BiDAF) significantly outperforms the trained model from Wang et al. (2018b) (PR+BiDAF) on the DuReader test set. As we can see, our complete model achieves the state-of-the-art performance in both ROUGE-L and BLEU-4, and greatly narrows the performance gap between MRC system and human in the challenging realworld setting.",
        "sentences": [
            "4.3.4 Comparison with State-of-the-art.",
            "Table 4 shows the performance of our model and other state-of-the-art models on the DuReader test set.",
            "First, we compare our passage extraction method with the paragraph ranking model from Wang et al. (2018b).",
            "Based on the same BiDAF model described in Section 3.4, our method (PE+BiDAF) significantly outperforms the trained model from Wang et al. (2018b) (PR+BiDAF) on the DuReader test set.",
            "As we can see, our complete model achieves the state-of-the-art performance in both ROUGE-L and BLEU-4, and greatly narrows the performance gap between MRC system and human in the challenging realworld setting."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "BiDAF (He et al. 2017)",
                "Match-LSTM (He et al. 2017)",
                "PR+BiDAF (Wang et al. 2018b)",
                "PE+BiDAF (ours)",
                "V-Net (Wang et al. 2018b)",
                "Our complete model"
            ],
            null,
            [
                "PE+BiDAF (ours)",
                "PR+BiDAF (Wang et al. 2018b)"
            ],
            [
                "Our complete model",
                "Human",
                "ROUGE-L",
                "BLEU-4"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_4",
        "paper_id": "D18-1235",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1239table_1",
        "description": "Short Questions Performance:. Table 1 shows that our model perfectly answers all test questions, demonstrating that it can learn challenging semantic operators and induce parse trees from end task supervision. Performance drops when using external parser, showing that our model learns an effective syntactic model for this domain. The RELATION NETWORK also achieves good performance, particularly on questions involving relations. LSTM baselines work well on questions not involving relations.",
        "sentences": [
            "Short Questions Performance:.",
            "Table 1 shows that our model perfectly answers all test questions, demonstrating that it can learn challenging semantic operators and induce parse trees from end task supervision.",
            "Performance drops when using external parser, showing that our model learns an effective syntactic model for this domain.",
            "The RELATION NETWORK also achieves good performance, particularly on questions involving relations.",
            "LSTM baselines work well on questions not involving relations."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Our Model (Pre-parsed)",
                "Our Model",
                "Boolean Questions",
                "Entity Set Questions",
                "Relation Questions"
            ],
            [
                "Our Model (Pre-parsed)",
                "Our Model"
            ],
            [
                "RELATION NETWORK",
                "Relation Questions"
            ],
            [
                "LSTM",
                "Relation Questions"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_1",
        "paper_id": "D18-1239",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1239table_3",
        "description": "Performance on Human-generated Language:. Table 3 shows the performance of our model on complex human-generated queries in GENX. Our approach outperforms strong LSTM and semantic parsing baselines, despite the semantic parser\u00e2\u20ac\u2122s use of hard-coded operators. These results suggest that our method represents an attractive middle ground between minimally structured and highly structured approaches to interpretation. Our model learns to interpret operators such as except that were not considered during development. This shows that our model can learn to parse human language, which contains greater lexical and structural diversity than synthetic questions. Trees induced by the model are linguistically plausible (see Appendix D).",
        "sentences": [
            "Performance on Human-generated Language:.",
            "Table 3 shows the performance of our model on complex human-generated queries in GENX.",
            "Our approach outperforms strong LSTM and semantic parsing baselines, despite the semantic parser\u00e2\u20ac\u2122s use of hard-coded operators.",
            "These results suggest that our method represents an attractive middle ground between minimally structured and highly structured approaches to interpretation.",
            "Our model learns to interpret operators such as except that were not considered during development.",
            "This shows that our model can learn to parse human language, which contains greater lexical and structural diversity than synthetic questions.",
            "Trees induced by the model are linguistically plausible (see Appendix D)."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "Our Model"
            ],
            [
                "Our Model",
                "LSTM",
                "SEMPRE"
            ],
            [
                "Our Model"
            ],
            [
                "Our Model"
            ],
            [
                "Our Model"
            ],
            [
                "Our Model"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_3",
        "paper_id": "D18-1239",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1240table_7",
        "description": "Semeval. Table 7 compares performance of Bcr + Ecr + V + E + SST system on Semeval to that of KeLP and ConvKN, the two top systems in the SemEval 2016 competition, and also to the performance of the recent DNN-based HyperQA and AI-CNN systems. In the Semeval 2016 competition, our model would have been the first, with #1 KeLP system being 0.6 MAP points behind. Then, it would have outperformed the state-of-theart AI-CNN system by 0.35 MAP points.",
        "sentences": [
            "Semeval.",
            "Table 7 compares performance of Bcr + Ecr + V + E + SST system on Semeval to that of KeLP and ConvKN, the two top systems in the SemEval 2016 competition, and also to the performance of the recent DNN-based HyperQA and AI-CNN systems.",
            "In the Semeval 2016 competition, our model would have been the first, with #1 KeLP system being 0.6 MAP points behind.",
            "Then, it would have outperformed the state-of-theart AI-CNN system by 0.35 MAP points."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Kelp [#1] (Filice et al. 2016)",
                "Conv-KN [#2] (Barron-Cedeno et al. 2016)",
                "CTKC +VQF (Tymoshenko et al. 2016b)",
                "HyperQA (Tay et al. 2018)",
                "AI-CNN (Zhang et al. 2017)",
                "Our model (V+Bcr+Ecr+E+SST)"
            ],
            [
                "Our model (V+Bcr+Ecr+E+SST)",
                "Kelp [#1] (Filice et al. 2016)",
                "MAP"
            ],
            [
                "Our model (V+Bcr+Ecr+E+SST)",
                "AI-CNN (Zhang et al. 2017)",
                "MAP"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_7",
        "paper_id": "D18-1240",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1244table_1",
        "description": "5.3 Results on the TACRED Dataset. We present our main results on the TACRED test set in Table 1. We observe that our GCN model outperforms all dependency-based models by at least 1.6 F1. By using contextualized word representations, the C-GCN model further outperforms the strong PA-LSTM model by 1.3 F1, and achieves a new state of the art. In addition, we find our model improves upon other dependency based models in both precision and recall. Comparing the C-GCN model with the GCN model, we find that the gain mainly comes from improved recall. We hypothesize that this is because the CGCN is more robust to parse errors by capturing local word patterns (see also Section 6.2). As we will show in Section 6.2, we find that our GCN models have complementary strengths when compared to the PA-LSTM. To leverage this result, we experiment with a simple interpolation strategy to combine these models. Given the output probabilities PG(r|x) from a GCN model and PS(r|x) from the sequence model for any relation r, we calculate the interpolated probability as P(r|x) = \u03b1 \u00b7 PG(r|x) + (1 - \u03b1) \u00b7 PS(r|x) where \u03b1 \u2208 [0, 1] is chosen on the dev set and set to 0.6. This simple interpolation between a GCN and a PA-LSTM achieves an F1 score of 67.1, outperforming each model alone by at least 2.0 F1. An interpolation between a C-GCN and a PA-LSTM further improves the result to 68.2. This complementary performance explains the gain we see in Table 1 when the two models are combined.",
        "sentences": [
            "5.3 Results on the TACRED Dataset.",
            "We present our main results on the TACRED test set in Table 1.",
            "We observe that our GCN model outperforms all dependency-based models by at least 1.6 F1.",
            "By using contextualized word representations, the C-GCN model further outperforms the strong PA-LSTM model by 1.3 F1, and achieves a new state of the art.",
            "In addition, we find our model improves upon other dependency based models in both precision and recall.",
            "Comparing the C-GCN model with the GCN model, we find that the gain mainly comes from improved recall.",
            "We hypothesize that this is because the CGCN is more robust to parse errors by capturing local word patterns (see also Section 6.2).",
            "As we will show in Section 6.2, we find that our GCN models have complementary strengths when compared to the PA-LSTM.",
            "To leverage this result, we experiment with a simple interpolation strategy to combine these models.",
            "Given the output probabilities PG(r|x) from a GCN model and PS(r|x) from the sequence model for any relation r, we calculate the interpolated probability as P(r|x) = \u03b1 \u00b7 PG(r|x) + (1 - \u03b1) \u00b7 PS(r|x) where \u03b1 \u2208 [0, 1] is chosen on the dev set and set to 0.6.",
            "This simple interpolation between a GCN and a PA-LSTM achieves an F1 score of 67.1, outperforming each model alone by at least 2.0 F1.",
            "An interpolation between a C-GCN and a PA-LSTM further improves the result to 68.2.",
            "This complementary performance explains the gain we see in Table 1 when the two models are combined."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            1,
            2,
            2,
            2,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "GCN",
                "C-GCN",
                "GCN + PA-LSTM",
                "C-GCN + PA-LSTM"
            ],
            [
                "GCN",
                "LR (Zhang+2017)",
                "SDP-LSTM (Xu+2015b)",
                "Tree-LSTM (Tai+2015)",
                "F1"
            ],
            [
                "C-GCN",
                "PA-LSTM (Zhang+2017)",
                "F1"
            ],
            [
                "C-GCN",
                "LR (Zhang+2017)",
                "SDP-LSTM (Xu+2015b)",
                "Tree-LSTM (Tai+2015)",
                "P",
                "R"
            ],
            [
                "GCN",
                "C-GCN",
                "R"
            ],
            [
                "C-GCN"
            ],
            [
                "GCN",
                "PA-LSTM (Zhang+2017)"
            ],
            [
                "GCN",
                "PA-LSTM (Zhang+2017)"
            ],
            [
                "GCN"
            ],
            [
                "GCN + PA-LSTM",
                "F1",
                "PA-LSTM (Zhang+2017)",
                "GCN"
            ],
            [
                "C-GCN + PA-LSTM",
                "F1"
            ],
            [
                "GCN + PA-LSTM",
                "C-GCN + PA-LSTM"
            ]
        ],
        "n_sentence": 13.0,
        "table_id": "table_1",
        "paper_id": "D18-1244",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1259table_4",
        "description": "The performance of our model on the benchmark settings is reported in Table 4, where all numbers are obtained with strong supervision over supporting facts. From the distractor setting to the full wiki setting, expanding the scope of the context increases the difficulty of question answering. The performance in the full wiki setting is substantially lower, which poses a challenge to existing techniques on retrieval-based question answering. Overall, model performance in all settings is significantly lower than human performance as shown in Section 5.3, which indicates that more technical advancements are needed in future work. We also investigate the explainability of our model by measuring supporting fact prediction performance. Our model achieves 60+ supporting fact prediction F1 and \u00e2\u02c6\u00bc40 joint F1, which indicates there is room for further improvement in terms of explainability.",
        "sentences": [
            "The performance of our model on the benchmark settings is reported in Table 4, where all numbers are obtained with strong supervision over supporting facts.",
            "From the distractor setting to the full wiki setting, expanding the scope of the context increases the difficulty of question answering.",
            "The performance in the full wiki setting is substantially lower, which poses a challenge to existing techniques on retrieval-based question answering.",
            "Overall, model performance in all settings is significantly lower than human performance as shown in Section 5.3, which indicates that more technical advancements are needed in future work.",
            "We also investigate the explainability of our model by measuring supporting fact prediction performance.",
            "Our model achieves 60+ supporting fact prediction F1 and \u00e2\u02c6\u00bc40 joint F1, which indicates there is room for further improvement in terms of explainability."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "distractor",
                "full wiki",
                "Answer"
            ],
            [
                "full wiki",
                "Answer",
                "Sup Fact",
                "Joint",
                "EM",
                "F1"
            ],
            [
                "distractor",
                "full wiki"
            ],
            [
                "Sup Fact"
            ],
            [
                "Sup Fact",
                "F1",
                "Joint",
                "distractor",
                "full wiki"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_4",
        "paper_id": "D18-1259",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1259table_9",
        "description": "Table 9 shows the comparison between train-medium split and hard examples like dev and test under retrieval metrics in full wiki setting. As we can see, the performance gap between trainmedium split and its dev/test is close, which implies that train-medium split has a similar level of difficulty as hard examples under the full wiki setting in which a retrieval model is necessary as the first processing step.",
        "sentences": [
            "Table 9 shows the comparison between train-medium split and hard examples like dev and test under retrieval metrics in full wiki setting.",
            "As we can see, the performance gap between trainmedium split and its dev/test is close, which implies that train-medium split has a similar level of difficulty as hard examples under the full wiki setting in which a retrieval model is necessary as the first processing step."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "train-medium",
                "dev",
                "test",
                "MAP",
                "Mean Rank",
                "CorAns Rank"
            ],
            [
                "train-medium",
                "dev",
                "test"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_9",
        "paper_id": "D18-1259",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1262table_3",
        "description": "Table 3 presents the results on English out-of-domain test set. Our models outperform the highest records achieved by He et al. (2018), with absolute improvements of 0.2-0.5% in F1 scores. These favorable results on both in-domain and outof-domain data demonstrate the effectiveness and robustness of our proposed unified framework.",
        "sentences": [
            "Table 3 presents the results on English out-of-domain test set.",
            "Our models outperform the highest records achieved by He et al. (2018), with absolute improvements of 0.2-0.5% in F1 scores.",
            "These favorable results on both in-domain and outof-domain data demonstrate the effectiveness and robustness of our proposed unified framework."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Ours (Syn-GCN)",
                "Ours (SA-LSTM)",
                "Ours (Tree-LSTM)",
                "F1",
                "He et al. (2018)"
            ],
            [
                "Ours (Syn-GCN)",
                "Ours (SA-LSTM)",
                "Ours (Tree-LSTM)"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_3",
        "paper_id": "D18-1262",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1262table_6",
        "description": "To further investigate the impact of deep encoder, we perform our Syn-GCN, SA-LSTM and Tree-LSTM models with another alternative configuration, using the same encoder as (Marcheggiani and Titov, 2017) (M&T encoder for short), which removes the residual connections from our framework. The corresponding results of our models are also summarized in Table 6 for comparison. Note that the first row is the results of our syntax-agnostic model. Surprisingly, we observe a dramatical performance decline of 1.2% F1 for our Syn-GCN model with M&T encoder. A less significant performance loss for our SALSTM (\u22120.4%) and Tree-LSTM (\u22120.5%) models shows that the Syn-GCN is more sensitive to contextual information. Nevertheless, the overall results show that applying deep encoder could receive higher gains.",
        "sentences": [
            "To further investigate the impact of deep encoder, we perform our Syn-GCN, SA-LSTM and Tree-LSTM models with another alternative configuration, using the same encoder as (Marcheggiani and Titov, 2017) (M&T encoder for short), which removes the residual connections from our framework.",
            "The corresponding results of our models are also summarized in Table 6 for comparison.",
            "Note that the first row is the results of our syntax-agnostic model.",
            "Surprisingly, we observe a dramatical performance decline of 1.2% F1 for our Syn-GCN model with M&T encoder.",
            "A less significant performance loss for our SALSTM (\u22120.4%) and Tree-LSTM (\u22120.5%) models shows that the Syn-GCN is more sensitive to contextual information.",
            "Nevertheless, the overall results show that applying deep encoder could receive higher gains."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "Syn-GCN",
                "SA-LSTM",
                "Tree-LSTM"
            ],
            [
                "Syn-GCN",
                "SA-LSTM",
                "Tree-LSTM"
            ],
            [
                "Baseline (syntax-agnostic)"
            ],
            [
                "Syn-GCN (M&T encoder)",
                "Syn-GCN",
                "F1"
            ],
            [
                "SA-LSTM",
                "Tree-LSTM",
                "SA-LSTM (M&T encoder)",
                "Tree-LSTM (M&T encoder)",
                "F1"
            ],
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_6",
        "paper_id": "D18-1262",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1262table_7",
        "description": "Comparison and Discussion. Table 7 presents the comprehensive results of our Syn-GCN model on the four syntactic inputs aforementioned of different quality together with previous SRL models. A number of observations can be made from these results. First, our model gives quite stable SRL performance no matter the syntactic input quality varies in a broad range, obtaining overall higher scores compared to previous state-of-the-arts. Second, It is interesting to note that the Sem-F1/LAS score of our model becomes relatively smaller as the syntactic input becomes better. Though not so surprised, these results show that our SRL component is even relatively stronger. Third, when we adopt a syntactic parser with higher parsing accuracy, our SRL system will achieve a better performance. Notably, our model yields a Sem-F1 of 90.5% taking gold syntax as input. It suggests that high-quality syntactic parse may indeed enhance SRL, which is consistent with the conclusion in (He et al., 2017).",
        "sentences": [
            "Comparison and Discussion.",
            "Table 7 presents the comprehensive results of our Syn-GCN model on the four syntactic inputs aforementioned of different quality together with previous SRL models.",
            "A number of observations can be made from these results.",
            "First, our model gives quite stable SRL performance no matter the syntactic input quality varies in a broad range, obtaining overall higher scores compared to previous state-of-the-arts.",
            "Second, It is interesting to note that the Sem-F1/LAS score of our model becomes relatively smaller as the syntactic input becomes better.",
            "Though not so surprised, these results show that our SRL component is even relatively stronger.",
            "Third, when we adopt a syntactic parser with higher parsing accuracy, our SRL system will achieve a better performance.",
            "Notably, our model yields a Sem-F1 of 90.5% taking gold syntax as input.",
            "It suggests that high-quality syntactic parse may indeed enhance SRL, which is consistent with the conclusion in (He et al., 2017)."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            1,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Our Syn-GCN (CoNLL-2009 predicted)",
                "Our Syn-GCN (Biaffine Parser)",
                "Our Syn-GCN (BIST Parser)",
                "Our Syn-GCN (Gold syntax)"
            ],
            null,
            [
                "Our Syn-GCN (CoNLL-2009 predicted)",
                "Our Syn-GCN (Biaffine Parser)",
                "Our Syn-GCN (BIST Parser)",
                "Our Syn-GCN (Gold syntax)",
                "P",
                "R",
                "Sem-F1"
            ],
            [
                "Our Syn-GCN (CoNLL-2009 predicted)",
                "Our Syn-GCN (Biaffine Parser)",
                "Our Syn-GCN (BIST Parser)",
                "Our Syn-GCN (Gold syntax)",
                "Sem-F1/LAS"
            ],
            null,
            [
                "Our Syn-GCN (CoNLL-2009 predicted)",
                "Our Syn-GCN (Biaffine Parser)",
                "Our Syn-GCN (BIST Parser)",
                "Our Syn-GCN (Gold syntax)",
                "P",
                "R",
                "Sem-F1"
            ],
            [
                "Our Syn-GCN (Gold syntax)",
                "Sem-F1"
            ],
            null
        ],
        "n_sentence": 9.0,
        "table_id": "table_7",
        "paper_id": "D18-1262",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1263table_3",
        "description": "DFS order matters. Table 3 depicts our model's performance when linearizing the graphs according to the different traversal orders discussed and exemplified in Table 2. Overall, we find that the \u201csmaller-first\u201d approach performs best across all datasets, and that imposing one of our orders is always preferable over random permutations. Intuitively, the \u201csmaller-first\u201d approach presents shorter, and likely easier, paths first, thus minimizing the amount of error-propagation for following decoding steps.",
        "sentences": [
            "DFS order matters.",
            "Table 3 depicts our model's performance when linearizing the graphs according to the different traversal orders discussed and exemplified in Table 2.",
            "Overall, we find that the \u201csmaller-first\u201d approach performs best across all datasets, and that imposing one of our orders is always preferable over random permutations.",
            "Intuitively, the \u201csmaller-first\u201d approach presents shorter, and likely easier, paths first, thus minimizing the amount of error-propagation for following decoding steps."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Random",
                "Sentence order",
                "Closest words",
                "Smaller-first"
            ],
            [
                "Smaller-first",
                "Random"
            ],
            [
                "Smaller-first"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "D18-1263",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1263table_4",
        "description": "From English to SDP. Table 4 presents the performance of our complete model (\u201cMTL PRIMARY+AUX\u201d) versus Peng et al. (2017a). On average, our model performs within 1% F1 point from the state-of-the art (outperforming it on the harder PSD task), despite using the more general sequence-to-sequence approach instead of a dedicated graph-parsing algorithm. In addition, an ablation study shows that multi-tasking the PRIMARY tasks is beneficial over a single task setting, which in turn is outperformed by the inclusion of the AUXILIARY tasks.",
        "sentences": [
            "From English to SDP.",
            "Table 4 presents the performance of our complete model (\u201cMTL PRIMARY+AUX\u201d) versus Peng et al. (2017a).",
            "On average, our model performs within 1% F1 point from the state-of-the art (outperforming it on the harder PSD task), despite using the more general sequence-to-sequence approach instead of a dedicated graph-parsing algorithm.",
            "In addition, an ablation study shows that multi-tasking the PRIMARY tasks is beneficial over a single task setting, which in turn is outperformed by the inclusion of the AUXILIARY tasks."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "MTL PRIMARY+AUX",
                "Peng et al. (2017a)"
            ],
            [
                "Peng et al. (2017a)",
                "MTL PRIMARY+AUX",
                "Avg.",
                "PSD"
            ],
            [
                "Single",
                "MTL PRIMARY",
                "MTL PRIMARY+AUX"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_4",
        "paper_id": "D18-1263",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1263table_5",
        "description": "Simulating disjoint annotations. In contrast with SDP's complete overlap of annotated sentences, multi-task learning often deals with disjoint training data. To simulate such scenario, we retrained the models on a randomly selected set of 33% of the train sentences for each representation (11, 886 sentences), such that the three representations overlap on only 10% (3, 565 sentences). The results in Table 5 show that our approach is more resilient to the decrease in annotation overlap, outperforming the state-of-the-art model on the DM and PSD task, as well as on the average score. We hypothesize that this is in part thanks to our ability to use the inter-task translations, even when these exist only for part of the annotations.",
        "sentences": [
            "Simulating disjoint annotations.",
            "In contrast with SDP's complete overlap of annotated sentences, multi-task learning often deals with disjoint training data.",
            "To simulate such scenario, we retrained the models on a randomly selected set of 33% of the train sentences for each representation (11, 886 sentences), such that the three representations overlap on only 10% (3, 565 sentences).",
            "The results in Table 5 show that our approach is more resilient to the decrease in annotation overlap, outperforming the state-of-the-art model on the DM and PSD task, as well as on the average score.",
            "We hypothesize that this is in part thanks to our ability to use the inter-task translations, even when these exist only for part of the annotations."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "Peng et al. (2017a)",
                "MTL PRIMARY+AUX"
            ],
            [
                "MTL PRIMARY+AUX",
                "Peng et al. (2017a)",
                "DM",
                "PSD",
                "Avg."
            ],
            [
                "MTL PRIMARY+AUX"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_5",
        "paper_id": "D18-1263",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1264table_3",
        "description": "Intrinsic Evaluation. Table 3 shows the intrinsic evaluation results, in which our alignment intrinsically outperforms JAMR aligner by achieving better alignment F1 score and leading to a higher scored oracle parser.",
        "sentences": [
            "Intrinsic Evaluation.",
            "Table 3 shows the intrinsic evaluation results, in which our alignment intrinsically outperforms JAMR aligner by achieving better alignment F1 score and leading to a higher scored oracle parser."
        ],
        "class_sentence": [
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "Our",
                "JAMR",
                "Alignment F1 (on hand-align)"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_3",
        "paper_id": "D18-1264",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1264table_4",
        "description": "Extrinsic Evaluation. Table 4 shows the results. From this table, we can see that our alignment consistently improves all the parsers by a margin ranging from 0.5 to 1.7. Both the intrinsic and the extrinsic evaluations show the effectiveness our aligner.",
        "sentences": [
            "Extrinsic Evaluation.",
            "Table 4 shows the results.",
            "From this table, we can see that our alignment consistently improves all the parsers by a margin ranging from 0.5 to 1.7.",
            "Both the intrinsic and the extrinsic evaluations show the effectiveness our aligner."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "JAMR parser: Word POS NER DEP",
                "CAMR parser: Word POS NER DEP",
                "+ JAMR aligner",
                "+ Our aligner",
                "all"
            ],
            [
                "+ Our aligner"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_4",
        "paper_id": "D18-1264",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1268table_3",
        "description": "Table 3 and Table 4 summarize the results of all the methods on the LEX-C dataset. Several points may be worth noticing. Firstly, the performance scores on LEX-C are not necessarily consistent with those on LEX-Z (Table 2) even if the methods and the language pairs are the same; this is not surprising as the two datasets differ in query words, word embedding quality, and training-set sizes. Secondly, the performance gap between the best supervised methods and the best unsupervised methods in both Table 3 and Table 4 are larger than that in Table 2. This is attributed to the large amount of good-quality supervision in LEX-C (5,000 human-annotated word pairs) and the larger candidate size in WE-C (200, 000 candidates). Thirdly, the average performance in Table 3 is lower than that in Table 4, indicating that the language pairs in the former are more difficult than that in the latter. Nevertheless, we can see that our method has much stronger performance than other unsupervised methods in Table 3, i.e., on the harder language pairs, and that it performed comparably with the model by Conneau et al. (2017) in Table 4 on the easier language pairs. Combining all these observations, we see that our method is highly robust for various language pairs and under different training conditions.",
        "sentences": [
            "Table 3 and Table 4 summarize the results of all the methods on the LEX-C dataset.",
            "Several points may be worth noticing.",
            "Firstly, the performance scores on LEX-C are not necessarily consistent with those on LEX-Z (Table 2) even if the methods and the language pairs are the same; this is not surprising as the two datasets differ in query words, word embedding quality, and training-set sizes.",
            "Secondly, the performance gap between the best supervised methods and the best unsupervised methods in both Table 3 and Table 4 are larger than that in Table 2.",
            "This is attributed to the large amount of good-quality supervision in LEX-C (5,000 human-annotated word pairs) and the larger candidate size in WE-C (200, 000 candidates).",
            "Thirdly, the average performance in Table 3 is lower than that in Table 4, indicating that the language pairs in the former are more difficult than that in the latter.",
            "Nevertheless, we can see that our method has much stronger performance than other unsupervised methods in Table 3, i.e., on the harder language pairs, and that it performed comparably with the model by Conneau et al. (2017) in Table 4 on the easier language pairs.",
            "Combining all these observations, we see that our method is highly robust for various language pairs and under different training conditions."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "Mikolov et al. (2013)",
                "Zhang et al. (2016)",
                "Xing et al. (2015)",
                "Shigeto et al. (2015)",
                "Artetxe et al. (2016)",
                "Artetxe et al. (2017)",
                "Conneau et al. (2017)",
                "Zhang et al. (2017a)",
                "Ours"
            ],
            null,
            null,
            [
                "Supervised",
                "Unsupervised"
            ],
            null,
            [
                "bg-en",
                "en-bg",
                "ca-en",
                "en-ca",
                "sv-en",
                "en-sv",
                "lv-en",
                "en-lv"
            ],
            [
                "Unsupervised",
                "Ours",
                "bg-en",
                "en-bg",
                "ca-en",
                "en-ca",
                "sv-en",
                "en-sv",
                "lv-en",
                "en-lv"
            ],
            [
                "Ours"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_3",
        "paper_id": "D18-1268",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1268table_4",
        "description": "Table 3 and Table 4 summarize the results of all the methods on the LEX-C dataset. Several points may be worth noticing. Firstly, the performance scores on LEX-C are not necessarily consistent with those on LEX-Z (Table 2) even if the methods and the language pairs are the same; this is not surprising as the two datasets differ in query words, word embedding quality, and training-set sizes. Secondly, the performance gap between the best supervised methods and the best unsupervised methods in both Table 3 and Table 4 are larger than that in Table 2. This is attributed to the large amount of good-quality supervision in LEX-C (5,000 human-annotated word pairs) and the larger candidate size in WE-C (200, 000 candidates). Thirdly, the average performance in Table 3 is lower than that in Table 4, indicating that the language pairs in the former are more difficult than that in the latter. Nevertheless, we can see that our method has much stronger performance than other unsupervised methods in Table 3, i.e., on the harder language pairs, and that it performed comparably with the model by Conneau et al. (2017) in Table 4 on the easier language pairs. Combining all these observations, we see that our method is highly robust for various language pairs and under different training conditions.",
        "sentences": [
            "Table 3 and Table 4 summarize the results of all the methods on the LEX-C dataset.",
            "Several points may be worth noticing.",
            "Firstly, the performance scores on LEX-C are not necessarily consistent with those on LEX-Z (Table 2) even if the methods and the language pairs are the same; this is not surprising as the two datasets differ in query words, word embedding quality, and training-set sizes.",
            "Secondly, the performance gap between the best supervised methods and the best unsupervised methods in both Table 3 and Table 4 are larger than that in Table 2.",
            "This is attributed to the large amount of good-quality supervision in LEX-C (5,000 human-annotated word pairs) and the larger candidate size in WE-C (200, 000 candidates).",
            "Thirdly, the average performance in Table 3 is lower than that in Table 4, indicating that the language pairs in the former are more difficult than that in the latter.",
            "Nevertheless, we can see that our method has much stronger performance than other unsupervised methods in Table 3, i.e., on the harder language pairs, and that it performed comparably with the model by Conneau et al. (2017) in Table 4 on the easier language pairs.",
            "Combining all these observations, we see that our method is highly robust for various language pairs and under different training conditions."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "Mikolov et al. (2013)",
                "Zhang et al. (2016)",
                "Xing et al. (2015)",
                "Shigeto et al. (2015)",
                "Artetxe et al. (2016)",
                "Artetxe et al. (2017)",
                "Conneau et al. (2017)",
                "Zhang et al. (2017a)",
                "Ours"
            ],
            null,
            null,
            [
                "Supervised",
                "Unsupervised"
            ],
            null,
            [
                "de-en",
                "en-de",
                "es-en",
                "en-es",
                "fr-en",
                "en-fr",
                "it-en",
                "en-it"
            ],
            [
                "Ours",
                "Conneau et al. (2017)",
                "de-en",
                "en-de",
                "es-en",
                "en-es",
                "fr-en",
                "en-fr",
                "it-en",
                "en-it"
            ],
            [
                "Ours"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_4",
        "paper_id": "D18-1268",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1268table_5",
        "description": "Table 5 summarizes the performance of all the methods in cross-lingual word similarity prediction. We can see that the unsupervised methods, including ours, perform equally well as the supervised methods, which is highly encouraging.",
        "sentences": [
            "Table 5 summarizes the performance of all the methods in cross-lingual word similarity prediction.",
            "We can see that the unsupervised methods, including ours, perform equally well as the supervised methods, which is highly encouraging."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "Mikolov et al. (2013)",
                "Zhang et al. (2016)",
                "Xing et al. (2015)",
                "Shigeto et al. (2015)",
                "Artetxe et al. (2016)",
                "Artetxe et al. (2017)",
                "Conneau et al. (2017)",
                "Zhang et al. (2017a)",
                "Ours"
            ],
            [
                "Unsupervised",
                "Supervised",
                "Ours"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_5",
        "paper_id": "D18-1268",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1270table_7",
        "description": "Is zero-shot XEL really effective?. To evaluate the effectiveness of the zero-shot XEL approach, we perform zero-shot XEL using XELMS on all datasets. Table 7 shows zero-shot XEL results on all datasets, both with and without using the prior during inference. Note that zero-shot XEL (with prior) is close to SoTA (Sil et al. (2018)) on TAC15-TEST, which also uses the prior for zeroshot XEL. However, for zero-shot XEL (without prior) performance drops by more than 20% for TAC15-Test, 2.4% for TH-Test and by 2.1% for McN-Test. This indicates that zero-shot XEL is not effective in a realistic zero-shot setting (i.e., when the prior is unavailable for inference).",
        "sentences": [
            "Is zero-shot XEL really effective?.",
            "To evaluate the effectiveness of the zero-shot XEL approach, we perform zero-shot XEL using XELMS on all datasets.",
            "Table 7 shows zero-shot XEL results on all datasets, both with and without using the prior during inference.",
            "Note that zero-shot XEL (with prior) is close to SoTA (Sil et al. (2018)) on TAC15-TEST, which also uses the prior for zeroshot XEL.",
            "However, for zero-shot XEL (without prior) performance drops by more than 20% for TAC15-Test, 2.4% for TH-Test and by 2.1% for McN-Test.",
            "This indicates that zero-shot XEL is not effective in a realistic zero-shot setting (i.e., when the prior is unavailable for inference)."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "XELMS (Z-S w/ prior)",
                "XELMS (Z-S w/o prior)",
                "TAC15-Test",
                "TH-Test",
                "McN-Test"
            ],
            [
                "XELMS (Z-S w/ prior)",
                "XELMS (Z-S w/o prior)",
                "TAC15-Test",
                "TH-Test",
                "McN-Test"
            ],
            [
                "XELMS (Z-S w/ prior)",
                "SoTA",
                "TAC15-Test"
            ],
            [
                "XELMS (Z-S w/ prior)",
                "XELMS (Z-S w/o prior)",
                "TAC15-Test",
                "TH-Test",
                "McN-Test"
            ],
            [
                "XELMS (Z-S w/o prior)"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_7",
        "paper_id": "D18-1270",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1273table_5",
        "description": "Table 5 illustrates that, for three different testing datasets, the entire generated corpus D achieves 74.1%, 80.6% and 84.2% on Ctrain:test, respectively, which are much higher than that of Trn13, Trn14 and Trn15. This difference may denote the validity of the generated corpus, with adequate spelling errors.",
        "sentences": [
            "Table 5 illustrates that, for three different testing datasets, the entire generated corpus D achieves 74.1%, 80.6% and 84.2% on Ctrain:test, respectively, which are much higher than that of Trn13, Trn14 and Trn15.",
            "This difference may denote the validity of the generated corpus, with adequate spelling errors."
        ],
        "class_sentence": [
            1,
            2
        ],
        "header_mention": [
            [
                "D : Tst13",
                "D : Tst14",
                "D : Tst15",
                "C (%)",
                "Trn13 : Tst13",
                "Trn14 : Tst14",
                "Trn15 : Tst15"
            ],
            null
        ],
        "n_sentence": 2.0,
        "table_id": "table_5",
        "paper_id": "D18-1273",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1273table_7",
        "description": "Table 7 shows the detection performance on three different testing datasets. We have the following observations. The size of training dataset is important for the model training. For Tst13, D-10k achieves a better F1 score than Trn13. A major reason may be the size of Trn13 (=350, see in Table 3), which is much smaller than the testing dataset. In this situation, the model can not learn enough information, resulting in being unable to detect unseen spelling errors. Besides, we can see that the detection performance shows a stable improvement as the size of our generated corpus is continuously enlarged. Therefore, for data-driven approaches, it is of great importance to train our model with enough instances having different spelling errors. The precision may be compromised if the training dataset contains too many \u201cnoisy\u201d spelling errors. From Table 7, although the overall performance (F1 score) keeps improving as the size of our generated corpus increases, the precision and the recall demonstrate different changing trends. It is observed that as the size of training dataset increases, the model achieves a better performance in terms of the recall. An possible reason is that with more instances containing different spelling error including in the training dataset, the number of unseen spelling error in the testing dataset is reduced, thus facilitating the model to detect more spelling errors. However, the improvement of the precision is not so obvious as that of the recall. Specifically, in Tst14 and Tst15, D-50k does not achieve a higher precision than D-40k. A possible explanation is that with a larger training dataset containing more spelling error instances, it may lead the model to misidentify some more correct characters, resulting in a lower precision. Compared with the limited training dataset manually annotated by human, our generated large-scale corpus can achieves a better performance. From Table 7, we can see that with a certain size of our generated corpus, it can train a model that achieve better detection performance than with the manually annotated datasets provided in the corresponding shared tasks. To some extent, this demonstrates the effectiveness of our generated corpus, thus confirms the validity of our approach.",
        "sentences": [
            "Table 7 shows the detection performance on three different testing datasets.",
            "We have the following observations.",
            "The size of training dataset is important for the model training.",
            "For Tst13, D-10k achieves a better F1 score than Trn13.",
            "A major reason may be the size of Trn13 (=350, see in Table 3), which is much smaller than the testing dataset.",
            "In this situation, the model can not learn enough information, resulting in being unable to detect unseen spelling errors.",
            "Besides, we can see that the detection performance shows a stable improvement as the size of our generated corpus is continuously enlarged.",
            "Therefore, for data-driven approaches, it is of great importance to train our model with enough instances having different spelling errors.",
            "The precision may be compromised if the training dataset contains too many \u201cnoisy\u201d spelling errors.",
            "From Table 7, although the overall performance (F1 score) keeps improving as the size of our generated corpus increases, the precision and the recall demonstrate different changing trends.",
            "It is observed that as the size of training dataset increases, the model achieves a better performance in terms of the recall.",
            "An possible reason is that with more instances containing different spelling error including in the training dataset, the number of unseen spelling error in the testing dataset is reduced, thus facilitating the model to detect more spelling errors.",
            "However, the improvement of the precision is not so obvious as that of the recall.",
            "Specifically, in Tst14 and Tst15, D-50k does not achieve a higher precision than D-40k.",
            "A possible explanation is that with a larger training dataset containing more spelling error instances, it may lead the model to misidentify some more correct characters, resulting in a lower precision.",
            "Compared with the limited training dataset manually annotated by human, our generated large-scale corpus can achieves a better performance.",
            "From Table 7, we can see that with a certain size of our generated corpus, it can train a model that achieve better detection performance than with the manually annotated datasets provided in the corresponding shared tasks.",
            "To some extent, this demonstrates the effectiveness of our generated corpus, thus confirms the validity of our approach."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            2,
            2,
            1,
            2,
            2,
            1,
            1,
            2,
            1,
            1,
            2,
            2,
            1,
            2
        ],
        "header_mention": [
            [
                "Tst13",
                "Tst14",
                "Tst15"
            ],
            null,
            null,
            [
                "D-10k",
                "Tst13",
                "Trn"
            ],
            [
                "Trn"
            ],
            [
                "Trn"
            ],
            [
                "D-10k",
                "D-20k",
                "D-30k",
                "D-40k",
                "D-50k"
            ],
            null,
            null,
            [
                "D-10k",
                "D-20k",
                "D-30k",
                "D-40k",
                "D-50k",
                "P",
                "R",
                "F1"
            ],
            [
                "D-10k",
                "D-20k",
                "D-30k",
                "D-40k",
                "D-50k",
                "R"
            ],
            null,
            [
                "D-10k",
                "D-20k",
                "D-30k",
                "D-40k",
                "D-50k",
                "P",
                "R"
            ],
            [
                "D-50k",
                "D-40k",
                "Tst14",
                "Tst15",
                "P"
            ],
            null,
            null,
            [
                "D-10k",
                "D-20k",
                "D-30k",
                "D-40k",
                "D-50k"
            ],
            [
                "D-10k",
                "D-20k",
                "D-30k",
                "D-40k",
                "D-50k"
            ]
        ],
        "n_sentence": 18.0,
        "table_id": "table_7",
        "paper_id": "D18-1273",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1277table_7",
        "description": "Impact of Tuning Set. Table 7 compares performance of the same experiments on the WSJ and Noun-Verb Challenge test sets, tuned either using the WSJ or the Noun-Verb development set. The only effect of the change in tuning set was for the Noun-Verb tuning to cause the early stopping to sometimes be a little earlier. When we tuned on the Noun-Verb development set, the WSJ results remained almost unchanged, while the NounVerb test set results increased significantly. We see that the performance on each dataset is best when matched with its tuning data. The effect was greatest on the unenhanced model, which improved 2.9% absolute on the Noun-Verb evaluation. The best overall Noun-Verb test set result was 89.3\u00c2\u00b10.2 when tuned this way.",
        "sentences": [
            "Impact of Tuning Set.",
            "Table 7 compares performance of the same experiments on the WSJ and Noun-Verb Challenge test sets, tuned either using the WSJ or the Noun-Verb development set.",
            "The only effect of the change in tuning set was for the Noun-Verb tuning to cause the early stopping to sometimes be a little earlier.",
            "When we tuned on the Noun-Verb development set, the WSJ results remained almost unchanged, while the NounVerb test set results increased significantly.",
            "We see that the performance on each dataset is best when matched with its tuning data.",
            "The effect was greatest on the unenhanced model, which improved 2.9% absolute on the Noun-Verb evaluation.",
            "The best overall Noun-Verb test set result was 89.3\u00c2\u00b10.2 when tuned this way."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "WSJ Test Set",
                "Noun-Verb Test Set",
                "WSJ",
                "NV"
            ],
            [
                "Tuning Set",
                "WSJ",
                "NV"
            ],
            [
                "NV",
                "WSJ Test Set",
                "Noun-Verb Test Set",
                "Bohnet et al. (2018)",
                "+ELMo",
                "+NV Data",
                "+ELMo+NV Data"
            ],
            [
                "Tuning Set"
            ],
            [
                "Noun-Verb Test Set",
                "Bohnet et al. (2018)",
                "WSJ",
                "NV"
            ],
            [
                "Noun-Verb Test Set",
                "+ELMo+NV Data",
                "NV"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_7",
        "paper_id": "D18-1277",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1278table_6",
        "description": "Table 6 summarizes the results on Czech, German, and Russian. We find augmenting the charlstm model with either oracle or predicted case improve its accuracy, although the effect is different across languages. The improvements from predicted case results are interesting, since in nonneural parsers, predicted case usually harms accuracy (Tsarfaty et al., 2010). However, we note that our taggers use gold POS, which might help. The MTL models achieve similar or slightly better performance than the character-only models, suggesting that supplying case in this way is beneficial. Curiously, the MTL parser is worse than the the pipeline parser, but the MTL case tagger is better than the pipeline case tagger (Table 7). This indicates that the MTL model must learn to encode case in the model\u00e2\u20ac\u2122s representation, but must not learn to effectively use it for parsing. Finally, we observe that augmenting the char-lstm with either gold or predicted case improves the parsing performance for all languages, and indeed closes the performance gap with the full oracle, which has access to all morphological features. This is especially interesting, because it shows using carefully targeted linguistic analyses can improve accuracy as much as wholesale linguistic analysis.",
        "sentences": [
            "Table 6 summarizes the results on Czech, German, and Russian.",
            "We find augmenting the charlstm model with either oracle or predicted case improve its accuracy, although the effect is different across languages.",
            "The improvements from predicted case results are interesting, since in nonneural parsers, predicted case usually harms accuracy (Tsarfaty et al., 2010).",
            "However, we note that our taggers use gold POS, which might help.",
            "The MTL models achieve similar or slightly better performance than the character-only models, suggesting that supplying case in this way is beneficial.",
            "Curiously, the MTL parser is worse than the the pipeline parser, but the MTL case tagger is better than the pipeline case tagger (Table 7).",
            "This indicates that the MTL model must learn to encode case in the model\u00e2\u20ac\u2122s representation, but must not learn to effectively use it for parsing.",
            "Finally, we observe that augmenting the char-lstm with either gold or predicted case improves the parsing performance for all languages, and indeed closes the performance gap with the full oracle, which has access to all morphological features.",
            "This is especially interesting, because it shows using carefully targeted linguistic analyses can improve accuracy as much as wholesale linguistic analysis."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            1,
            2,
            2,
            1,
            2
        ],
        "header_mention": [
            [
                "Czech",
                "German",
                "Russian"
            ],
            [
                "char + predicted case",
                "oracle",
                "Dev",
                "Test",
                "Czech",
                "German",
                "Russian"
            ],
            [
                "char + predicted case"
            ],
            null,
            [
                "char"
            ],
            null,
            null,
            [
                "char + gold case",
                "char + predicted case",
                "Czech",
                "German",
                "Russian",
                "oracle"
            ],
            null
        ],
        "n_sentence": 9.0,
        "table_id": "table_6",
        "paper_id": "D18-1278",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1279table_2",
        "description": "5.2 State-of-the-art Results. Table 2 presents our proposed model in comparison with state-of-the-art results. LSTM-CRF is our baseline which uses fine-tuned pre-trained word embeddings. Its comparison with LSTMCRF using random initializations for word embeddings, as shown in Table 1, confirms that pre-trained word embeddings are useful for sequence labeling. Since the training corpus for sequence labeling is relatively small, pre-trained embeddings learned from a huge unlabeled corpus can help to enhance word semantics. Furthermore, we adopt and re-implement two stateof-the-art character models, char-LSTM and charCNN, by combining with LSTM-CRF, which we refer to as LSTM-CRF-char-LSTM and LSTMCRF-char-CNN. These experiments show that our char-IntNet generally improves results across different models and datasets. The improvement is more pronounced for non-English datasets, for example, IntNet improves the F-1 score over the stateof-the-art results by more than 2% for Dutch and Spanish. It also shows that the results of LSTM-CRF are significantly improved after adding character-to-word models, which confirms that word shape information is very important for sequence labeling. Figure 2 presents the details of training epochs in comparison with other state-ofthe-art character models for different languages. It shows that char-CNN and char-LSTM converge early whereas char-IntNet takes more epochs to converge and generally performs better. It alludes to the fact that IntNet is suitable for reducing overfitting, since we have used early stopping while training.",
        "sentences": [
            "5.2 State-of-the-art Results.",
            "Table 2 presents our proposed model in comparison with state-of-the-art results.",
            "LSTM-CRF is our baseline which uses fine-tuned pre-trained word embeddings.",
            "Its comparison with LSTMCRF using random initializations for word embeddings, as shown in Table 1, confirms that pre-trained word embeddings are useful for sequence labeling.",
            "Since the training corpus for sequence labeling is relatively small, pre-trained embeddings learned from a huge unlabeled corpus can help to enhance word semantics.",
            "Furthermore, we adopt and re-implement two stateof-the-art character models, char-LSTM and charCNN, by combining with LSTM-CRF, which we refer to as LSTM-CRF-char-LSTM and LSTMCRF-char-CNN.",
            "These experiments show that our char-IntNet generally improves results across different models and datasets.",
            "The improvement is more pronounced for non-English datasets, for example, IntNet improves the F-1 score over the stateof-the-art results by more than 2% for Dutch and Spanish.",
            "It also shows that the results of LSTM-CRF are significantly improved after adding character-to-word models, which confirms that word shape information is very important for sequence labeling.",
            "Figure 2 presents the details of training epochs in comparison with other state-ofthe-art character models for different languages.",
            "It shows that char-CNN and char-LSTM converge early whereas char-IntNet takes more epochs to converge and generally performs better.",
            "It alludes to the fact that IntNet is suitable for reducing overfitting, since we have used early stopping while training."
        ],
        "class_sentence": [
            2,
            1,
            2,
            2,
            2,
            2,
            1,
            1,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "Conv-CRF+Lexicon (Collobert et al. 2011)",
                "LSTM-CRF+Lexicon (Huang et al. 2015)",
                "LSTM-CRF+Lexicon+char-CNN (Chiu and Nichols 2016)",
                "LSTM-Softmax+char-LSTM (Ling et al. 2015)",
                "LSTM-CRF+char-LSTM (Lample et al. 2016)",
                "LSTM-CRF+char-CNN (Ma and Hovy 2016)",
                "GRM-CRF+char-GRU (Yang et al. 2017)",
                "LSTM-CRF",
                "LSTM-CRF+char-LSTM",
                "LSTM-CRF+char-CNN",
                "LSTM-CRF+char-IntNet-9",
                "LSTM-CRF+char-IntNet-5"
            ],
            [
                "LSTM-CRF"
            ],
            [
                "LSTM-CRF"
            ],
            null,
            [
                "LSTM-CRF+char-LSTM",
                "LSTM-CRF+char-CNN"
            ],
            [
                "LSTM-CRF+char-IntNet-9",
                "LSTM-CRF+char-IntNet-5"
            ],
            [
                "LSTM-CRF+char-IntNet-9",
                "LSTM-CRF+char-IntNet-5",
                "Spanish",
                "Dutch",
                "German",
                "LSTM-CRF+char-LSTM (Lample et al. 2016)",
                "GRM-CRF+char-GRU (Yang et al. 2017)"
            ],
            [
                "LSTM-CRF",
                "LSTM-CRF+char-LSTM",
                "LSTM-CRF+char-CNN"
            ],
            null,
            [
                "LSTM-CRF+char-LSTM",
                "LSTM-CRF+char-CNN",
                "LSTM-CRF+char-IntNet-9",
                "LSTM-CRF+char-IntNet-5"
            ],
            null
        ],
        "n_sentence": 12.0,
        "table_id": "table_2",
        "paper_id": "D18-1279",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1280table_4",
        "description": "6 Results. Tables 4 and 5 present the results on the IEMOCAP and SEMAINE testing sets, respectively. In Table 4, we evaluate the mean classification performance using Weighted Accuracy (acc.) and F1-Score (F1) on the discrete emotion categories. ICON performs better than the compared models with significant performance increase in emotions (\u00e2\u02c6\u00bc2.1% acc.). For each emotion, ICON outperforms all the compared models except for happiness emotion. However, its performance is still at par with cLSTM without a significant gap. Also, ICON manages to correctly identify the relatively similar excitement emotion by a large margin.",
        "sentences": [
            "6 Results.",
            "Tables 4 and 5 present the results on the IEMOCAP and SEMAINE testing sets, respectively.",
            "In Table 4, we evaluate the mean classification performance using Weighted Accuracy (acc.) and F1-Score (F1) on the discrete emotion categories.",
            "ICON performs better than the compared models with significant performance increase in emotions (\u00e2\u02c6\u00bc2.1% acc.).",
            "For each emotion, ICON outperforms all the compared models except for happiness emotion.",
            "However, its performance is still at par with cLSTM without a significant gap.",
            "Also, ICON manages to correctly identify the relatively similar excitement emotion by a large margin."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "IEMOCAP: Emotion Categories"
            ],
            [
                "acc.",
                "F1"
            ],
            [
                "ICON",
                "acc."
            ],
            [
                "ICON",
                "Sad",
                "Neutral",
                "Angry",
                "Excited",
                "Frustrated"
            ],
            [
                "ICON",
                "cLSTM",
                "Happy"
            ],
            [
                "ICON"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_4",
        "paper_id": "D18-1280",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1280table_6",
        "description": "Multimodality:. We investigate the importance of multimodal features for our task. Table 6 presents the results for different combinations of modes used by ICON on IEMOCAP. As seen, the trimodal network provides the best performance which is preceded by the bimodal variants. Among unimodals, language modality performs the best, reaffirming its significance in multimodal systems. Interestingly, the audio and visual modality, on their own, do not provide good performance, but when used with text, complementary data is shared to improve overall performance.",
        "sentences": [
            "Multimodality:.",
            "We investigate the importance of multimodal features for our task.",
            "Table 6 presents the results for different combinations of modes used by ICON on IEMOCAP.",
            "As seen, the trimodal network provides the best performance which is preceded by the bimodal variants.",
            "Among unimodals, language modality performs the best, reaffirming its significance in multimodal systems.",
            "Interestingly, the audio and visual modality, on their own, do not provide good performance, but when used with text, complementary data is shared to improve overall performance."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Modality",
                "T",
                "A",
                "V",
                "A+V",
                "T+A",
                "T+V",
                "T+A+V"
            ],
            [
                "Modality",
                "T+A+V",
                "A+V",
                "T+A",
                "T+V"
            ],
            [
                "Modality",
                "T"
            ],
            [
                "Modality",
                "A",
                "V",
                "T+A",
                "T+V"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_6",
        "paper_id": "D18-1280",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1282table_5",
        "description": "Table 5 gives the mean accuracy obtained on the test data (of 100 runs). Our ImgObjLoc vectors outperform all comparison models on motion verbs, including CNN-based image features and the best-performing models of (Gella et al., 2018), namely Gella\u2013CNN+O and Gella\u2013CNN+C (CNN features concatenated with predicted object labels and image captions, respectively). On non-motion verbs, the best models, including our own, perform only comparably to the most frequent sense heuristic. Note that we examine the simplest representation ImgObjLoc can yield, i.e., frame-semantic representations for individual images. More complex representations are left for future work.",
        "sentences": [
            "Table 5 gives the mean accuracy obtained on the test data (of 100 runs).",
            "Our ImgObjLoc vectors outperform all comparison models on motion verbs, including CNN-based image features and the best-performing models of (Gella et al., 2018), namely Gella\u2013CNN+O and Gella\u2013CNN+C (CNN features concatenated with predicted object labels and image captions, respectively).",
            "On non-motion verbs, the best models, including our own, perform only comparably to the most frequent sense heuristic.",
            "Note that we examine the simplest representation ImgObjLoc can yield, i.e., frame-semantic representations for individual images.",
            "More complex representations are left for future work."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "ImgObjLoc",
                "Random",
                "MFS",
                "CNN",
                "Gella\u2013CNN+O",
                "Gella\u2013CNN+C",
                "CNN (reproduced)",
                "Motion"
            ],
            [
                "Non-motion",
                "MFS",
                "CNN",
                "Gella\u2013CNN+O",
                "Gella\u2013CNN+C",
                "CNN (reproduced)",
                "ImgObjLoc"
            ],
            [
                "ImgObjLoc"
            ],
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_5",
        "paper_id": "D18-1282",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1283table_4",
        "description": "7.2 Experimental Results. Table 4 shows the comparison results among various models and the upper bound where the gold commonsense evidence provided to the human. It\u00e2\u20ac\u2122s not surprising that performance on common ground is worse in the hard configuration as the distracting verbs are more similar to the target action. The CVAE-based method is better than the attention-based method in facilitating common ground.",
        "sentences": [
            "7.2 Experimental Results.",
            "Table 4 shows the comparison results among various models and the upper bound where the gold commonsense evidence provided to the human.",
            "It\u00e2\u20ac\u2122s not surprising that performance on common ground is worse in the hard configuration as the distracting verbs are more similar to the target action.",
            "The CVAE-based method is better than the attention-based method in facilitating common ground."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Attenton",
                "CVAE",
                "CVAE+SV",
                "Gold"
            ],
            [
                "Hard",
                "Attenton",
                "CVAE",
                "CVAE+SV",
                "Gold"
            ],
            [
                "CVAE",
                "Attenton"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_4",
        "paper_id": "D18-1283",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1289table_3",
        "description": "5.1 Predicting Human Complexity Judgmen. To asses the contribution of the linguistic features to predict the judgment of sentence complexity we trained a linear SVM regression model with default parameters. We performed a 3-fold cross validation over each subset of agreed sentences at agreement 10 and 14. We measured two performance metrics: the mean absolute error to evaluate the accuracy of the model to predict the same complexity judgment assigned by humans; the Spearman correlation to evaluate the correlation between the ranking of features produced by the regression model with the ranking produced by the human judgments. Table 3 reports the results and the average score of the two metrics. As it can be seen, the model is very accurate and achieves a very high correlation (>0.56 with p <0.001) with an average error difference (avg mean abs err) below 1. In particular, the model obtained higher performance in predicting the ranking of features extracted from sentences at agreement 14. This might be due to the fact these sentences are characterized by a more uniform distribution of linguistic phenomena and that these phenomena contribute to predict the same judgment of complexity. This is in line with the results obtained by the SVM classifier in predicting agreement (Table 2). This is particularly the case of English and it possibly suggests that the set of sentences similarly judged by humans are characterized by a lower variability of the values of the features.",
        "sentences": [
            "5.1 Predicting Human Complexity Judgmen.",
            "To asses the contribution of the linguistic features to predict the judgment of sentence complexity we trained a linear SVM regression model with default parameters.",
            "We performed a 3-fold cross validation over each subset of agreed sentences at agreement 10 and 14.",
            "We measured two performance metrics: the mean absolute error to evaluate the accuracy of the model to predict the same complexity judgment assigned by humans; the Spearman correlation to evaluate the correlation between the ranking of features produced by the regression model with the ranking produced by the human judgments.",
            "Table 3 reports the results and the average score of the two metrics.",
            "As it can be seen, the model is very accurate and achieves a very high correlation (>0.56 with p <0.001) with an average error difference (avg mean abs err) below 1.",
            "In particular, the model obtained higher performance in predicting the ranking of features extracted from sentences at agreement 14.",
            "This might be due to the fact these sentences are characterized by a more uniform distribution of linguistic phenomena and that these phenomena contribute to predict the same judgment of complexity.",
            "This is in line with the results obtained by the SVM classifier in predicting agreement (Table 2).",
            "This is particularly the case of English and it possibly suggests that the set of sentences similarly judged by humans are characterized by a lower variability of the values of the features."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            1,
            1,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "IT-10",
                "IT-14",
                "EN-10",
                "EN-14"
            ],
            [
                "mean abs err 1",
                "Spearman 1",
                "mean abs err 2",
                "Spearman 2",
                "mean abs err 3",
                "Spearman 3"
            ],
            [
                "mean abs err 1",
                "Spearman 1",
                "mean abs err 2",
                "Spearman 2",
                "mean abs err 3",
                "Spearman 3",
                "avg mean abs err",
                "avg Spearman"
            ],
            [
                "Spearman 1",
                "Spearman 2",
                "Spearman 3",
                "avg Spearman",
                "avg mean abs err"
            ],
            [
                "IT-14",
                "EN-14"
            ],
            null,
            null,
            [
                "EN-10",
                "EN-14"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_3",
        "paper_id": "D18-1289",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1292table_1",
        "description": "Table 1 shows parsing results on the WSJ20dev dataset. The Best result is from an arbitrary sample at convergence of the oracle best run. The Best with PIoC is the same run, but with PIoC to aggregate 100 posterior samples at convergence. All with PIoC uses 100 posterior samples from all of the 10 chosen runs, and finally All with PIoC without best excludes the best run in PIoC calculation. There is almost a point of gain in precision going from Best to Best with PIoC with virtually no recall loss, showing that the posterior uncertainty is helpful in flattening binary trees. As more samples from the posterior are collected, as shown in All with PIoC without best, the precision gain is even more substantial. This shows that with PIoC there is no need to know which sample from which run is the best. Model selection in this case is only needed to weed out the runs with very low likelihood.",
        "sentences": [
            "Table 1 shows parsing results on the WSJ20dev dataset.",
            "The Best result is from an arbitrary sample at convergence of the oracle best run.",
            "The Best with PIoC is the same run, but with PIoC to aggregate 100 posterior samples at convergence.",
            "All with PIoC uses 100 posterior samples from all of the 10 chosen runs, and finally All with PIoC without best excludes the best run in PIoC calculation.",
            "There is almost a point of gain in precision going from Best to Best with PIoC with virtually no recall loss, showing that the posterior uncertainty is helpful in flattening binary trees.",
            "As more samples from the posterior are collected, as shown in All with PIoC without best, the precision gain is even more substantial.",
            "This shows that with PIoC there is no need to know which sample from which run is the best.",
            "Model selection in this case is only needed to weed out the runs with very low likelihood."
        ],
        "class_sentence": [
            1,
            2,
            2,
            2,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            [
                "Best",
                "Best w/ PIoC",
                "Rec",
                "Prec"
            ],
            [
                "All w/ PIoC w/o best",
                "Prec",
                "Best w/ PIoC"
            ],
            [
                "All w/ PIoC w/o best"
            ],
            null
        ],
        "n_sentence": 8.0,
        "table_id": "table_1",
        "paper_id": "D18-1292",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1296table_2",
        "description": "Table 2 presents recognition results, comparing baseline LSTM-LMs to the full session-based LSTM-LMs. Both the letter-trigram and one-word word encoding versions are reported. The different models may also be used jointly, using log-linear score combination in rescoring, shown in the third section of the table. We also tried iterating the session LM rescoring, after the recognized word histories were updated from the first rescoring pass (shown as \u201c2nd iteration\u201d in the table). Results show that the session-based LM yields between 1% and 4% relative word error reduction for the two word encodings, and test sets. When the two word encoding types are combined by log-linear combination of model scores, the gain from session-based modeling is preserved. Iterating the session LM rescoring to improve the word histories did not give consistent gains. Even though the session-based LSTM subsumes all the information used in the standard LSTM, there is an additional gain to be had from combining those two model types (last row in the table). Thus, the overall gain from adding the session-based models to the two baseline models is 3-5% relative word error reduction.",
        "sentences": [
            "Table 2 presents recognition results, comparing baseline LSTM-LMs to the full session-based LSTM-LMs.",
            "Both the letter-trigram and one-word word encoding versions are reported.",
            "The different models may also be used jointly, using log-linear score combination in rescoring, shown in the third section of the table.",
            "We also tried iterating the session LM rescoring, after the recognized word histories were updated from the first rescoring pass (shown as \u201c2nd iteration\u201d in the table).",
            "Results show that the session-based LM yields between 1% and 4% relative word error reduction for the two word encodings, and test sets.",
            "When the two word encoding types are combined by log-linear combination of model scores, the gain from session-based modeling is preserved.",
            "Iterating the session LM rescoring to improve the word histories did not give consistent gains.",
            "Even though the session-based LSTM subsumes all the information used in the standard LSTM, there is an additional gain to be had from combining those two model types (last row in the table).",
            "Thus, the overall gain from adding the session-based models to the two baseline models is 3-5% relative word error reduction."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "LSTM-LM",
                "Session LSTM-LM"
            ],
            [
                "Letter 3gram",
                "One-hot"
            ],
            [
                "Letter 3gram + One-hot"
            ],
            [
                "Session LSTM-LM 2nd iteration"
            ],
            [
                "Session LSTM-LM",
                "Letter 3gram",
                "One-hot",
                "Letter 3gram + One-hot",
                "WER",
                "test",
                "SWB",
                "CH"
            ],
            [
                "Letter 3gram + One-hot",
                "Session LSTM-LM"
            ],
            [
                "Session LSTM-LM",
                "Session LSTM-LM 2nd iteration"
            ],
            [
                "LSTM-LM + Session LSTM-LM"
            ],
            [
                "Letter 3gram + One-hot",
                "LSTM-LM",
                "Session LSTM-LM",
                "LSTM-LM + Session LSTM-LM"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_2",
        "paper_id": "D18-1296",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1303table_2",
        "description": "See Table 2 for multi-label classification results, where the Hamming score for the multi-label CNN-RNN model is 82.5%, showing potential for real-world use as well as substantial future research scope.",
        "sentences": [
            "See Table 2 for multi-label classification results, where the Hamming score for the multi-label CNN-RNN model is 82.5%, showing potential for real-world use as well as substantial future research scope."
        ],
        "class_sentence": [
            1
        ],
        "header_mention": [
            [
                "CNN-RNN (bidirec + char)",
                "Hamming"
            ]
        ],
        "n_sentence": 1.0,
        "table_id": "table_2",
        "paper_id": "D18-1303",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1309table_2",
        "description": "4.1 Nested NER. Table 2 shows the comparison of our model with several previous state-of-the nested NER models on the test dataset. Our model outperforms the state-of-the-art models in terms of F-score. Our results on Table 2 is based on bidirectional LSTM with character embeddings and the maximum region size is 10.",
        "sentences": [
            "4.1 Nested NER.",
            "Table 2 shows the comparison of our model with several previous state-of-the nested NER models on the test dataset.",
            "Our model outperforms the state-of-the-art models in terms of F-score.",
            "Our results on Table 2 is based on bidirectional LSTM with character embeddings and the maximum region size is 10."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Exhaustive Model",
                "Ju et al. (2018)",
                "Katiyar and Cardie",
                "Muis and Lu (2017)",
                "Lu and Roth (2015)",
                "Finkel and Manning"
            ],
            [
                "Exhaustive Model",
                "F(%)"
            ],
            [
                "Exhaustive Model"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "D18-1309",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1309table_3",
        "description": "Table 3 describes the performances of our model on different entity levels on the test dataset. The model performs well on multi-token and top-level entities. This is interesting because they are often considered difficult for sequential labeling models.",
        "sentences": [
            "Table 3 describes the performances of our model on different entity levels on the test dataset.",
            "The model performs well on multi-token and top-level entities.",
            "This is interesting because they are often considered difficult for sequential labeling models."
        ],
        "class_sentence": [
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "Single-token",
                "Multi-token",
                "Top Level",
                "Nested",
                "All entities"
            ],
            [
                "Multi-token",
                "Top Level"
            ],
            null
        ],
        "n_sentence": 3.0,
        "table_id": "table_3",
        "paper_id": "D18-1309",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1309table_4",
        "description": "Table 4 shows the performances on the five entity types on the test dataset. We here show the performance by Finkel and Manning (2009) (F&M) for the reference. Our system performs better than their model except for the RNA type.",
        "sentences": [
            "Table 4 shows the performances on the five entity types on the test dataset.",
            "We here show the performance by Finkel and Manning (2009) (F&M) for the reference.",
            "Our system performs better than their model except for the RNA type."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "DNA",
                "RNA",
                "cell line",
                "cell type",
                "protein"
            ],
            [
                "F&M F(%)"
            ],
            [
                "F(%)",
                "F&M F(%)",
                "DNA",
                "cell line",
                "cell type",
                "protein"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_4",
        "paper_id": "D18-1309",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1309table_5",
        "description": "Table 5 shows the coverage ratio and the performance with different maximum region sizes. Since the average entity mention length of GENIA dataset is less than 4, the system can cover almost all the entities for the maximum sizes of 6 or more. The longer maximum region size is desirable to cover all the mentions, but it requires more computational costs. Fortunately, the performance did not degrade with the long maximum region size, despite the fact that it introduces more out-of-entity regions.",
        "sentences": [
            "Table 5 shows the coverage ratio and the performance with different maximum region sizes.",
            "Since the average entity mention length of GENIA dataset is less than 4, the system can cover almost all the entities for the maximum sizes of 6 or more.",
            "The longer maximum region size is desirable to cover all the mentions, but it requires more computational costs.",
            "Fortunately, the performance did not degrade with the long maximum region size, despite the fact that it introduces more out-of-entity regions."
        ],
        "class_sentence": [
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            [
                "Ratio(%)",
                "Region",
                "size = 3",
                "size = 6",
                "size = 8",
                "size = 10"
            ],
            null,
            [
                "Region"
            ],
            [
                "Region",
                "size = 6",
                "size = 8",
                "size = 10"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_5",
        "paper_id": "D18-1309",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1309table_6",
        "description": "Ablations on character embeddings in Table 6 also show the importance of character embeddings. It also shows that both the boundary information and the inside information, i.e., average of the embeddings in a region, are necessary to improve the performance.",
        "sentences": [
            "Ablations on character embeddings in Table 6 also show the importance of character embeddings.",
            "It also shows that both the boundary information and the inside information, i.e., average of the embeddings in a region, are necessary to improve the performance."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "Bi-LSTM",
                "Bi-LSTM + Character*",
                "F(%)"
            ],
            [
                "Boundary*",
                "Inside*",
                "Boundary+Inside*",
                "F(%)"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_6",
        "paper_id": "D18-1309",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1309table_7",
        "description": "4.3 Flat NER. We evaluated our model on JNLPBA as a flat dataset, where nested and discontinuous entities are removed. Table 7 shows the performances of our model on JNLPBA dataset. We compared our result with the state-of-the-art result of Gridach (2017) which achieved 75.8% in F-score, where our model obtained 78.4% in terms of F-score.",
        "sentences": [
            "4.3 Flat NER.",
            "We evaluated our model on JNLPBA as a flat dataset, where nested and discontinuous entities are removed.",
            "Table 7 shows the performances of our model on JNLPBA dataset.",
            "We compared our result with the state-of-the-art result of Gridach (2017) which achieved 75.8% in F-score, where our model obtained 78.4% in terms of F-score."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "overall",
                "F(%)"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_7",
        "paper_id": "D18-1309",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1315table_1",
        "description": "In order to assure fair comparison, we perform the paradigm completion experiment described in Malouf (2017), where 90% of the word forms in the data set is used for training and the remaining 10% for testing. As the results in Table 1 show, our results very closely replicate those reported by Malouf (2017).",
        "sentences": [
            "In order to assure fair comparison, we perform the paradigm completion experiment described in Malouf (2017), where 90% of the word forms in the data set is used for training and the remaining 10% for testing.",
            "As the results in Table 1 show, our results very closely replicate those reported by Malouf (2017)."
        ],
        "class_sentence": [
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "Our baseline",
                "Malouf (2017)"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_1",
        "paper_id": "D18-1315",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1315table_4",
        "description": "Table 4 shows results for completing tables for common lexemes. Our system significantly outperforms the baseline on all other datasets apart from German nouns. We believe that the reason for the German outlier is the high degree of syncretism in German noun tables.",
        "sentences": [
            "Table 4 shows results for completing tables for common lexemes.",
            "Our system significantly outperforms the baseline on all other datasets apart from German nouns.",
            "We believe that the reason for the German outlier is the high degree of syncretism in German noun tables."
        ],
        "class_sentence": [
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Our system",
                "Baseline",
                "FINNISH NOUNS",
                "FINNISH VERBS",
                "FRENCH VERBS",
                "GERMAN VERBS",
                "LATVIAN NOUNS",
                "SPANISH VERBS",
                "TURKISH NOUNS"
            ],
            [
                "GERMAN NOUNS"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_4",
        "paper_id": "D18-1315",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1316table_3",
        "description": "Sample outputs produced by our attack are shown in Tables 1 and 2. Additional outputs can be found in the supplementary material. Table 3 shows the attack success rate and mean percentage of modified words on each task. We compare to the Perturb baseline, which greedily applies the Perturb subroutine, to validate the use of population-based optimization. As can be seen from our results, we are able to achieve high success rate with a limited number of modifications on both tasks. In addition, the genetic algorithm significantly outperformed the Perturb baseline in both success rate and percentage of words modified, demonstrating the additional benefit yielded by using population-based optimization. Testing using a single TitanX GPU, for sentiment analysis and textual entailment, we measured average runtimes on success to be 43.5 and 5 seconds per example, respectively. The high success rate and reasonable runtimes demonstrate the practicality of our approach, even when scaling to long sentences, such as those found in the IMDB dataset. Speaking of which, our success rate on textual entailment is lower due to the large disparity in sentence length. On average, hypothesis sentences in the SNLI corpus are 9 words long, which is very short compared to IMDB (229 words, limited to 100 for experiments). With sentences that short, applying successful perturbations becomes much harder, however we were still able to achieve a success rate of 70%. For the same reason, we didn\u00e2\u20ac\u2122t apply the Perturb baseline on the textual entailment task, as the Perturb baseline fails to achieve any success under the limits of the maximum allowed changes constraint.",
        "sentences": [
            "Sample outputs produced by our attack are shown in Tables 1 and 2.",
            "Additional outputs can be found in the supplementary material.",
            "Table 3 shows the attack success rate and mean percentage of modified words on each task.",
            "We compare to the Perturb baseline, which greedily applies the Perturb subroutine, to validate the use of population-based optimization.",
            "As can be seen from our results, we are able to achieve high success rate with a limited number of modifications on both tasks.",
            "In addition, the genetic algorithm significantly outperformed the Perturb baseline in both success rate and percentage of words modified, demonstrating the additional benefit yielded by using population-based optimization.",
            "Testing using a single TitanX GPU, for sentiment analysis and textual entailment, we measured average runtimes on success to be 43.5 and 5 seconds per example, respectively.",
            "The high success rate and reasonable runtimes demonstrate the practicality of our approach, even when scaling to long sentences, such as those found in the IMDB dataset.",
            "Speaking of which, our success rate on textual entailment is lower due to the large disparity in sentence length.",
            "On average, hypothesis sentences in the SNLI corpus are 9 words long, which is very short compared to IMDB (229 words, limited to 100 for experiments).",
            "With sentences that short, applying successful perturbations becomes much harder, however we were still able to achieve a success rate of 70%.",
            "For the same reason, we didn\u00e2\u20ac\u2122t apply the Perturb baseline on the textual entailment task, as the Perturb baseline fails to achieve any success under the limits of the maximum allowed changes constraint."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1,
            1,
            2,
            2,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "Genetic attack"
            ],
            null,
            [
                "Sentiment Analysis",
                "Textual Entailment",
                "% success",
                "% modified"
            ],
            [
                "Perturb baseline"
            ],
            [
                "Genetic attack",
                "Sentiment Analysis",
                "Textual Entailment",
                "% success"
            ],
            [
                "Genetic attack",
                "Perturb baseline",
                "% success",
                "% modified"
            ],
            [
                "Sentiment Analysis",
                "Textual Entailment"
            ],
            [
                "Genetic attack"
            ],
            [
                "Genetic attack",
                "Textual Entailment",
                "% success"
            ],
            null,
            [
                "Genetic attack",
                "Textual Entailment",
                "% success"
            ],
            [
                "Perturb baseline",
                "Textual Entailment"
            ]
        ],
        "n_sentence": 12.0,
        "table_id": "table_3",
        "paper_id": "D18-1316",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1321table_3",
        "description": "Effect of Gated Attention Mechanism. Table 3 shows the Effect of gated attention mechanism. We compared models with Gated C+ P2C and Simple C+ P2C. The MIU accuracy of the P2C model has over 10% improvement when changing the operate pattern of the extra information proves the effect of GA mechanism. The Gated C+ P2C achieves the best in DC corpus, suggesting that the gated-attention works extremely well for handling long and diverse context. Main Result. Our model is compared to other models in Table 3. So far, (Huang et al., 2015) and (Zhang et al., 2017) reported the state-of-theart results among statistical models. We list the top-5 accuracy contrast to all baselines with top10 results, and the comparison indicates the noticeable advancement of our P2C model. To our surprise, the top-5 result on PD of our best Gated C+ P2C system approaches the top-10 accuracy of Google IME. On DC corpus, our Gated C+ P2C model with the best setting achieves 90.14% accuracy, surpassing all the baselines. The comparison shows our gated-attention system outperforms all state-of-the-art baselines with better user experience.",
        "sentences": [
            "Effect of Gated Attention Mechanism.",
            "Table 3 shows the Effect of gated attention mechanism.",
            "We compared models with Gated C+ P2C and Simple C+ P2C.",
            "The MIU accuracy of the P2C model has over 10% improvement when changing the operate pattern of the extra information proves the effect of GA mechanism.",
            "The Gated C+ P2C achieves the best in DC corpus, suggesting that the gated-attention works extremely well for handling long and diverse context.",
            "Main Result.",
            "Our model is compared to other models in Table 3.",
            "So far, (Huang et al., 2015) and (Zhang et al., 2017) reported the state-of-theart results among statistical models.",
            "We list the top-5 accuracy contrast to all baselines with top10 results, and the comparison indicates the noticeable advancement of our P2C model.",
            "To our surprise, the top-5 result on PD of our best Gated C+ P2C system approaches the top-10 accuracy of Google IME.",
            "On DC corpus, our Gated C+ P2C model with the best setting achieves 90.14% accuracy, surpassing all the baselines.",
            "The comparison shows our gated-attention system outperforms all state-of-the-art baselines with better user experience."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            2,
            1,
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "basic P2C",
                "Simple C+ P2C",
                "Gated C+ P2C"
            ],
            [
                "basic P2C"
            ],
            [
                "Gated C+ P2C",
                "DC"
            ],
            null,
            [
                "basic P2C",
                "Google IME",
                "CoCat",
                "OMWA"
            ],
            [
                "CoCat",
                "OMWA"
            ],
            [
                "basic P2C",
                "Google IME",
                "CoCat",
                "OMWA",
                "Top-5",
                "Top-10"
            ],
            [
                "Gated C+ P2C",
                "Google IME",
                "Top-5",
                "Top-10",
                "PD"
            ],
            [
                "Gated C+ P2C",
                "DC",
                "Top-5"
            ],
            [
                "Gated C+ P2C"
            ]
        ],
        "n_sentence": 12.0,
        "table_id": "table_3",
        "paper_id": "D18-1321",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1323table_1",
        "description": "3.3 Language modelling results. We present the LM results for the standard nontied model, the tied model as in Inan et al. (2017) and Press and Wolf (2017), and our tied model with an additional linear transformation (tied+L) in Tables 1 (PTB) and 2 (Wiki). Table 1 confirms that tying generally brings gains with respect to not tying. This is also true for the cases when the hidden and embedding sizes are different (e.g. 400/200 and 600/400), where our tied+L model outperforms the non-tied model by 5 to 6.4 points having around 40% less parameters. Furthermore, our decoupled model slightly but consistently improves results with respect to standard tying, confirming our intuition that the coupling of the hidden state to the embedding representation is a limiting constraint. Smaller tied+L models perform well compared to larger tied models. In particular, the tied+L model with 600/400 units has perplexity of 76.0, compared to 76.1 of the tied 600/600 model, with 55% the number of parametres. Note that our results are comparable to previously reported perplexity values on PTB for similar models. Our best results of 75.5 test perplexity is only 1.2 points behind the large tied model with 1500 units reported in Press and Wolf (2017) and is only 1.6 points behind the medium tied model with 650 units and variational dropout (Gal and Ghahramani, 2016) reported in Inan et al. (2017).",
        "sentences": [
            "3.3 Language modelling results.",
            "We present the LM results for the standard nontied model, the tied model as in Inan et al. (2017) and Press and Wolf (2017), and our tied model with an additional linear transformation (tied+L) in Tables 1 (PTB) and 2 (Wiki).",
            "Table 1 confirms that tying generally brings gains with respect to not tying.",
            "This is also true for the cases when the hidden and embedding sizes are different (e.g. 400/200 and 600/400), where our tied+L model outperforms the non-tied model by 5 to 6.4 points having around 40% less parameters.",
            "Furthermore, our decoupled model slightly but consistently improves results with respect to standard tying, confirming our intuition that the coupling of the hidden state to the embedding representation is a limiting constraint.",
            "Smaller tied+L models perform well compared to larger tied models.",
            "In particular, the tied+L model with 600/400 units has perplexity of 76.0, compared to 76.1 of the tied 600/600 model, with 55% the number of parametres.",
            "Note that our results are comparable to previously reported perplexity values on PTB for similar models.",
            "Our best results of 75.5 test perplexity is only 1.2 points behind the large tied model with 1500 units reported in Press and Wolf (2017) and is only 1.6 points behind the medium tied model with 650 units and variational dropout (Gal and Ghahramani, 2016) reported in Inan et al. (2017)."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "tied+L",
                "Inan2017 VD tied 650",
                "P&W2016 tied 1500"
            ],
            null,
            [
                "tied+L",
                "non-tied",
                "Hid",
                "Emb"
            ],
            [
                "tied+L",
                "tied"
            ],
            [
                "tied",
                "tied+L",
                "Hid",
                "Emb"
            ],
            [
                "tied",
                "tied+L",
                "Hid",
                "600",
                "Emb",
                "400",
                "Test"
            ],
            [
                "Inan2017 VD tied 650",
                "Zaremba2014 1500",
                "P&W2016 tied 1500"
            ],
            [
                "non-tied",
                "Hid",
                "600",
                "Emb",
                "Test",
                "Inan2017 VD tied 650",
                "P&W2016 tied 1500"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_1",
        "paper_id": "D18-1323",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1325table_3",
        "description": "Table 3 shows the performance of our best HAN model with a varying number k of previous sentences in the test-set. We can see that the best performance for TED talks and news is archived with 3, while for subtitles it is similar between 3 and 7.",
        "sentences": [
            "Table 3 shows the performance of our best HAN model with a varying number k of previous sentences in the test-set.",
            "We can see that the best performance for TED talks and news is archived with 3, while for subtitles it is similar between 3 and 7."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "k"
            ],
            [
                "k",
                "3",
                "7",
                "TED Talks",
                "News",
                "Subtitles"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_3",
        "paper_id": "D18-1325",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1326table_1",
        "description": "5.1 Our Strategies vs. Baseline. Table 1 reports the main translation results of Zh\u2192En/Ja and En\u2192De/Fr translation tasks. Our ultimate goal is to make the universal one-to-many framework as good as or better than the individually trained systems. We conduct universal one-to-many translation using Johnson et al. (2017) method on Transformer framework as our baseline system (briefly, O2M method). From the first two lines, we can see that the O2M method cannot perform on par with the individually trained systems in most cases. We mentioned before that our goal is to improve the universal one-to-many multilingual translation framework while maintaining the parameter sharing property. We can observe from the table that all our proposed strategies (last part in Table 1) improve the translation performance compared to the baseline (O2M). Specifically, the combined use of three strategies performs best and it can achieve the improvements up to 1.96 BLEU points (45.51 vs. 43.55 on Zh\u2192En MT04). As for languagedependent positional embedding, we find that both fixed and dynamic styles perform similarly. Our ultimate goal is to make the universal oneto-many framework as good as or better than the individually trained systems. Table 1 demonstrates some encouraging results. It is shown in the table that the universal one-to-many architecture enhanced with our strategies can outperform the individually trained models on three out of four language translations (Zh\u2192En, Zh\u2192Ja, En\u2192Fr). The results verify the effectiveness of our proposed methods.",
        "sentences": [
            "5.1 Our Strategies vs. Baseline.",
            "Table 1 reports the main translation results of Zh\u2192En/Ja and En\u2192De/Fr translation tasks.",
            "Our ultimate goal is to make the universal one-to-many framework as good as or better than the individually trained systems.",
            "We conduct universal one-to-many translation using Johnson et al. (2017) method on Transformer framework as our baseline system (briefly, O2M method).",
            "From the first two lines, we can see that the O2M method cannot perform on par with the individually trained systems in most cases.",
            "We mentioned before that our goal is to improve the universal one-to-many multilingual translation framework while maintaining the parameter sharing property.",
            "We can observe from the table that all our proposed strategies (last part in Table 1) improve the translation performance compared to the baseline (O2M).",
            "Specifically, the combined use of three strategies performs best and it can achieve the improvements up to 1.96 BLEU points (45.51 vs. 43.55 on Zh\u2192En MT04).",
            "As for languagedependent positional embedding, we find that both fixed and dynamic styles perform similarly.",
            "Our ultimate goal is to make the universal oneto-many framework as good as or better than the individually trained systems.",
            "Table 1 demonstrates some encouraging results.",
            "It is shown in the table that the universal one-to-many architecture enhanced with our strategies can outperform the individually trained models on three out of four language translations (Zh\u2192En, Zh\u2192Ja, En\u2192Fr).",
            "The results verify the effectiveness of our proposed methods."
        ],
        "class_sentence": [
            2,
            1,
            2,
            2,
            1,
            2,
            1,
            1,
            1,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Zh\u2192En",
                "Zh\u2192Ja",
                "En\u2192De",
                "En\u2192Fr"
            ],
            [
                "Indiv",
                "O2M"
            ],
            [
                "O2M"
            ],
            [
                "Indiv",
                "O2M"
            ],
            [
                "O2M"
            ],
            [
                "O2M",
                "O2M + \u2460",
                "O2M + \u2460 + \u2461 (Dyn)",
                "O2M + \u2460 + \u2461 (Fixed)",
                "O2M + \u2460 + \u2462",
                "O2M + \u2460 + \u2461 (Dyn)+ 3"
            ],
            [
                "O2M + \u2460 + \u2461 (Dyn)+ 3",
                "O2M",
                "Zh\u2192En",
                "MT04"
            ],
            [
                "O2M + \u2460 + \u2461 (Dyn)",
                "O2M + \u2460 + \u2461 (Fixed)"
            ],
            [
                "Indiv",
                "O2M"
            ],
            null,
            [
                "Indiv",
                "O2M + \u2460",
                "O2M + \u2460 + \u2461 (Dyn)",
                "O2M + \u2460 + \u2461 (Fixed)",
                "O2M + \u2460 + \u2462",
                "O2M + \u2460 + \u2461 (Dyn)+ 3",
                "Zh\u2192En",
                "Zh\u2192Ja",
                "En\u2192Fr"
            ],
            [
                "O2M + \u2460",
                "O2M + \u2460 + \u2461 (Dyn)",
                "O2M + \u2460 + \u2461 (Fixed)",
                "O2M + \u2460 + \u2462",
                "O2M + \u2460 + \u2461 (Dyn)+ 3"
            ]
        ],
        "n_sentence": 13.0,
        "table_id": "table_1",
        "paper_id": "D18-1326",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1330table_1",
        "description": "4.2 The MUSE benchmark,2,\nTable 1 reports the comparison of RCSLS with standard supervised and unsupervised approaches on 5 language pairs (in both directions) of the MUSE benchmark (Conneau et al., 2017). Every approach uses the Wikipedia fastText vectors and supervision comes in the form of a lexicon composed of 5k words and their translations. Regardless of the relaxation, RCSLS outperforms the state of the art by, on average, 3 to 4% in accuracy. This shows the importance of using the same criterion during training and inference. Note that the refinement step (\u201crefine\u201d) also uses CSLS to finetune the alignments but leads to a marginal gain for supervised methods. Interestingly, RCSLS achieves a better performance without constraints (+0.8%) for all pairs. Contrary to observations made in previous works, this result suggests that preserving the distance between word vectors is not essential for word translation. Indeed, previous works used a l2 loss where, indeed, orthogonal constraints lead to an improvement of +5.3% (Procrustes versus Least Square Error). This suggests that a linear mapping W with no constraints works well only if it is learned with a proper criterion.",
        "sentences": [
            "4.2 The MUSE benchmark,2,\nTable 1 reports the comparison of RCSLS with standard supervised and unsupervised approaches on 5 language pairs (in both directions) of the MUSE benchmark (Conneau et al., 2017).",
            "Every approach uses the Wikipedia fastText vectors and supervision comes in the form of a lexicon composed of 5k words and their translations.",
            "Regardless of the relaxation, RCSLS outperforms the state of the art by, on average, 3 to 4% in accuracy.",
            "This shows the importance of using the same criterion during training and inference.",
            "Note that the refinement step (\u201crefine\u201d) also uses CSLS to finetune the alignments but leads to a marginal gain for supervised methods.",
            "Interestingly, RCSLS achieves a better performance without constraints (+0.8%) for all pairs.",
            "Contrary to observations made in previous works, this result suggests that preserving the distance between word vectors is not essential for word translation.",
            "Indeed, previous works used a l2 loss where, indeed, orthogonal constraints lead to an improvement of +5.3% (Procrustes versus Least Square Error).",
            "This suggests that a linear mapping W with no constraints works well only if it is learned with a proper criterion."
        ],
        "class_sentence": [
            1,
            2,
            1,
            2,
            2,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "RCSLS",
                "Adversarial + refine"
            ],
            [
                "RCSLS"
            ],
            null,
            [
                "RCSLS",
                "RCSLS + spectral",
                "avg."
            ],
            [
                "RCSLS + spectral",
                "RCSLS"
            ],
            [
                "Least Square Error",
                "Procrustes",
                "avg."
            ],
            null
        ],
        "n_sentence": 9.0,
        "table_id": "table_1",
        "paper_id": "D18-1330",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1330table_3",
        "description": "4.3 The WaCky dataset. Dinu et al. (2014) introduce a setting where word vectors are learned on the WaCky datasets (Baroni et al., 2009) and aligned with a noisy bilingual lexicon. We select the number of epochs within {1, 2, 5, 10} on a validation set. Table 3 shows that RCSLS is on par with the state of the art. RCSLS is thus robust to relatively poor word vectors and noisy lexicons.",
        "sentences": [
            "4.3 The WaCky dataset.",
            "Dinu et al. (2014) introduce a setting where word vectors are learned on the WaCky datasets (Baroni et al., 2009) and aligned with a noisy bilingual lexicon.",
            "We select the number of epochs within {1, 2, 5, 10} on a validation set.",
            "Table 3 shows that RCSLS is on par with the state of the art.",
            "RCSLS is thus robust to relatively poor word vectors and noisy lexicons."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "Adversarial + refine + CSLS",
                "RCSLS",
                "Procrustes + CSLS"
            ],
            [
                "RCSLS"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_3",
        "paper_id": "D18-1330",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1341table_1",
        "description": "4.1 Results. Table 1 shows the result on different training datasets to compare our model against the baselines. Original MT is the strong standard donothing baseline, copying the MT translation as the PE output. In all settings, our MT+AG+LM models outperforms the MT+AG and monolingual/multi-source SEQ2SEQ models. Specifically, our model outperform MT+AG in 500K+12K training condition by almost 1 BLEU score on test2017. As expected, the models trained on 23K data perform better than those trained on 12K; further gains are obtained by adding 500K synthetic data. Interestingly, training MT+AG and MT+AG+LM models on 23K data lead to better TER/BLEU than those trained on 500K+12K. This implies the importance of in-domain training data, as the synthetic corpus is created using general domain Common-Crawl corpus.",
        "sentences": [
            "4.1 Results.",
            "Table 1 shows the result on different training datasets to compare our model against the baselines.",
            "Original MT is the strong standard donothing baseline, copying the MT translation as the PE output.",
            "In all settings, our MT+AG+LM models outperforms the MT+AG and monolingual/multi-source SEQ2SEQ models.",
            "Specifically, our model outperform MT+AG in 500K+12K training condition by almost 1 BLEU score on test2017.",
            "As expected, the models trained on 23K data perform better than those trained on 12K; further gains are obtained by adding 500K synthetic data.",
            "Interestingly, training MT+AG and MT+AG+LM models on 23K data lead to better TER/BLEU than those trained on 500K+12K.",
            "This implies the importance of in-domain training data, as the synthetic corpus is created using general domain Common-Crawl corpus."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Original MT",
                "TGT \u2192 PE",
                "SRC+TGT \u2192 PE",
                "MT+AG",
                "MT+AG+LM",
                "dev",
                "test2016",
                "test2017"
            ],
            [
                "Original MT"
            ],
            [
                "MT+AG+LM",
                "TGT \u2192 PE",
                "SRC+TGT \u2192 PE",
                "MT+AG"
            ],
            [
                "MT+AG+LM",
                "MT+AG",
                "500K+12K",
                "test2017",
                "BLEU"
            ],
            [
                "12K",
                "500K+12K",
                "23K",
                "500K+23K"
            ],
            [
                "MT+AG",
                "MT+AG+LM",
                "23K",
                "500K+12K",
                "TER",
                "BLEU"
            ],
            null
        ],
        "n_sentence": 8.0,
        "table_id": "table_1",
        "paper_id": "D18-1341",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1345table_2",
        "description": "We compare the CLM\u00e2\u20ac\u2122s Entity Identification against two state-of-the-art NER systems: CogCompNER (Khashabi et al., 2018) and LSTMCRF (Lample et al., 2016). We train the NER systems as usual, but at test time we convert all predictions into binary token-level annotations to get\nthe final score. As Table 2 shows, the result of Ngram CLM, which yields the highest performance, is remarkably close to the result of state-of-theart NER systems (especially for English) given the simplicity of the model.",
        "sentences": [
            "We compare the CLM\u00e2\u20ac\u2122s Entity Identification against two state-of-the-art NER systems: CogCompNER (Khashabi et al., 2018) and LSTMCRF (Lample et al., 2016).",
            "We train the NER systems as usual, but at test time we convert all predictions into binary token-level annotations to get\nthe final score.",
            "As Table 2 shows, the result of Ngram CLM, which yields the highest performance, is remarkably close to the result of state-of-theart NER systems (especially for English) given the simplicity of the model."
        ],
        "class_sentence": [
            1,
            2,
            1
        ],
        "header_mention": [
            [
                "SRILM",
                "CogCompNER (ceiling)",
                "Lample et al. (2016) (ceiling)"
            ],
            null,
            [
                "SRILM"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_2",
        "paper_id": "D18-1345",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1345table_3",
        "description": "The results in Table 3 show that for six of the eight languages we studied, the baseline NER can be significantly improved by adding simple CLM features; for English and Arabic, it performs better even than the neural NER model of (Lample et al., 2016). For Tagalog, however, adding CLM features actually impairs system performance. In the same table, the rows marked \u201cunseen\u201d report systems\u2019 performance on named entities in Test that were not seen in the training data. This setting more directly assesses the robustness of a system to identify named entities in new data. By this measure, Farsi NER is not improved by nameonly CLM features and Tagalog is impaired. Benefits for English, Hindi, and Somali are limited, but are quite significant for Amharic, Arabic, and Bengali.",
        "sentences": [
            "The results in Table 3 show that for six of the eight languages we studied, the baseline NER can be significantly improved by adding simple CLM features; for English and Arabic, it performs better even than the neural NER model of (Lample et al., 2016).",
            "For Tagalog, however, adding CLM features actually impairs system performance.",
            "In the same table, the rows marked \u201cunseen\u201d report systems\u2019 performance on named entities in Test that were not seen in the training data.",
            "This setting more directly assesses the robustness of a system to identify named entities in new data.",
            "By this measure, Farsi NER is not improved by nameonly CLM features and Tagalog is impaired.",
            "Benefits for English, Hindi, and Somali are limited, but are quite significant for Amharic, Arabic, and Bengali."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "CogCompNER+LM",
                "Lample et al. (2016)",
                "eng",
                "ara"
            ],
            [
                "CogCompNER",
                "tgl"
            ],
            [
                "Unseen"
            ],
            [
                "Unseen"
            ],
            [
                "CogCompNER+LM",
                "Unseen",
                "fas",
                "tgl"
            ],
            [
                "CogCompNER+LM",
                "Unseen",
                "eng",
                "amh",
                "ara",
                "ben",
                "hin",
                "som"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_3",
        "paper_id": "D18-1345",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1349table_4",
        "description": "5 Results and Discussion. Table 4 compares our model against the best performing models in the literature (Dernoncourt et al. 2016; Liu et al. 2013). There are two variants of our model in terms of different implementations of the sentence encoding layer: the model that uses bi-RNN to encode the sentence is called HSLN-RNN; while the model that uses the CNN module is named HSLN-CNN. We have evaluated both model variants on all datasets. And as evidenced by Table 4, our best model can improve the F1 scores by 2%-3% in absolute number compared with the previous best published results for all datasets. For the PubMed 20k and 200k datasets, our HSLN-RNN model achieves better results; however, for the NICTA dataset, the HSLN-CNN model performs better. This makes sense because the CNN sentence encoder has fewer parameters to be optimized, thus the HSLN-CNN model is less likely to over-fit in a smaller dataset such as NICTA. With sufficient data, however, the increased capacity of the HSLN-RNN model offers performance benefits. To be noted, this performance gap between RNN and CNN sentence encoder gets larger as the dataset size increases from 20k to 200k for the PubMed dataset.",
        "sentences": [
            "5 Results and Discussion.",
            "Table 4 compares our model against the best performing models in the literature (Dernoncourt et al. 2016; Liu et al. 2013).",
            "There are two variants of our model in terms of different implementations of the sentence encoding layer: the model that uses bi-RNN to encode the sentence is called HSLN-RNN; while the model that uses the CNN module is named HSLN-CNN.",
            "We have evaluated both model variants on all datasets.",
            "And as evidenced by Table 4, our best model can improve the F1 scores by 2%-3% in absolute number compared with the previous best published results for all datasets.",
            "For the PubMed 20k and 200k datasets, our HSLN-RNN model achieves better results; however, for the NICTA dataset, the HSLN-CNN model performs better.",
            "This makes sense because the CNN sentence encoder has fewer parameters to be optimized, thus the HSLN-CNN model is less likely to over-fit in a smaller dataset such as NICTA.",
            "With sufficient data, however, the increased capacity of the HSLN-RNN model offers performance benefits.",
            "To be noted, this performance gap between RNN and CNN sentence encoder gets larger as the dataset size increases from 20k to 200k for the PubMed dataset."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            1,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "Best Published",
                "Our Models"
            ],
            [
                "HSLN-CNN",
                "HSLN-RNN"
            ],
            [
                "HSLN-CNN",
                "HSLN-RNN",
                "PubMed",
                "NICTA"
            ],
            [
                "Marco Lui (Lui 2012)",
                "bi-ANN (Dernoncourt et al. 2016)",
                "HSLN-CNN",
                "HSLN-RNN"
            ],
            [
                "HSLN-CNN",
                "HSLN-RNN",
                "PubMed",
                "20k",
                "200k",
                "NICTA"
            ],
            [
                "HSLN-CNN",
                "NICTA"
            ],
            [
                "HSLN-RNN"
            ],
            [
                "HSLN-CNN",
                "HSLN-RNN",
                "PubMed",
                "20k",
                "200k"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_4",
        "paper_id": "D18-1349",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1349table_9",
        "description": "In order to test the importance of pretrained word embeddings, we performed experiments with different sets of publicly published word embeddings, as well as our locally curated word embeddings, to initialize our model. Table 9 gives the performance of six different word embeddings for our HSLN-RNN model trained on the PubMed 20k dataset. According to Table 9, the training methods that create the word embeddings do not have a strong influence on model performance, but the corpus they are trained on does. The combination of Wikipedia and PubMed abstracts as the corpus for unsupervised word embedding training yields the best result, and the individual use of either the Wikipedia corpus or the PubMed abstracts performs much worse. Although the dataset we are using for evaluation is also from PubMed abstracts, using only the PubMed abstracts together with MIMIC notes without the Wikipedia corpus does not guarantee better result (see the \u201cFastTextP.M.+MIMIC\u201d embeddings in Table 9), which may be because the corpus size of PubMed abstracts plus MIMIC notes (about 12.8 million abstracts and 1 million notes) is not large enough for good embedding training compared with the corpus consisting of at least billion tokens such as the Wikipedia.",
        "sentences": [
            "In order to test the importance of pretrained word embeddings, we performed experiments with different sets of publicly published word embeddings, as well as our locally curated word embeddings, to initialize our model.",
            "Table 9 gives the performance of six different word embeddings for our HSLN-RNN model trained on the PubMed 20k dataset.",
            "According to Table 9, the training methods that create the word embeddings do not have a strong influence on model performance, but the corpus they are trained on does.",
            "The combination of Wikipedia and PubMed abstracts as the corpus for unsupervised word embedding training yields the best result, and the individual use of either the Wikipedia corpus or the PubMed abstracts performs much worse.",
            "Although the dataset we are using for evaluation is also from PubMed abstracts, using only the PubMed abstracts together with MIMIC notes without the Wikipedia corpus does not guarantee better result (see the \u201cFastTextP.M.+MIMIC\u201d embeddings in Table 9), which may be because the corpus size of PubMed abstracts plus MIMIC notes (about 12.8 million abstracts and 1 million notes) is not large enough for good embedding training compared with the corpus consisting of at least billion tokens such as the Wikipedia."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Glove-wiki",
                "FastText-wiki",
                "FastText-P.M.+MIMIC",
                "Word2vec-News",
                "Word2vec-wiki",
                "Word2vec-wiki+P.M.",
                "P.M. 20k"
            ],
            [
                "Glove-wiki",
                "FastText-wiki",
                "FastText-P.M.+MIMIC",
                "Word2vec-News",
                "Word2vec-wiki",
                "Word2vec-wiki+P.M.",
                "P.M. 20k"
            ],
            [
                "Word2vec-News",
                "Word2vec-wiki",
                "Word2vec-wiki+P.M."
            ],
            [
                "FastText-P.M.+MIMIC",
                "P.M. 20k"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_9",
        "paper_id": "D18-1349",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1352table_2",
        "description": "Results. Table 2 shows the results for MIMIC II. Because the label set for each medical record is augmented using the ICD-9 hierarchy, we expect methods that use the hierarchy to have an advantage. Table 2 results do not rely on thresholding because we evaluate using the relative ranking of groups with similar frequencies. ACNN performs best on frequent labels. For few-shot labels, ZAGCNN outperforms ACNN by over 10% in R@10 and by 8% in R@5; compared to these R@k gains for few-shot labels, our loss on frequent labels is minimal (< 1%). We find that the word embedding derived label vectors work best for ESZSL on zero-shot labels. However, this setup is outperformed by GRALS derived label vectors on the frequent and few-shot labels. On zero-shot labels, ZAGCNN outperforms the best ESZSL variant by over 16% for both R@5 and R@10. Also, we find that the GCNN layers help both fe- and zero- shot labels. Finally, similar to the setup in Xian et al. (2017), we also compute the harmonic average across all R@5 and all R@10 scores. The metric is only computed for methods that can predict zero-shot classes. We find that ZAGCNN outperforms ZACNN by 4% for R@10.",
        "sentences": [
            "Results.",
            "Table 2 shows the results for MIMIC II.",
            "Because the label set for each medical record is augmented using the ICD-9 hierarchy, we expect methods that use the hierarchy to have an advantage.",
            "Table 2 results do not rely on thresholding because we evaluate using the relative ranking of groups with similar frequencies.",
            "ACNN performs best on frequent labels.",
            "For few-shot labels, ZAGCNN outperforms ACNN by over 10% in R@10 and by 8% in R@5; compared to these R@k gains for few-shot labels, our loss on frequent labels is minimal (< 1%).",
            "We find that the word embedding derived label vectors work best for ESZSL on zero-shot labels.",
            "However, this setup is outperformed by GRALS derived label vectors on the frequent and few-shot labels.",
            "On zero-shot labels, ZAGCNN outperforms the best ESZSL variant by over 16% for both R@5 and R@10.",
            "Also, we find that the GCNN layers help both fe- and zero- shot labels.",
            "Finally, similar to the setup in Xian et al. (2017), we also compute the harmonic average across all R@5 and all R@10 scores.",
            "The metric is only computed for methods that can predict zero-shot classes.",
            "We find that ZAGCNN outperforms ZACNN by 4% for R@10."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            2,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            [
                "ACNN (Mullenbach et al. 2018) *",
                "S"
            ],
            [
                "ZAGCNN",
                "ACNN (Mullenbach et al. 2018) *",
                "F",
                "R@5",
                "R@10",
                "S"
            ],
            [
                "ESZSL + W2V",
                "ESZSL + W2V 2",
                "Z"
            ],
            [
                "ESZSL + GRALS",
                "F",
                "S"
            ],
            [
                "Z",
                "ZAGCNN",
                "ESZSL + W2V 2",
                "R@5",
                "R@10"
            ],
            [
                "ZAGCNN",
                "F",
                "Z"
            ],
            [
                "Harmonic Average",
                "R@5",
                "R@10"
            ],
            [
                "Harmonic Average"
            ],
            [
                "ZAGCNN",
                "ZACNN",
                "Harmonic Average",
                "R@10"
            ]
        ],
        "n_sentence": 13.0,
        "table_id": "table_2",
        "paper_id": "D18-1352",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1353table_3",
        "description": "Table 3 gives human evaluation results. MRL achieves better results than the other two models. Since fluency is quite easy to be optimized, our method gets close to human-authored poems on Fluency. The biggest gap between MRL and GT lies on Meaning. It\u00e2\u20ac\u2122s a complex criterion involving the use of words, topic, emotion expression and so on. The utilization of TF-IDF does ameliorate the use of words on diversity and innovation, hence improving Meaningfulness to some extent, but there are still lots to do.",
        "sentences": [
            "Table 3 gives human evaluation results.",
            "MRL achieves better results than the other two models.",
            "Since fluency is quite easy to be optimized, our method gets close to human-authored poems on Fluency.",
            "The biggest gap between MRL and GT lies on Meaning.",
            "It\u00e2\u20ac\u2122s a complex criterion involving the use of words, topic, emotion expression and so on.",
            "The utilization of TF-IDF does ameliorate the use of words on diversity and innovation, hence improving Meaningfulness to some extent, but there are still lots to do."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "MRL",
                "Base",
                "Mem"
            ],
            [
                "MRL"
            ],
            [
                "MRL",
                "GT"
            ],
            null,
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_3",
        "paper_id": "D18-1353",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1358table_6",
        "description": "4.4.2 Experimental Results. Finally, the evaluation results in Table 6 lead to the following findings:. (1) Our models outperform other baselines on WN11 and FB15k, and obtain comparable results with baselines on FB13, which validate the effectiveness of our models;. (2) The extended models TransE-HRS, TransH-HRS and DistMult-HRS achieve substantial improvements against the original models. On WN11, TransE-HRS outperforms TransE with a margin as large as 10.9%. These improvements indicates the technique of utilizing the HRS information is capable to be extended to different KGE models.",
        "sentences": [
            "4.4.2 Experimental Results.",
            "Finally, the evaluation results in Table 6 lead to the following findings:.",
            "(1) Our models outperform other baselines on WN11 and FB15k, and obtain comparable results with baselines on FB13, which validate the effectiveness of our models;.",
            "(2) The extended models TransE-HRS, TransH-HRS and DistMult-HRS achieve substantial improvements against the original models.",
            "On WN11, TransE-HRS outperforms TransE with a margin as large as 10.9%.",
            "These improvements indicates the technique of utilizing the HRS information is capable to be extended to different KGE models."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "TransE-HRS",
                "TransH-HRS",
                "DistMult-HRS",
                "WN11",
                "FB13",
                "FB15k"
            ],
            [
                "TransE-HRS",
                "TransH-HRS",
                "DistMult-HRS",
                "TransE",
                "TransH",
                "DistMult"
            ],
            [
                "WN11",
                "TransE-HRS",
                "TransE"
            ],
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_6",
        "paper_id": "D18-1358",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1359table_4",
        "description": "Relation Breakdown. We perform additional analysis on the YAGO dataset to gain a deeper understanding of the performance of our model using ConvE method. Table 4 compares our models on some of the most frequent relations. As shown, the model that includes textual description significantly benefits isAffiliatedTo, and playsFor relations, as this information often appears in text. Moreover, images are useful for hasGender and isMarriedTo, while for the relation isConnectedTo, numerical (dates) are more effective than images.",
        "sentences": [
            "Relation Breakdown.",
            "We perform additional analysis on the YAGO dataset to gain a deeper understanding of the performance of our model using ConvE method.",
            "Table 4 compares our models on some of the most frequent relations.",
            "As shown, the model that includes textual description significantly benefits isAffiliatedTo, and playsFor relations, as this information often appears in text.",
            "Moreover, images are useful for hasGender and isMarriedTo, while for the relation isConnectedTo, numerical (dates) are more effective than images."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "isAffiliatedTo",
                "playsFor",
                "hasGender",
                "isConnectedTo",
                "isMarriedTo"
            ],
            [
                "isAffiliatedTo",
                "playsFor",
                "+Description"
            ],
            [
                "hasGender",
                "isMarriedTo",
                "isConnectedTo",
                "+Images",
                "+Numbers"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_4",
        "paper_id": "D18-1359",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1360table_4",
        "description": "Results on SemEval 17. Table 4 compares the results of our model with the state of the art on the SemEval 17 dataset for tasks of span identification, keyphrase extraction and relation extraction as well as the overall score. Span identification aims at identifying spans of entities. Keyphrase classification and relation extraction has the same setting with the entity and relation extraction in SCIERC. Our model outperforms all the previous models that use hand-designed features. We observe more significant improvement in span identification than keyphrase classification. This confirms the benefit of our model in enumerating spans (rather than BIO tagging in state-of-the-art systems). Moreover, we have competitive results compared to the previous state of the art in relation extraction. We observe less gain compared to the SCIERC dataset mainly because there are no coference links, and the relation types are not comprehensive.",
        "sentences": [
            "Results on SemEval 17.",
            "Table 4 compares the results of our model with the state of the art on the SemEval 17 dataset for tasks of span identification, keyphrase extraction and relation extraction as well as the overall score.",
            "Span identification aims at identifying spans of entities.",
            "Keyphrase classification and relation extraction has the same setting with the entity and relation extraction in SCIERC.",
            "Our model outperforms all the previous models that use hand-designed features.",
            "We observe more significant improvement in span identification than keyphrase classification.",
            "This confirms the benefit of our model in enumerating spans (rather than BIO tagging in state-of-the-art systems).",
            "Moreover, we have competitive results compared to the previous state of the art in relation extraction.",
            "We observe less gain compared to the SCIERC dataset mainly because there are no coference links, and the relation types are not comprehensive."
        ],
        "class_sentence": [
            2,
            1,
            2,
            2,
            1,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Span Indentification",
                "Keyphrase Extraction",
                "Relation Extraction",
                " Overall"
            ],
            [
                "Span Indentification"
            ],
            [
                "Keyphrase Extraction",
                "Relation Extraction"
            ],
            [
                "SCIIE"
            ],
            [
                "SCIIE",
                "Span Indentification",
                "Keyphrase Extraction"
            ],
            [
                "SCIIE",
                "Span Indentification"
            ],
            [
                "SCIIE",
                "Best SemEval",
                "Relation Extraction"
            ],
            null
        ],
        "n_sentence": 9.0,
        "table_id": "table_4",
        "paper_id": "D18-1360",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1362table_2",
        "description": "5 Results 5.1 Model Comparison . Table 2 shows the evaluation results of our proposed approach and the baselines. The top part presents embedding based approaches and the bottom part presents multi-hop reasoning approaches. We find embedding based models perform strongly on several datasets, achieving overall best evaluation metrics on UMLS, Kinship, FB15K237 and NELL-995 despite their simplicity. While previous path based approaches achieve comparable performance on some of the datasets (WN18RR, NELL-995, and UMLS), they perform significantly worse than the embedding based models on the other datasets (9.1 and 14.2 absolute points lower on Kinship and FB15k-237 respectively). A possible reason for this is that embedding based methods map every link in the KG into the same embedding space, which implicitly encodes the connectivity of the whole graph. In contrast, path based models use the discrete represen tation of a KG as input, and therefore have to leave out a significant proportion of the combinatorial path space by selection. For some path based approaches, computation cost is a bottleneck. In particular, NeuralLP and NTP-\u03bb failed to scale to the larger datasets and their results are omitted from the table, as Das et al. (2018) reported. Ours is the first multi-hop reasoning approach which is consistently comparable or better than embedding based approaches on all five datasets. The best single model, Ours(ConvE), improves the SOTA performance of path-based models on three datasets (UMLS, Kinship, and FB15k-237) by 4%, 9%, and 39% respectively. On NELL-995, our approach did not significantly improve over existing SOTA. The NELL-995 dataset consists of only 12 relations in the test set and, as we further detail in the analysis (\u00a7 5.3.3), our approach is less effective for those relation types. The model variations using different reward shaping modules perform similarly. While a better reward shaping module typically results in a better overall model, an exception is WN18RR, where ComplEx performs slightly worse on its own but is more helpful for reward shaping. We left the study of the relationship between the reward shaping module accuracy and the overall model performance as future work.",
        "sentences": [
            "5 Results 5.1 Model Comparison .",
            "Table 2 shows the evaluation results of our proposed approach and the baselines.",
            "The top part presents embedding based approaches and the bottom part presents multi-hop reasoning approaches.",
            "We find embedding based models perform strongly on several datasets, achieving overall best evaluation metrics on UMLS, Kinship, FB15K237 and NELL-995 despite their simplicity.",
            "While previous path based approaches achieve comparable performance on some of the datasets (WN18RR, NELL-995, and UMLS), they perform significantly worse than the embedding based models on the other datasets (9.1 and 14.2 absolute points lower on Kinship and FB15k-237 respectively).",
            "A possible reason for this is that embedding based methods map every link in the KG into the same embedding space, which implicitly encodes the connectivity of the whole graph.",
            "In contrast, path based models use the discrete represen tation of a KG as input, and therefore have to leave out a significant proportion of the combinatorial path space by selection.",
            "For some path based approaches, computation cost is a bottleneck.",
            "In particular, NeuralLP and NTP-\u03bb failed to scale to the larger datasets and their results are omitted from the table, as Das et al. (2018) reported.",
            "Ours is the first multi-hop reasoning approach which is consistently comparable or better than embedding based approaches on all five datasets.",
            "The best single model, Ours(ConvE), improves the SOTA performance of path-based models on three datasets (UMLS, Kinship, and FB15k-237) by 4%, 9%, and 39% respectively.",
            "On NELL-995, our approach did not significantly improve over existing SOTA.",
            "The NELL-995 dataset consists of only 12 relations in the test set and, as we further detail in the analysis (\u00a7 5.3.3), our approach is less effective for those relation types.",
            "The model variations using different reward shaping modules perform similarly.",
            "While a better reward shaping module typically results in a better overall model, an exception is WN18RR, where ComplEx performs slightly worse on its own but is more helpful for reward shaping.",
            "We left the study of the relationship between the reward shaping module accuracy and the overall model performance as future work."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            2,
            2,
            2,
            1,
            1,
            1,
            1,
            2,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "DistMult (Yang et al. 2014)",
                "ComplEx (Trouillon et al. 2016)",
                "ConvE (Dettmers et al. 2018)",
                "NeuralLP (Yang et al. 2017)",
                "NTP-\u03bb (Rocktaschel et. al. 2017)",
                "MINERVA (Das et al. 2018)",
                "Ours(ComplEx)",
                "Ours(ConvE)"
            ],
            [
                "DistMult (Yang et al. 2014)",
                "ComplEx (Trouillon et al. 2016)",
                "ConvE (Dettmers et al. 2018)",
                "UMLS",
                "Kinship",
                "FB15k-237",
                "NELL-995"
            ],
            [
                "DistMult (Yang et al. 2014)",
                "ComplEx (Trouillon et al. 2016)",
                "ConvE (Dettmers et al. 2018)",
                "UMLS",
                "Kinship",
                "FB15k-237",
                "WN18RR",
                "NELL-995"
            ],
            null,
            null,
            null,
            [
                "NeuralLP (Yang et al. 2017)",
                "NTP-\u03bb (Rocktaschel et. al. 2017)"
            ],
            [
                "Ours(ComplEx)",
                "Ours(ConvE)"
            ],
            [
                "Ours(ConvE)",
                "UMLS",
                "Kinship",
                "FB15k-237"
            ],
            [
                "Ours(ComplEx)",
                "Ours(ConvE)",
                "NELL-995"
            ],
            [
                "NELL-995"
            ],
            null,
            [
                "ComplEx (Trouillon et al. 2016)"
            ],
            null
        ],
        "n_sentence": 16.0,
        "table_id": "table_2",
        "paper_id": "D18-1362",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1362table_5",
        "description": "Table 5 shows the percentage of examples associated with seen and unseen queries on each dev dataset and the corresponding MRR evaluation metrics of previously studied models. On most datasets, the ratio of seen vs. unseen queries is similar to that of to-many vs. to-one relations (Table 4) as a result of random data split, with the exception of WN18RR. On some datasets, all models perform better on seen queries (UMLS, Kinship, WN18RR) while others reveal the opposite trend. We leave the study of these model behaviors to future work. On NELL-995 both of our proposed enhancements are not effective over the seen queries. In most cases, our proposed enhancements improve the performance over unseen queries, with AD being more effective.",
        "sentences": [
            "Table 5 shows the percentage of examples associated with seen and unseen queries on each dev dataset and the corresponding MRR evaluation metrics of previously studied models.",
            "On most datasets, the ratio of seen vs. unseen queries is similar to that of to-many vs. to-one relations (Table 4) as a result of random data split, with the exception of WN18RR.",
            "On some datasets, all models perform better on seen queries (UMLS, Kinship, WN18RR) while others reveal the opposite trend.",
            "We leave the study of these model behaviors to future work.",
            "On NELL-995 both of our proposed enhancements are not effective over the seen queries.",
            "In most cases, our proposed enhancements improve the performance over unseen queries, with AD being more effective."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "%",
                "Seen Queries",
                "Unseen Queries"
            ],
            [
                "UMLS",
                "Kinship",
                "FB15k-237",
                "NELL-995",
                "%",
                "Seen Queries",
                "Unseen Queries"
            ],
            [
                "Seen Queries",
                "UMLS",
                "Kinship",
                "WN18RR"
            ],
            null,
            [
                "NELL-995",
                "-RS",
                "-AD",
                "Seen Queries"
            ],
            [
                "Unseen Queries",
                "-RS",
                "-AD"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_5",
        "paper_id": "D18-1362",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1363table_1",
        "description": "4.4 Results. Our results are shown in Table 1. For SET1, SIG17+SHIP obtains the highest accuracy, while, for SET2 and SET3, MED+PT+SHIP performs best. This difference can be easily explained by the fact that the performance of neural networks decreases rapidly for smaller training sets, and, while paradigm transduction strongly mitigates this problem, it cannot completely eliminate it. Overall, however, SIG17+SHIP, MED+PT, and MED+PT+SHIP all outperform the baselines by a wide margin for all settings. Effect of paradigm transduction. On average, MED+PT clearly outperforms SIG17, the strongest baseline: by .0796 (.5808-.5012) on SET1, .0910 (.7486-.6576) on SET2, and .0747 (.8454-.7707) on SET3. However, looking at each language individually (refer to Appendix A for those results), we find that MED+PT performs poorly for a few languages, namely Danish, English, and Norwegian (Bokmal & Nynorsk). We hypothesize that this can most likely be explained by the size of the input subset of those languages being small (cf. Figure 2 for average input subset sizes per language). Recall that the input subset is explored by the model during transduction. Most poorly performing languages have input subsets containing only the lemma; in this case paradigm transduction reduces to autoencoding the lemma. Thus, we conclude that paradigm transduction can only improve over MED if two or more sources are given. Conversely, if we consider only the languages with an average input subset size of more than 15 (Basque, Haida, Hindi, Khaling, Persian, and Quechua), the average accuracy of MED+PT for SET1 is 0.9564, compared to an overall average of 0.5808. This observation shows clearly that paradigm transduction obtains strong results if many forms per paradigm are given. Effect of SHIP. Further, Table 1 shows that SIG17+SHIP is better than SIG17 by .0959 (.5971-.5012) on SET1, .0779 (.7355-.6576) on SET2, and .0301 (.8008-.7707) on SET3. Stronger effects for smaller amounts of training data indicate that SHIP's strategy of selecting a single reliable source is more important for weaker final models; in these cases, selecting the most deterministic source reduces errors due to noise. In contrast, the performance of MED, the neural model, is relatively independent of the choice of source; this is in line with earlier findings (Cotterell et al., 2016). However, even for MED+PT, adding SHIP (i.e., MED+PT+SHIP) slightly increases accuracy by .0061 (.7547-.7486) on SET2, and .0029 (.8483-.8454) on SET3 (L53). Ablation. MED does not perform well for either SET1 or SET2. In contrast, on SET3 it even outperforms SIG17 for a few languages. However, MED loses against MED+PT in all cases, highlighting the positive effect of paradigm transduction. Looking at PT next, even though PT does not have a zero accuracy for any setting or language, it performs consistently worse than MED+PT. For SET3, PT is even lower than MED on average, by .3436 (.4211-.0775). Note that, in contrast to the other methods, PT\u2019s performance is not dependent on the size of the training set. The main determinant for PT\u2019s performance is the size of the input subset during transductive inference. If the input subset is large, PT can perform better than MED, e.g., for Hindi and Urdu. For Khaling SET1, PT even outperforms both MED and SIG17. However, in most cases, PT does not perform well on its own. MED+PT outperforms both MED and PT. This confirms our initial intuition: MED and PT learn complementary information for paradigm completion. The base model learns the general structure of the language (i.e., correspondences between tags and inflections) while paradigm transduction teaches the model which character sequences are common in a specific test paradigm.",
        "sentences": [
            "4.4 Results.",
            "Our results are shown in Table 1.",
            "For SET1, SIG17+SHIP obtains the highest accuracy, while, for SET2 and SET3, MED+PT+SHIP performs best.",
            "This difference can be easily explained by the fact that the performance of neural networks decreases rapidly for smaller training sets, and, while paradigm transduction strongly mitigates this problem, it cannot completely eliminate it.",
            "Overall, however, SIG17+SHIP, MED+PT, and MED+PT+SHIP all outperform the baselines by a wide margin for all settings.",
            "Effect of paradigm transduction.",
            "On average, MED+PT clearly outperforms SIG17, the strongest baseline: by .0796 (.5808-.5012) on SET1, .0910 (.7486-.6576) on SET2, and .0747 (.8454-.7707) on SET3.",
            "However, looking at each language individually (refer to Appendix A for those results), we find that MED+PT performs poorly for a few languages, namely Danish, English, and Norwegian (Bokmal & Nynorsk).",
            "We hypothesize that this can most likely be explained by the size of the input subset of those languages being small (cf. Figure 2 for average input subset sizes per language).",
            "Recall that the input subset is explored by the model during transduction.",
            "Most poorly performing languages have input subsets containing only the lemma; in this case paradigm transduction reduces to autoencoding the lemma.",
            "Thus, we conclude that paradigm transduction can only improve over MED if two or more sources are given.",
            "Conversely, if we consider only the languages with an average input subset size of more than 15 (Basque, Haida, Hindi, Khaling, Persian, and Quechua), the average accuracy of MED+PT for SET1 is 0.9564, compared to an overall average of 0.5808.",
            "This observation shows clearly that paradigm transduction obtains strong results if many forms per paradigm are given.",
            "Effect of SHIP.",
            "Further, Table 1 shows that SIG17+SHIP is better than SIG17 by .0959 (.5971-.5012) on SET1, .0779 (.7355-.6576) on SET2, and .0301 (.8008-.7707) on SET3.",
            "Stronger effects for smaller amounts of training data indicate that SHIP's strategy of selecting a single reliable source is more important for weaker final models; in these cases, selecting the most deterministic source reduces errors due to noise.",
            "In contrast, the performance of MED, the neural model, is relatively independent of the choice of source; this is in line with earlier findings (Cotterell et al., 2016).",
            "However, even for MED+PT, adding SHIP (i.e., MED+PT+SHIP) slightly increases accuracy by .0061 (.7547-.7486) on SET2, and .0029 (.8483-.8454) on SET3 (L53).",
            "Ablation.",
            "MED does not perform well for either SET1 or SET2.",
            "In contrast, on SET3 it even outperforms SIG17 for a few languages.",
            "However, MED loses against MED+PT in all cases, highlighting the positive effect of paradigm transduction.",
            "Looking at PT next, even though PT does not have a zero accuracy for any setting or language, it performs consistently worse than MED+PT.",
            "For SET3, PT is even lower than MED on average, by .3436 (.4211-.0775).",
            "Note that, in contrast to the other methods, PT\u2019s performance is not dependent on the size of the training set.",
            "The main determinant for PT\u2019s performance is the size of the input subset during transductive inference.",
            "If the input subset is large, PT can perform better than MED, e.g., for Hindi and Urdu.",
            "For Khaling SET1, PT even outperforms both MED and SIG17.",
            "However, in most cases, PT does not perform well on its own.",
            "MED+PT outperforms both MED and PT.",
            "This confirms our initial intuition: MED and PT learn complementary information for paradigm completion.",
            "The base model learns the general structure of the language (i.e., correspondences between tags and inflections) while paradigm transduction teaches the model which character sequences are common in a specific test paradigm."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            1,
            2,
            1,
            2,
            2,
            2,
            2,
            2,
            1,
            2,
            2,
            1,
            2,
            1,
            1,
            2,
            1,
            2,
            1,
            1,
            1,
            2,
            2,
            2,
            2,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "SET1",
                "SET2",
                "SET3",
                "SIG17+SHIP",
                "MED+PT+SHIP"
            ],
            null,
            [
                "SIG17+SHIP",
                "MED+PT",
                "MED+PT+SHIP"
            ],
            null,
            [
                "MED+PT",
                "BL: SIG17",
                "SET1",
                "SET2",
                "SET3"
            ],
            [
                "MED+PT"
            ],
            null,
            null,
            null,
            null,
            [
                "MED+PT",
                "SET1"
            ],
            null,
            null,
            [
                "SIG17+SHIP",
                "BL: SIG17",
                "SET1",
                "SET2",
                "SET3"
            ],
            [
                "SIG17+SHIP"
            ],
            null,
            [
                "MED+PT",
                "MED+PT+SHIP",
                "SET2",
                "SET3"
            ],
            null,
            [
                "BL: MED",
                "SET1",
                "SET2"
            ],
            [
                "BL: MED",
                "BL: SIG17",
                "SET3"
            ],
            [
                "BL: MED",
                "MED+PT"
            ],
            [
                "BL: PT",
                "MED+PT"
            ],
            [
                "BL: PT",
                "BL: MED",
                "SET3"
            ],
            [
                "BL: PT"
            ],
            [
                "BL: PT"
            ],
            [
                "BL: PT",
                "BL: MED"
            ],
            [
                "SET1",
                "BL: MED",
                "BL: SIG17",
                "BL: PT"
            ],
            [
                "BL: MED",
                "BL: SIG17",
                "BL: PT"
            ],
            [
                "MED+PT",
                "BL: PT",
                "BL: MED"
            ],
            [
                "BL: PT",
                "BL: MED"
            ],
            null
        ],
        "n_sentence": 33.0,
        "table_id": "table_1",
        "paper_id": "D18-1363",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1366table_3",
        "description": "For example lemma + morph means lemma and morph embeddings are first pre-trained on the resoucerich language and then used to initialize the respective lemma and morph representations for the low resource language. Monolingual Experiments:. Table 3 shows our results on all languages. We get +5.8 F1 points for Turkish, +4.8 F1 for Uyghur, +0.8 F1 for Hindi and +0.7 F1 for Bengali over the existing methods. We observe that a combination of character-ngrams, lemma and morphological properties gives the best performance for Uyghur and Bengali. Adding morph hurts in Turkish, in contrast to Hindi, where it helps. Section 5.3.3 discuses plausible reasons for this.",
        "sentences": [
            "For example lemma + morph means lemma and morph embeddings are first pre-trained on the resoucerich language and then used to initialize the respective lemma and morph representations for the low resource language.",
            "Monolingual Experiments:.",
            "Table 3 shows our results on all languages.",
            "We get +5.8 F1 points for Turkish, +4.8 F1 for Uyghur, +0.8 F1 for Hindi and +0.7 F1 for Bengali over the existing methods.",
            "We observe that a combination of character-ngrams, lemma and morphological properties gives the best performance for Uyghur and Bengali.",
            "Adding morph hurts in Turkish, in contrast to Hindi, where it helps.",
            "Section 5.3.3 discuses plausible reasons for this."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "Turkish",
                "Uyghur",
                "Hindi",
                "Bengali"
            ],
            [
                "Char-ngrams + Lemma + Morph",
                "Char-ngrams + Lemma",
                "Char-ngrams + Morph",
                "Turkish",
                "Uyghur",
                "Hindi",
                "Bengali"
            ],
            [
                "Char-ngrams + Lemma + Morph",
                "Turkish",
                "Uyghur",
                "Bengali"
            ],
            [
                "Char-ngrams + Morph",
                "Turkish",
                "Hindi"
            ],
            null
        ],
        "n_sentence": 7.0,
        "table_id": "table_3",
        "paper_id": "D18-1366",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1367table_1",
        "description": "Cross-validation is conducted in two settings. In one (Hype-Par), the non-hyperbolic sentences are paraphrases, in the other (Hype-Min), literal data come from the Minimal Units Corpus. The results are shown in Table 1 and Table 2. While in the Hype-Min setting the performance is not satisfying, estimators achieve above chance accuracy using paraphrases as literal inputs. In fact, in Table 1, the accuracy scores based on quantity-quality vectors (QQ column) suggest that our handcrafted features are actually useful for detecting hyperboles. Therefore, to gain further insight on their informativeness, we conduct a recurrent feature ablation and observed how different subsets affect predictions. Figure 1 illustrates that 5 features can maximize the accuracy of LR. SVM and LDA behave the same with a set of the same size, and they all assign high weights to imageability, unexpectedness and subjectivity. The three models become comparable to, and yet do not outperform, the second and third baselines. In the attempt to improve the models\u00e2\u20ac\u2122 description of the data, we repeat the experiment with yet another set of features. We merge the QQ with the Skip-Gram and GloVe features, by separately concatenating the two types of vectors to our data representations (Skip-Gram+QQ and GloVe+QQ columns). An interesting trend appears both for Hype-Par and Hype-Min: with Skip-Gram+QQ, algorithms perform better than relying on Skip-Gram or QQ alone, and the same happens for Glove+QQ. The new sets of features produce a consistent improvement over the baselines and over our own features. LR outstands other classifiers in the SkipGram+QQ combination, reaching .72 mean accuracy and .76 average F1-score (see Table 3).",
        "sentences": [
            "Cross-validation is conducted in two settings.",
            "In one (Hype-Par), the non-hyperbolic sentences are paraphrases, in the other (Hype-Min), literal data come from the Minimal Units Corpus.",
            "The results are shown in Table 1 and Table 2.",
            "While in the Hype-Min setting the performance is not satisfying, estimators achieve above chance accuracy using paraphrases as literal inputs.",
            "In fact, in Table 1, the accuracy scores based on quantity-quality vectors (QQ column) suggest that our handcrafted features are actually useful for detecting hyperboles.",
            "Therefore, to gain further insight on their informativeness, we conduct a recurrent feature ablation and observed how different subsets affect predictions.",
            "Figure 1 illustrates that 5 features can maximize the accuracy of LR.",
            "SVM and LDA behave the same with a set of the same size, and they all assign high weights to imageability, unexpectedness and subjectivity.",
            "The three models become comparable to, and yet do not outperform, the second and third baselines.",
            "In the attempt to improve the models\u00e2\u20ac\u2122 description of the data, we repeat the experiment with yet another set of features.",
            "We merge the QQ with the Skip-Gram and GloVe features, by separately concatenating the two types of vectors to our data representations (Skip-Gram+QQ and GloVe+QQ columns).",
            "An interesting trend appears both for Hype-Par and Hype-Min: with Skip-Gram+QQ, algorithms perform better than relying on Skip-Gram or QQ alone, and the same happens for Glove+QQ.",
            "The new sets of features produce a consistent improvement over the baselines and over our own features.",
            "LR outstands other classifiers in the SkipGram+QQ combination, reaching .72 mean accuracy and .76 average F1-score (see Table 3)."
        ],
        "class_sentence": [
            2,
            2,
            1,
            2,
            1,
            2,
            2,
            2,
            1,
            2,
            2,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            [
                "QQ"
            ],
            null,
            [
                "LR"
            ],
            [
                "SVM",
                "LDA"
            ],
            [
                "LR",
                "SVM",
                "LDA"
            ],
            null,
            [
                "Skip-gram+QQ",
                "GloVe+QQ"
            ],
            [
                "QQ",
                "Skip-gram",
                "GloVe",
                "Skip-gram+QQ",
                "GloVe+QQ"
            ],
            [
                "Skip-gram+QQ",
                "GloVe+QQ"
            ],
            [
                "Skip-gram+QQ",
                "LR"
            ]
        ],
        "n_sentence": 14.0,
        "table_id": "table_1",
        "paper_id": "D18-1367",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1368table_4",
        "description": "Table 4 shows cross-genre experimental results of our neural network models on the training set of MASC+Wiki by treating each genre as one crossvalidation fold. As we expected, both the macroaverage F1-score and class-wise F1 scores are lower compared with the results in Table 2 where in-genre data were used for model training as well. But the performance drop on the paragraph-level models is little, which clearly outperform the previous system (Friedrich et al., 2016) and the baseline model by a large margin.",
        "sentences": [
            "Table 4 shows cross-genre experimental results of our neural network models on the training set of MASC+Wiki by treating each genre as one crossvalidation fold.",
            "As we expected, both the macroaverage F1-score and class-wise F1 scores are lower compared with the results in Table 2 where in-genre data were used for model training as well.",
            "But the performance drop on the paragraph-level models is little, which clearly outperform the previous system (Friedrich et al., 2016) and the baseline model by a large margin."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Paragraph-level Model",
                "Macro",
                "F1 STA",
                "F1 EVE",
                "F1 REP",
                "F1 GENI",
                "F1 GENA",
                "F1 QUE",
                "F1 IMP"
            ],
            [
                "Paragraph-level Model",
                "CRF (Friedrich et al. 2016)",
                "Clause-level Bi-LSTM"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_4",
        "paper_id": "D18-1368",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1369table_4",
        "description": "Table 4 shows the thread reconstruction results of our model and the baseline models in the Wikipedia conversation dataset. Since the HDHP model does not infer the parent event, we reconstruct threads in the form of chronologically ordered linked list of posts in each local cluster that inferred from HDHP. From the F1node score of the results, we establish our model performs better than other baseline models.",
        "sentences": [
            "Table 4 shows the thread reconstruction results of our model and the baseline models in the Wikipedia conversation dataset.",
            "Since the HDHP model does not infer the parent event, we reconstruct threads in the form of chronologically ordered linked list of posts in each local cluster that inferred from HDHP.",
            "From the F1node score of the results, we establish our model performs better than other baseline models."
        ],
        "class_sentence": [
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "HDHP"
            ],
            [
                "HD-GMHP"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_4",
        "paper_id": "D18-1369",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1371table_5",
        "description": "Bottom rows in Table 5 report the end-to-end performance of our five systems on both domains. On both labeled and unlabeled parsing, our basic neural model with only lexical input performs comparable to the logistic regression model. And our enriched neural model with only three simple linguistic features outperforms both the logistic regression model and the basic neural model on news, improving the performance by more than 10%. However, our models only slightly improve the unlabeled parsing over the simple baseline on narrative Grimm data. This is probably due to (1) it is a very strong baseline to link every node to its immediate previous node, since in an narrative discourse linear temporal sequences are very common; and (2) most events breaking the temporal linearity in a narrative discourse are implicit stative descriptions which are harder to model with only lexical and distance features. Finally, attention mechanism improves temporal relation labeling on both domains. 5.3.2 Temporal Relation Evaluation. To facilitate comparison with previous work where gold events are used as parser input, we report our results on temporal dependency parsing with gold time expression and event spans in Table 5 (top rows). These results are in the same ballpark as what is reported in previous work on temporal relation extraction. The best performance in Kolomiyets et al. (2012) are 0.84 and 0.65 fscores for unlabeled and labeled parses, achieved by temporal structure parsers trained and evaluated on narrative children\u00e2\u20ac\u2122s stories. Our best performing model (Neural-attention) reports 0.81 and 0.70 f-scores on unlabeled and labeled parses respectively, showing similar performance. It is important to note, however, that these two works use different data sets, and are not directly comparable. Finally, parsing accuracy with gold time/event spans as input is substantially higher than that with predicted spans, showing the effects of error propagation.",
        "sentences": [
            "Bottom rows in Table 5 report the end-to-end performance of our five systems on both domains.",
            "On both labeled and unlabeled parsing, our basic neural model with only lexical input performs comparable to the logistic regression model.",
            "And our enriched neural model with only three simple linguistic features outperforms both the logistic regression model and the basic neural model on news, improving the performance by more than 10%.",
            "However, our models only slightly improve the unlabeled parsing over the simple baseline on narrative Grimm data.",
            "This is probably due to (1) it is a very strong baseline to link every node to its immediate previous node, since in an narrative discourse linear temporal sequences are very common; and (2) most events breaking the temporal linearity in a narrative discourse are implicit stative descriptions which are harder to model with only lexical and distance features.",
            "Finally, attention mechanism improves temporal relation labeling on both domains.",
            "5.3.2 Temporal Relation Evaluation.",
            "To facilitate comparison with previous work where gold events are used as parser input, we report our results on temporal dependency parsing with gold time expression and event spans in Table 5 (top rows).",
            "These results are in the same ballpark as what is reported in previous work on temporal relation extraction.",
            "The best performance in Kolomiyets et al. (2012) are 0.84 and 0.65 fscores for unlabeled and labeled parses, achieved by temporal structure parsers trained and evaluated on narrative children\u00e2\u20ac\u2122s stories.",
            "Our best performing model (Neural-attention) reports 0.81 and 0.70 f-scores on unlabeled and labeled parses respectively, showing similar performance.",
            "It is important to note, however, that these two works use different data sets, and are not directly comparable.",
            "Finally, parsing accuracy with gold time/event spans as input is substantially higher than that with predicted spans, showing the effects of error propagation."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            1,
            2,
            1,
            1,
            2,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "Neural-basic",
                "Baseline-logistic",
                "unlabeled f",
                "labeled f",
                "end-to-end systems with automatic spans"
            ],
            [
                "Baseline-logistic",
                "Neural-basic",
                "Neural-enriched",
                "news",
                "end-to-end systems with automatic spans"
            ],
            [
                "grimm",
                "unlabeled f",
                "Baseline-simple",
                "Neural-basic",
                "Neural-enriched",
                "Neural-attention",
                "end-to-end systems with automatic spans"
            ],
            null,
            [
                "Neural-attention",
                "news",
                "grimm",
                "end-to-end systems with automatic spans"
            ],
            null,
            null,
            [
                "temporal relation parsing with gold spans"
            ],
            [
                "temporal relation parsing with gold spans"
            ],
            [
                "Neural-attention",
                "news",
                "unlabeled f",
                "labeled f",
                "test",
                "temporal relation parsing with gold spans"
            ],
            null,
            null
        ],
        "n_sentence": 13.0,
        "table_id": "table_5",
        "paper_id": "D18-1371",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1373table_2",
        "description": "3.4 Compare with State-of-the-art. First, we compare LRMM with state-of-the-art methods listed in Sec. 3.2. In this setting, LRMM is trained with all data modalities and tested with different missing modality regimes. Table 2 lists the results on the four datasets. By leveraging multimodal correlations, LRMM significantly outperforms MF-based models (i.e. NMF, SVD++) and topic-based methods (i.e., URP, CTR, RMR, and HFT). LRMM also outperforms recent deep learning models (i.e., NRT, DeepCoNN) with respect to almost all metrics. LRMM is the only method with a robust performance for the cold-start recommendation problem where user review or item review texts are removed. While the cold-start recommendation is more challenging, LRMM(-U) and LRMM(-O) are still able to achieve a similar performance to the baselines in the standard recommendation setting. For example, RMSE 1.101 (LRMM(-O)) to 1.107 (NRT) on Electronics, MAE 0.680 (LRMM(-O)) to 0.667 (DeepCoNN) on S&O. We conjecture that the cross-modality dependencies (Srivastava and Salakhutdinov, 2012) make LRMM more robust when modalities are missing. Table 5 lists some randomly selected rating predictions. Similar to Table 2, missing user (-U) and item (-O) preference significantly deteriorates the performance.",
        "sentences": [
            "3.4 Compare with State-of-the-art.",
            "First, we compare LRMM with state-of-the-art methods listed in Sec. 3.2.",
            "In this setting, LRMM is trained with all data modalities and tested with different missing modality regimes.",
            "Table 2 lists the results on the four datasets.",
            "By leveraging multimodal correlations, LRMM significantly outperforms MF-based models (i.e. NMF, SVD++) and topic-based methods (i.e., URP, CTR, RMR, and HFT).",
            "LRMM also outperforms recent deep learning models (i.e., NRT, DeepCoNN) with respect to almost all metrics.",
            "LRMM is the only method with a robust performance for the cold-start recommendation problem where user review or item review texts are removed.",
            "While the cold-start recommendation is more challenging, LRMM(-U) and LRMM(-O) are still able to achieve a similar performance to the baselines in the standard recommendation setting.",
            "For example, RMSE 1.101 (LRMM(-O)) to 1.107 (NRT) on Electronics, MAE 0.680 (LRMM(-O)) to 0.667 (DeepCoNN) on S&O.",
            "We conjecture that the cross-modality dependencies (Srivastava and Salakhutdinov, 2012) make LRMM more robust when modalities are missing.",
            "Table 5 lists some randomly selected rating predictions.",
            "Similar to Table 2, missing user (-U) and item (-O) preference significantly deteriorates the performance."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            1,
            1,
            2,
            1,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            [
                "LRMM(+F)",
                "NMF",
                "SVD++",
                "URP",
                "RMR",
                "HFT"
            ],
            [
                "LRMM(+F)",
                "HFT",
                "DeepCoNN"
            ],
            [
                "LRMM(+F)"
            ],
            [
                "LRMM(-U)",
                "LRMM(-O)"
            ],
            [
                "LRMM(-O)",
                "NRT",
                "DeepCoNN",
                "Electronics",
                "S&O",
                "RMSE",
                "MAE"
            ],
            null,
            null,
            [
                "LRMM(-U)",
                "LRMM(-O)"
            ]
        ],
        "n_sentence": 12.0,
        "table_id": "table_2",
        "paper_id": "D18-1373",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1373table_3",
        "description": "3.6 Missing Modality Imputation. The proposed m-drop and m-auto methods allow LRMM to be more robust to missing data modalities. Table 3 lists the results of training LRMM with missing data modalities for the modality dropout ratio pm = 0.5 on the S&O and H&P datasets, respectively. Both RMSE and MAE of LRMM deteriorate but are still comparable to the MF-based approaches NMF and SVD++.",
        "sentences": [
            "3.6 Missing Modality Imputation.",
            "The proposed m-drop and m-auto methods allow LRMM to be more robust to missing data modalities.",
            "Table 3 lists the results of training LRMM with missing data modalities for the modality dropout ratio pm = 0.5 on the S&O and H&P datasets, respectively.",
            "Both RMSE and MAE of LRMM deteriorate but are still comparable to the MF-based approaches NMF and SVD++."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "LRMM(+F)",
                "S&O",
                "H&P"
            ],
            [
                "LRMM(+F)",
                "RMSE",
                "MAE"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "D18-1373",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1374table_1",
        "description": "6 Experiments. In Table 1 we report results from the experiments on the 10M web documents dataset for prec and NDCG-IDF metrics for k = 1, 3, 5, 7, limiting k to small values as is common in the recommendation problems from large sets of items (Jain et al., 2016). The p(F ) baseline always predicts entities according to their frequency over the training set. It can be viewed as maximum likelihood estimate (MLE) for the model which is only composed of a bias vector (i.e., input entities are ignored). Notice the relatively high performance when the most popular entities are taken. For example, in 36.76% of cases entity Research (the most popular future entity from the corpus) is in the future of the document (as can be also seen from Figure 1, where the entity Research corresponds to the leftmost point of the graph). This constitutes a high value, as the vocabulary consists of 100K entities. Among the linear models, P(F|C) yields the highest scores, significantly outperforming the baselines. PPMI(F|C) model yields relatively high NCG-IDF scores (although in most cases lower than P(F|C)), and very low precision scores. Notice that the SVD methods are consistently worse than the linear models. This shows that no additional generalization is gained when lowering the number of parameters of the linear models. When experimenting with higher ranks for SVD decomposition we found the performance increases, but does not improve over the linear models. NNs improve over linear models according to both NCG-IDF and prec scores. This is especially apparent for NCG-IDF, where the relative improvement is very significant. XML-CNN is in all cases better than the Youtube model, which shows how utilizing more linguistic structure than simply bag of entities is helpful in the NN framework. Both Youtube and XML-CNN models with a modified loss function improve over the basic NN models in terms of the NCG-IDF metrics, showing that a simple adjustment of a loss function in the NN framework can lead to more rare entities being recommended. This comes at the cost of lowering prec@k scores, which however correlates with user judgments to a lesser extent, as we showed in Section 3.2. The Random Forest models turn out to be the most competitive. Notice that no linguistic structure is captured in FastXML models, only the bags of entities. This is in contrast with XML-CNN approach which looks at local contexts of feature entities. FastXML performs particularly well on the precision scores, which however is not necessarily useful, as demonstrated in the examples discussed later. Last, we inspect the sizes of the different models reported in the rightmost column of Table 1. Model size is an important factor to consider in practical applications, e.g. when deploying a system on the device. PFastreXML model takes 150GB, by far the most of all methods, resulting in its capability in recommending tail entities. The linear models take 4.7GB related to the fact that in the full 100K \u00c3\u2014 100K co-occurrence matrix approximately 11% of entries are non-zero. Applying SVD matrix decomposition helps reduce this size significantly. The NN models take around 2GB, significantly less than the random forests.",
        "sentences": [
            "6 Experiments.",
            "In Table 1 we report results from the experiments on the 10M web documents dataset for prec and NDCG-IDF metrics for k = 1, 3, 5, 7, limiting k to small values as is common in the recommendation problems from large sets of items (Jain et al., 2016).",
            "The p(F ) baseline always predicts entities according to their frequency over the training set.",
            "It can be viewed as maximum likelihood estimate (MLE) for the model which is only composed of a bias vector (i.e., input entities are ignored).",
            "Notice the relatively high performance when the most popular entities are taken.",
            "For example, in 36.76% of cases entity Research (the most popular future entity from the corpus) is in the future of the document (as can be also seen from Figure 1, where the entity Research corresponds to the leftmost point of the graph).",
            "This constitutes a high value, as the vocabulary consists of 100K entities.",
            "Among the linear models, P(F|C) yields the highest scores, significantly outperforming the baselines.",
            "PPMI(F|C) model yields relatively high NCG-IDF scores (although in most cases lower than P(F|C)), and very low precision scores.",
            "Notice that the SVD methods are consistently worse than the linear models.",
            "This shows that no additional generalization is gained when lowering the number of parameters of the linear models.",
            "When experimenting with higher ranks for SVD decomposition we found the performance increases, but does not improve over the linear models.",
            "NNs improve over linear models according to both NCG-IDF and prec scores.",
            "This is especially apparent for NCG-IDF, where the relative improvement is very significant.",
            "XML-CNN is in all cases better than the Youtube model, which shows how utilizing more linguistic structure than simply bag of entities is helpful in the NN framework.",
            "Both Youtube and XML-CNN models with a modified loss function improve over the basic NN models in terms of the NCG-IDF metrics, showing that a simple adjustment of a loss function in the NN framework can lead to more rare entities being recommended.",
            "This comes at the cost of lowering prec@k scores, which however correlates with user judgments to a lesser extent, as we showed in Section 3.2.",
            "The Random Forest models turn out to be the most competitive.",
            "Notice that no linguistic structure is captured in FastXML models, only the bags of entities.",
            "This is in contrast with XML-CNN approach which looks at local contexts of feature entities.",
            "FastXML performs particularly well on the precision scores, which however is not necessarily useful, as demonstrated in the examples discussed later.",
            "Last, we inspect the sizes of the different models reported in the rightmost column of Table 1.",
            "Model size is an important factor to consider in practical applications, e.g. when deploying a system on the device.",
            "PFastreXML model takes 150GB, by far the most of all methods, resulting in its capability in recommending tail entities.",
            "The linear models take 4.7GB related to the fact that in the full 100K \u00c3\u2014 100K co-occurrence matrix approximately 11% of entries are non-zero.",
            "Applying SVD matrix decomposition helps reduce this size significantly.",
            "The NN models take around 2GB, significantly less than the random forests."
        ],
        "class_sentence": [
            2,
            1,
            2,
            2,
            2,
            1,
            2,
            1,
            1,
            1,
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            2,
            2,
            2,
            1,
            1,
            2,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "NCG-IDF @1",
                "NCG-IDF @3",
                "NCG-IDF @5",
                "NCG-IDF @7",
                "prec @1",
                "prec @3",
                "prec @5",
                "prec @7"
            ],
            [
                "p(F)"
            ],
            [
                "p(F)"
            ],
            [
                "p(F)"
            ],
            [
                "p(F)",
                "prec @1"
            ],
            null,
            [
                "P(F|C)"
            ],
            [
                "PPMI(C F)",
                "NCG-IDF @1",
                "NCG-IDF @3",
                "NCG-IDF @5",
                "NCG-IDF @7"
            ],
            [
                "SVD"
            ],
            null,
            [
                "SVD"
            ],
            [
                "Youtube",
                "XML-CNN",
                "base",
                "NCG-IDF @1",
                "NCG-IDF @3",
                "NCG-IDF @5",
                "NCG-IDF @7",
                "prec @1",
                "prec @3",
                "prec @5",
                "prec @7"
            ],
            [
                "Youtube",
                "XML-CNN",
                "NCG-IDF @1",
                "NCG-IDF @3",
                "NCG-IDF @5",
                "NCG-IDF @7"
            ],
            [
                "Youtube",
                "XML-CNN"
            ],
            [
                "Youtube",
                "XML-CNN",
                "base",
                "IDF",
                "NCG-IDF @1",
                "NCG-IDF @3",
                "NCG-IDF @5",
                "NCG-IDF @7"
            ],
            [
                "prec @1",
                "prec @3",
                "prec @5",
                "prec @7",
                "IDF"
            ],
            [
                "FastXML",
                "PFastreXML"
            ],
            [
                "FastXML"
            ],
            [
                "FastXML",
                "XML-CNN"
            ],
            [
                "FastXML",
                "prec @1",
                "prec @3",
                "prec @5",
                "prec @7"
            ],
            [
                "model size"
            ],
            null,
            [
                "PFastreXML",
                "model size"
            ],
            [
                "N(C F)",
                "P(F|C)",
                "PPMI(C F)",
                "base",
                "model size"
            ],
            [
                "SVD"
            ],
            [
                "Youtube",
                "XML-CNN",
                "FastXML",
                "PFastreXML",
                "model size"
            ]
        ],
        "n_sentence": 27.0,
        "table_id": "table_1",
        "paper_id": "D18-1374",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1378table_8",
        "description": "To understand the contributions of incorporating authors, discourse relations, and word embeddings, we evaluate variants of Limbic for SEUlevel sentiment classification on two datasets: tSEU and tSEU(D). We create tSEU by randomly selecting 200 hotel reviews by seven authors. We manually annotate the sentiments of each SEU, obtaining 2,692 SEUs. We create tSEU(D) by selecting reviews in tSEU containing at least one Comparison or Expansion. We define three variants of Limbic (L): LA with just authors, no discourse relations or word embeddings; LAD with authors and discourse relations but no word embeddings; LAW with authors and word embeddings but no discourse relations. Table 8 compares Limbic with LA LAD, and LAW. We observe that for both datasets, incorporating discourse relations improves accuracy. By incorporating word embeddings, LAW yields better accuracy than LAD, showing that word embeddings add more value to Limbic than discourse relations do.",
        "sentences": [
            "To understand the contributions of incorporating authors, discourse relations, and word embeddings, we evaluate variants of Limbic for SEUlevel sentiment classification on two datasets: tSEU and tSEU(D).",
            "We create tSEU by randomly selecting 200 hotel reviews by seven authors.",
            "We manually annotate the sentiments of each SEU, obtaining 2,692 SEUs.",
            "We create tSEU(D) by selecting reviews in tSEU containing at least one Comparison or Expansion.",
            "We define three variants of Limbic (L): LA with just authors, no discourse relations or word embeddings; LAD with authors and discourse relations but no word embeddings; LAW with authors and word embeddings but no discourse relations.",
            "Table 8 compares Limbic with LA, LAD, and LAW.",
            "We observe that for both datasets, incorporating discourse relations improves accuracy.",
            "By incorporating word embeddings, LAW yields better accuracy than LAD, showing that word embeddings add more value to Limbic than discourse relations do."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "tSEU",
                "tSEU(D)"
            ],
            [
                "tSEU"
            ],
            null,
            [
                "tSEU(D)"
            ],
            [
                "LA",
                "LAD",
                "LAW"
            ],
            [
                "LA",
                "LAD",
                "LAW"
            ],
            [
                "tSEU",
                "tSEU(D)",
                "LAD"
            ],
            [
                "LAD",
                "LAW"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_8",
        "paper_id": "D18-1378",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1380table_2",
        "description": "4.3 Overall Performance Comparison. Table 2 shows the performance comparison results of MGAN with other baseline methods. We can have the following observations. (1) Majority performs worst since it only utilizes the data distribution information. Feature+SVM can achieve much better performance on all the datasets, with the well-designed feature engineering. Our method MGAN outperforms Majority and Feature+SVM since MGAN could learn the high quality representation for prediction. (2) ATAE-LSTM is better than LSTM since it employs attention mechanism on the hidden states and combines with attention embedding to generate the final representation. TD-LSTM performs slightly better than ATAE-LSTM, and it employs two LSTM networks to capture the left and right context of the aspect. TD-LSTM performs worse than our method MGAN since it could not properly pay more attentions on the important parts of the context. (3) IAN achieves slightly better results with the previous LSTM-based methods, which interactively learns the attended aspect and context vector as final representation. Our method consistently performs better than IAN since we utilize the finegrained attention vectors to relieve the information loss in IAN. MemNet continuously learns the attended vector on the context word embedding memory, and updates the query vector at each hop. BILSTM-ATT-G models left context and right context using attention-based LSTMs, which achieves better performance than MemNet. RAM performs better than other baselines. It employs bidirectional LSTM network to generate contextual memory, and learns the multiple attended vector on the memory. Similar with MemNet, it utilizes the averaged aspect vector to learn the attention weights on context words. Our proposed MGAN consistently performs better than MemNet, BILSTM-ATT-G and RAM on all three datasets. On one hand, they only consider to learn the attention weights on context towards the aspect, and do not consider to learn the weights on aspect words towards the context. On the other hand, they just use the averaged aspect vector to guide the attention, which will lose some information, especially on the aspects with multiple words. From another perspective, our method employs the aspect alignment loss, which can bring extra useful information from the aspectlevel interactions.",
        "sentences": [
            "4.3 Overall Performance Comparison.",
            "Table 2 shows the performance comparison results of MGAN with other baseline methods.",
            "We can have the following observations.",
            "(1) Majority performs worst since it only utilizes the data distribution information.",
            "Feature+SVM can achieve much better performance on all the datasets, with the well-designed feature engineering.",
            "Our method MGAN outperforms Majority and Feature+SVM since MGAN could learn the high quality representation for prediction.",
            "(2) ATAE-LSTM is better than LSTM since it employs attention mechanism on the hidden states and combines with attention embedding to generate the final representation.",
            "TD-LSTM performs slightly better than ATAE-LSTM, and it employs two LSTM networks to capture the left and right context of the aspect.",
            "TD-LSTM performs worse than our method MGAN since it could not properly pay more attentions on the important parts of the context.",
            "(3) IAN achieves slightly better results with the previous LSTM-based methods, which interactively learns the attended aspect and context vector as final representation.",
            "Our method consistently performs better than IAN since we utilize the finegrained attention vectors to relieve the information loss in IAN.",
            "MemNet continuously learns the attended vector on the context word embedding memory, and updates the query vector at each hop.",
            "BILSTM-ATT-G models left context and right context using attention-based LSTMs, which achieves better performance than MemNet.",
            "RAM performs better than other baselines.",
            "It employs bidirectional LSTM network to generate contextual memory, and learns the multiple attended vector on the memory.",
            "Similar with MemNet, it utilizes the averaged aspect vector to learn the attention weights on context words.",
            "Our proposed MGAN consistently performs better than MemNet, BILSTM-ATT-G and RAM on all three datasets.",
            "On one hand, they only consider to learn the attention weights on context towards the aspect, and do not consider to learn the weights on aspect words towards the context.",
            "On the other hand, they just use the averaged aspect vector to guide the attention, which will lose some information, especially on the aspects with multiple words.",
            "From another perspective, our method employs the aspect alignment loss, which can bring extra useful information from the aspectlevel interactions."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            2,
            1,
            1,
            2,
            2,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "MGAN"
            ],
            null,
            [
                "Majority"
            ],
            [
                "Feature-SVM"
            ],
            [
                "MGAN",
                "Majority",
                "Feature-SVM"
            ],
            [
                "ATAE-LSTM"
            ],
            [
                "TD-LSTM",
                "ATAE-LSTM"
            ],
            [
                "MGAN",
                "TD-LSTM"
            ],
            [
                "ATAE-LSTM",
                "TD-LSTM",
                "IAN"
            ],
            [
                "MGAN",
                "IAN"
            ],
            [
                "MemNet"
            ],
            [
                "MemNet",
                "BILSTM-ATT-G"
            ],
            [
                "RAM"
            ],
            [
                "RAM"
            ],
            [
                "RAM",
                "MemNet"
            ],
            [
                "MGAN",
                "MemNet",
                "BILSTM-ATT-G",
                "RAM"
            ],
            [
                "MemNet",
                "BILSTM-ATT-G",
                "RAM"
            ],
            [
                "MemNet",
                "BILSTM-ATT-G",
                "RAM"
            ],
            [
                "MGAN"
            ]
        ],
        "n_sentence": 20.0,
        "table_id": "table_2",
        "paper_id": "D18-1380",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1380table_3",
        "description": "4.4 Analysis of MGAN model. Table 3 shows the performance comparison among the variants of MGAN model. We can have the following observations. (1) the proposed fine-grained attention mechanism MGAN-F, which is responsible for linking and fusing the information between the context and aspect word, achieves competitive performance compared with MGAN-C, especially on laptop dataset. To investigate this case, we collect the percentage of aspects with different word lengths in Table 4. We can find that laptop dataset has the highest percentage on the aspects with more than two words, and the second-highest percentage on two words. It demonstrates MGAN-F has better performance on aspects with more words, and make use of the word-level interactions to relieve the information loss occurred in coarsegrained attention mechanism. (2) MGAN-CF is better than both MGAN-C and MGAN-F, which demonstrates the coarsegrained attentions and fine-grained attentions could improve the performance from different perspectives. Compared with MGAN-CF, the complete MGAN model gains further improvement by bringing the aspect alignment loss, which is designed to capture the aspect level interactions.",
        "sentences": [
            "4.4 Analysis of MGAN model.",
            "Table 3 shows the performance comparison among the variants of MGAN model.",
            "We can have the following observations.",
            "(1) the proposed fine-grained attention mechanism MGAN-F, which is responsible for linking and fusing the information between the context and aspect word, achieves competitive performance compared with MGAN-C, especially on laptop dataset.",
            "To investigate this case, we collect the percentage of aspects with different word lengths in Table 4.",
            "We can find that laptop dataset has the highest percentage on the aspects with more than two words, and the second-highest percentage on two words.",
            "It demonstrates MGAN-F has better performance on aspects with more words, and make use of the word-level interactions to relieve the information loss occurred in coarsegrained attention mechanism.",
            "(2) MGAN-CF is better than both MGAN-C and MGAN-F, which demonstrates the coarsegrained attentions and fine-grained attentions could improve the performance from different perspectives.",
            "Compared with MGAN-CF, the complete MGAN model gains further improvement by bringing the aspect alignment loss, which is designed to capture the aspect level interactions."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "MGAN"
            ],
            null,
            [
                "MGAN-F",
                "MGAN-C",
                "Laptop"
            ],
            null,
            [
                "Laptop"
            ],
            [
                "MGAN-F"
            ],
            [
                "MGAN-CF",
                "MGAN-C",
                "MGAN-F"
            ],
            [
                "MGAN-CF",
                "MGAN"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_3",
        "paper_id": "D18-1380",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1381table_1",
        "description": "4.2 Experimental Results. Table 1 and Table 2 report the results of our experiments. The results on TRAIN-ALL are higher than TRAIN for SemEval16 in lieu of the larger dataset. Firstly, we observe that our proposed AGLR outperforms all neural baselines on 3-way classification. The overall performance of AGLR achieves state-of-the-art performance. On average, AGLR outperforms Lexicon RNN and AT-BiLSTM by 1% \u00e2\u02c6\u2019 3% in terms of F1 score. We also observe that AGLR always improves AT-BiLSTM which ascertains the effectiveness of learning auxiliary lexicon embeddings. The key idea here is that the auxiliary lexicon embeddings provide a different view of the sentence which supports the network in making predictions. We also observe that Lexicon RNN does not handle 3-way classification well. Even though it has achieved good performance on binary classification, the performance on 3-way classification is lackluster (the performance of AGLR outperforms Lexicon RNN by up to 8% on SemEval16 TRAIN). This could also be attributed to the MSE based loss function. Conversely, by learning an auxiliary embedding (instead of a scalar score), our model becomes more flexible at the final layer and can be adapted to using a k softmax function. Finally, we observe that BiLSTM and AT-BiLSTM outperform Lexicon RNN on average with Lexicon RNN being slightly better on binary classification.",
        "sentences": [
            "4.2 Experimental Results.",
            "Table 1 and Table 2 report the results of our experiments.",
            "The results on TRAIN-ALL are higher than TRAIN for SemEval16 in lieu of the larger dataset.",
            "Firstly, we observe that our proposed AGLR outperforms all neural baselines on 3-way classification.",
            "The overall performance of AGLR achieves state-of-the-art performance.",
            "On average, AGLR outperforms Lexicon RNN and AT-BiLSTM by 1% \u00e2\u02c6\u2019 3% in terms of F1 score.",
            "We also observe that AGLR always improves AT-BiLSTM which ascertains the effectiveness of learning auxiliary lexicon embeddings.",
            "The key idea here is that the auxiliary lexicon embeddings provide a different view of the sentence which supports the network in making predictions.",
            "We also observe that Lexicon RNN does not handle 3-way classification well.",
            "Even though it has achieved good performance on binary classification, the performance on 3-way classification is lackluster (the performance of AGLR outperforms Lexicon RNN by up to 8% on SemEval16 TRAIN).",
            "This could also be attributed to the MSE based loss function.",
            "Conversely, by learning an auxiliary embedding (instead of a scalar score), our model becomes more flexible at the final layer and can be adapted to using a k softmax function.",
            "Finally, we observe that BiLSTM and AT-BiLSTM outperform Lexicon RNN on average with Lexicon RNN being slightly better on binary classification."
        ],
        "class_sentence": [
            2,
            1,
            0,
            1,
            1,
            1,
            1,
            2,
            1,
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "AGLR",
                "3-way"
            ],
            [
                "AGLR"
            ],
            [
                "AGLR",
                "AT-BiLSTM",
                "Lexicon RNN",
                "AVG",
                "F1"
            ],
            [
                "AGLR",
                "AT-BiLSTM"
            ],
            null,
            [
                "Lexicon RNN",
                "3-way"
            ],
            [
                "Lexicon RNN",
                "Binary"
            ],
            null,
            [
                "AGLR"
            ],
            [
                "BiLSTM",
                "AT-BiLSTM",
                "Lexicon RNN",
                "Binary",
                "AVG"
            ]
        ],
        "n_sentence": 13.0,
        "table_id": "table_1",
        "paper_id": "D18-1381",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1381table_3",
        "description": "Comparisons against Top SemEval Systems. Table 3 reports the results of our proposed approach against the top team of each SemEval run, i.e., NRC-Canada (Mohammad et al., 2013) for 2013 Task 2, Team-X (Miura et al., 2014) for 2014 Task 9, SwissCheese (Deriu et al., 2016) for 2016 Task 4. We follow the exact training datasets allowed for each SemEval run. Following the competition setting, with the exception of accuracy for SemEval 2016, all metrics reported are the macro averaged F1 score of positive and negative classes. We observe that AGLR achieves competitive performance relative to the top runs in SemEval 2013, 2014 and 2016. It is good to note that SemEval approaches are often heavily engineered containing ensembles and many handcrafted features which include extensive use of sentiment lexicons, POS tags and negation detectors. Recent SemEval runs gravitate towards neural ensembles. For instance, the winning approach for SwissCheese (SemEval 2016) uses an ensemble of 6 CNN models along with a meta-classifier (random forest classifier). On the other hand, our proposed model is a single neural model. In addition, SwissCheese also uses emoticon-based distant supervision which exploits a huge corpus of sentences (millions) for training. Conversely, our approach only uses the 2013 and 2016 training sets which are significantly smaller. Given these conditions, we find it remarkable that our single model is able to achieve competitive performance relative to the extensively engineered approach of SwissCheese. Moreover, we actually outperform significantly in terms of pure accuracy. AGLR performs competitively on SemEval 2013 and 2014 as well. The good performance on the sarcasm dataset could be attributed to our contrastive attention mechanism.",
        "sentences": [
            "Comparisons against Top SemEval Systems.",
            "Table 3 reports the results of our proposed approach against the top team of each SemEval run, i.e., NRC-Canada (Mohammad et al., 2013) for 2013 Task 2, Team-X (Miura et al., 2014) for 2014 Task 9, SwissCheese (Deriu et al., 2016) for 2016 Task 4.",
            "We follow the exact training datasets allowed for each SemEval run.",
            "Following the competition setting, with the exception of accuracy for SemEval 2016, all metrics reported are the macro averaged F1 score of positive and negative classes.",
            "We observe that AGLR achieves competitive performance relative to the top runs in SemEval 2013, 2014 and 2016.",
            "It is good to note that SemEval approaches are often heavily engineered containing ensembles and many handcrafted features which include extensive use of sentiment lexicons, POS tags and negation detectors.",
            "Recent SemEval runs gravitate towards neural ensembles.",
            "For instance, the winning approach for SwissCheese (SemEval 2016) uses an ensemble of 6 CNN models along with a meta-classifier (random forest classifier).",
            "On the other hand, our proposed model is a single neural model.",
            "In addition, SwissCheese also uses emoticon-based distant supervision which exploits a huge corpus of sentences (millions) for training.",
            "Conversely, our approach only uses the 2013 and 2016 training sets which are significantly smaller.",
            "Given these conditions, we find it remarkable that our single model is able to achieve competitive performance relative to the extensively engineered approach of SwissCheese.",
            "Moreover, we actually outperform significantly in terms of pure accuracy.",
            "AGLR performs competitively on SemEval 2013 and 2014 as well.",
            "The good performance on the sarcasm dataset could be attributed to our contrastive attention mechanism."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            1,
            2,
            0,
            2,
            1,
            2,
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Ours",
                "Top System"
            ],
            null,
            [
                "SemEval16"
            ],
            [
                "Ours",
                "Top System",
                "SemEval13",
                "SemEval14",
                "SemEval16"
            ],
            null,
            null,
            [
                "SemEval16",
                "Top System"
            ],
            [
                "Ours"
            ],
            [
                "SemEval16",
                "Top System"
            ],
            [
                "Ours"
            ],
            [
                "Ours",
                "SemEval16",
                "Top System"
            ],
            [
                "Ours",
                "SemEval16",
                "Top System",
                "Tweets (Acc)"
            ],
            [
                "SemEval14",
                "Top System",
                "Ours"
            ],
            null
        ],
        "n_sentence": 15.0,
        "table_id": "table_3",
        "paper_id": "D18-1381",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1385table_2",
        "description": "6.2 Results. We describe the task setting results in Table 2, and detailed per-event results in Table 3. Although TFG does not achieve the highest F1-score in the task setting, it is mainly due to the split of the dataset. More than half of the tweets in the test set do not have images. Thus we can only leverage cross-platform information by searching videos\u00e2\u20ac\u2122 URLs, which results in less accurate crosslingual cross-platform features. In the event setting, which has a more fair comparison, TFG outperformed other methods with a big margin (p<0.001). It is surprising to see that only 10 features extracted from external resources indexed by search engines leveraged by a simple classifier can bring such a big performance boost. 7.1 Results. Experiment results using the task setting are shown in Table 2 and the detailed per-event results are listed in Table 3. Similar to the problem in TFG, we can not obtain any Chinese webpages related to events such as Syrian boy, Varoufakis and zdf, which cover most tweets in the test set. Those missing features make TFB perform poorly in the task setting. However, TFB performs better than two of the baselines in the event setting. If we exclude events without Baidu webpages (event 10, 16 and 17), the average F1-score of UoS, MCG and CER are 0.130, 0.732 and 0.660, which are all lower than TFB's. The performance of TFB proves that our method can be generalized across languages or platforms. To further test the robustness of our crosslingual cross-platform features, we also examined if it would still work when leveraging external information that contains different languages. We extracted the cross-lingual cross-platform features for tweets leveraging Google and Baidu webpages together (Combo) and accessed the performance of Combo using MLP similarly. The performance of Combo is also listed in Table 2. Since Combo would contain noise introduced from combining webpages indexed by different search engines, it is not surprising that Combo performs slightly worse than TFG extracted from Google webpages which already cover a wide range of information solely. However, Combo performs much better than TFB which only leverages Baidu webpages. It proves that our cross-lingual cross-platform features are robust enough to utilize combined external information from different languages and platforms.",
        "sentences": [
            "6.2 Results.",
            "We describe the task setting results in Table 2, and detailed per-event results in Table 3.",
            "Although TFG does not achieve the highest F1-score in the task setting, it is mainly due to the split of the dataset.",
            "More than half of the tweets in the test set do not have images.",
            "Thus we can only leverage cross-platform information by searching videos\u00e2\u20ac\u2122 URLs, which results in less accurate crosslingual cross-platform features.",
            "In the event setting, which has a more fair comparison, TFG outperformed other methods with a big margin (p<0.001).",
            "It is surprising to see that only 10 features extracted from external resources indexed by search engines leveraged by a simple classifier can bring such a big performance boost.",
            "7.1 Results.",
            "Experiment results using the task setting are shown in Table 2 and the detailed per-event results are listed in Table 3.",
            "Similar to the problem in TFG, we can not obtain any Chinese webpages related to events such as Syrian boy, Varoufakis and zdf, which cover most tweets in the test set.",
            "Those missing features make TFB perform poorly in the task setting.",
            "However, TFB performs better than two of the baselines in the event setting.",
            "If we exclude events without Baidu webpages (event 10, 16 and 17), the average F1-score of UoS, MCG and CER are 0.130, 0.732 and 0.660, which are all lower than TFB's.",
            "The performance of TFB proves that our method can be generalized across languages or platforms.",
            "To further test the robustness of our crosslingual cross-platform features, we also examined if it would still work when leveraging external information that contains different languages.",
            "We extracted the cross-lingual cross-platform features for tweets leveraging Google and Baidu webpages together (Combo) and accessed the performance of Combo using MLP similarly.",
            "The performance of Combo is also listed in Table 2.",
            "Since Combo would contain noise introduced from combining webpages indexed by different search engines, it is not surprising that Combo performs slightly worse than TFG extracted from Google webpages which already cover a wide range of information solely.",
            "However, Combo performs much better than TFB which only leverages Baidu webpages.",
            "It proves that our cross-lingual cross-platform features are robust enough to utilize combined external information from different languages and platforms."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            2,
            1,
            2,
            2,
            1,
            2,
            0,
            0,
            0,
            0,
            2,
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "TFG",
                "F1-Task"
            ],
            null,
            null,
            [
                "TFG",
                "F1-Event"
            ],
            null,
            null,
            null,
            [
                "TFG"
            ],
            null,
            null,
            null,
            null,
            null,
            [
                "Combo"
            ],
            [
                "Combo"
            ],
            [
                "Combo",
                "TFG"
            ],
            [
                "Combo"
            ],
            null
        ],
        "n_sentence": 20.0,
        "table_id": "table_2",
        "paper_id": "D18-1385",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1385table_5",
        "description": "8.1 Results. Table 5 lists the detailed results of our transfer learning experiment. We achieved much better performance compared to the baseline with statistical significance (p<0.001), which indicates that our cross-lingual cross-platform feature set can be generalized to rumors in different languages. It enables the trained classifier to leverage the information learned from one language to another. 8.2 Analysis. In event 11 (Pig fish), Transfer achieves much higher performance than the random baseline. Generally, Baidu webpages\u00e2\u20ac\u2122 titles are semantically different from tweets. However, in this particular event, the textual information of those titles and tweets are semantically close. As a result, models learned from English rumors can easily work on Chinese rumors, which is helpful for our transfer learning. Figure 4 shows three Twitter-Baidu rumor pairs with similar meaning in this event. Transfer obtains pretty low F1-scores in event 07 (Passport hoax). The annotation conflict caused its weak performance. This event is about a Child drew all over his dads passport and made his dad stuck in South Korea. During the manual annotation process, we found out that it is a real event confirmed by official accounts according to one news article from Chinese social media, while CCMR Twitter labeled such tweets as fake. Since Transfer is pre-trained using Twitter dataset, it is not surprising that Transfer achieves 0 in F1-score on this event. The annotation conflict also brings out that rumor verification will benefit from utilizing cross-lingual and cross-platform information.",
        "sentences": [
            "8.1 Results.",
            "Table 5 lists the detailed results of our transfer learning experiment.",
            "We achieved much better performance compared to the baseline with statistical significance (p<0.001), which indicates that our cross-lingual cross-platform feature set can be generalized to rumors in different languages.",
            "It enables the trained classifier to leverage the information learned from one language to another.",
            "8.2 Analysis.",
            "In event 11 (Pig fish), Transfer achieves much higher performance than the random baseline.",
            "Generally, Baidu webpages\u00e2\u20ac\u2122 titles are semantically different from tweets.",
            "However, in this particular event, the textual information of those titles and tweets are semantically close.",
            "As a result, models learned from English rumors can easily work on Chinese rumors, which is helpful for our transfer learning.",
            "Figure 4 shows three Twitter-Baidu rumor pairs with similar meaning in this event.",
            "Transfer obtains pretty low F1-scores in event 07 (Passport hoax).",
            "The annotation conflict caused its weak performance.",
            "This event is about a Child drew all over his dads passport and made his dad stuck in South Korea.",
            "During the manual annotation process, we found out that it is a real event confirmed by official accounts according to one news article from Chinese social media, while CCMR Twitter labeled such tweets as fake.",
            "Since Transfer is pre-trained using Twitter dataset, it is not surprising that Transfer achieves 0 in F1-score on this event.",
            "The annotation conflict also brings out that rumor verification will benefit from utilizing cross-lingual and cross-platform information."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            2,
            1,
            2,
            2,
            2,
            2,
            1,
            2,
            2,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Transfer"
            ],
            [
                "Transfer",
                "Random"
            ],
            null,
            null,
            [
                "Random",
                "Transfer",
                "ID",
                "11",
                "Pig fish"
            ],
            null,
            [
                "Pig fish"
            ],
            null,
            null,
            [
                "Transfer",
                "ID",
                "07",
                "Passport hoax"
            ],
            [
                "Passport hoax"
            ],
            [
                "Passport hoax"
            ],
            [
                "Passport hoax"
            ],
            [
                "Transfer",
                "Passport hoax"
            ],
            null
        ],
        "n_sentence": 16.0,
        "table_id": "table_5",
        "paper_id": "D18-1385",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1386table_1",
        "description": "Table 1 displays the results. The difference in performance between the three baselines that don't use a RNN generator and the three model variants that do demonstrates the importance of context in recognizing personal attacks within text. The relative performance of the three variants of the Lei et al. model show that both modifications, setting the bias term and the addition of the adversarial predictor, lead to marginal improvements in tokenwise F1. The best-performing model approaches average human performance on this metric. The phrasewise metric is relaxed. It allows a contiguous personal attack sequence to be considered captured if even a single token from the sequence is captured. The results on this metric show that in an absolute sense, 87% of personal attacks are at least partially captured by the algorithm. The simplest baseline, which produces rationales by thresholding the coefficients of a logistic regression model, does deceptively well on this metric by only identifying attacking words like \u201djerk\u201d and \u201da**hole\u201d, but its poor tokenwise performance shows that it doesn\u2019t mimic human highlighting very well.",
        "sentences": [
            "Table 1 displays the results.",
            "The difference in performance between the three baselines that don't use a RNN generator and the three model variants that do demonstrates the importance of context in recognizing personal attacks within text.",
            "The relative performance of the three variants of the Lei et al. model show that both modifications, setting the bias term and the addition of the adversarial predictor, lead to marginal improvements in tokenwise F1.",
            "The best-performing model approaches average human performance on this metric.",
            "The phrasewise metric is relaxed.",
            "It allows a contiguous personal attack sequence to be considered captured if even a single token from the sequence is captured.",
            "The results on this metric show that in an absolute sense, 87% of personal attacks are at least partially captured by the algorithm.",
            "The simplest baseline, which produces rationales by thresholding the coefficients of a logistic regression model, does deceptively well on this metric by only identifying attacking words like \u201djerk\u201d and \u201da**hole\u201d, but its poor tokenwise performance shows that it doesn\u2019t mimic human highlighting very well."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "RNN predictor",
                "RNN predictor + sigmoid generator",
                "RNN predictor + LIME"
            ],
            [
                "Lei2016",
                "Lei2016 + bias",
                "Lei2016 + bias + inverse (EAN)"
            ],
            [
                "Mean human performance"
            ],
            [
                "Phrasewise"
            ],
            [
                "Phrasewise"
            ],
            [
                "Lei2016 + bias + inverse (EAN)",
                "Phrasewise",
                "Rec."
            ],
            null
        ],
        "n_sentence": 8.0,
        "table_id": "table_1",
        "paper_id": "D18-1386",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1387table_7",
        "description": "6.3 Predicting Vague Sentences. In Table 7 we present results on classifying privacy policy sentences into four categories: clear, somewhat clear, vague, and extremely vague. We compare AC-GAN with three baselines: CNN and LSTM trained on human-annotated sentences, and a majority baseline that assigns the most frequent label to all test sentences. We observe that the ACGAN models (using CNN discriminator) perform strongly, surpassing all baseline approaches. CNN shows strong performance, yielding an F-score of 50.92%. A similar effect has been demonstrated on other sentence classification tasks, where CNN outperforms LSTM and logistic regression classifiers (Kim, 2014; Zhang and Wallace, 2015). We report results of AC-GAN using the CNN discriminator. Comparing \"Full Model\" with \"Vagueness Only\",we found that allowing the AC-GAN to only discriminate sentences of different levels of vagueness, but not real/fake sentences, yields better results. We conjecture this is because training GAN models, especially with a multitask learning objective, can be unstable and more effort is required to balance the two objectives (LS and LC).",
        "sentences": [
            "6.3 Predicting Vague Sentences.",
            "In Table 7 we present results on classifying privacy policy sentences into four categories: clear, somewhat clear, vague, and extremely vague.",
            "We compare AC-GAN with three baselines: CNN and LSTM trained on human-annotated sentences, and a majority baseline that assigns the most frequent label to all test sentences.",
            "We observe that the ACGAN models (using CNN discriminator) perform strongly, surpassing all baseline approaches.",
            "CNN shows strong performance, yielding an F-score of 50.92%.",
            "A similar effect has been demonstrated on other sentence classification tasks, where CNN outperforms LSTM and logistic regression classifiers (Kim, 2014; Zhang and Wallace, 2015).",
            "We report results of AC-GAN using the CNN discriminator.",
            "Comparing \"Full Model\" with \"Vagueness Only\",we found that allowing the AC-GAN to only discriminate sentences of different levels of vagueness, but not real/fake sentences, yields better results.",
            "We conjecture this is because training GAN models, especially with a multitask learning objective, can be unstable and more effort is required to balance the two objectives (LS and LC)."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            2,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "AC-GAN (Full Model)",
                "AC-GAN (Vagueness Only)",
                "Baseline (Majority)",
                "LSTM",
                "CNN"
            ],
            [
                "AC-GAN (Full Model)",
                "AC-GAN (Vagueness Only)"
            ],
            [
                "CNN"
            ],
            [
                "CNN"
            ],
            [
                "AC-GAN (Full Model)",
                "AC-GAN (Vagueness Only)"
            ],
            [
                "AC-GAN (Full Model)",
                "AC-GAN (Vagueness Only)"
            ],
            null
        ],
        "n_sentence": 9.0,
        "table_id": "table_7",
        "paper_id": "D18-1387",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1388table_1",
        "description": "5.1 Results and Analysis. Quantitative Results. Table 1 shows the results of the evaluation. First note that the logistic regression classifier and the CNN model using the Title outperforms the CHANCE classifier significantly (F1: 59.12,59.24 vs 34.53). Second, only modeling the network structure yields a F1 of 55.10 but still significantly better than the chance baseline. This confirms our intuition that modeling the network structure can be useful in prediction of ideology. Third, note that modeling the content (HDAM) significantly outperforms all previous baselines (F1:68.92). This suggests that content cues can be very strong indicators of ideology. Finally, all flavors of our model outperform the baselines. Specifically, observe that incorporating the network cues outperforms all uni-modal models that only model either the title, the network, or the content. It is also worth noting that without the network, only the title and the content show only a small improvement over the best performing baseline (69.54 vs 68.92) suggesting that the network yields distinctive cues from both the title, and the content. Finally, the best performing model effectively uses all three modalities to yield a F1 score of 79.67 outperforming the state of the art baseline by 10 percentage points. Altogether our results suggest the superiority of our model over competitive baselines. In order to obtain deeper insights into our model, we also perform a qualitative analysis of our model\u00e2\u20ac\u2122s predictions.",
        "sentences": [
            "5.1 Results and Analysis.",
            "Quantitative Results.",
            "Table 1 shows the results of the evaluation.",
            "First note that the logistic regression classifier and the CNN model using the Title outperforms the CHANCE classifier significantly (F1: 59.12,59.24 vs 34.53).",
            "Second, only modeling the network structure yields a F1 of 55.10 but still significantly better than the chance baseline.",
            "This confirms our intuition that modeling the network structure can be useful in prediction of ideology.",
            "Third, note that modeling the content (HDAM) significantly outperforms all previous baselines (F1:68.92).",
            "This suggests that content cues can be very strong indicators of ideology.",
            "Finally, all flavors of our model outperform the baselines.",
            "Specifically, observe that incorporating the network cues outperforms all uni-modal models that only model either the title, the network, or the content.",
            "It is also worth noting that without the network, only the title and the content show only a small improvement over the best performing baseline (69.54 vs 68.92) suggesting that the network yields distinctive cues from both the title, and the content.",
            "Finally, the best performing model effectively uses all three modalities to yield a F1 score of 79.67 outperforming the state of the art baseline by 10 percentage points.",
            "Altogether our results suggest the superiority of our model over competitive baselines.",
            "In order to obtain deeper insights into our model, we also perform a qualitative analysis of our model\u00e2\u20ac\u2122s predictions."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1,
            2,
            1,
            2,
            1,
            2,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "LR",
                "CNN",
                "CHANCE",
                "F1"
            ],
            [
                "FNN",
                "CHANCE",
                "F1"
            ],
            [
                "FNN"
            ],
            [
                "HDAM",
                "F1"
            ],
            [
                "HDAM"
            ],
            [
                "MVDAM",
                "Title Network",
                "Title Content",
                "Title Network Content"
            ],
            [
                "Title Network Content"
            ],
            [
                "MVDAM",
                "Title Content",
                "HDAM",
                "F1",
                "Network"
            ],
            [
                "MVDAM",
                "Title Network Content",
                "HDAM",
                "F1"
            ],
            [
                "MVDAM"
            ],
            null
        ],
        "n_sentence": 14.0,
        "table_id": "table_1",
        "paper_id": "D18-1388",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1389table_4",
        "description": "4.3 Results and Discussion. We present in Table 4 the results of using features from the different sources proposed in Section 3. We start by describing the contribution of each feature type towards factuality and bias. We can see that the textual features extracted from the ARTICLES yielded the best performance on factuality. They also perform well on bias, being the only type that beats the baseline on MAE. These results indicate the importance of analyzing the contents of the target website. They also show that using the titles only is not enough, and that the article bodies contain important information that should not be ignored. Overall, the WIKIPEDIA features are less useful for factuality, and perform reasonably well for bias. The best features from this family are those about the page content, which includes a general description of the medium, its history, ideology and other information that can be potentially helpful. Interestingly, the has page feature alone yields sizable improvement over the baseline, especially for factuality. This makes sense given that trustworthy websites are more likely to have Wikipedia pages; yet, this feature does not help much for predicting political bias. The TWITTER features perform moderately for factuality and poorly for bias. This is not surprising, as we normally may not be able to tell much about the political ideology of a website just by looking at its Twitter profile (not its tweets!) unless something is mentioned in its description, which turns out to perform better than the rest of the features from this family. We can see that the has twitter feature is less effective than has wiki for factuality, which makes sense given that Twitter is less regulated than Wikipedia. Note that the counts features yield reasonable performance, indicating that information about activity (e.g., number of statuses) and social connectivity (e.g., number of followers) is useful. Overall, the TWITTER features seem to complement each other, as their union yields the best performance on factuality. The URL features are better used for factuality rather than bias prediction. This is mainly due to the nature of these features, which are aimed at detecting phishing websites, as we mentioned in Section 3. Overall, this feature family yields slight improvements, suggesting that it can be useful when used together with other features. Finally, the Alexa rank does not improve over the baseline, which suggests that more sophisticated TRAFFIC-related features might be needed.",
        "sentences": [
            "4.3 Results and Discussion.",
            "We present in Table 4 the results of using features from the different sources proposed in Section 3.",
            "We start by describing the contribution of each feature type towards factuality and bias.",
            "We can see that the textual features extracted from the ARTICLES yielded the best performance on factuality.",
            "They also perform well on bias, being the only type that beats the baseline on MAE.",
            "These results indicate the importance of analyzing the contents of the target website.",
            "They also show that using the titles only is not enough, and that the article bodies contain important information that should not be ignored.",
            "Overall, the WIKIPEDIA features are less useful for factuality, and perform reasonably well for bias.",
            "The best features from this family are those about the page content, which includes a general description of the medium, its history, ideology and other information that can be potentially helpful.",
            "Interestingly, the has page feature alone yields sizable improvement over the baseline, especially for factuality.",
            "This makes sense given that trustworthy websites are more likely to have Wikipedia pages; yet, this feature does not help much for predicting political bias.",
            "The TWITTER features perform moderately for factuality and poorly for bias.",
            "This is not surprising, as we normally may not be able to tell much about the political ideology of a website just by looking at its Twitter profile (not its tweets!) unless something is mentioned in its description, which turns out to perform better than the rest of the features from this family.",
            "We can see that the has twitter feature is less effective than has wiki for factuality, which makes sense given that Twitter is less regulated than Wikipedia.",
            "Note that the counts features yield reasonable performance, indicating that information about activity (e.g., number of statuses) and social connectivity (e.g., number of followers) is useful.",
            "Overall, the TWITTER features seem to complement each other, as their union yields the best performance on factuality.",
            "The URL features are better used for factuality rather than bias prediction.",
            "This is mainly due to the nature of these features, which are aimed at detecting phishing websites, as we mentioned in Section 3.",
            "Overall, this feature family yields slight improvements, suggesting that it can be useful when used together with other features.",
            "Finally, the Alexa rank does not improve over the baseline, which suggests that more sophisticated TRAFFIC-related features might be needed."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            1,
            2,
            1,
            1,
            2,
            1,
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Traffic",
                "URL",
                "Twitter",
                "Wikipedia",
                "Articles"
            ],
            [
                "Factuality",
                "Bias"
            ],
            [
                "Articles",
                "Factuality"
            ],
            [
                "Articles",
                "Majority Baseline",
                "Bias",
                "MAE"
            ],
            null,
            [
                "Articles",
                "title",
                "body"
            ],
            [
                "Wikipedia",
                "Factuality",
                "Bias"
            ],
            [
                "Wikipedia",
                "content"
            ],
            [
                "Wikipedia",
                "has page",
                "Factuality",
                "Majority Baseline"
            ],
            [
                "Wikipedia"
            ],
            [
                "Twitter",
                "Factuality",
                "Bias"
            ],
            [
                "Twitter",
                "description"
            ],
            [
                "Twitter",
                "Wikipedia"
            ],
            [
                "counts"
            ],
            [
                "Twitter",
                "Factuality"
            ],
            [
                "URL",
                "Factuality",
                "Bias"
            ],
            [
                "URL"
            ],
            [
                "URL"
            ],
            [
                "Traffic",
                "Majority Baseline",
                "Alexa rank"
            ]
        ],
        "n_sentence": 20.0,
        "table_id": "table_4",
        "paper_id": "D18-1389",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1392table_1",
        "description": "Table 1 compares results in terms of variance explained, when using the three hand-picked factors vs. using all 11 extra-linguistic factors (Since past work has also used the Pearson-r metric, Table 2 shows the same results for all factors in terms of Pearson-r). As the table shows, FA outperforms controls only, added-controls, and residualized control. RFA does even better and outperforms FA on both the hand-picked factors and when using the entire set of factors. These results demonstrate the complementary nature of the residualized control and factor adaptation approaches and the benefits of combining them. Even though adding controls directly, as in the \u201cadded-controls\u201d column, works better than language-only and controls-only models, it is worse than any other model that exploits both language and extra-linguistic data. This motivates the need for combining different types of features in both an additive (residualized control) and multiplicative (factor adaptation) style. Overall, these results show the power of RFA over the other models. RFA\u2019s improvement over FA was statistically significant for 4 out of 5 outcomes, and 3 out of 5 for residualized control. Recall that added-controls, residualized control, FA, and RFA all have access to the same set of information. The gains of RFA over FA show that RFA\u2019s structure utilizing residualized control is better suited for combining extra-linguistic and language-only features.",
        "sentences": [
            "Table 1 compares results in terms of variance explained, when using the three hand-picked factors vs. using all 11 extra-linguistic factors (Since past work has also used the Pearson-r metric, Table 2 shows the same results for all factors in terms of Pearson-r).",
            "As the table shows, FA outperforms controls only, added-controls, and residualized control.",
            "RFA does even better and outperforms FA on both the hand-picked factors and when using the entire set of factors.",
            "These results demonstrate the complementary nature of the residualized control and factor adaptation approaches and the benefits of combining them.",
            "Even though adding controls directly, as in the \u201cadded-controls\u201d column, works better than language-only and controls-only models, it is worse than any other model that exploits both language and extra-linguistic data.",
            "This motivates the need for combining different types of features in both an additive (residualized control) and multiplicative (factor adaptation) style.",
            "Overall, these results show the power of RFA over the other models.",
            "RFA\u2019s improvement over FA was statistically significant for 4 out of 5 outcomes, and 3 out of 5 for residualized control.",
            "Recall that added-controls, residualized control, FA, and RFA all have access to the same set of information.",
            "The gains of RFA over FA show that RFA\u2019s structure utilizing residualized control is better suited for combining extra-linguistic and language-only features."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            1,
            2,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "Controls Only",
                "Added-Controls",
                "RC",
                "FA"
            ],
            [
                "RFA",
                "FA",
                "3 Socio-Demographic Factors",
                "All Factors"
            ],
            null,
            [
                "Lang.",
                "Controls Only",
                "Added-Controls"
            ],
            [
                "FA",
                "RFA"
            ],
            [
                "RFA"
            ],
            [
                "RC",
                "FA",
                "RFA"
            ],
            [
                "Added-Controls",
                "RC",
                "FA",
                "RFA"
            ],
            [
                "RFA"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_1",
        "paper_id": "D18-1392",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1395table_2",
        "description": "Table 2 depicts the results obtained by combining character trigrams, tokens, and spelling features (Sections 3.5.1, 3.5.2). As expected, these content features yield excellent results in-domain, but the accuracy deteriorates out-of-domain, especially in the most challenging task of NLI.",
        "sentences": [
            "Table 2 depicts the results obtained by combining character trigrams, tokens, and spelling features (Sections 3.5.1, 3.5.2).",
            "As expected, these content features yield excellent results in-domain, but the accuracy deteriorates out-of-domain, especially in the most challenging task of NLI."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "In-domain",
                "Out-of-domain",
                "NLI"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_2",
        "paper_id": "D18-1395",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1395table_4",
        "description": "Table 4 shows the results obtained by combining the spelling features with the grammar features (Section 3.5.2). Clearly, these two feature types reflect somewhat different phenomena, as the results are better than using any of the two alone.",
        "sentences": [
            "Table 4 shows the results obtained by combining the spelling features with the grammar features (Section 3.5.2).",
            "Clearly, these two feature types reflect somewhat different phenomena, as the results are better than using any of the two alone."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            null
        ],
        "n_sentence": 2.0,
        "table_id": "table_4",
        "paper_id": "D18-1395",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1395table_5",
        "description": "Table 5 shows the accuracy obtained by all the centrality features (Section 3.5.4), excluding the most popular subreddits. As expected, the contribution of these features is small, and is most evident on the binary task. The signal of the native language reflected by these features is very subtle, but is nonetheless present, as the results are consistently higher than the baseline.",
        "sentences": [
            "Table 5 shows the accuracy obtained by all the centrality features (Section 3.5.4), excluding the most popular subreddits.",
            "As expected, the contribution of these features is small, and is most evident on the binary task.",
            "The signal of the native language reflected by these features is very subtle, but is nonetheless present, as the results are consistently higher than the baseline."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Binary"
            ],
            null
        ],
        "n_sentence": 3.0,
        "table_id": "table_5",
        "paper_id": "D18-1395",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1396table_3",
        "description": "Same as last section, we evaluate the quality of the left and right half of the translation results generated by both the left-to-right and right-toleft models. The results are summarized in Table 3. For comparison, we also include the BLEU scores of normal translation (without teacher forcing). We have several findings from Table 3 as follows:. \u2022 Exposure bias exists. The accuracy of both left and right half tokens in the normal translation is lower than that in teacher forcing, which feeds the ground-truth tokens as inputs. This demonstrates that feeding the previously generated tokens (which might be in correct) in inference indeed hurts translation accuracy. \u2022 Error propagation does exist. We find the error is accumulated along the sequential generation of the sentence. Taking En-Zh and the left-to-right NMT model as an example, the BLEU score improvement of the right half (the second half of the generation) of teacher forcing over normal translation is 2.64, which is much larger than the accuracy improvement of the left half (the first half of the generation): 1.70. Similarly, for En-Zh with the right-to-left NMT model, the BLEU score improvement of the left half (the second half of the generation) of teacher forcing over normal translation is 2.82, which is much larger than the accuracy improvement of the right half (the first half of the generation): 1.77. \u2022 Other causes exist. Taking En-De translation with the left-to-right model as an example, the accuracy of the left half (9.43) is higher than that of the right half (8.36) when there is no error propagation with teacher forcing. Similar results can be found in other language pairs and models. This suggests that there must be some other causes leading to accuracy drop, which will be studied in the next section.",
        "sentences": [
            "Same as last section, we evaluate the quality of the left and right half of the translation results generated by both the left-to-right and right-toleft models.",
            "The results are summarized in Table 3.",
            "For comparison, we also include the BLEU scores of normal translation (without teacher forcing).",
            "We have several findings from Table 3 as follows:.",
            "\u2022 Exposure bias exists.",
            "The accuracy of both left and right half tokens in the normal translation is lower than that in teacher forcing, which feeds the ground-truth tokens as inputs.",
            "This demonstrates that feeding the previously generated tokens (which might be in correct) in inference indeed hurts translation accuracy.",
            "\u2022 Error propagation does exist.",
            "We find the error is accumulated along the sequential generation of the sentence.",
            "Taking En-Zh and the left-to-right NMT model as an example, the BLEU score improvement of the right half (the second half of the generation) of teacher forcing over normal translation is 2.64, which is much larger than the accuracy improvement of the left half (the first half of the generation): 1.70.",
            "Similarly, for En-Zh with the right-to-left NMT model, the BLEU score improvement of the left half (the second half of the generation) of teacher forcing over normal translation is 2.82, which is much larger than the accuracy improvement of the right half (the first half of the generation): 1.77.",
            "\u2022 Other causes exist.",
            "Taking En-De translation with the left-to-right model as an example, the accuracy of the left half (9.43) is higher than that of the right half (8.36) when there is no error propagation with teacher forcing.",
            "Similar results can be found in other language pairs and models.",
            "This suggests that there must be some other causes leading to accuracy drop, which will be studied in the next section."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            2,
            1,
            2,
            2,
            2,
            1,
            1,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "left-to-right",
                "right-to-left"
            ],
            null,
            [
                "0"
            ],
            null,
            null,
            [
                "Left",
                "Right",
                "0",
                "1"
            ],
            null,
            null,
            null,
            [
                "En-Zh",
                "Left",
                "Right",
                "left-to-right",
                "\u0394"
            ],
            [
                "En-Zh",
                "Left",
                "Right",
                "right-to-left",
                "\u0394"
            ],
            null,
            [
                "En-De",
                "Left",
                "Right",
                "left-to-right",
                "1"
            ],
            null,
            null
        ],
        "n_sentence": 15.0,
        "table_id": "table_3",
        "paper_id": "D18-1396",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1397table_2",
        "description": "Table 2 shows that the learning of baseline reward does not help RL training. This contradicts with previous observations (Ranzato et al., 2016), and seems to suggest that the variance of gradient estimation in NMT is not as large as we expected. The reason might be that the probability mass on the target-side language space induced by the NMT model is highly concentrated, making the sampled y representative enough in terms of estimating the expectation. Therefore, for the economic perspective, it is not necessary to add the additional steps of using baseline reward on RL training for NMT.",
        "sentences": [
            "Table 2 shows that the learning of baseline reward does not help RL training.",
            "This contradicts with previous observations (Ranzato et al., 2016), and seems to suggest that the variance of gradient estimation in NMT is not as large as we expected.",
            "The reason might be that the probability mass on the target-side language space induced by the NMT model is highly concentrated, making the sampled y representative enough in terms of estimating the expectation.",
            "Therefore, for the economic perspective, it is not necessary to add the additional steps of using baseline reward on RL training for NMT."
        ],
        "class_sentence": [
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            [
                "RL (baseline function)"
            ],
            null,
            null,
            [
                "RL (baseline function)"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "D18-1397",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1397table_3",
        "description": "From Table 3 and 4, we have several observations. First, monolingual data helps RL training, improving BLEU score from 25.04 to 25.22 (\u03c1 < 0.05) in Table 3. Second, when we only add monolingual data for RL training, the model achieves similar performance compared to MLE training with bilingual and monolingual data (e.g., 25.15 vs. 25.24 (\u03c1 < 0.05) in Table 4).",
        "sentences": [
            "From Table 3 and 4, we have several observations.",
            "First, monolingual data helps RL training, improving BLEU score from 25.04 to 25.22 (\u03c1 < 0.05) in Table 3.",
            "Second, when we only add monolingual data for RL training, the model achieves similar performance compared to MLE training with bilingual and monolingual data (e.g., 25.15 vs. 25.24 (\u03c1 < 0.05) in Table 4)."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "[B] (MLE) + [Ms] (RL)",
                "[B] (MLE) + [B] (RL)",
                "Test"
            ],
            [
                "[B] (MLE) + [Ms] (RL)",
                "[B & Ms] (MLE)"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_3",
        "paper_id": "D18-1397",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1397table_5",
        "description": "With both Source-Side and Target-Side Monolingual Data. We have two approaches to use both source-side and target-side monolingual data, as described in subsection 4.3. The results are reported in Table 5. From Table 5, we can observe that the sequential training of monolingual data can benefit the model performance. Taking the last three rows as an example, the BLEU score of the MLE model trained on the combination of bilingual data and target-side monolingual data is 25.24; based on this model, RL training using the source-side monolingual data further improves the model performance by 0.7 (\u00cf\u0081 < 0.01) BLEU points.",
        "sentences": [
            "With both Source-Side and Target-Side Monolingual Data.",
            "We have two approaches to use both source-side and target-side monolingual data, as described in subsection 4.3.",
            "The results are reported in Table 5.",
            "From Table 5, we can observe that the sequential training of monolingual data can benefit the model performance.",
            "Taking the last three rows as an example, the BLEU score of the MLE model trained on the combination of bilingual data and target-side monolingual data is 25.24; based on this model, RL training using the source-side monolingual data further improves the model performance by 0.7 (\u00cf\u0081 < 0.01) BLEU points."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            [
                "[B & Mt] (MLE)",
                "[B & Mt] (MLE) + [Ms] (RL)",
                "Test"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_5",
        "paper_id": "D18-1397",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1399table_3",
        "description": "6.3 Comparison with supervised systems. So as to put our results into perspective, Table 3 comprises the results of different supervised methods in the same test sets. More concretely, we report the results of the Transformer (Vaswani et al., 2017), an NMT system based on self-attention that is the current state-of-the-art in machine translation, along with the scores obtained by the best performing system in each WMT shared task at the time, and those of a standard phrase-based SMT system trained on Europarl and tuned on newstest2013 using Moses. We also report the effect of removing lexical reordering from the latter as we do in our initial system (Section 4), restricting the vocabulary to the most frequent unigram, bigram and trigrams as we do when training our embeddings (Section 3), and using our unsupervised tuning procedure over a subset of the monolingual corpus (Section 4.2) instead of using standard MERT tuning over newstest2013. Quite surprisingly, our proposed system, trained exclusively on monolingual corpora, is relatively close to a comparable phrase-based SMT system trained on Europarl, with differences below 5 BLEU points in all cases and as little as 2.5 in some. Note that both systems use the exact same language model trained on News Crawl, making them fully comparable in terms of the monolingual corpora they have access to. While more of a baseline than the state-of-the-art, note that Moses+Europarl is widely used as a reference system in machine translation. As such, we think that our results are very encouraging, as they show that our fully unsupervised system is already quite close to this competitive baseline. In addition to that, the results for the constrained variants of this SMT system justify some of the simplifications required by our approach. In particular, removing lexical reordering and constraining the phrase table to the most frequent n-grams, as we do for our initial system, has a relatively small effect, with a drop of less than 1 BLEU point in all cases, and as little as 0.28 in some. Replacing standard MERT tuning with our unsupervised variant does cause a considerable drop in performance, although it is below 2.5 BLEU points even in the worst case, and our unsupervised tuning method is still better than using default weights as reported in Table 2. This shows the importance of tuning in SMT, suggesting that these results could be further improved if one had access to a small parallel corpus for tuning.",
        "sentences": [
            "6.3 Comparison with supervised systems.",
            "So as to put our results into perspective, Table 3 comprises the results of different supervised methods in the same test sets.",
            "More concretely, we report the results of the Transformer (Vaswani et al., 2017), an NMT system based on self-attention that is the current state-of-the-art in machine translation, along with the scores obtained by the best performing system in each WMT shared task at the time, and those of a standard phrase-based SMT system trained on Europarl and tuned on newstest2013 using Moses.",
            "We also report the effect of removing lexical reordering from the latter as we do in our initial system (Section 4), restricting the vocabulary to the most frequent unigram, bigram and trigrams as we do when training our embeddings (Section 3), and using our unsupervised tuning procedure over a subset of the monolingual corpus (Section 4.2) instead of using standard MERT tuning over newstest2013.",
            "Quite surprisingly, our proposed system, trained exclusively on monolingual corpora, is relatively close to a comparable phrase-based SMT system trained on Europarl, with differences below 5 BLEU points in all cases and as little as 2.5 in some.",
            "Note that both systems use the exact same language model trained on News Crawl, making them fully comparable in terms of the monolingual corpora they have access to.",
            "While more of a baseline than the state-of-the-art, note that Moses+Europarl is widely used as a reference system in machine translation.",
            "As such, we think that our results are very encouraging, as they show that our fully unsupervised system is already quite close to this competitive baseline.",
            "In addition to that, the results for the constrained variants of this SMT system justify some of the simplifications required by our approach.",
            "In particular, removing lexical reordering and constraining the phrase table to the most frequent n-grams, as we do for our initial system, has a relatively small effect, with a drop of less than 1 BLEU point in all cases, and as little as 0.28 in some.",
            "Replacing standard MERT tuning with our unsupervised variant does cause a considerable drop in performance, although it is below 2.5 BLEU points even in the worst case, and our unsupervised tuning method is still better than using default weights as reported in Table 2.",
            "This shows the importance of tuning in SMT, suggesting that these results could be further improved if one had access to a small parallel corpus for tuning."
        ],
        "class_sentence": [
            2,
            1,
            2,
            2,
            1,
            2,
            2,
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "NMT (transformer)"
            ],
            [
                "+ w/o lexical reord.",
                "+ constrained vocab.",
                "+ unsup. tuning"
            ],
            [
                "Proposed system",
                "Supervised SMT (europarl)"
            ],
            [
                "Proposed system",
                "Supervised SMT (europarl)"
            ],
            null,
            [
                "Proposed system"
            ],
            [
                "Supervised SMT (europarl)",
                "+ w/o lexical reord.",
                "+ constrained vocab.",
                "+ unsup. tuning"
            ],
            [
                "Supervised SMT (europarl)",
                "+ w/o lexical reord.",
                "+ constrained vocab."
            ],
            [
                "Supervised SMT (europarl)",
                "+ unsup. tuning",
                "Proposed system"
            ],
            null
        ],
        "n_sentence": 12.0,
        "table_id": "table_3",
        "paper_id": "D18-1399",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1403table_4",
        "description": "Table 4 (top) reports the results using micro-averaged F1. Our models outperform both variants of ABAE across domains. ABAEinit improves upon the vanilla model, affirming that informed aspect initialization can facilitate the task. The richer multi-seed representation of MATE, however, helps our model achieve a 3.2% increase in F1. Further improvements are gained by the multi-task model, which boosts performance by 2.7%. Results are shown in Table 4 (bottom). The combined use of polarity and aspect information improves the retrieval of salient opinions across domains, as all model variants that use our salience formula of Equation (12) outperform the MILNET and aspect-only baselines. When comparing between aspect-based alternatives, we observe that the extraction accuracy correlates with the quality of aspect prediction. In particular, ranking using MILNET+MATE+MT gives best results, with a 2.6% increase in MAP against MILNET+MATE and 4.6% against MILNET+ABAEinit. The trend persists even when MILNET polarities are ignored, although the quality of rankings is worse in this case. Opinion Summaries We now turn to the summarization task itself, where we compare our best performing model (MILNET+MATE+MT), with and without a redundancy filter (RD), against the following methods: a baseline that selects segments randomly; a Lead baseline that only selects the leading segments from each review; SumBasic,a generic frequency-based extractive summarizer (Nenkova and Vanderwende, 2005); LexRank, a generic graph-based extractive summarizer(Erkan and Radev, 2004); Opinosis, a graph-based abstractive summarizer that is designed for opinion summarization (Ganesan et al., 2010).",
        "sentences": [
            "Table 4 (top) reports the results using micro-averaged F1.",
            "Our models outperform both variants of ABAE across domains.",
            "ABAEinit improves upon the vanilla model, affirming that informed aspect initialization can facilitate the task.",
            "The richer multi-seed representation of MATE, however, helps our model achieve a 3.2% increase in F1.",
            "Further improvements are gained by the multi-task model, which boosts performance by 2.7%.",
            "Results are shown in Table 4 (bottom).",
            "The combined use of polarity and aspect information improves the retrieval of salient opinions across domains, as all model variants that use our salience formula of Equation (12) outperform the MILNET and aspect-only baselines.",
            "When comparing between aspect-based alternatives, we observe that the extraction accuracy correlates with the quality of aspect prediction.",
            "In particular, ranking using MILNET+MATE+MT gives best results, with a 2.6% increase in MAP against MILNET+MATE and 4.6% against MILNET+ABAEinit.",
            "The trend persists even when MILNET polarities are ignored, although the quality of rankings is worse in this case.",
            "Opinion Summaries We now turn to the summarization task itself, where we compare our best performing model (MILNET+MATE+MT), with and without a redundancy filter (RD), against the following methods: a baseline that selects segments randomly; a Lead baseline that only selects the leading segments from each review; SumBasic,a generic frequency-based extractive summarizer (Nenkova and Vanderwende, 2005); LexRank, a generic graph-based extractive summarizer(Erkan and Radev, 2004); Opinosis, a graph-based abstractive summarizer that is designed for opinion summarization (Ganesan et al., 2010)."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            2,
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            [
                "Aspect Extraction (F1)"
            ],
            [
                "ABAE",
                "ABAEinit",
                "MATE"
            ],
            [
                "ABAE",
                "ABAEinit"
            ],
            [
                "ABAEinit",
                "MATE",
                "AVG"
            ],
            [
                "MATE",
                "MATE+MT",
                "AVG"
            ],
            [
                "Salience (MAP/P@5)"
            ],
            [
                "MILNET+ABAEinit",
                "MILNET+MATE",
                "MILNET+MATE+MT"
            ],
            null,
            [
                "MILNET+MATE+MT",
                "MILNET+ABAEinit",
                "MILNET+MATE",
                "AVG"
            ],
            [
                "MILNET"
            ],
            [
                "MILNET+MATE+MT"
            ]
        ],
        "n_sentence": 11.0,
        "table_id": "table_4",
        "paper_id": "D18-1403",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1403table_5",
        "description": "Table 5 presents ROUGE-1, ROUGE-2 and ROUGE-L F1 scores, averaged across domains. Our model (MILNET+MATE+MT) significantly outperforms all comparison systems (p < 0.05; paired bootstrap resampling; Koehn 2004), whilst using a redundancy filter slightly improves performance. Assisting Opinosis with aspect predictions is beneficial, however, it remains significantly inferior to our model (see the supplementary material for additional results).",
        "sentences": [
            "Table 5 presents ROUGE-1, ROUGE-2 and ROUGE-L F1 scores, averaged across domains.",
            "Our model (MILNET+MATE+MT) significantly outperforms all comparison systems (p < 0.05; paired bootstrap resampling; Koehn 2004), whilst using a redundancy filter slightly improves performance.",
            "Assisting Opinosis with aspect predictions is beneficial, however, it remains significantly inferior to our model (see the supplementary material for additional results)."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "ROUGE-1",
                "ROUGE-2",
                "ROUGE-L"
            ],
            [
                "MILNET+MATE+MT"
            ],
            [
                "MILNET+MATE+MT",
                "Opinosis+MATE+MT"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_5",
        "paper_id": "D18-1403",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1410table_2",
        "description": "Results. Table 2 compares the performances of our NRR model to the state-of-the-art results reported by Paetzold and Specia (2017). We use precision of the simplest candidate (P@1) and Pearson correlation to measure performance. P@1 is equivalent to TRank (Specia et al., 2012), the official metric for the SemEval 2012 English Lexical Simplification task. While P@1 captures the practical utility of an approach, Pearson correlation indicates how well the system\u2019s rankings correlate with human judgment. We train our NRR model with all the features (NRRall) mentioned in \u00a73.2 except the word2vec embedding features to avoid overfitting on the small training set. Our full model (NRRall+binning+WC) exhibits a statistically significant improvement over the state-of-the-art for both measures. We use paired bootstrap test (Berg-Kirkpatrick et al., 2012; Efron and Tibshirani, 1993) as it can be applied to any performance metric. We also conducted ablation experiments to show the effectiveness of the Gaussianbased feature vectorization layer (+binning) and the word-complexity lexicon (+W C).",
        "sentences": [
            "Results.",
            "Table 2 compares the performances of our NRR model to the state-of-the-art results reported by Paetzold and Specia (2017).",
            "We use precision of the simplest candidate (P@1) and Pearson correlation to measure performance.",
            "P@1 is equivalent to TRank (Specia et al., 2012), the official metric for the SemEval 2012 English Lexical Simplification task.",
            "While P@1 captures the practical utility of an approach, Pearson correlation indicates how well the system\u2019s rankings correlate with human judgment.",
            "We train our NRR model with all the features (NRRall) mentioned in \u00a73.2 except the word2vec embedding features to avoid overfitting on the small training set.",
            "Our full model (NRRall+binning+WC) exhibits a statistically significant improvement over the state-of-the-art for both measures.",
            "We use paired bootstrap test (Berg-Kirkpatrick et al., 2012; Efron and Tibshirani, 1993) as it can be applied to any performance metric.",
            "We also conducted ablation experiments to show the effectiveness of the Gaussianbased feature vectorization layer (+binning) and the word-complexity lexicon (+W C)."
        ],
        "class_sentence": [
            2,
            1,
            2,
            2,
            2,
            2,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "P@1",
                "Pearson"
            ],
            [
                "P@1"
            ],
            [
                "Pearson"
            ],
            [
                "NRRall",
                "NRRall+binning",
                "NRRall+binning+WC"
            ],
            [
                "NRRall+binning+WC",
                "P@1",
                "Pearson"
            ],
            null,
            [
                "NRRall+binning",
                "NRRall+binning+WC"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_2",
        "paper_id": "D18-1410",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1410table_4",
        "description": "Results. Following the evaluation setup in previous work (Pavlick and Callison-Burch, 2016), we compare accuracy and precision by 10-fold cross-validation. Folds are constructed in such a way that the training and test vocabularies are disjoint. Table 4 shows the performance of our model compared to SimplePPDB and other baselines. We use all the features (NRRall) in  \u00a73.2 except for the context features as we are classifying paraphrase rules in PPDB that come with no context. SimplePPDB used the same features plus additional discrete features, such as POS tags, character unigrams and bigrams. Our neural readability ranking model alone with Gaussian binning (NRRall+binning) achieves better accuracy and precision while using less features. Leveraging the lexicon (NRRall+binning+WC) shows statistically signi?cant improvements over SimplePPDB rankings based on the paired bootstrap test. The accuracy increases by 3.2 points, the precision for \u2018simplifying\u2019 class improves by 7.4 points and the precision for \u2018complicating\u2019 class improves by 4.0 points.",
        "sentences": [
            "Results.",
            "Following the evaluation setup in previous work (Pavlick and Callison-Burch, 2016), we compare accuracy and precision by 10-fold cross-validation.",
            "Folds are constructed in such a way that the training and test vocabularies are disjoint.",
            "Table 4 shows the performance of our model compared to SimplePPDB and other baselines.",
            "We use all the features (NRRall) in  \u00a73.2 except for the context features as we are classifying paraphrase rules in PPDB that come with no context.",
            "SimplePPDB used the same features plus additional discrete features, such as POS tags, character unigrams and bigrams.",
            "Our neural readability ranking model alone with Gaussian binning (NRRall+binning) achieves better accuracy and precision while using less features.",
            "Leveraging the lexicon (NRRall+binning+WC) shows statistically signi?cant improvements over SimplePPDB rankings based on the paired bootstrap test.",
            "The accuracy increases by 3.2 points, the precision for \u2018simplifying\u2019 class improves by 7.4 points and the precision for \u2018complicating\u2019 class improves by 4.0 points."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            2,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Acc.",
                "P+1",
                "P-1"
            ],
            null,
            null,
            [
                "NRRall"
            ],
            [
                "SimplePPDB"
            ],
            [
                "NRRall",
                "NRRall+binning",
                "Acc.",
                "P+1",
                "P-1"
            ],
            [
                "NRRall+binning+WC",
                "SimplePPDB"
            ],
            [
                "NRRall+binning+WC",
                "SimplePPDB",
                "Acc.",
                "P+1",
                "P-1"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_4",
        "paper_id": "D18-1410",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1410table_5",
        "description": "Results. Table 5 shows the comparison of SimplePPDB and SimplePPDB++ on the number of substitutions generated for each target, the mean average precision and precision@1 for the final ranked list of candidate substitutions. This is a fair and direct comparison between SimplePPDB++ and SimplePPDB, as both methods have access to the same paraphrase rules in PPDB as potential candidates. The better NRR model we used in creating SimplePPDB++ allows improved selections and rankings of simplifying paraphrase rules than the previous version of SimplePPDB. As an additional reference, we also include the measurements for the other existing methods based on (Pavlick and Callison-Burch, 2016), which, by evaluation design, are focused on the comparison of precision while PPDB has full coverage.",
        "sentences": [
            "Results.",
            "Table 5 shows the comparison of SimplePPDB and SimplePPDB++ on the number of substitutions generated for each target, the mean average precision and precision@1 for the final ranked list of candidate substitutions.",
            "This is a fair and direct comparison between SimplePPDB++ and SimplePPDB, as both methods have access to the same paraphrase rules in PPDB as potential candidates.",
            "The better NRR model we used in creating SimplePPDB++ allows improved selections and rankings of simplifying paraphrase rules than the previous version of SimplePPDB.",
            "As an additional reference, we also include the measurements for the other existing methods based on (Pavlick and Callison-Burch, 2016), which, by evaluation design, are focused on the comparison of precision while PPDB has full coverage."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "SimplePPDB",
                "SimplePPDB++",
                "#PPs",
                "MAP",
                "P@1"
            ],
            [
                "SimplePPDB++",
                "SimplePPDB"
            ],
            [
                "SimplePPDB++"
            ],
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_5",
        "paper_id": "D18-1410",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1410table_6",
        "description": "Results. We compare our enhanced approaches (SV000gg+W C and NC+W C) and lexicon only approach (W C-only), with the state-of-the-art and baseline threshold-based methods. For measuring performance, we use F-score and accuracy as well as G-score, the harmonic mean of accuracy and recall. G-score is the official metric of the CWI task of Semeval 2016. Table 6 shows that the wordcomplexity lexicon improves the performance of SV000gg and the nearest centroid classifier in all the three metrics. The improvements are statistically significant according to the paired bootstrap test with p < 0.01. The word-complexity lexicon alone (WC-only) performs satisfactorily on the CWIG3G2 dataset, which effectively is a simple table look-up approach with extreme time and space efficiency. For CWI SemEval 2016 dataset, WC-only approach gives the best accuracy and Fscore, though this can be attributed to the skewed distribution of dataset (only 5% of the test instances are 'complex'.",
        "sentences": [
            "Results.",
            "We compare our enhanced approaches (SV000gg+W C and NC+W C) and lexicon only approach (W C-only), with the state-of-the-art and baseline threshold-based methods.",
            "For measuring performance, we use F-score and accuracy as well as G-score, the harmonic mean of accuracy and recall.",
            "G-score is the official metric of the CWI task of Semeval 2016.",
            "Table 6 shows that the wordcomplexity lexicon improves the performance of SV000gg and the nearest centroid classifier in all the three metrics.",
            "The improvements are statistically significant according to the paired bootstrap test with p < 0.01.",
            "The word-complexity lexicon alone (WC-only) performs satisfactorily on the CWIG3G2 dataset, which effectively is a simple table look-up approach with extreme time and space efficiency.",
            "For CWI SemEval 2016 dataset, WC-only approach gives the best accuracy and Fscore, though this can be attributed to the skewed distribution of dataset (only 5% of the test instances are 'complex'."
        ],
        "class_sentence": [
            2,
            1,
            2,
            2,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "WC-only",
                "NearestCentroid+WC",
                "SV000gg+WC"
            ],
            [
                "G-score",
                "F-score",
                "Accuracy"
            ],
            [
                "G-score"
            ],
            [
                "NearestCentroid+WC",
                "SV000gg+WC"
            ],
            null,
            [
                "WC-only",
                "CWIG3G2 2018"
            ],
            [
                "WC-only",
                "CWI SemEval 2016",
                "F-score",
                "Accuracy"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_6",
        "paper_id": "D18-1410",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1412table_2",
        "description": "PropBank SRL. We use the OntoNotes data from the CoNLL shared task in 2012 (Pradhan et al., 2013) for Propbank SRL. Table 2 reports results using gold predicates. Recent competitive systems for PropBank SRL follow the approach of Zhou and Xu (2015), employing deep architectures, and forgoing the use of any syntax. He et al. (2017) improve on those results, and in analysis experiments, show that constraints derived using syntax may further improve performance. Tan et al. (2018) employ a similar approach but use feed-forward networks with self-attention. He et al. (2018a) use a span-based classification to jointly identify and label argument spans. Our syntax-agnostic semi-CRF baseline model improves on prior work (excluding ELMo), showing again the value of global normalization in semantic structure prediction. We obtain further improvement of 0.8 absolute F1 with the best syntactic scaffold from the frame SRL task. This indicates that a syntactic inductive bias is beneficial even when using sophisticated neural architectures. He et al. (2018a) also provide a setup where initialization was done with deep contextualized embeddings, ELMo (Peters et al., 2018), resulting in 85.5 F1 on the OntoNotes test set. The improvements from ELMo are methodologically orthogonal to syntactic scaffolds.",
        "sentences": [
            "PropBank SRL.",
            "We use the OntoNotes data from the CoNLL shared task in 2012 (Pradhan et al., 2013) for Propbank SRL.",
            "Table 2 reports results using gold predicates.",
            "Recent competitive systems for PropBank SRL follow the approach of Zhou and Xu (2015), employing deep architectures, and forgoing the use of any syntax.",
            "He et al. (2017) improve on those results, and in analysis experiments, show that constraints derived using syntax may further improve performance.",
            "Tan et al. (2018) employ a similar approach but use feed-forward networks with self-attention.",
            "He et al. (2018a) use a span-based classification to jointly identify and label argument spans.",
            "Our syntax-agnostic semi-CRF baseline model improves on prior work (excluding ELMo), showing again the value of global normalization in semantic structure prediction.",
            "We obtain further improvement of 0.8 absolute F1 with the best syntactic scaffold from the frame SRL task.",
            "This indicates that a syntactic inductive bias is beneficial even when using sophisticated neural architectures.",
            "He et al. (2018a) also provide a setup where initialization was done with deep contextualized embeddings, ELMo (Peters et al., 2018), resulting in 85.5 F1 on the OntoNotes test set.",
            "The improvements from ELMo are methodologically orthogonal to syntactic scaffolds."
        ],
        "class_sentence": [
            2,
            2,
            1,
            2,
            2,
            2,
            2,
            2,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "Zhou and Xu (2015)"
            ],
            [
                "He et al. (2017)"
            ],
            [
                "Tan et al. (2018)"
            ],
            [
                "He et al. (2018a)"
            ],
            [
                "Semi-CRF baseline"
            ],
            [
                "Semi-CRF baseline",
                "+ common nonterminals",
                "F1"
            ],
            [
                "+ common nonterminals"
            ],
            [
                "He et al. (2018a)"
            ],
            null
        ],
        "n_sentence": 12.0,
        "table_id": "table_2",
        "paper_id": "D18-1412",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1414table_9",
        "description": "Table 9 further shows F-scores for the baseline and the both-retrained model relative to each role type in detail. Given that the F-scores for both models are equal to 0 on A3 and A4, we just omit this part. From the figure we can observe that, all the semantic roles achieve significant improvements in performances.",
        "sentences": [
            "Table 9 further shows F-scores for the baseline and the both-retrained model relative to each role type in detail.",
            "Given that the F-scores for both models are equal to 0 on A3 and A4, we just omit this part.",
            "From the figure we can observe that, all the semantic roles achieve significant improvements in performances."
        ],
        "class_sentence": [
            1,
            2,
            1
        ],
        "header_mention": [
            [
                "Baseline",
                "Both retrained"
            ],
            [
                "Baseline",
                "Both retrained"
            ],
            [
                "Both retrained"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_9",
        "paper_id": "D18-1414",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1415table_1",
        "description": "After obtaining the original system S1, we deploy it to interact with Sim1 and Sim2 respectively, under different LU error rates (Li et al., 2017a). In each condition, we simulate 3200 episodes to obtain the performance. Table 1 shows the details of the test performance. Table 2 shows the statistics of turns when S1 interacts with Sim2. As shown in Table 1, S1 achieves higher dialog success rate and rewards when testing with Sim1.",
        "sentences": [
            "After obtaining the original system S1, we deploy it to interact with Sim1 and Sim2 respectively, under different LU error rates (Li et al., 2017a).",
            "In each condition, we simulate 3200 episodes to obtain the performance.",
            "Table 1 shows the details of the test performance.",
            "Table 2 shows the statistics of turns when S1 interacts with Sim2.",
            "As shown in Table 1, S1 achieves higher dialog success rate and rewards when testing with Sim1."
        ],
        "class_sentence": [
            2,
            2,
            1,
            0,
            1
        ],
        "header_mention": [
            [
                "Sim1",
                "Sim2"
            ],
            null,
            null,
            null,
            [
                "Succ.",
                "Reward",
                "Sim1"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_1",
        "paper_id": "D18-1415",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1417table_1",
        "description": "4.4 Independent Learning. The results of separate training for slot filling and intent detection are reported in Table 1 and Table 2 respectively. On the independent slot filling task, we fixed the intent information as the ground truth labels in the dataset. But on the independent intent detection task, there is no interaction with slot labels. Table 1 compares F1-score of slot filling between our proposed architecture and some previous works. Our model achieves state-of-the-art results and outperforms previous best model by 0.56% in terms of F1-score. We attribute the improvement of our model to the following reasons: 1) The attention used in (Liu and Lane, 2016a) is vanilla attention, which is used to compute the decoding states. It is not suitable for our model since the embeddings are composed of several parts. Self-attention allows the model to attend to information jointly from different representation parts, so as to better understand the utterance. 2) intentaugmented gating layer connects the semantics of sequence slot labels, which captures complex interactions between the two tasks.",
        "sentences": [
            "4.4 Independent Learning.",
            "The results of separate training for slot filling and intent detection are reported in Table 1 and Table 2 respectively.",
            "On the independent slot filling task, we fixed the intent information as the ground truth labels in the dataset.",
            "But on the independent intent detection task, there is no interaction with slot labels.",
            "Table 1 compares F1-score of slot filling between our proposed architecture and some previous works.",
            "Our model achieves state-of-the-art results and outperforms previous best model by 0.56% in terms of F1-score.",
            "We attribute the improvement of our model to the following reasons: 1) The attention used in (Liu and Lane, 2016a) is vanilla attention, which is used to compute the decoding states.",
            "It is not suitable for our model since the embeddings are composed of several parts.",
            "Self-attention allows the model to attend to information jointly from different representation parts, so as to better understand the utterance.",
            "2) intentaugmented gating layer connects the semantics of sequence slot labels, which captures complex interactions between the two tasks."
        ],
        "class_sentence": [
            2,
            1,
            0,
            2,
            1,
            1,
            2,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            [
                "Our Model",
                "F1-score"
            ],
            [
                "Our Model",
                "F1-score"
            ],
            [
                "Attention BiRNN (Liu and Lane 2016a)"
            ],
            [
                "Our Model"
            ],
            [
                "Our Model"
            ],
            [
                "Our Model"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_1",
        "paper_id": "D18-1417",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1417table_4",
        "description": "Table 4 shows the joint learning performance of our model on ATIS data set by removing one module at a time. We find that all variants of our model perform well based on our gate mechanism. As listed in the table, all features contribute to both slot filling and intent classification task. If we remove the self-attention from the holistic model or just in the intent-augmented gating layer, the performance drops dramatically. The result can be interpreted that self-attention mechanism computes context representation separately and enhances the interaction of features in the same aspect. We can see that self-attention does improve performance a lot in a large scale, which is consistent with findings of previous work (Vaswani et al., 2017; Lin et al., 2017). If we remove character-level embeddings and only use word-level embeddings, we see 0.22% drop in terms of F1-score. Though word-level embeddings represent the semantics of each word, character-level embeddings can better handle the out-of-vocabulary (OOV) problem which is essential to determine the slot labels.",
        "sentences": [
            "Table 4 shows the joint learning performance of our model on ATIS data set by removing one module at a time.",
            "We find that all variants of our model perform well based on our gate mechanism.",
            "As listed in the table, all features contribute to both slot filling and intent classification task.",
            "If we remove the self-attention from the holistic model or just in the intent-augmented gating layer, the performance drops dramatically.",
            "The result can be interpreted that self-attention mechanism computes context representation separately and enhances the interaction of features in the same aspect.",
            "We can see that self-attention does improve performance a lot in a large scale, which is consistent with findings of previous work (Vaswani et al., 2017; Lin et al., 2017).",
            "If we remove character-level embeddings and only use word-level embeddings, we see 0.22% drop in terms of F1-score.",
            "Though word-level embeddings represent the semantics of each word, character-level embeddings can better handle the out-of-vocabulary (OOV) problem which is essential to determine the slot labels."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "W/O char-embedding",
                "W/O self-attention",
                "W/O attention-gating",
                "Full Model"
            ],
            null,
            [
                "W/O self-attention"
            ],
            [
                "W/O self-attention"
            ],
            [
                "W/O self-attention"
            ],
            [
                "Full Model",
                "W/O char-embedding",
                "F1-Score"
            ],
            [
                "W/O char-embedding"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_4",
        "paper_id": "D18-1417",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1418table_2",
        "description": "6.3 Model comparison. Our results for our proposed model and comparison with other models for permuted-bAbI dialog task are given in Table 2. Table 2 follows the same format as Table 1, except we show results for different models on permuted-bAbI dialog task. We show results for three models memN2N, memN2N + all-answers and our proposed model, Mask-memN2N. In the memN2N + all-answers model, we extend the baseline memN2N model and though not realistic, we provide information on all correct next utterances during training, instead of providing only one correct next utterance. The memN2N + all-answers model has an element-wise sigmoid at the output layer instead of a softmax, allowing it to predict multiple correct answers. This model serves as an important additional baseline, and clearly demonstrates the benefit of our proposed approach. From Table 2, we observe that the memN2N + all-answers model performs poorly, in comparison to the memN2N baseline model both in standard setup and OOV setting, as well as with and without match-type features. This shows that the existing methods do not improve the accuracy of a dialog system even if all correct next utterances are known and used during training the model. Our proposed model performs better than both the baseline models. In the standard setup, the perdialog accuracy increases from 22% to 32%. Using match-type features, the per-dialog accuracy increases considerably from 30.3% to 47.3%. In the OOV setting, all models perform poorly and achieve per-dialog accuracy of 0-1% both with and without match-type features. These results are similar to results for original-bAbI dialog Task 5 from Bordes and Weston (2016b) and our results with the baseline model. Overall, Mask-memN2N is able to handle multiple correct next utterances present in permutedbAbI dialog task better than the baseline models. This indicates that permuted-bAbI dialog task is a better and effective evaluation proxy compared to original-bAbI dialog task for real-world data. This also shows that we need better neural approaches, similar to our proposed model, Mask-memN2N, for goal-oriented dialog in addition to better testbeds for benchmarking goal-oriented dialogs systems.",
        "sentences": [
            "6.3 Model comparison.",
            "Our results for our proposed model and comparison with other models for permuted-bAbI dialog task are given in Table 2.",
            "Table 2 follows the same format as Table 1, except we show results for different models on permuted-bAbI dialog task.",
            "We show results for three models memN2N, memN2N + all-answers and our proposed model, Mask-memN2N.",
            "In the memN2N + all-answers model, we extend the baseline memN2N model and though not realistic, we provide information on all correct next utterances during training, instead of providing only one correct next utterance.",
            "The memN2N + all-answers model has an element-wise sigmoid at the output layer instead of a softmax, allowing it to predict multiple correct answers.",
            "This model serves as an important additional baseline, and clearly demonstrates the benefit of our proposed approach.",
            "From Table 2, we observe that the memN2N + all-answers model performs poorly, in comparison to the memN2N baseline model both in standard setup and OOV setting, as well as with and without match-type features.",
            "This shows that the existing methods do not improve the accuracy of a dialog system even if all correct next utterances are known and used during training the model.",
            "Our proposed model performs better than both the baseline models.",
            "In the standard setup, the perdialog accuracy increases from 22% to 32%.",
            "Using match-type features, the per-dialog accuracy increases considerably from 30.3% to 47.3%.",
            "In the OOV setting, all models perform poorly and achieve per-dialog accuracy of 0-1% both with and without match-type features.",
            "These results are similar to results for original-bAbI dialog Task 5 from Bordes and Weston (2016b) and our results with the baseline model.",
            "Overall, Mask-memN2N is able to handle multiple correct next utterances present in permutedbAbI dialog task better than the baseline models.",
            "This indicates that permuted-bAbI dialog task is a better and effective evaluation proxy compared to original-bAbI dialog task for real-world data.",
            "This also shows that we need better neural approaches, similar to our proposed model, Mask-memN2N, for goal-oriented dialog in addition to better testbeds for benchmarking goal-oriented dialogs systems."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            2,
            2,
            2,
            1,
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "Mask-memN2N"
            ],
            null,
            [
                "memN2N",
                "memN2N + all-answers",
                "Mask-memN2N"
            ],
            [
                "memN2N + all-answers"
            ],
            [
                "memN2N + all-answers"
            ],
            [
                "memN2N + all-answers"
            ],
            [
                "memN2N + all-answers",
                "memN2N"
            ],
            null,
            [
                "Mask-memN2N",
                "memN2N",
                "memN2N + all-answers"
            ],
            [
                "Mask-memN2N",
                "memN2N",
                "Per-dialog",
                "no match-type"
            ],
            [
                "Mask-memN2N",
                "memN2N",
                "+ match-type",
                "Per-dialog"
            ],
            [
                "OOV: memN2N",
                "OOV: memN2N + all-answers",
                "OOV: Mask-memN2N",
                "Per-dialog"
            ],
            [
                "OOV: Mask-memN2N"
            ],
            [
                "Mask-memN2N"
            ],
            null,
            [
                "Mask-memN2N"
            ]
        ],
        "n_sentence": 17.0,
        "table_id": "table_2",
        "paper_id": "D18-1418",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1418table_3",
        "description": "6.4 Ablation study. Here, we study the different parts of our model for better understanding of how the different parts influence the overall model performance. Our results for ablation study are given in Table 3. We show results for Mask-memN2N in various settings - a) without entropy, b) without pre-training mask c) reinforcement learning phase only. Adding entropy for the RL phase seems to have improved performance a bit by assisting better exploration in the RL phase. When we remove L2 mask pre-training, there is a huge drop in performance. In the RL phase, the action space is large. In the bAbI dialog task, which is a retrieval task, it is all the candidate answers that can be retrieved which forms the action set. L2 mask pre-training would help the RL phase to try more relevant actions from the very start. From Table 3 it is clear that the RL phase individually does not perform well; it is the combination of both the phases that gives the best performance. When we do only the RL phase, it might be very tough for the system to learn everything by trial and error, especially because the action space is so large. Preceding it with the SL phase and L2 mask pre-training would have put the system and its parameters at a good spot from which the RL phase can improve performance. Note that it would not be valid to check performance of the SL phase in the test set as the SL phase requires the actual answers for it to create the mask.",
        "sentences": [
            "6.4 Ablation study.",
            "Here, we study the different parts of our model for better understanding of how the different parts influence the overall model performance.",
            "Our results for ablation study are given in Table 3.",
            "We show results for Mask-memN2N in various settings - a) without entropy, b) without pre-training mask c) reinforcement learning phase only.",
            "Adding entropy for the RL phase seems to have improved performance a bit by assisting better exploration in the RL phase.",
            "When we remove L2 mask pre-training, there is a huge drop in performance.",
            "In the RL phase, the action space is large.",
            "In the bAbI dialog task, which is a retrieval task, it is all the candidate answers that can be retrieved which forms the action set.",
            "L2 mask pre-training would help the RL phase to try more relevant actions from the very start.",
            "From Table 3 it is clear that the RL phase individually does not perform well; it is the combination of both the phases that gives the best performance.",
            "When we do only the RL phase, it might be very tough for the system to learn everything by trial and error, especially because the action space is so large.",
            "Preceding it with the SL phase and L2 mask pre-training would have put the system and its parameters at a good spot from which the RL phase can improve performance.",
            "Note that it would not be valid to check performance of the SL phase in the test set as the SL phase requires the actual answers for it to create the mask."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            2,
            1,
            2,
            2,
            2,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "Mask-memN2N (w/o entropy)",
                "Mask-memN2N (w/o L2 mask pre-training)",
                "Mask-memN2N (Reinforcement learning phase only)"
            ],
            null,
            [
                "Mask-memN2N (w/o L2 mask pre-training)"
            ],
            null,
            null,
            null,
            [
                "Mask-memN2N",
                "Mask-memN2N (Reinforcement learning phase only)"
            ],
            [
                "Mask-memN2N (Reinforcement learning phase only)"
            ],
            null,
            null
        ],
        "n_sentence": 13.0,
        "table_id": "table_3",
        "paper_id": "D18-1418",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1421table_2",
        "description": "Automatic evaluation. Table 2 shows the performances of the models on Quora datasets. In both settings, we find that the proposed RbM-SL and RbM-IRL models outperform the baseline models in terms of all the evaluation measures. Particularly in Quora-II, RbM-SL and RbM-IRL make significant improvements over the baselines, which demonstrates their higher ability in learning for paraphrase generation. On Quora dataset, RbM-SL is constantly better than RbM-IRL for all the automatic measures, which is reasonable because RbM-SL makes use of additional labeled data to train the evaluator. Quora datasets contains a large number of high-quality non-paraphrases, i.e., they are literally similar but semantically different, for instance are analogue clocks better than digital and is analogue better than digital. Trained with the data, the evaluator tends to become more capable in paraphrase identification. With additional evaluation on Quora data, the evaluator used in RbM-SL can achieve an accuracy of 87% on identifying positive and negative pairs of paraphrases.",
        "sentences": [
            "Automatic evaluation.",
            "Table 2 shows the performances of the models on Quora datasets.",
            "In both settings, we find that the proposed RbM-SL and RbM-IRL models outperform the baseline models in terms of all the evaluation measures.",
            "Particularly in Quora-II, RbM-SL and RbM-IRL make significant improvements over the baselines, which demonstrates their higher ability in learning for paraphrase generation.",
            "On Quora dataset, RbM-SL is constantly better than RbM-IRL for all the automatic measures, which is reasonable because RbM-SL makes use of additional labeled data to train the evaluator.",
            "Quora datasets contains a large number of high-quality non-paraphrases, i.e., they are literally similar but semantically different, for instance are analogue clocks better than digital and is analogue better than digital.",
            "Trained with the data, the evaluator tends to become more capable in paraphrase identification.",
            "With additional evaluation on Quora data, the evaluator used in RbM-SL can achieve an accuracy of 87% on identifying positive and negative pairs of paraphrases."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "RbM-SL (ours)",
                "RbM-IRL (ours)"
            ],
            [
                "Quora-II",
                "RbM-SL (ours)",
                "RbM-IRL (ours)"
            ],
            [
                "RbM-SL (ours)",
                "RbM-IRL (ours)"
            ],
            null,
            null,
            [
                "RbM-SL (ours)"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_2",
        "paper_id": "D18-1421",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1421table_4",
        "description": "Table 4 demonstrates the average ratings for each model, including the ground-truth references. Our models of RbM-SL and RbM-IRL get better scores in terms of relevance and fluency than the baseline models, and their differences are statistically significant (paired t-test, p-value < 0.01). We note that in human evaluation, RbM-SL achieves the best relevance score while RbM-IRL achieves the best fluency score.",
        "sentences": [
            "Table 4 demonstrates the average ratings for each model, including the ground-truth references.",
            "Our models of RbM-SL and RbM-IRL get better scores in terms of relevance and fluency than the baseline models, and their differences are statistically significant (paired t-test, p-value < 0.01).",
            "We note that in human evaluation, RbM-SL achieves the best relevance score while RbM-IRL achieves the best fluency score."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "RbM-SL (ours)",
                "RbM-IRL (ours)",
                "Relevance",
                "Fluency"
            ],
            [
                "RbM-SL (ours)",
                "RbM-IRL (ours)",
                "Relevance",
                "Fluency"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_4",
        "paper_id": "D18-1421",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1424table_1",
        "description": "3.3 Evaluation We conduct automatic evaluation with metrics: BLEU 1, BLEU 2, BLEU 3, BLEU 4 (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and ROUGE-L (Lin, 2004), and use evaluation package released by (Sharma et al., 2017) to compute them. 4 Results and Analysis. 4.1 Comparison of Techniques. Table 1 shows evaluation results for different models on SQuAD Split1. Results with both sentence-level and paragraph-level inputs are included. Similar results also have been observed on SQuAD Split2. The definitions of the models under comparison are:. s2s: basic sequence to sequence model. s2s-a: s2s + attention mechanism. s2s-a-at: s2s-a + answer tagging. s2s-a-at-cp: s2s-a-at + copy mechanism. s2s-a-at-mp: s2s-a-at + maxout pointer mechanism. s2s-a-at-mp-gsa: s2s-a-at-mp + gated self-attention. Attention Mechanism. s2s-a vs. s2s: we can see attention brings in large improvement on both sentence and paragraph inputs. The lower performance on paragraph indicates the challenge of encoding paragraph-level information. Answer Tagging. s2s-a-at vs. s2s-a: answer tagging dramatically boosts the performance, which confirms the importance of answer-aware QG: to generate good question, we need to control/learn which part of the context the generated question is asking about. More importantly, answer tagging clearly reduces the gap between sentence and paragraph inputs, which could be explained with: by providing guidance on answer words, we can make the model learn to neglect noise when processing a long context. Copy Mechanism. s2s-a-at-cp vs. s2s-a-at: as expected, copy mechanism further improves the performance on the QG task. (Du et al., 2017) pointed out most of the sentence-question pairs in SQuAD have over 50% overlaps in non-stop-words. Our results prove that sequence to sequence models with copy mechanism can very well learn when to generate a word and when to copy one from input on such QG task. More interestingly, the performance is lower when paragraph is given as input than sentence as input. The gap, again, reveals the challenge of leveraging longer context. We found that, when paragraph is given, the model tends to generate more repetitive words, and those words (often entities/concepts) usually appear multiple times in the context, Figure 3. The repetition issue can also be seen for sentence input, but it is more severe for paragraph. Maxout Pointer. s2s-a-at-mp vs. s2s-a-at-cp: Maxout pointer is designed to resolve the repetition issue brought by the basic copy mechanism, for example Figure 3. The maxout pointer mechanism outperforms the basic copy mechanism in all metrics. Moreover, the effectiveness of maxout pointer is more significant when paragraph is given as the model input, as it reverses the performance gap between models trained with sentence and paragraph inputs, Table 1. Gated Self-attention. s2s-a-at-mp-gsa vs. s2s-a-at-mp: the results demonstrate the effectiveness of gated selfattention, in particular, when working with paragraph inputs. This is the first time, as we know, taking paragraph as input is better than sentence for neural QG tasks. The observation is consistent across all metrics. Gated self-attention helps refine encoded context by fusing important information with the context\u2019s self representation properly, especially when the context is long. To better understand how gated self-attention works, we visualize the self alignment vectors at each time step of the encoded sequence for one example, in Figure 5. This example corresponds to the example 1 in Figure 1. We can see the alignments distribution concentrates near the answer sequence and the most relevant context: \u201dthomas davis\u201d in this example. Such alignments in turn would help to promote the most valuable information for decoding.",
        "sentences": [
            "3.3 Evaluation We conduct automatic evaluation with metrics: BLEU 1, BLEU 2, BLEU 3, BLEU 4 (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and ROUGE-L (Lin, 2004), and use evaluation package released by (Sharma et al., 2017) to compute them.",
            "4 Results and Analysis.",
            "4.1 Comparison of Techniques.",
            "Table 1 shows evaluation results for different models on SQuAD Split1.",
            "Results with both sentence-level and paragraph-level inputs are included.",
            "Similar results also have been observed on SQuAD Split2.",
            "The definitions of the models under comparison are:.",
            "s2s: basic sequence to sequence model.",
            "s2s-a: s2s + attention mechanism.",
            "s2s-a-at: s2s-a + answer tagging.",
            "s2s-a-at-cp: s2s-a-at + copy mechanism.",
            "s2s-a-at-mp: s2s-a-at + maxout pointer mechanism.",
            "s2s-a-at-mp-gsa: s2s-a-at-mp + gated self-attention.",
            "Attention Mechanism.",
            "s2s-a vs. s2s: we can see attention brings in large improvement on both sentence and paragraph inputs.",
            "The lower performance on paragraph indicates the challenge of encoding paragraph-level information.",
            "Answer Tagging.",
            "s2s-a-at vs. s2s-a: answer tagging dramatically boosts the performance, which confirms the importance of answer-aware QG: to generate good question, we need to control/learn which part of the context the generated question is asking about.",
            "More importantly, answer tagging clearly reduces the gap between sentence and paragraph inputs, which could be explained with: by providing guidance on answer words, we can make the model learn to neglect noise when processing a long context.",
            "Copy Mechanism.",
            "s2s-a-at-cp vs. s2s-a-at: as expected, copy mechanism further improves the performance on the QG task.",
            "(Du et al., 2017) pointed out most of the sentence-question pairs in SQuAD have over 50% overlaps in non-stop-words.",
            "Our results prove that sequence to sequence models with copy mechanism can very well learn when to generate a word and when to copy one from input on such QG task.",
            "More interestingly, the performance is lower when paragraph is given as input than sentence as input.",
            "The gap, again, reveals the challenge of leveraging longer context.",
            "We found that, when paragraph is given, the model tends to generate more repetitive words, and those words (often entities/concepts) usually appear multiple times in the context, Figure 3.",
            "The repetition issue can also be seen for sentence input, but it is more severe for paragraph.",
            "Maxout Pointer.",
            "s2s-a-at-mp vs. s2s-a-at-cp: Maxout pointer is designed to resolve the repetition issue brought by the basic copy mechanism, for example Figure 3.",
            "The maxout pointer mechanism outperforms the basic copy mechanism in all metrics.",
            "Moreover, the effectiveness of maxout pointer is more significant when paragraph is given as the model input, as it reverses the performance gap between models trained with sentence and paragraph inputs, Table 1.",
            "Gated Self-attention.",
            "s2s-a-at-mp-gsa vs. s2s-a-at-mp: the results demonstrate the effectiveness of gated selfattention, in particular, when working with paragraph inputs.",
            "This is the first time, as we know, taking paragraph as input is better than sentence for neural QG tasks.",
            "The observation is consistent across all metrics.",
            "Gated self-attention helps refine encoded context by fusing important information with the context\u2019s self representation properly, especially when the context is long.",
            "To better understand how gated self-attention works, we visualize the self alignment vectors at each time step of the encoded sequence for one example, in Figure 5.",
            "This example corresponds to the example 1 in Figure 1.",
            "We can see the alignments distribution concentrates near the answer sequence and the most relevant context: \u201dthomas davis\u201d in this example.",
            "Such alignments in turn would help to promote the most valuable information for decoding."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            1,
            2,
            2,
            1,
            2,
            2,
            1,
            2,
            2,
            1,
            2,
            2,
            2,
            2,
            2,
            1,
            1,
            2,
            1,
            2,
            1,
            2,
            2,
            0,
            0,
            0
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            [
                "s2s"
            ],
            [
                "s2s-a"
            ],
            [
                "s2s-a-at"
            ],
            [
                "s2s-a-at-cp"
            ],
            [
                "s2s-a-at-mcp"
            ],
            [
                "s2s-a-at-mcp-gsa"
            ],
            null,
            [
                "s2s",
                "s2s-a",
                "Sen.",
                "Par."
            ],
            [
                "Par."
            ],
            null,
            [
                "s2s-a",
                "s2s-a-at"
            ],
            [
                "Sen.",
                "Par.",
                "s2s-a-at"
            ],
            null,
            [
                "s2s-a-at",
                "s2s-a-at-cp"
            ],
            null,
            [
                "s2s-a-at-cp"
            ],
            [
                "s2s-a-at-cp",
                "Par.",
                "Sen."
            ],
            null,
            null,
            null,
            null,
            null,
            [
                "s2s-a-at-cp",
                "s2s-a-at-mcp"
            ],
            [
                "s2s-a-at-mcp",
                "Sen.",
                "Par."
            ],
            null,
            [
                "s2s-a-at-mcp-gsa",
                "Par."
            ],
            null,
            null,
            null,
            null,
            null,
            null,
            null
        ],
        "n_sentence": 40.0,
        "table_id": "table_1",
        "paper_id": "D18-1424",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1429table_8",
        "description": "The results of our experiments are summarized in Table 8 - 10. The first column for each table shows the manner in which the noisy training data was created. The second column shows the BLEU4 score of the noisy questions when compared to the original reference questions (thus it tells us the perceived quality of these questions under the BLEU4 metric). We consider BLEU4 because of all the current metrics used for AQG it is the most popular. Similarly, the third column tells us the perceived quality of these questions under the Q-BLEU4 metric. Ideally, we would want that the performance of the model should correlate better with the perceived quality of the training questions as identified by a given metric. We observe that the general trend is better w.r.t. the Q-BLEU4 metric than the BLEU4 metric (i.e., in general, higher Q-BLEU4 indicates better performance and lower Q-BLEU4 indicates poor performance).  In particular, notice that BLEU4 gives much importance to stop words, but these words hardly have any influence on the final performance. We believe that such an extrinsic evaluation should also be used while designing better metrics and it would help us get better insights.",
        "sentences": [
            "The results of our experiments are summarized in Table 8 - 10.",
            "The first column for each table shows the manner in which the noisy training data was created.",
            "The second column shows the BLEU4 score of the noisy questions when compared to the original reference questions (thus it tells us the perceived quality of these questions under the BLEU4 metric).",
            "We consider BLEU4 because of all the current metrics used for AQG it is the most popular.",
            "Similarly, the third column tells us the perceived quality of these questions under the Q-BLEU4 metric.",
            "Ideally, we would want that the performance of the model should correlate better with the perceived quality of the training questions as identified by a given metric.",
            "We observe that the general trend is better w.r.t. the Q-BLEU4 metric than the BLEU4 metric (i.e., in general, higher Q-BLEU4 indicates better performance and lower Q-BLEU4 indicates poor performance).",
            "In particular, notice that BLEU4 gives much importance to stop words, but these words hardly have any influence on the final performance.",
            "We believe that such an extrinsic evaluation should also be used while designing better metrics and it would help us get better insights."
        ],
        "class_sentence": [
            "1",
            "1",
            "1",
            "2",
            "1",
            "2",
            "1",
            "2",
            "2"
        ],
        "header_mention": [
            null,
            [
                "None",
                "Stop Words",
                "Question Type",
                "Content Words",
                "Named Entity"
            ],
            [
                "BLEU"
            ],
            [
                "BLEU"
            ],
            [
                "QBLEU"
            ],
            null,
            [
                "BLEU",
                "QBLEU"
            ],
            null,
            null
        ],
        "n_sentence": 9.0,
        "table_id": "table_8",
        "paper_id": "D18-1429",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1434table_3",
        "description": "5.2 Baseline and State-of-the-Art. The comparison of our method with various baselines and state-of-the-art methods is provided in table 2 for VQA 1.0 and table 3 for VQG-COCO dataset. The comparable baselines for our method are the image based and caption based models in which we use either only the image or the caption embedding and generate the question. In both the tables, the first block consists of the current stateof-the-art methods on that dataset and the second contains the baselines. We observe that for the VQA dataset we achieve an improvement of 8% in BLEU and 7% in METEOR metric scores over the baselines, whereas for VQG-COCO dataset this is 15% for both the metrics. We improve over the previous state-of-the-art (Yang et al., 2015) for VQA dataset by around 6% in BLEU score and 10% in METEOR score. In the VQG-COCO dataset, we improve over (Mostafazadeh et al., 2016) by 3.7% and (Jain et al., 2017) by 3.5% in terms of METEOR scores.",
        "sentences": [
            "5.2 Baseline and State-of-the-Art.",
            "The comparison of our method with various baselines and state-of-the-art methods is provided in table 2 for VQA 1.0 and table 3 for VQG-COCO dataset.",
            "The comparable baselines for our method are the image based and caption based models in which we use either only the image or the caption embedding and generate the question.",
            "In both the tables, the first block consists of the current stateof-the-art methods on that dataset and the second contains the baselines.",
            "We observe that for the VQA dataset we achieve an improvement of 8% in BLEU and 7% in METEOR metric scores over the baselines, whereas for VQG-COCO dataset this is 15% for both the metrics.",
            "We improve over the previous state-of-the-art (Yang et al., 2015) for VQA dataset by around 6% in BLEU score and 10% in METEOR score.",
            "In the VQG-COCO dataset, we improve over (Mostafazadeh et al., 2016) by 3.7% and (Jain et al., 2017) by 3.5% in terms of METEOR scores."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            1,
            0,
            1
        ],
        "header_mention": [
            null,
            [
                "Natural 2016",
                "Creative 2017",
                "Image Only",
                "Caption Only",
                "Tag-Hadamard",
                "Place CNN-Joint",
                "Diff.Image-Joint",
                "MDN-Joint (Ours)",
                "Humans 2016"
            ],
            [
                "Image Only",
                "Caption Only"
            ],
            [
                "Natural 2016",
                "Creative 2017",
                "Image Only",
                "Caption Only"
            ],
            [
                "MDN-Joint (Ours)",
                "Image Only",
                "Caption Only",
                "BLEU1",
                "METEOR"
            ],
            null,
            [
                "MDN-Joint (Ours)",
                "Natural 2016",
                "Creative 2017",
                "METEOR"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_3",
        "paper_id": "D18-1434",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1435table_2",
        "description": "Table 2 shows the performances of template generator based on coarse-grained and finegrained type respectively, and Figure 5 shows an example of the template generated. Coarse templates are the ones after we replace names with these coarse-grained types. Entity Linking classifies names into more fine-grained types, so the corresponding templates are fine templates. The generalization method of replacing the named entities with entity types can reduce the vocabulary size significantly, which reduces the impact of sparse named entities in training data. The template generation achieves close performance with stateof-the-art generic image captioning on MSCOCO dataset (Xu et al., 2015). The template generator based on coarse-grained entity type outperforms the one based on fine-grained entity type for two reasons: (1) fine template relies on EDL, and incorrect linkings import noise; (2) named entities usually has multiple types, but we only choose one during generalization. Thus the caption, \u2018Bob Dylan performs at the Wiltern Theatre in Los Angeles\u2019, is generalized into \u2018<Writer> performs at the <Theater> in <Loaction>\u2019, but the correct type for Bob Dylan in this context should be Artist.",
        "sentences": [
            "Table 2 shows the performances of template generator based on coarse-grained and finegrained type respectively, and Figure 5 shows an example of the template generated.",
            "Coarse templates are the ones after we replace names with these coarse-grained types.",
            "Entity Linking classifies names into more fine-grained types, so the corresponding templates are fine templates.",
            "The generalization method of replacing the named entities with entity types can reduce the vocabulary size significantly, which reduces the impact of sparse named entities in training data.",
            "The template generation achieves close performance with stateof-the-art generic image captioning on MSCOCO dataset (Xu et al., 2015).",
            "The template generator based on coarse-grained entity type outperforms the one based on fine-grained entity type for two reasons: (1) fine template relies on EDL, and incorrect linkings import noise; (2) named entities usually has multiple types, but we only choose one during generalization.",
            "Thus the caption, \u2018Bob Dylan performs at the Wiltern Theatre in Los Angeles\u2019, is generalized into \u2018<Writer> performs at the <Theater> in <Loaction>\u2019, but the correct type for Bob Dylan in this context should be Artist."
        ],
        "class_sentence": [
            1,
            2,
            2,
            2,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Coarse Template"
            ],
            [
                "Fine Template"
            ],
            [
                "Vocabulary Size"
            ],
            null,
            [
                "Coarse Template",
                "Fine Template"
            ],
            null
        ],
        "n_sentence": 7.0,
        "table_id": "table_2",
        "paper_id": "D18-1435",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1438table_4",
        "description": "We use the 1-image and 2-image random selected image summaries as the baselines which we compare our models with. The top 1 or 2 images ranked by our model are selected out to form the summaries. Results in Table 4 show that HNNattTI outperforms the random baseline, while HNNattTC and HNNattTIC perform worse. This implies that attending images can generate better sentence-image alignment in the multimodal summaries than the model attending captions does. And this can also partly explain why our summarization model attending images when decoding text summaries than the one attending captions does. To show the influence of our OOV replacement mechanism, we eliminate the mechanism from our models, and show the evaluation results in Table 4 and Table 5. We can see from the two tables that the scores are lower than the corresponding scores in Table 2 and Table 3. Our OOV replacement mechanism improves the summarization models, though the mechanism is relatively simple.",
        "sentences": [
            "We use the 1-image and 2-image random selected image summaries as the baselines which we compare our models with.",
            "The top 1 or 2 images ranked by our model are selected out to form the summaries.",
            "Results in Table 4 show that HNNattTI outperforms the random baseline, while HNNattTC and HNNattTIC perform worse.",
            "This implies that attending images can generate better sentence-image alignment in the multimodal summaries than the model attending captions does.",
            "And this can also partly explain why our summarization model attending images when decoding text summaries than the one attending captions does.",
            "To show the influence of our OOV replacement mechanism, we eliminate the mechanism from our models, and show the evaluation results in Table 4 and Table 5.",
            "We can see from the two tables that the scores are lower than the corresponding scores in Table 2 and Table 3.",
            "Our OOV replacement mechanism improves the summarization models, though the mechanism is relatively simple."
        ],
        "class_sentence": [
            2,
            2,
            1,
            2,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "HNNattTI-3-OOV",
                "HNNattTC-3-OOV",
                "HNNTattTIC-3-OOV"
            ],
            null,
            null,
            [
                "HNNattTI-3-OOV",
                "HNNattTC-3-OOV",
                "HNNTattTIC-3-OOV",
                "HNNattT-3-OOV"
            ],
            [
                "HNNattTI-3-OOV",
                "HNNattTC-3-OOV",
                "HNNTattTIC-3-OOV",
                "HNNattT-3-OOV"
            ],
            null
        ],
        "n_sentence": 8.0,
        "table_id": "table_4",
        "paper_id": "D18-1438",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1442table_1",
        "description": "6 Experiment analysis. Table 1 shows the performance comparison of our model with other baselines on the DailyMail dataset with respect to Rouge score at 75 bytes and 275 bytes of summary length. Our model performs consistently and significantly better than other models on 75 bytes, while on 275 bytes, the improvement margin is smaller. One possible interpretation is that our model has high precision on top rank outputs, but the accuracy is lower for lower rank sentences. In addition, (Cheng and Lapata, 2016) used additional supervised training to create sentence-level extractive labels to train their model, while our model uses an unsupervised greedy approximation instead.",
        "sentences": [
            "6 Experiment analysis.",
            "Table 1 shows the performance comparison of our model with other baselines on the DailyMail dataset with respect to Rouge score at 75 bytes and 275 bytes of summary length.",
            "Our model performs consistently and significantly better than other models on 75 bytes, while on 275 bytes, the improvement margin is smaller.",
            "One possible interpretation is that our model has high precision on top rank outputs, but the accuracy is lower for lower rank sentences.",
            "In addition, (Cheng and Lapata, 2016) used additional supervised training to create sentence-level extractive labels to train their model, while our model uses an unsupervised greedy approximation instead."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "Lead-3",
                "LReg(500)",
                "Cheng et.al 16",
                "SummaRuNNer",
                "REFRESH",
                "Hybrid MemNet",
                "ITS",
                "b75",
                "b275"
            ],
            [
                "ITS",
                "b75",
                "b275"
            ],
            [
                "ITS"
            ],
            [
                "ITS"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_1",
        "paper_id": "D18-1442",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1442table_5",
        "description": "Human Evaluation:. We gave human evaluators three system-generated summaries, generated by Lead-3, Hybrid MemNet, ITS, as well as the human-written gold standard summary, and asked them to rank these summaries based on summary informativeness and coherence. Table 5 shows the percentages of summaries of different models under each rank scored by human experts. It is not surprising that gold standard has the most summaries of the highest quality. Our model has the most summaries under 2nd rank, thus can be considered 2nd best, following are Hybrid MemNet and Lead-3, as they are ranked mostly 3rd and 4th. By case study, we found that a number of summaries generated by Hybrid MemNet have two sentences the same as ITS out of three, however, the third distinct sentence from our model always leads to a better evaluation result considering overall informativeness and coherence.",
        "sentences": [
            "Human Evaluation:.",
            "We gave human evaluators three system-generated summaries, generated by Lead-3, Hybrid MemNet, ITS, as well as the human-written gold standard summary, and asked them to rank these summaries based on summary informativeness and coherence.",
            "Table 5 shows the percentages of summaries of different models under each rank scored by human experts.",
            "It is not surprising that gold standard has the most summaries of the highest quality.",
            "Our model has the most summaries under 2nd rank, thus can be considered 2nd best, following are Hybrid MemNet and Lead-3, as they are ranked mostly 3rd and 4th.",
            "By case study, we found that a number of summaries generated by Hybrid MemNet have two sentences the same as ITS out of three, however, the third distinct sentence from our model always leads to a better evaluation result considering overall informativeness and coherence."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Lead-3",
                "Hybrid MemNet",
                "ITS",
                "Gold"
            ],
            [
                "Lead-3",
                "Hybrid MemNet",
                "ITS",
                "Gold"
            ],
            [
                "Gold",
                "1st"
            ],
            [
                "ITS",
                "2nd",
                "Lead-3",
                "Hybrid MemNet",
                "3rd",
                "4th"
            ],
            [
                "Hybrid MemNet",
                "ITS"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_5",
        "paper_id": "D18-1442",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1443table_2",
        "description": "Table 2 shows experiments with the same systems on the NYT corpus. We see that the 2 point improvement compared to the baseline PointerGenerator maximum-likelihood approach carries over to this dataset. Here, the model outperforms the RL based model by Paulus et al. (2017) in ROUGE-1 and 2, but not L, and is comparable to the results of (Celikyilmaz et al., 2018) except for ROUGE-L. The same can be observed when comparing ML and our Pointer-Generator. We suspect that a difference in summary lengths due to our inference parameter choices leads to this difference, but did not have access to their models or summaries to investigate this claim. This shows that a bottom-up approach achieves competitive results even to models that are trained on summary-specific objectives.",
        "sentences": [
            "Table 2 shows experiments with the same systems on the NYT corpus.",
            "We see that the 2 point improvement compared to the baseline PointerGenerator maximum-likelihood approach carries over to this dataset.",
            "Here, the model outperforms the RL based model by Paulus et al. (2017) in ROUGE-1 and 2, but not L, and is comparable to the results of (Celikyilmaz et al., 2018) except for ROUGE-L.",
            "The same can be observed when comparing ML and our Pointer-Generator.",
            "We suspect that a difference in summary lengths due to our inference parameter choices leads to this difference, but did not have access to their models or summaries to investigate this claim.",
            "This shows that a bottom-up approach achieves competitive results even to models that are trained on summary-specific objectives."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "Bottom-Up Summarization",
                "Point.Gen. + Coverage Pen."
            ],
            [
                "Bottom-Up Summarization",
                "ML*",
                "ML+RL*",
                "DCA\u2020",
                "R-1",
                "R-2",
                "R-L"
            ],
            [
                "Point.Gen. + Coverage Pen.",
                "ML*",
                "R-1",
                "R-2",
                "R-L"
            ],
            null,
            [
                "Bottom-Up Summarization"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_2",
        "paper_id": "D18-1443",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1445table_3",
        "description": "Table 3 shows the performance of RL and ILP on the DUC\u201904 dataset. TD(\u03bb) significantly outperforms LSTD(\u03bb) in terms of all ROUGE scores we consider. Although the least-square RL algorithms (which LSTD belongs to) have been proved to achieve better performance than standard TD methods in large-scale problems (see Lagoudakis and Parr, 2003), their performance is sensitive to many factors, e.g., initialisation values in the diagonal matrix, regularisation parameters, etc. We note that a similar observation about the inferior performance of least-square RL in EMDS is reported by Rioux et al. (2014). TD(\u03bb) also significantly outperforms ILP in terms of all metrics except ROUGE-2. This is not surprising, because the bigram-based ILP is optimised for ROUGE-2, whereas our reward function U\u2217 considers other metrics as well (see Eq. (6)). Since ILP is widely used as a strong baseline for EMDS, these results confirm the advantage of using RL for EMDS problems.",
        "sentences": [
            "Table 3 shows the performance of RL and ILP on the DUC\u201904 dataset.",
            "TD(\u03bb) significantly outperforms LSTD(\u03bb) in terms of all ROUGE scores we consider.",
            "Although the least-square RL algorithms (which LSTD belongs to) have been proved to achieve better performance than standard TD methods in large-scale problems (see Lagoudakis and Parr, 2003), their performance is sensitive to many factors, e.g., initialisation values in the diagonal matrix, regularisation parameters, etc.",
            "We note that a similar observation about the inferior performance of least-square RL in EMDS is reported by Rioux et al. (2014).",
            "TD(\u03bb) also significantly outperforms ILP in terms of all metrics except ROUGE-2.",
            "This is not surprising, because the bigram-based ILP is optimised for ROUGE-2, whereas our reward function U\u2217 considers other metrics as well (see Eq. (6)).",
            "Since ILP is widely used as a strong baseline for EMDS, these results confirm the advantage of using RL for EMDS problems."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            1,
            2,
            2
        ],
        "header_mention": [
            [
                "TD(\u03bb)",
                "LSTD(\u03bb)",
                "ILP"
            ],
            [
                "TD(\u03bb)",
                "LSTD(\u03bb)",
                "R1",
                "R2",
                "RL",
                "RSU4"
            ],
            [
                "TD(\u03bb)",
                "LSTD(\u03bb)"
            ],
            [
                "LSTD(\u03bb)"
            ],
            [
                "TD(\u03bb)",
                "ILP",
                "R1",
                "RL",
                "RSU4"
            ],
            [
                "ILP",
                "R2"
            ],
            [
                "ILP",
                "TD(\u03bb)",
                "LSTD(\u03bb)"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_3",
        "paper_id": "D18-1445",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1447table_4",
        "description": "The experimental results are shown in Table 4 which indicate that: 1) though trained on scientific papers, our models still have the ability to generate keyphrases for news articles, illustrating that our models have learned some universal features between the two domains; and 2) semi-supervised learning by leveraging unlabeled data improves the generation performances more, indicating that our proposed method is reasonably effective when being tested on cross-domain data. Though unsupervised methods are still superior, for future work, we can leverage unlabeled out-of-domain corpora to improve cross-domain keyphrase generation performance, which could be a promising direction for domain adaption or transfer learning.",
        "sentences": [
            "The experimental results are shown in Table 4 which indicate that: 1) though trained on scientific papers, our models still have the ability to generate keyphrases for news articles, illustrating that our models have learned some universal features between the two domains; and 2) semi-supervised learning by leveraging unlabeled data improves the generation performances more, indicating that our proposed method is reasonably effective when being tested on cross-domain data.",
            "Though unsupervised methods are still superior, for future work, we can leverage unlabeled out-of-domain corpora to improve cross-domain keyphrase generation performance, which could be a promising direction for domain adaption or transfer learning."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "Our Models"
            ],
            [
                "Unsupervised",
                "F1"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_4",
        "paper_id": "D18-1447",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1454table_2",
        "description": "5.1 Main Experiment. The results of our model on both NarrativeQA and WikiHop with and without commonsense incorporation are shown in Table 2 and Table 3. We see empirically that our model outperforms all generative models on NarrativeQA, and is competitive with the top span prediction models. Furthermore, with the NOIC commonsense integration, we were able to further improve performance (p < 0.001 on all metrics6), establishing a new state-of-the-art for the task. We also see that our model performs well on WikiHop,7 and is further improved via the addition of commonsense (p < 0.001), demonstrating the generalizability of both our model and commonsense addition techniques.8.",
        "sentences": [
            "5.1 Main Experiment.",
            "The results of our model on both NarrativeQA and WikiHop with and without commonsense incorporation are shown in Table 2 and Table 3.",
            "We see empirically that our model outperforms all generative models on NarrativeQA, and is competitive with the top span prediction models.",
            "Furthermore, with the NOIC commonsense integration, we were able to further improve performance (p < 0.001 on all metrics6), establishing a new state-of-the-art for the task.",
            "We also see that our model performs well on WikiHop,7 and is further improved via the addition of commonsense (p < 0.001), demonstrating the generalizability of both our model and commonsense addition techniques.8."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "MHPGM",
                "MHPGM+ NOIC"
            ],
            [
                "MHPGM",
                "Seq2Seq (Kocisky et al. 2018)",
                "ASR (Kocisky et al. 2018)",
                "BiDAF (Kocisky et al. 2018)",
                "BiAttn + MRU-LSTM (Tay et al. 2018)"
            ],
            [
                "MHPGM+ NOIC"
            ],
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_2",
        "paper_id": "D18-1454",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1462table_2",
        "description": "4.5 Experimental Results. Table 2 shows the results of automatic evaluation. The proposed model performs the best according to BLEU. In particular, the differences between the existing state-of-the-art models are within 0.07, while the proposed model supersedes the best of them by 0.13.",
        "sentences": [
            "4.5 Experimental Results.",
            "Table 2 shows the results of automatic evaluation.",
            "The proposed model performs the best according to BLEU.",
            "In particular, the differences between the existing state-of-the-art models are within 0.07, while the proposed model supersedes the best of them by 0.13."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Proposed Model",
                "BLEU"
            ],
            [
                "EE-Seq2Seq",
                "DE-Seq2Seq",
                "GE-Seq2Seq",
                "Proposed Model",
                "BLEU"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "D18-1462",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1462table_6",
        "description": "Table 6 shows the human evaluation results. The slight improvement with the skeleton extraction module in BLEU reflecs as the decreases in both fluency and coherence. It suggests the necessity of human evaluation. The decreased results can be explained by the fact that the style of the dataset for pre-training the skeleton extraction module is very different from the narrative story dataset. While it may help extract some useful skeletons, it is likely that many of them are not suitable for learning the dependency of sentences. Finally, when the skeleton extraction module is trained on the target domain using reinforcement learning, the human evaluation is improved significantly by 14% on G-score.",
        "sentences": [
            "Table 6 shows the human evaluation results.",
            "The slight improvement with the skeleton extraction module in BLEU reflecs as the decreases in both fluency and coherence.",
            "It suggests the necessity of human evaluation.",
            "The decreased results can be explained by the fact that the style of the dataset for pre-training the skeleton extraction module is very different from the narrative story dataset.",
            "While it may help extract some useful skeletons, it is likely that many of them are not suitable for learning the dependency of sentences.",
            "Finally, when the skeleton extraction module is trained on the target domain using reinforcement learning, the human evaluation is improved significantly by 14% on G-score."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "Fluency",
                "Coherence",
                "G-Score",
                "+Skeleton Extraction Module"
            ],
            [
                "+Skeleton Extraction Module"
            ],
            [
                "+Skeleton Extraction Module"
            ],
            null,
            [
                "G-Score",
                "+Reinforcement Learning"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_6",
        "paper_id": "D18-1462",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1463table_1",
        "description": "Table 1 reports the embedding scores on both datasets. NEXUS network significantly outperforms the best baseline model in most cases. Notably, NEXUS can absorb the advantages from both NEXUS-H and NEXUS-F. The history and future information seem to help the model from different perspectives. Taking into account both of them does not create a conflict and the combination leads to an overall improvement. RL performs rather poorly on this metric, which is understandable as it does not target the ground-truth responses during training (Li et al., 2016c).",
        "sentences": [
            "Table 1 reports the embedding scores on both datasets.",
            "NEXUS network significantly outperforms the best baseline model in most cases.",
            "Notably, NEXUS can absorb the advantages from both NEXUS-H and NEXUS-F.",
            "The history and future information seem to help the model from different perspectives.",
            "Taking into account both of them does not create a conflict and the combination leads to an overall improvement.",
            "RL performs rather poorly on this metric, which is understandable as it does not target the ground-truth responses during training (Li et al., 2016c)."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            2,
            2
        ],
        "header_mention": [
            [
                "DailyDialog",
                "Twitter"
            ],
            [
                "NEXUS"
            ],
            [
                "NEXUS-H",
                "NEXUS-F",
                "NEXUS"
            ],
            null,
            [
                "NEXUS"
            ],
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_1",
        "paper_id": "D18-1463",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1463table_2",
        "description": "BLEU Score. BLEU is a popular metric that measures the geometric mean of the modified ngram precision with a length penalty (Papineni et al., 2002). Table 2 reports the BLEU 1-3 scores. Compared with embedding-based metrics, the BLEU score quantifies the word-overlap between generated responses and the ground-truth. One challenge of evaluating dialogue generation by BLEU score is the difficulty of accessing multiple references for the one-to-many alignment relation. Following Sordoni et al. (2015); Zhao et al. (2017); Shen et al. (2018), for each context, 10 more candidate references are acquired by using information retrieval methods (see Appendix A.4 for more details). All candidates are then passed to human annotators to filter unsuitable ones, resulting in 6.74 and 5.13 references for DailyDialog and Twitter dataset respectively. The human annotation is costly, so we evaluate it on 1000 sampled test cases for each dataset. As the BLEU score is not the simple mean of individual sentence scores, we compute the 95% significance interval by bootstrap resampling (Koehn, 2004; Riezler and Maxwell, 2005). As can be seen, NEXUS network achieves best or near-best performances with only greedy decoders. NEXUS-H generally outperforms NEXUS-F as the connection with future context is not explicitly addressed by the BLEU score metric. MMI and VHRED bring minor improvements over the seq2seq model. Even when evaluated on multiple references, RL still performs worse than most models.",
        "sentences": [
            "BLEU Score.",
            "BLEU is a popular metric that measures the geometric mean of the modified ngram precision with a length penalty (Papineni et al., 2002).",
            "Table 2 reports the BLEU 1-3 scores.",
            "Compared with embedding-based metrics, the BLEU score quantifies the word-overlap between generated responses and the ground-truth.",
            "One challenge of evaluating dialogue generation by BLEU score is the difficulty of accessing multiple references for the one-to-many alignment relation.",
            "Following Sordoni et al. (2015); Zhao et al. (2017); Shen et al. (2018), for each context, 10 more candidate references are acquired by using information retrieval methods (see Appendix A.4 for more details).",
            "All candidates are then passed to human annotators to filter unsuitable ones, resulting in 6.74 and 5.13 references for DailyDialog and Twitter dataset respectively.",
            "The human annotation is costly, so we evaluate it on 1000 sampled test cases for each dataset.",
            "As the BLEU score is not the simple mean of individual sentence scores, we compute the 95% significance interval by bootstrap resampling (Koehn, 2004; Riezler and Maxwell, 2005).",
            "As can be seen, NEXUS network achieves best or near-best performances with only greedy decoders.",
            "NEXUS-H generally outperforms NEXUS-F as the connection with future context is not explicitly addressed by the BLEU score metric.",
            "MMI and VHRED bring minor improvements over the seq2seq model.",
            "Even when evaluated on multiple references, RL still performs worse than most models."
        ],
        "class_sentence": [
            2,
            2,
            1,
            2,
            2,
            2,
            2,
            2,
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "BLEU-1",
                "BLEU-2",
                "BLEU-3"
            ],
            [
                "BLEU-1",
                "BLEU-2",
                "BLEU-3"
            ],
            null,
            null,
            [
                "DailyDialog",
                "Twitter"
            ],
            [
                "DailyDialog",
                "Twitter"
            ],
            null,
            [
                "NEXUS"
            ],
            [
                "NEXUS-H",
                "NEXUS-F"
            ],
            [
                "Greedy",
                "Beam",
                "MMI",
                "VHRED"
            ],
            [
                "RL"
            ]
        ],
        "n_sentence": 13.0,
        "table_id": "table_2",
        "paper_id": "D18-1463",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1464table_1",
        "description": "Results. Table 1 summarizes the results of different systems for the readability assessment task. CohEmb significantly outperforms the graph-based coherence model proposed by Mesgar and Strube (2016) by a large margin (6%), showing that our model captures coherence better than their model. In our model, the CNN layer automatically learns which connections are important to be considered for coherence patterns, whereas this is performed in Mesgar and Strube (2016) by defining a threshold for eliminating connections. CohLSTM significantly outperforms both the coherence model proposed by Mesgar and Strube (2016) and the CohEmb model by 11% and 5%, respectively, and defines a new state-of-the-art on this dataset. CohLSTM, unlike Mesgar and Strube (2016)\u00e2\u20ac\u2122s model and CohEmb, considers words of sentences in their sentence context. This supports our intuition that actual context information of words contributes to the perceived coherence of texts. CohLSTM, which captures exclusively local coherence, even outperforms the readability system proposed by De Clercq and Hoste (2016), which relies on a wide range of lexical, syntactic and semantic features.",
        "sentences": [
            "Results.",
            "Table 1 summarizes the results of different systems for the readability assessment task.",
            "CohEmb significantly outperforms the graph-based coherence model proposed by Mesgar and Strube (2016) by a large margin (6%), showing that our model captures coherence better than their model.",
            "In our model, the CNN layer automatically learns which connections are important to be considered for coherence patterns, whereas this is performed in Mesgar and Strube (2016) by defining a threshold for eliminating connections.",
            "CohLSTM significantly outperforms both the coherence model proposed by Mesgar and Strube (2016) and the CohEmb model by 11% and 5%, respectively, and defines a new state-of-the-art on this dataset.",
            "CohLSTM, unlike Mesgar and Strube (2016)\u00e2\u20ac\u2122s model and CohEmb, considers words of sentences in their sentence context.",
            "This supports our intuition that actual context information of words contributes to the perceived coherence of texts.",
            "CohLSTM, which captures exclusively local coherence, even outperforms the readability system proposed by De Clercq and Hoste (2016), which relies on a wide range of lexical, syntactic and semantic features."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "Mesgar and Strube (2016)",
                "CohEmb",
                "CohLSTM",
                "De Clercq and Hoste (2016)"
            ],
            [
                "CohEmb",
                "Mesgar and Strube (2016)",
                "CohLSTM",
                "Accuracy (%)"
            ],
            [
                "CohLSTM"
            ],
            [
                "CohLSTM",
                "Mesgar and Strube (2016)",
                "CohEmb",
                "Accuracy (%)"
            ],
            [
                "CohLSTM",
                "Mesgar and Strube (2016)",
                "CohEmb"
            ],
            null,
            [
                "CohLSTM",
                "De Clercq and Hoste (2016)"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_1",
        "paper_id": "D18-1464",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1465table_4",
        "description": "Since the first and the last sentences of the text are more special to discern (Chen et al., 2016; Gong et al., 2016), we also evaluate the ratio of correctly predicting the first and the last sentences. Table 4 summarizes our performances on arXiv abstract and SIND caption. As we see, all models show fair well in predicting the first sentence, and the prediction accuracy declines for the last one. It is observed that ATTOrderNet still achieves a boost in predicting two positions compared to the previous state-of-the-art system on both datasets.",
        "sentences": [
            "Since the first and the last sentences of the text are more special to discern (Chen et al., 2016; Gong et al., 2016), we also evaluate the ratio of correctly predicting the first and the last sentences.",
            "Table 4 summarizes our performances on arXiv abstract and SIND caption.",
            "As we see, all models show fair well in predicting the first sentence, and the prediction accuracy declines for the last one.",
            "It is observed that ATTOrderNet still achieves a boost in predicting two positions compared to the previous state-of-the-art system on both datasets."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "arXiv abstract",
                "SIND caption"
            ],
            [
                "Random",
                "Pairwise Ranking Model",
                "CNN+PtrNet",
                "LSTM+PtrNet",
                "ATTOrderNet (ATT)",
                "ATTOrderNet (CNN)",
                "ATTOrderNet",
                "head",
                "tail"
            ],
            [
                "ATTOrderNet",
                "arXiv abstract",
                "SIND caption",
                "head",
                "tail"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_4",
        "paper_id": "D18-1465",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1465table_5",
        "description": "3.4.2 Results. Table 5 reports the results of ATTOrderNet and currently competing architectures in this evaluation task. ATTOrderNet also achieves the stateof-the-art performance, showing a remarkable advancement of about 1.8% gain on Accident dataset and further improving the pairwise accuracy to 99.8 on Earthquake dataset. LSTM+PtrNet and CNN+ PtrNet (Gong et al., 2016) fall short of Varient-LSTM+PtrNet (Logeswaran et al., 2018) in performance. This could also be blamed for their paragraph encoder. Documents in both datasets are much longer than those in others, which brings more trouble for LSTMs in paragraph encoder to build logical representations. Compared to the result in the sentence ordering task, Entity Grid (Barzilay and Lapata, 2008) achieves a good performance in this task and even outperforms Recurrent neural networks and Recursive neural networks (Li and Hovy, 2014) on Accident dataset. However, Entity Grid requires hand-engineered features and heavily relies on linguistic knowledge which restrain the model to be adapted to other tasks.",
        "sentences": [
            "3.4.2 Results.",
            "Table 5 reports the results of ATTOrderNet and currently competing architectures in this evaluation task.",
            "ATTOrderNet also achieves the stateof-the-art performance, showing a remarkable advancement of about 1.8% gain on Accident dataset and further improving the pairwise accuracy to 99.8 on Earthquake dataset.",
            "LSTM+PtrNet and CNN+ PtrNet (Gong et al., 2016) fall short of Varient-LSTM+PtrNet (Logeswaran et al., 2018) in performance.",
            "This could also be blamed for their paragraph encoder.",
            "Documents in both datasets are much longer than those in others, which brings more trouble for LSTMs in paragraph encoder to build logical representations.",
            "Compared to the result in the sentence ordering task, Entity Grid (Barzilay and Lapata, 2008) achieves a good performance in this task and even outperforms Recurrent neural networks and Recursive neural networks (Li and Hovy, 2014) on Accident dataset.",
            "However, Entity Grid requires hand-engineered features and heavily relies on linguistic knowledge which restrain the model to be adapted to other tasks."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "ATTOrderNet",
                "Random",
                "Graph",
                "HMM+Entity",
                "HMM",
                "Entity Grid",
                "Recurrent",
                "Recursive",
                "Discriminative Model",
                "Varient-LSTM+PtrNet",
                "CNN+PtrNet",
                "LSTM+PtrNet"
            ],
            [
                "ATTOrderNet",
                "Varient-LSTM+PtrNet",
                "Accident",
                "Earthquake"
            ],
            [
                "Varient-LSTM+PtrNet",
                "CNN+PtrNet",
                "LSTM+PtrNet"
            ],
            null,
            [
                "Accident",
                "Earthquake"
            ],
            [
                "Entity Grid",
                "Recurrent",
                "Recursive",
                "Accident"
            ],
            [
                "Entity Grid"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_5",
        "paper_id": "D18-1465",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1483table_5",
        "description": "We test two different scenarios for optimizing the similarity threshold \u03c4 for the crosslingual case. Table 5 shows the results for these experiments. First, we consider the simpler case of adjusting a global \u03c4 parameter for the crosslingual distances, as also described for the monolingual case. As shown, this method works poorly, since the \u03c4 grid-search could not find a reasonable \u03c4 which worked well for every possible language pair. Subsequently, we also consider the case of using English as a pivot language (see \u00a73), where distances for every other language are only compared to English, and crosslingual clustering decisions are made only based on this distance. This yielded our best crosslingual score of F1=84.0, confirming that crosslingual similarity is of higher quality between each language and English, for the embeddings we used. This score represents only a small degradation in respect to the monolingual results, since clustering across different languages is a harder problem.",
        "sentences": [
            "We test two different scenarios for optimizing the similarity threshold \u03c4 for the crosslingual case.",
            "Table 5 shows the results for these experiments.",
            "First, we consider the simpler case of adjusting a global \u03c4 parameter for the crosslingual distances, as also described for the monolingual case.",
            "As shown, this method works poorly, since the \u03c4 grid-search could not find a reasonable \u03c4 which worked well for every possible language pair.",
            "Subsequently, we also consider the case of using English as a pivot language (see \u00a73), where distances for every other language are only compared to English, and crosslingual clustering decisions are made only based on this distance.",
            "This yielded our best crosslingual score of F1=84.0, confirming that crosslingual similarity is of higher quality between each language and English, for the embeddings we used.",
            "This score represents only a small degradation in respect to the monolingual results, since clustering across different languages is a harder problem."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "\u03c4search (global)"
            ],
            [
                "\u03c4search (global)"
            ],
            [
                "\u03c4search (pivot)"
            ],
            [
                "\u03c4search (pivot)",
                "F1"
            ],
            [
                "\u03c4search (pivot)",
                "F1"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_5",
        "paper_id": "D18-1483",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1484table_4",
        "description": "where in Zero Update, we ignore the training set of C and just evaluate our model on the testing set. As Table 4 shows, Before Update denotes the model trained on the old tasks before the new tasks are involved, so only evaluations on the old tasks are conducted. Cold Update re-trains the model of Before Update with both the old tasks and the new tasks, thus achieving similar performances with the last line in Table 3. Different from Cold Update, Hot Update resumes training only on the new tasks, requires much less training time, while still obtains competitive results for all tasks. The new tasks like IMDB and Kitchen benefit more from Hot Update than the old tasks, as the parameters are further tuned according to annotations from these new tasks. Zero Update provides inspiring possibilities for completely unannotated tasks. There are no more annotations for additional training from the new tasks, so we just apply the model of Before Update for evaluations on the testing sets of the new tasks. Zero Update achieves competitive performances in Case 1 (90.9 for IMDB) and Case 2 (86.7 for Kitchen), as tasks from these two cases all belong to sentiment datasets of different cardinalities or domains that contain rich semantic correlations with each other. However, the result for IMDB in Case 3 is only 74.2, as sentiment shares less relevance with topic and question type, thus leading to poor transferring performances.",
        "sentences": [
            "where in Zero Update, we ignore the training set of C and just evaluate our model on the testing set.",
            "As Table 4 shows, Before Update denotes the model trained on the old tasks before the new tasks are involved, so only evaluations on the old tasks are conducted.",
            "Cold Update re-trains the model of Before Update with both the old tasks and the new tasks, thus achieving similar performances with the last line in Table 3.",
            "Different from Cold Update, Hot Update resumes training only on the new tasks, requires much less training time, while still obtains competitive results for all tasks.",
            "The new tasks like IMDB and Kitchen benefit more from Hot Update than the old tasks, as the parameters are further tuned according to annotations from these new tasks.",
            "Zero Update provides inspiring possibilities for completely unannotated tasks.",
            "There are no more annotations for additional training from the new tasks, so we just apply the model of Before Update for evaluations on the testing sets of the new tasks.",
            "Zero Update achieves competitive performances in Case 1 (90.9 for IMDB) and Case 2 (86.7 for Kitchen), as tasks from these two cases all belong to sentiment datasets of different cardinalities or domains that contain rich semantic correlations with each other.",
            "However, the result for IMDB in Case 3 is only 74.2, as sentiment shares less relevance with topic and question type, thus leading to poor transferring performances."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Before Update"
            ],
            [
                "Cold Update",
                "Before Update"
            ],
            [
                "Cold Update",
                "Hot Update"
            ],
            [
                "Hot Update",
                "IMDB",
                "K"
            ],
            [
                "Zero Update"
            ],
            [
                "Before Update"
            ],
            [
                "Zero Update",
                "Case 1",
                "IMDB",
                "Case 2",
                "K"
            ],
            [
                "Zero Update",
                "Case 3",
                "IMDB"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_4",
        "paper_id": "D18-1484",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1484table_5",
        "description": "As Table 5 shows, MTLE achieves competitive or better performances on most tasks except for QC, as it contains less correlations with other tasks. Tree-LSTM outperforms our model on SST1 (50.6 against 49.8), but it requires an external parser to get the sentence topological structure and utilizes treebank annotations. PV slightly surpasses MTLE on IMDB (91.7 against 91.3), as sentences from IMDB are much longer than SST and MDSD, which require stronger abilities of long-term dependency learning.",
        "sentences": [
            "As Table 5 shows, MTLE achieves competitive or better performances on most tasks except for QC, as it contains less correlations with other tasks.",
            "Tree-LSTM outperforms our model on SST1 (50.6 against 49.8), but it requires an external parser to get the sentence topological structure and utilizes treebank annotations.",
            "PV slightly surpasses MTLE on IMDB (91.7 against 91.3), as sentences from IMDB are much longer than SST and MDSD, which require stronger abilities of long-term dependency learning."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "MTLE",
                "SST-1",
                "SST-2",
                "IMDB",
                "Books",
                "DVDs",
                "Electronics",
                "Kitchen"
            ],
            [
                "Tree-LSTM",
                "MTLE",
                "SST-1"
            ],
            [
                "PV",
                "MTLE",
                "IMDB"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_5",
        "paper_id": "D18-1484",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1485table_5",
        "description": "We present the results of the evaluation on Table 5, where it can be found that our model with fewer parameters still outperforms the hierarchical model with the deterministic setting of sentence or phrase. Moreover, in order to alleviate the influence of the deterministic sentence boundary, we compare the performance of different hierarchical models with different boundaries, which sets the boundaries at the end of every 5, 10, 15 and 20 words respectively. The results in Table 5 show that the hierarchical models achieve similar performances, which are all higher than the performances of the baselines. This shows that highlevel representations can contribute to the performance of the Seq2Seq model on the multi-label text classification task. Furthermore, as these performances are no better than that of our proposed model, it can reflect that the learnable high-level representations can contribute more than deterministic sentence-level representations, as it can be more flexible to represent information of diverse levels, instead of fixed phrase or sentence level.",
        "sentences": [
            "We present the results of the evaluation on Table 5, where it can be found that our model with fewer parameters still outperforms the hierarchical model with the deterministic setting of sentence or phrase.",
            "Moreover, in order to alleviate the influence of the deterministic sentence boundary, we compare the performance of different hierarchical models with different boundaries, which sets the boundaries at the end of every 5, 10, 15 and 20 words respectively.",
            "The results in Table 5 show that the hierarchical models achieve similar performances, which are all higher than the performances of the baselines.",
            "This shows that highlevel representations can contribute to the performance of the Seq2Seq model on the multi-label text classification task.",
            "Furthermore, as these performances are no better than that of our proposed model, it can reflect that the learnable high-level representations can contribute more than deterministic sentence-level representations, as it can be more flexible to represent information of diverse levels, instead of fixed phrase or sentence level."
        ],
        "class_sentence": [
            1,
            2,
            1,
            2,
            2
        ],
        "header_mention": [
            [
                "Our model"
            ],
            [
                "Hier-5",
                "Hier-10",
                "Hier-15",
                "Hier-20"
            ],
            [
                "Hier-5",
                "Hier-10",
                "Hier-15",
                "Hier-20"
            ],
            null,
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_5",
        "paper_id": "D18-1485",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1486table_2",
        "description": "4.3 Single-Task Learning Results. We first test our approach on six datasets for text classification under the scheme of single-task. As Table 2 shows, our single-task network enhanced by capsules is already a strong model. CapsNet1 that has one kernel size obtains the best accuracy on 2 out of 6 datasets, and gets competitive results on the others. And CapsNet-2 with multiple kernel sizes further improves the performance and get best accuracy on 4 datasets. This proves our capsule networks are effective for text. Particularly, our capsule network outperforms conventional CNNs like DCNN, CNN-MC and VD-CNN with a large margin (by average 1.1%, 0.7% and 1.0% respectively), which shows the advantages of capsule network over conventional CNNs for clustering features and leveraging the position information. Ablation Study on Orphan Category. Orphan category in class capsule layer helps collect the noise capsules that contain the \u2018background\u2019 information like stop words, punctuations or any unrelated words. We conduct the ablation experiment on orphan category, and result (Table 2) shows that network with orphan category perform better than the without one by 0.4%. This demonstrates the effectiveness of orphan category.",
        "sentences": [
            "4.3 Single-Task Learning Results.",
            "We first test our approach on six datasets for text classification under the scheme of single-task.",
            "As Table 2 shows, our single-task network enhanced by capsules is already a strong model.",
            "CapsNet1 that has one kernel size obtains the best accuracy on 2 out of 6 datasets, and gets competitive results on the others.",
            "And CapsNet-2 with multiple kernel sizes further improves the performance and get best accuracy on 4 datasets.",
            "This proves our capsule networks are effective for text.",
            "Particularly, our capsule network outperforms conventional CNNs like DCNN, CNN-MC and VD-CNN with a large margin (by average 1.1%, 0.7% and 1.0% respectively), which shows the advantages of capsule network over conventional CNNs for clustering features and leveraging the position information.",
            "Ablation Study on Orphan Category.",
            "Orphan category in class capsule layer helps collect the noise capsules that contain the \u2018background\u2019 information like stop words, punctuations or any unrelated words.",
            "We conduct the ablation experiment on orphan category, and result (Table 2) shows that network with orphan category perform better than the without one by 0.4%.",
            "This demonstrates the effectiveness of orphan category."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1,
            2,
            1,
            2,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "CapsNet-1",
                "CapsNet-2",
                "MR",
                "SST-1",
                "SST-2",
                "Subj",
                "TREC",
                "AG\u2019s"
            ],
            [
                "CapsNet-1",
                "CapsNet-2"
            ],
            [
                "CapsNet-1",
                "MR",
                "SST-1",
                "SST-2",
                "Subj",
                "TREC",
                "AG\u2019s"
            ],
            [
                "CapsNet-2",
                "MR",
                "SST-1",
                "Subj",
                "AG\u2019s"
            ],
            [
                "CapsNet-1",
                "CapsNet-2"
            ],
            [
                "CapsNet-2",
                "VD-CNN",
                "DCNN",
                "CNN-MC"
            ],
            null,
            null,
            [
                "CapsNet-2",
                "- Orphan"
            ],
            [
                "- Orphan"
            ]
        ],
        "n_sentence": 11.0,
        "table_id": "table_2",
        "paper_id": "D18-1486",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1490table_5",
        "description": "Finally, we compare the ACNN model to stateof-the-art methods from the literature, evaluated on the Switchboard test set. Table 5 shows that the ACNN model is competitive with recent models from the literature. The three models that score more highly than the ACNN all rely on handcrafted features, additional information sources such as partial-word features (which would not be available in a realistic ASR application), or external resources such as dependency parsers and language models. The ACNN, on the other hand, only uses whole-word inputs and learns the \u201crough copy\u201d dependencies between words without requiring any manual feature engineering.",
        "sentences": [
            "Finally, we compare the ACNN model to stateof-the-art methods from the literature, evaluated on the Switchboard test set.",
            "Table 5 shows that the ACNN model is competitive with recent models from the literature.",
            "The three models that score more highly than the ACNN all rely on handcrafted features, additional information sources such as partial-word features (which would not be available in a realistic ASR application), or external resources such as dependency parsers and language models.",
            "The ACNN, on the other hand, only uses whole-word inputs and learns the \u201crough copy\u201d dependencies between words without requiring any manual feature engineering."
        ],
        "class_sentence": [
            2,
            1,
            2,
            2
        ],
        "header_mention": [
            [
                "ACNN"
            ],
            [
                "ACNN"
            ],
            [
                "Ferguson et al. (2015)",
                "Zayats et al. (2016)",
                "Jamshid Lou et al. (2017)"
            ],
            [
                "ACNN"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_5",
        "paper_id": "D18-1490",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1494table_7",
        "description": "We train W2V, siW2V, and SSPMI over each corpus by setting the number of context window size to 5. Furthermore, the dimension of word embeddings generated from all models is set to 50 according to (Lai et al., 2016). The values of word similarity on ISEAR and YouTube are respectively shown in Table 7 and Table 8, where the best results are highlighted in boldface. We can observe that SLTM outperforms baselines for all cases. The results indicate that word embeddings learned from the global label-specific topic information are better than those from the local context information without any external corpora.",
        "sentences": [
            "We train W2V, siW2V, and SSPMI over each corpus by setting the number of context window size to 5.",
            "Furthermore, the dimension of word embeddings generated from all models is set to 50 according to (Lai et al., 2016).",
            "The values of word similarity on ISEAR and YouTube are respectively shown in Table 7 and Table 8, where the best results are highlighted in boldface.",
            "We can observe that SLTM outperforms baselines for all cases.",
            "The results indicate that word embeddings learned from the global label-specific topic information are better than those from the local context information without any external corpora."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "W2V",
                "siW2V",
                "SSPMI"
            ],
            [
                "W2V",
                "siW2V",
                "SSPMI"
            ],
            null,
            [
                "SLTM"
            ],
            [
                "SLTM"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_7",
        "paper_id": "D18-1494",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1495table_3",
        "description": "Effect of the mini-corpus. To study the effect of our sampling strategy which has been discussed in section 3.3. Table 3 shows the performance of our model with different sample size for a mini-corpus. For the 20 Newsgroups dataset, the best performance is achieved when the sample size is 3. When we do not use our sample strategy (mini-corpus is 1), the performance drops by a large margin. From Table 1, we see that the average size of documents size in 20 Newsgroups is relatively short (88 compared with 302 in All News). Therefore, the 20 Newsgroups dataset may suffer from the sparsity problem. The experiment shows that our sampling strategy can help to overcome this problem. When sample size increases, the performance drops again. The biterm graph with large sample size may bring the same problem of the original BTM (insufficient topic representation). Compared to 20 Newsgroups dataset, documents in the All News dataset is longer and carried more topic information, so the best performance is achieved without sampling. We find that when the sample size is larger than an optimized value, the topic coherence starts to drop.",
        "sentences": [
            "Effect of the mini-corpus.",
            "To study the effect of our sampling strategy which has been discussed in section 3.3.",
            "Table 3 shows the performance of our model with different sample size for a mini-corpus.",
            "For the 20 Newsgroups dataset, the best performance is achieved when the sample size is 3.",
            "When we do not use our sample strategy (mini-corpus is 1), the performance drops by a large margin.",
            "From Table 1, we see that the average size of documents size in 20 Newsgroups is relatively short (88 compared with 302 in All News).",
            "Therefore, the 20 Newsgroups dataset may suffer from the sparsity problem.",
            "The experiment shows that our sampling strategy can help to overcome this problem.",
            "When sample size increases, the performance drops again.",
            "The biterm graph with large sample size may bring the same problem of the original BTM (insufficient topic representation).",
            "Compared to 20 Newsgroups dataset, documents in the All News dataset is longer and carried more topic information, so the best performance is achieved without sampling.",
            "We find that when the sample size is larger than an optimized value, the topic coherence starts to drop."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1,
            2,
            2,
            2,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "20 News (k=50)",
                "20 News (k=100)",
                "All news (k=50)",
                "All news (k=100)",
                "# Samples"
            ],
            [
                "20 News (k=50)",
                "20 News (k=100)",
                "# Samples",
                "3"
            ],
            [
                "# Samples",
                "1",
                "3"
            ],
            [
                "20 News (k=50)",
                "20 News (k=100)"
            ],
            [
                "20 News (k=50)",
                "20 News (k=100)"
            ],
            null,
            [
                "# Samples",
                "Score"
            ],
            null,
            [
                "20 News (k=50)",
                "20 News (k=100)",
                "All news (k=50)",
                "All news (k=100)",
                "# Samples",
                "1"
            ],
            null
        ],
        "n_sentence": 12.0,
        "table_id": "table_3",
        "paper_id": "D18-1495",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1497table_6",
        "description": "In Table 6 we present cross AUC evaluations. Rows correspond to the embedding used and columns to the aspect evaluated against. As expected, aspect-embeddings perform better w.r.t. the aspects for which they code, suggesting some disentanglement. However, the reduction in performance when using one aspect representation to discriminate w.r.t. others is not as pronounced as above. This is because aspect ratings are highly correlated: if taste is positive, aroma is very likely to be as well. Effectively, here sentiment entangles all of these aspects.",
        "sentences": [
            "In Table 6 we present cross AUC evaluations.",
            "Rows correspond to the embedding used and columns to the aspect evaluated against.",
            "As expected, aspect-embeddings perform better w.r.t. the aspects for which they code, suggesting some disentanglement.",
            "However, the reduction in performance when using one aspect representation to discriminate w.r.t. others is not as pronounced as above.",
            "This is because aspect ratings are highly correlated: if taste is positive, aroma is very likely to be as well.",
            "Effectively, here sentiment entangles all of these aspects."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "Look",
                "Aroma",
                "Palate",
                "Taste"
            ],
            null,
            null,
            [
                "Aroma",
                "Taste"
            ],
            [
                "Look",
                "Aroma",
                "Palate",
                "Taste"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_6",
        "paper_id": "D18-1497",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1498table_4",
        "description": "5.2 Part-of-Speech Tagging. Table 4 summarizes our results on POS tagging. Again, our approach consistently achieves the best performance across different settings and tasks. Adding Twitter as a source leads to a drop in performance for the unified model, as a result of negative transfer. Our method, however, robustly handles negative transfer and manages to even benefit from this additional source.",
        "sentences": [
            "5.2 Part-of-Speech Tagging.",
            "Table 4 summarizes our results on POS tagging.",
            "Again, our approach consistently achieves the best performance across different settings and tasks.",
            "Adding Twitter as a source leads to a drop in performance for the unified model, as a result of negative transfer.",
            "Our method, however, robustly handles negative transfer and manages to even benefit from this additional source."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "MoE"
            ],
            [
                "uni-MS",
                "uni-MS\u2020"
            ],
            [
                "MoE"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_4",
        "paper_id": "D18-1498",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1508table_1",
        "description": "Results. Table 1 shows the results of our model and the baselines in the emoji prediction task for the different evaluation splits. The evaluation metrics used are: F1, Accuracy@k (A@k, where k \u2208 {1, 5}), and Coverage Error (CE) (Tsoumakas et al., 2009). We note that the latter metric is not normally used in emoji prediction settings. However, with many emojis being \u201cnear synonyms\u201d (in the sense of being often used almost interchangeably), it seems natural to evaluate the performance of an emoji prediction system in terms of how far we would need to go through the predicted emojis to recover the true label. The results show that our proposed 2-BiLSTMsl method outperforms all baselines for F1 in three out of four settings, and for CE in all of them. In the following section we shed light on the reasons behind this performance, and we try to understand how these predictions were made.",
        "sentences": [
            "Results.",
            "Table 1 shows the results of our model and the baselines in the emoji prediction task for the different evaluation splits.",
            "The evaluation metrics used are: F1, Accuracy@k (A@k, where k \u2208 {1, 5}), and Coverage Error (CE) (Tsoumakas et al., 2009).",
            "We note that the latter metric is not normally used in emoji prediction settings.",
            "However, with many emojis being \u201cnear synonyms\u201d (in the sense of being often used almost interchangeably), it seems natural to evaluate the performance of an emoji prediction system in terms of how far we would need to go through the predicted emojis to recover the true label.",
            "The results show that our proposed 2-BiLSTMsl method outperforms all baselines for F1 in three out of four settings, and for CE in all of them.",
            "In the following section we shed light on the reasons behind this performance, and we try to understand how these predictions were made."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "FastText",
                "2-BiLSTM",
                "2-BiLSTMa",
                "2-BiLSTMl",
                "F1",
                "A@1",
                "A@5",
                "CE"
            ],
            [
                "F1",
                "A@1",
                "A@5",
                "CE"
            ],
            null,
            null,
            [
                "2-BiLSTMl",
                "Lab",
                "50",
                "100",
                "200",
                "F1",
                "CE"
            ],
            [
                "2-BiLSTMl"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_1",
        "paper_id": "D18-1508",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1510table_1",
        "description": "Performance. Table 1 shows the translation performance on test sets measured in BLEU score. Simply training NMT model by the probabilistic 2-gram precision achieves an improvement of 1.5 BLEU points, which significantly outperforms the reinforcement-based algorithms. We also test the precision of other n-grams and their combinations, but do not notice significant improvements over P-P2. Notice that our method only changes the loss function, without any modification on model structure and training data.",
        "sentences": [
            "Performance.",
            "Table 1 shows the translation performance on test sets measured in BLEU score.",
            "Simply training NMT model by the probabilistic 2-gram precision achieves an improvement of 1.5 BLEU points, which significantly outperforms the reinforcement-based algorithms.",
            "We also test the precision of other n-grams and their combinations, but do not notice significant improvements over P-P2.",
            "Notice that our method only changes the loss function, without any modification on model structure and training data."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "P-P2",
                "BaseNMT",
                "RF"
            ],
            [
                "P-BLEU",
                "P-GLEU",
                "P-P2"
            ],
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_1",
        "paper_id": "D18-1510",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1516table_3",
        "description": "4.3 Results. Table 3 and 4 list the results for the KG completion tasks. TA-TRANSE and TA-DISTMULT systematically improve TRANSE and DISTMULT in MRR, hits@10 and hits@1 in almost all cases. Mean rank is a metric that is very susceptible to outliers and hence these improvements are not consistent. TTRANSE learns independent representations for each timestamp contained in the training set. At test time, timestamps unseen during training are represented by null vectors. This explains that TTRANSE is only competitive in YAGO15K, wherein the number of distinct timestamps is very small (see #Distinct TS in Table 2) and thus enough training examples exist to learn robust timestamp embeddings. TTRANSE\u00e2\u20ac\u2122s performance is similar to that of TA-TRANSE, our time-aware version of TRANSE, in WIKIDATA. Similarly, TTRANSE can learn robust timestamp representations because of the small number of distinct timestamps of this data set.",
        "sentences": [
            "4.3 Results.",
            "Table 3 and 4 list the results for the KG completion tasks.",
            "TA-TRANSE and TA-DISTMULT systematically improve TRANSE and DISTMULT in MRR, hits@10 and hits@1 in almost all cases.",
            "Mean rank is a metric that is very susceptible to outliers and hence these improvements are not consistent.",
            "TTRANSE learns independent representations for each timestamp contained in the training set.",
            "At test time, timestamps unseen during training are represented by null vectors.",
            "This explains that TTRANSE is only competitive in YAGO15K, wherein the number of distinct timestamps is very small (see #Distinct TS in Table 2) and thus enough training examples exist to learn robust timestamp embeddings.",
            "TTRANSE\u00e2\u20ac\u2122s performance is similar to that of TA-TRANSE, our time-aware version of TRANSE, in WIKIDATA.",
            "Similarly, TTRANSE can learn robust timestamp representations because of the small number of distinct timestamps of this data set."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            2,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "TA-TRANSE",
                "TA-DISTMULT",
                "TRANSE",
                "DISTMULT",
                "MRR",
                "Hits@10",
                "Hits@1"
            ],
            [
                "MR"
            ],
            [
                "TTRANSE"
            ],
            [
                "TTRANSE"
            ],
            [
                "TTRANSE",
                "YAGO15K"
            ],
            [
                "TTRANSE",
                "TA-TRANSE",
                "WIKIDATA"
            ],
            [
                "TTRANSE"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_3",
        "paper_id": "D18-1516",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1525table_1",
        "description": "4 Discussion. We find that almost all of the proposed loss functions outperform the vanilla autoencoder trained with cross-entropy on all three tasks (see Table 1). The only exception is the weighted similarity loss function. Compared to the logarithm-based losses, this loss applies softer penalties when the groundtruth tokens are predicted to have lower probabilities. We conclude that the non-linearity introduced by a logarithm function contributes to more efficient training. Among the models we tested, the best scores were achieved by the weighted cross-entropy loss for MSRP (68.2%), the weighted similarity loss for SNLI (69.1%) and by the soft label loss for SICK-E (72.4%). We observe that for the paraphrase task, all the soft label losses behaved similarly, while for the inference/entailment, increasing the number of neighbors improved performance.",
        "sentences": [
            "4 Discussion.",
            "We find that almost all of the proposed loss functions outperform the vanilla autoencoder trained with cross-entropy on all three tasks (see Table 1).",
            "The only exception is the weighted similarity loss function.",
            "Compared to the logarithm-based losses, this loss applies softer penalties when the groundtruth tokens are predicted to have lower probabilities.",
            "We conclude that the non-linearity introduced by a logarithm function contributes to more efficient training.",
            "Among the models we tested, the best scores were achieved by the weighted cross-entropy loss for MSRP (68.2%), the weighted similarity loss for SNLI (69.1%) and by the soft label loss for SICK-E (72.4%).",
            "We observe that for the paraphrase task, all the soft label losses behaved similarly, while for the inference/entailment, increasing the number of neighbors improved performance."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Cross-entropy (vanilla AE)",
                "Soft label N = 3",
                "Soft label N = 5",
                "Soft label N = 10",
                "Weighted cross-entropy",
                "Acc"
            ],
            [
                "Acc",
                "Weighted similarity"
            ],
            null,
            null,
            [
                "Acc",
                "Weighted cross-entropy",
                "Weighted similarity",
                "MSRP",
                "SNLI",
                "SICK-E",
                "Soft label N = 10"
            ],
            [
                "Soft label N = 10"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_1",
        "paper_id": "D18-1525",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1529table_3",
        "description": "Table 3 contains results achieved without using any pretrained embeddings. Our model achieves the best results among NN models on 6/7 datasets. In addition, while the majority of datasets work the best if the pretrained embedding matrix is treated as constant, the MSR dataset is an outlier: fine-tuning embeddings yields a very large improvement. We observe that the likely cause is a low OOV rate in the MSR evaluation set compared to other datasets.",
        "sentences": [
            "Table 3 contains results achieved without using any pretrained embeddings.",
            "Our model achieves the best results among NN models on 6/7 datasets.",
            "In addition, while the majority of datasets work the best if the pretrained embedding matrix is treated as constant, the MSR dataset is an outlier: fine-tuning embeddings yields a very large improvement.",
            "We observe that the likely cause is a low OOV rate in the MSR evaluation set compared to other datasets."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "Ours",
                "AS",
                "CITYU",
                "CTB6",
                "CTB7",
                "MSR",
                "UD"
            ],
            [
                "AS",
                "CITYU",
                "CTB6",
                "CTB7",
                "PKU",
                "UD"
            ],
            [
                "MSR"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "D18-1529",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1529table_6",
        "description": "3.2 Ablation Experiments. To see which decisions had the greatest impact on the result, we performed ablation experiments on the holdout sets of the different corpora. Starting with our proposed system, we remove one decision, perform hyperparameter tuning, and see the change in performance. The results are summarized in Table 6. Negative numbers in Table 6 correspond to decreases in performance for the ablated system. Note that although each of the components help performance on average, there are cases where we observe no impact. For example using recurrent dropout on AS and MSR rarely affects accuracy.",
        "sentences": [
            "3.2 Ablation Experiments.",
            "To see which decisions had the greatest impact on the result, we performed ablation experiments on the holdout sets of the different corpora.",
            "Starting with our proposed system, we remove one decision, perform hyperparameter tuning, and see the change in performance.",
            "The results are summarized in Table 6.",
            "Negative numbers in Table 6 correspond to decreases in performance for the ablated system.",
            "Note that although each of the components help performance on average, there are cases where we observe no impact.",
            "For example using recurrent dropout on AS and MSR rarely affects accuracy."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "This work"
            ],
            null,
            null,
            [
                "This work",
                "-LSTM dropout",
                "-stacked bi-LSTM",
                "-pretrain"
            ],
            [
                "AS",
                "MSR",
                "-LSTM dropout"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_6",
        "paper_id": "D18-1529",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1531table_2",
        "description": "Table 2 shows the results. We can find from the table that only 1024 guideline sentences can improve the performance of \u201cSLM-4\u201d significantly. While rule-based post-processing is very effective, \u201cSLM-4\u2020\u201d can outperform \u201cSLM-4*\u201d on all the four datasets. Moreover, performance drops when applying the rule-based post-processing to \u201cSLM-4\u2020\u201d on three datasets. These indicate that SLMs can learn the empirical rules for word segmentation given only a small amount of training data. And these guideline data can improve the performance of SLMs naturally, superior to using explicit rules.",
        "sentences": [
            "Table 2 shows the results.",
            "We can find from the table that only 1024 guideline sentences can improve the performance of \u201cSLM-4\u201d significantly.",
            "While rule-based post-processing is very effective, \u201cSLM-4\u2020\u201d can outperform \u201cSLM-4*\u201d on all the four datasets.",
            "Moreover, performance drops when applying the rule-based post-processing to \u201cSLM-4\u2020\u201d on three datasets.",
            "These indicate that SLMs can learn the empirical rules for word segmentation given only a small amount of training data.",
            "And these guideline data can improve the performance of SLMs naturally, superior to using explicit rules."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "SLM-4\u2020",
                "SLM-4"
            ],
            [
                "SLM-4*",
                "SLM-4\u2020",
                "PKU",
                "MSR",
                "AS",
                "CityU"
            ],
            [
                "SLM-4\u2020",
                "SLM-4\u2020*",
                "PKU",
                "AS",
                "CityU"
            ],
            null,
            [
                "SLM-4"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_2",
        "paper_id": "D18-1531",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1538table_1",
        "description": "Does training with joint objective help?. We trained 3 models with random 1%, 10% and whole 100% of the training set with joint objective (\u03b11 = \u03b12 = 0.5). For comparison, we trained 3 SOTA models with the same training sets. All models were trained for max 150 epochs and with a patience of 20 epochs. Table 1 reports the results of this experiment. We see that models trained with joint objective (JX) improve over baseline models (BX), both in terms of F1 and average disagreement rate. These improvements provide evidence for answering (Q1-3) favorably. Further, gains are more in low resource scenarios because by training models jointly to satisfy syntactic constraints helps in better generalization when trained with limited SRL corpora.",
        "sentences": [
            "Does training with joint objective help?.",
            "We trained 3 models with random 1%, 10% and whole 100% of the training set with joint objective (\u03b11 = \u03b12 = 0.5).",
            "For comparison, we trained 3 SOTA models with the same training sets.",
            "All models were trained for max 150 epochs and with a patience of 20 epochs.",
            "Table 1 reports the results of this experiment.",
            "We see that models trained with joint objective (JX) improve over baseline models (BX), both in terms of F1 and average disagreement rate.",
            "These improvements provide evidence for answering (Q1-3) favorably.",
            "Further, gains are more in low resource scenarios because by training models jointly to satisfy syntactic constraints helps in better generalization when trained with limited SRL corpora."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "J100",
                "J10",
                "J1"
            ],
            [
                "B100",
                "B10",
                "B1"
            ],
            [
                "B100",
                "B10",
                "B1",
                "J100",
                "J10",
                "J1"
            ],
            null,
            [
                "B100",
                "B10",
                "B1",
                "J100",
                "J10",
                "J1",
                "Test F1",
                "Average disagreement rate (%)"
            ],
            [
                "J100",
                "J10",
                "J1"
            ],
            [
                "J100",
                "J10",
                "J1"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_1",
        "paper_id": "D18-1538",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1544table_2",
        "description": "3 Experimental Results. Table 2 shows our results for language modeling. PRPN-UP, configured as-is with parsing criterion and language modeling criterion, performs dramatically worse than the standard PRPN-LM (a vs. d and e). However, this is not a fair comparison as the larger vocabulary gives PRPN-UP a harder task to solve. Adjusting the vocabulary of PRPNUP down to 10k to make a fairer comparison possible, the PPL of PRPN-UP improves significantly (c vs. d), but not enough to match PRPN-LM (a vs. c). We also observe that early stopping on parsing leads to incomplete training and a substantial decrease in perplexity (a vs. b and d vs. e). The models stop training at around the 13th epoch when we early-stop on parsing objective, while they stop training around the 65th epoch when we early-stop on language modeling objective. Both PRPN models trained on AllNLI do even worse (f and g), though the mismatch in vocabulary and domain may explain this effect. In addition, since it takes much longer to train PRPN on the larger AllNLI dataset, we train PRPN on AllNLI for only 15 epochs while we train the PRPN on WSJ for 100 epochs. Although the parsing objective converges within 15 epochs, we notice that language modeling perplexity is still improving. We expect that the perplexity of the PRPN models trained on AllNLI could be lower if we increase the number of training epochs.",
        "sentences": [
            "3 Experimental Results.",
            "Table 2 shows our results for language modeling.",
            "PRPN-UP, configured as-is with parsing criterion and language modeling criterion, performs dramatically worse than the standard PRPN-LM (a vs. d and e).",
            "However, this is not a fair comparison as the larger vocabulary gives PRPN-UP a harder task to solve.",
            "Adjusting the vocabulary of PRPNUP down to 10k to make a fairer comparison possible, the PPL of PRPN-UP improves significantly (c vs. d), but not enough to match PRPN-LM (a vs. c).",
            "We also observe that early stopping on parsing leads to incomplete training and a substantial decrease in perplexity (a vs. b and d vs. e).",
            "The models stop training at around the 13th epoch when we early-stop on parsing objective, while they stop training around the 65th epoch when we early-stop on language modeling objective.",
            "Both PRPN models trained on AllNLI do even worse (f and g), though the mismatch in vocabulary and domain may explain this effect.",
            "In addition, since it takes much longer to train PRPN on the larger AllNLI dataset, we train PRPN on AllNLI for only 15 epochs while we train the PRPN on WSJ for 100 epochs.",
            "Although the parsing objective converges within 15 epochs, we notice that language modeling perplexity is still improving.",
            "We expect that the perplexity of the PRPN models trained on AllNLI could be lower if we increase the number of training epochs."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            1,
            1,
            2,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "PRPN-LM",
                "PRPN-UP"
            ],
            [
                "PRPN-UP",
                "a",
                "d",
                "e",
                "PPL Median"
            ],
            [
                "PRPN-UP"
            ],
            [
                "a",
                "c",
                "d",
                "PRPN-LM",
                "PRPN-UP",
                "Vocab Size",
                "10k"
            ],
            [
                "a",
                "b",
                "d",
                "e",
                "Stopping Criterion",
                "LM",
                "UP"
            ],
            [
                "Stopping Criterion",
                "LM",
                "UP"
            ],
            [
                "Training Data",
                "AllNLI Train",
                "f",
                "g"
            ],
            [
                "WSJ Train",
                "AllNLI Train"
            ],
            [
                "PRPN-UP"
            ],
            [
                "PRPN-LM",
                "PRPN-UP",
                "AllNLI Train"
            ]
        ],
        "n_sentence": 11.0,
        "table_id": "table_2",
        "paper_id": "D18-1544",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1544table_3",
        "description": "In addition, Table 3 shows that the PRPN-UP models achieve the median parsing F1 scores of 46.3 and 48.6 respectively on the MultiNLI dev set while PRPN-LM performs the median F1 of 45.7; setting the state of the art in parsing performance on this dataset among latent tree models by a large margin. We conclude that PRPN does acquire some substantial knowledge of syntax, and that this knowledge agrees with Penn Treebank (PTB) grammar significantly better than chance.",
        "sentences": [
            "In addition, Table 3 shows that the PRPN-UP models achieve the median parsing F1 scores of 46.3 and 48.6 respectively on the MultiNLI dev set while PRPN-LM performs the median F1 of 45.7; setting the state of the art in parsing performance on this dataset among latent tree models by a large margin.",
            "We conclude that PRPN does acquire some substantial knowledge of syntax, and that this knowledge agrees with Penn Treebank (PTB) grammar significantly better than chance."
        ],
        "class_sentence": [
            1,
            2
        ],
        "header_mention": [
            [
                "PRPN-LM",
                "PRPN-UP",
                "F1 wrt.",
                "SP"
            ],
            [
                "PRPN-LM",
                "PRPN-UP"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_3",
        "paper_id": "D18-1544",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1547table_4",
        "description": "We trained the same neural architecture (taking into account different number of domains) on both MultiWOZ and Cam676 datasets. The best results on the Cam676 corpus were obtained with bidirectional GRU cell. In the case of MultiWOZ dataset, the LSTM cell serving as a decoder and an encoder achieved the highest score with the global type of attention (Bahdanau et al., 2014). Table 4 presents the results of a various of model architectures and shows several challenges. As expected, the model achieves almost perfect score on the Inform metric on the Cam676 dataset taking the advantage of an oracle belief state signal. However, even with the perfect dialogue state tracking of the user intent, the baseline models obtain almost 30% lower score on the Inform metric on the new corpus. The addition of the attention improves the score on the Success metric on the new dataset by less than 1%. Nevertheless, as expected, the best model on MultiWOZ is still falling behind by a large margin in comparison to the results on the Cam676 corpus taking into account both Inform and Success metrics. As most of dialogues span over at least two domains, the model has to be much more effective in order to execute a successful dialogue. Moreover, the BLEU score on the MultiWOZ is lower than the one reported on the Cam676 dataset.",
        "sentences": [
            "We trained the same neural architecture (taking into account different number of domains) on both MultiWOZ and Cam676 datasets.",
            "The best results on the Cam676 corpus were obtained with bidirectional GRU cell.",
            "In the case of MultiWOZ dataset, the LSTM cell serving as a decoder and an encoder achieved the highest score with the global type of attention (Bahdanau et al., 2014).",
            "Table 4 presents the results of a various of model architectures and shows several challenges.",
            "As expected, the model achieves almost perfect score on the Inform metric on the Cam676 dataset taking the advantage of an oracle belief state signal.",
            "However, even with the perfect dialogue state tracking of the user intent, the baseline models obtain almost 30% lower score on the Inform metric on the new corpus.",
            "The addition of the attention improves the score on the Success metric on the new dataset by less than 1%.",
            "Nevertheless, as expected, the best model on MultiWOZ is still falling behind by a large margin in comparison to the results on the Cam676 corpus taking into account both Inform and Success metrics.",
            "As most of dialogues span over at least two domains, the model has to be much more effective in order to execute a successful dialogue.",
            "Moreover, the BLEU score on the MultiWOZ is lower than the one reported on the Cam676 dataset."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            1,
            1,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            [
                "Cam676",
                "MultiWOZ"
            ],
            [
                "Cam676"
            ],
            [
                "MultiWOZ"
            ],
            null,
            [
                "Cam676",
                "Inform (%)"
            ],
            [
                "Inform (%)"
            ],
            [
                "w/o attention",
                "w/ attention",
                "Success (%)"
            ],
            [
                "Cam676",
                "MultiWOZ",
                "Inform (%)",
                "Success (%)"
            ],
            null,
            [
                "Cam676",
                "MultiWOZ",
                "BLEU"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_4",
        "paper_id": "D18-1547",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1001table_1",
        "description": "We submitted the best BISON model out of the random three of Table 1 to be evaluated on the hidden test set and report results in comparison to the best model on the leaderboard,3 E3 (Zhong and Zettlemoyer, 2019) in Table 2. BISON outperforms E3 by 5.6 BLEU-4 points, while it is only slightly worse than E3 in terms of accuracy.",
        "sentences": [
            "We submitted the best BISON model out of the random three of Table 1 to be evaluated on the hidden test set and report results in comparison to the best model on the leaderboard,3 E3 (Zhong and Zettlemoyer, 2019) in Table 2.",
            "BISON outperforms E3 by 5.6 BLEU-4 points, while it is only slightly worse than E3 in terms of accuracy."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "BISON"
            ],
            [
                "BISON"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_1",
        "paper_id": "D19-1001",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1005table_1",
        "description": "Perplexity. Table 1 compares masked LM perplexity for KnowBert with BERTBASE and BERTLARGE. To rule out minor differences due to our data preparation, the BERT models are finetuned on our training data before being evaluated. Overall, KnowBert improves the masked LM perplexity, with all KnowBert models outperforming BERTLARGE, despite being derived from BERTBASE.",
        "sentences": [
            "Perplexity.",
            "Table 1 compares masked LM perplexity for KnowBert with BERTBASE and BERTLARGE.",
            "To rule out minor differences due to our data preparation, the BERT models are finetuned on our training data before being evaluated.",
            "Overall, KnowBert improves the masked LM perplexity, with all KnowBert models outperforming BERTLARGE, despite being derived from BERTBASE."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "PPL",
                "KnowBert-Wiki",
                "KnowBert-WordNet",
                "KnowBert-W+W",
                "BERTBASE",
                "BERTLARGE"
            ],
            null,
            [
                "KnowBert-Wiki",
                "KnowBert-WordNet",
                "KnowBert-W+W",
                "masked LM",
                "BERTLARGE",
                "BERTBASE"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_1",
        "paper_id": "D19-1005",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1009table_4",
        "description": "Sentence similarity . We used the SICK dataset (Marelli et al., 2014) for this task. It consists of 9841 sentence pairs that had been annotated with relatedness scores on a 5-point rating scale. We used the test split of this dataset that contains 4906 sentence pairs. The aim of this experiment was to test if disambiguated sense vectors can provide a better representation of sentences than word vectors. We used a simple method to test the two representations: it consisted of representing a sentence as the sum of the disambiguated sense vectors in one case and as the sum of word vectors in the other case. Once the sentence representations had been obtained for both methods the cosine similarity was used to measure their relatedness. The results of this experiment are reported in Table 4 as Pearson and Spearman correlation and Mean Squared Error (MSE). We used the ? configuration of our model with Chen2014 to represent senses and BERT-l-u-4 to represent words. As we can see the simplicity of the method leads to low performances for both representations, but sense vectors correlate better than word vectors.",
        "sentences": [
            "Sentence similarity .",
            "We used the SICK dataset (Marelli et al., 2014) for this task.",
            "It consists of 9841 sentence pairs that had been annotated with relatedness scores on a 5-point rating scale.",
            "We used the test split of this dataset that contains 4906 sentence pairs.",
            "The aim of this experiment was to test if disambiguated sense vectors can provide a better representation of sentences than word vectors.",
            "We used a simple method to test the two representations: it consisted of representing a sentence as the sum of the disambiguated sense vectors in one case and as the sum of word vectors in the other case.",
            "Once the sentence representations had been obtained for both methods the cosine similarity was used to measure their relatedness.",
            "The results of this experiment are reported in Table 4 as Pearson and Spearman correlation and Mean Squared Error (MSE).",
            "We used the ? configuration of our model with Chen2014 to represent senses and BERT-l-u-4 to represent words.",
            "As we can see the simplicity of the method leads to low performances for both representations, but sense vectors correlate better than word vectors."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            [
                "Pearson",
                "Spearman",
                "MSE"
            ],
            null,
            [
                "sense",
                "word"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_4",
        "paper_id": "D19-1009",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1010table_3",
        "description": "The performance of each approach that interact swith the agenda-based user simulator is shown in Table 3. GDPL achieves extremely high perfor-mance in the task success on account of the substantial improvement in inform F1 and match rateover the baselines.  Since the reward estimator of GDPL evaluates stateaction pairs,  it can always guide  the  dialog  policy  during  the  conversation thus leading the dialog policy to a successful strategy, which also indirectly demonstrates that the reward estimator has learned a reasonable reward at each dialog turn. Surprisingly, GDPL even outperforms human in completing the task,  and its average dialog turns are close to those of humans, though GDPL is inferior in terms of match rate. Humans almost manage to make a reservation in each session, which contributes to high task success.  However,  it  is  also  interesting  to  find  that human have low inform F1, and that may explain why the task is not always completed successfully. Actually, there have high recall (86.75%) but low precision  (54.43%)  in  human  dialogs  when  answering  the  requested  information.  This  is  possibly because during data collection human users forget  to  ask  for  all  required  information  of  the task, as reported in (Su et al., 2016).",
        "sentences": [
            "The performance of each approach that interact swith the agenda-based user simulator is shown in Table 3.",
            "GDPL achieves extremely high perfor-mance in the task success on account of the substantial improvement in inform F1 and match rateover the baselines.",
            " Since the reward estimator of GDPL evaluates stateaction pairs,  it can always guide  the  dialog  policy  during  the  conversation thus leading the dialog policy to a successful strategy, which also indirectly demonstrates that the reward estimator has learned a reasonable reward at each dialog turn.",
            "Surprisingly, GDPL even outperforms human in completing the task,  and its average dialog turns are close to those of humans, though GDPL is inferior in terms of match rate. Humans almost manage to make a reservation in each session, which contributes to high task success.",
            " However,  it  is  also  interesting  to  find  that human have low inform F1, and that may explain why the task is not always completed successfully.",
            "Actually, there have high recall (86.75%) but low precision  (54.43%)  in  human  dialogs  when  answering  the  requested  information.",
            " This  is  possibly because during data collection human users forget  to  ask  for  all  required  information  of  the task, as reported in (Su et al., 2016)."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "GDPL",
                "Success"
            ],
            [
                "GDPL"
            ],
            [
                "GDPL",
                "Human",
                "Match"
            ],
            [
                "Human"
            ],
            [
                "Human"
            ],
            null
        ],
        "n_sentence": 7.0,
        "table_id": "table_3",
        "paper_id": "D19-1010",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1020table_3",
        "description": "7.8 Main results on PGR . Table 3 shows the comparison with previous work on the PGR testset, where our models are significantly better than the existing models. This is likely because the previous models do not utilize all the information from inputs: BO-LSTM only takes the words (without arc labels) along the shortest dependency path between the target mentions; the pretrained weights of BioBERT are kept constant during training for relation extraction.",
        "sentences": [
            "7.8 Main results on PGR .",
            "Table 3 shows the comparison with previous work on the PGR testset, where our models are significantly better than the existing models.",
            "This is likely because the previous models do not utilize all the information from inputs: BO-LSTM only takes the words (without arc labels) along the shortest dependency path between the target mentions; the pretrained weights of BioBERT are kept constant during training for relation extraction."
        ],
        "class_sentence": [
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "EDGEWISEPS"
            ],
            [
                "BO-LSTM (Lamurias et al., 2019)\u2020",
                "BioBERT (Lee et al., 2019)\u2020"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_3",
        "paper_id": "D19-1020",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1021table_1",
        "description": "Experimental Result Analysis. Table 1 shows the experimental results, from which we can observe that:. (1) RSN models outperform all baseline models on precision, recall, and F1-score, among which Weakly-supervised RSN (SN-L+CV) achieves state-of-the-art performances. This indicates that RSN is capable of understanding new relations\u2019 semantic meanings within sentences. (2) Supervised and distantly-supervised relational representations improve clustering performances. Compared with RW-HAC, SN-HAC achieves better clustering results because of its supervised relational representation and similarity metric. Specifically, unsupervised baselines mainly use sparse one-hot features. RW-HAC uses word embeddings, but integrates them in a rule-based way. In contrast, RSN uses distributed feature representations, and can optimize information integration process according to supervision. (3) Louvain outperforms HAC for clustering with RSN, comparing SN-HAC with SN-L. One explanation is that our model does not put additional constraints on the prior distribution of relational vectors, and therefore the relation clusters might have odd shapes in violation of HAC\u2019s assumption. Moreover, when representations are not distinguishable enough, forcing HAC to find fine-grained clusters may harm recall while contributing minimally to precision. In practice, we do observe that the number of relations SN-L extracts is constantly less than the true number 16.",
        "sentences": [
            "Experimental Result Analysis.",
            "Table 1 shows the experimental results, from which we can observe that:.",
            "(1) RSN models outperform all baseline models on precision, recall, and F1-score, among which Weakly-supervised RSN (SN-L+CV) achieves state-of-the-art performances.",
            "This indicates that RSN is capable of understanding new relations\u2019 semantic meanings within sentences.",
            "(2) Supervised and distantly-supervised relational representations improve clustering performances.",
            "Compared with RW-HAC, SN-HAC achieves better clustering results because of its supervised relational representation and similarity metric.",
            "Specifically, unsupervised baselines mainly use sparse one-hot features.",
            "RW-HAC uses word embeddings, but integrates them in a rule-based way.",
            "In contrast, RSN uses distributed feature representations, and can optimize information integration process according to supervision.",
            "(3) Louvain outperforms HAC for clustering with RSN, comparing SN-HAC with SN-L.",
            "One explanation is that our model does not put additional constraints on the prior distribution of relational vectors, and therefore the relation clusters might have odd shapes in violation of HAC\u2019s assumption.",
            "Moreover, when representations are not distinguishable enough, forcing HAC to find fine-grained clusters may harm recall while contributing minimally to precision.",
            "In practice, we do observe that the number of relations SN-L extracts is constantly less than the true number 16."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            2,
            1,
            2,
            2,
            2,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "SN-HAC",
                "SN-L",
                "SN-L+V",
                "SN-L+C",
                "SN-L+CV1",
                "P",
                "R",
                "F1"
            ],
            null,
            null,
            [
                "RW-HAC",
                "SN-HAC"
            ],
            null,
            [
                "RW-HAC"
            ],
            null,
            [
                "SN-HAC",
                "SN-L"
            ],
            null,
            [
                "SN-HAC"
            ],
            [
                "SN-L"
            ]
        ],
        "n_sentence": 13.0,
        "table_id": "table_1",
        "paper_id": "D19-1021",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1022table_1",
        "description": "5.3 Results and Analysis.  5.3.1 Results on TACRED dataset . Table 1 shows the results of baseline as well as our proposed models on TACRED dataset. It is observed that our proposed knowledge-attention encoder outperforms all CNN-based and RNNbased models by at least 1.3 F1. Meanwhile, it achieves comparable results with C-GCN and selfattention encoder, which are the current start-ofthe-art single-model systems.",
        "sentences": [
            "5.3 Results and Analysis.",
            " 5.3.1 Results on TACRED dataset .",
            "Table 1 shows the results of baseline as well as our proposed models on TACRED dataset.",
            "It is observed that our proposed knowledge-attention encoder outperforms all CNN-based and RNNbased models by at least 1.3 F1.",
            "Meanwhile, it achieves comparable results with C-GCN and selfattention encoder, which are the current start-ofthe-art single-model systems."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "Knwl-attn",
                "CNN",
                "LSTM",
                "F1"
            ],
            [
                "C-GCN",
                "Self-attn"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_1",
        "paper_id": "D19-1022",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1025table_2",
        "description": "6.2 Results on Low-Resource Languages . Table 2 shows the overall performance of our proposed model as well as the baseline methods (P and R denote Precision and Recall). We can see:. Our method consistently outperforms all baselines in five languages w.r.t F1, mainly because we greatly improve recall (2.7% to 9.34% on average) by taking best advantage of WL data and being robust to noise via two modules. As for the precision, partial-CRFs perform poorly compared with CRFs due to the uncertainty of unlabeled words, while our method alleviates this issue by introducing linguistic features in non-entity sampling. An exception  occurs  in  CY,  because  it  has  the  most training data, which may bring more accurate information than sampling. Actually,  we can tune hyper-parameter non-entity ratio ? to improve precision, more studies can be found in Section 6.5. Besides, the sampling technique can utilize more prior features if available, we leave it in future.",
        "sentences": [
            "6.2 Results on Low-Resource Languages .",
            "Table 2 shows the overall performance of our proposed model as well as the baseline methods (P and R denote Precision and Recall).",
            "We can see:.",
            "Our method consistently outperforms all baselines in five languages w.r.t F1, mainly because we greatly improve recall (2.7% to 9.34% on average) by taking best advantage of WL data and being robust to noise via two modules.",
            "As for the precision, partial-CRFs perform poorly compared with CRFs due to the uncertainty of unlabeled words, while our method alleviates this issue by introducing linguistic features in non-entity sampling.",
            "An exception  occurs  in  CY,  because  it  has  the  most training data, which may bring more accurate information than sampling.",
            "Actually,  we can tune hyper-parameter non-entity ratio ? to improve precision, more studies can be found in Section 6.5.",
            "Besides, the sampling technique can utilize more prior features if available, we leave it in future."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "Ours",
                "P",
                "R"
            ],
            null,
            [
                "Ours",
                "F1"
            ],
            [
                "BiLSTM-PCRFs",
                "CNN-CRFs",
                "BiLSTM-CRFs",
                "Trans-CRFs"
            ],
            [
                "CY"
            ],
            null,
            null
        ],
        "n_sentence": 8.0,
        "table_id": "table_2",
        "paper_id": "D19-1025",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1026table_3",
        "description": "Table 3 shows the results on the five crossdomain datasets. As shown, none of existing methods can consistently win on all datasets. DCA-based models achieve state-of-the-art performance on the MSBNC and the ACE2004 dataset. On remaining datasets, DCA-RL achieves comparable performance with other complex global models. In addition, RL-based models show on average 1.1% improvement on F1 score over the SL-based models across all the crossdomain datasets. At the same time, DCA-based methods are much more efficient, both in time complexity and in resource requirement. Detailed efficiency analysis will be presented in following sections.",
        "sentences": [
            "Table 3 shows the results on the five crossdomain datasets.",
            "As shown, none of existing methods can consistently win on all datasets.",
            "DCA-based models achieve state-of-the-art performance on the MSBNC and the ACE2004 dataset.",
            "On remaining datasets, DCA-RL achieves comparable performance with other complex global models.",
            "In addition, RL-based models show on average 1.1% improvement on F1 score over the SL-based models across all the crossdomain datasets.",
            "At the same time, DCA-based methods are much more efficient, both in time complexity and in resource requirement.",
            "Detailed efficiency analysis will be presented in following sections."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "Berkeley-CNN + DCA-SL",
                "Berkeley-CNN + DCA-RL",
                "ETHZ-Attn + DCA-SL",
                "ETHZ-Attn + DCA-RL"
            ],
            [
                "Berkeley-CNN + DCA-RL",
                "ETHZ-Attn + DCA-RL"
            ],
            [
                "Berkeley-CNN + DCA-RL",
                "Berkeley-CNN + DCA-SL"
            ],
            [
                "Berkeley-CNN + DCA-SL",
                "Berkeley-CNN + DCA-RL",
                "ETHZ-Attn + DCA-SL",
                "ETHZ-Attn + DCA-RL"
            ],
            null
        ],
        "n_sentence": 7.0,
        "table_id": "table_3",
        "paper_id": "D19-1026",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1026table_4",
        "description": "2. Effect of neighbor entities. In contrast to traditional global models, we include both previously linked entities and their close neighbors for global signal. Table 4 shows the effectiveness of this strategy. We observe that incorporating these neighbor significantly improve the performance (compared to 1-hop) by introducing more related information. And our analysis shows that on average 0.72% and 3.56% relative improvement of 2-hop DCA-(SL/RL) over 1-hop DCA-(SL/RL) or baseline-SL (without DCA) is statistically significant (with P-value < 0.005). This is consistent with our design of DCA.",
        "sentences": [
            "2. Effect of neighbor entities.",
            "In contrast to traditional global models, we include both previously linked entities and their close neighbors for global signal.",
            "Table 4 shows the effectiveness of this strategy.",
            "We observe that incorporating these neighbor significantly improve the performance (compared to 1-hop) by introducing more related information.",
            "And our analysis shows that on average 0.72% and 3.56% relative improvement of 2-hop DCA-(SL/RL) over 1-hop DCA-(SL/RL) or baseline-SL (without DCA) is statistically significant (with P-value < 0.005).",
            "This is consistent with our design of DCA."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "ETHZ-Attn + 1-hop DCA"
            ],
            [
                "ETHZ-Attn + 1-hop DCA",
                "ETHZ-Attn + 2-hop DCA"
            ],
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_4",
        "paper_id": "D19-1026",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1028table_2",
        "description": "5.2 Experimental Results . Comparison with three baseline methods on Google Web 1T. Table 2 shows the performance of different bootstrapping methods on Google Web 1T. We can see that our full model outperforms three baseline methods: comparing with POS, our method achieves 41% improvement in P@100, 35% improvement in P@200 and 45% improvement in MAP; comparing with MEB, our method achieves 24% improvement in P@100 and 18% improvement in P@200; comparing with COB, our method achieves 3% improvement in both P@100 and P@200 metrics, and 2% improvement in MAP. The above findings indicate that our method can extract more correct entities with higher ranking scores than the baselines.",
        "sentences": [
            "5.2 Experimental Results .",
            "Comparison with three baseline methods on Google Web 1T.",
            "Table 2 shows the performance of different bootstrapping methods on Google Web 1T.",
            "We can see that our full model outperforms three baseline methods: comparing with POS, our method achieves 41% improvement in P@100, 35% improvement in P@200 and 45% improvement in MAP; comparing with MEB, our method achieves 24% improvement in P@100 and 18% improvement in P@200; comparing with COB, our method achieves 3% improvement in both P@100 and P@200 metrics, and 2% improvement in MAP.",
            "The above findings indicate that our method can extract more correct entities with higher ranking scores than the baselines."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "Ours full",
                "MAP",
                "MEB",
                "COB*",
                "P@100",
                "P@200"
            ],
            [
                "Ours full"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_2",
        "paper_id": "D19-1028",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1030table_7",
        "description": "Table 7 shows the results of event argument role labeling on Chinese and Arabic entity mentions automatically extracted by Stanford CoreNLP instead of manually annotated mentions. The system extracted entity mentions introduce noise and thus decrease the performance of the model, but the overall results are still promising.",
        "sentences": [
            "Table 7 shows the results of event argument role labeling on Chinese and Arabic entity mentions automatically extracted by Stanford CoreNLP instead of manually annotated mentions.",
            "The system extracted entity mentions introduce noise and thus decrease the performance of the model, but the overall results are still promising."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "Chinese",
                "Arabic"
            ],
            null
        ],
        "n_sentence": 2.0,
        "table_id": "table_7",
        "paper_id": "D19-1030",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1034table_4",
        "description": "Table 4 describes the performances of our model on the five categories on the test dataset. Our model outperforms the model described in Ju et al. (2018) and Sohrab and Miwa (2018) with F-score value on all categories.",
        "sentences": [
            "Table 4 describes the performances of our model on the five categories on the test dataset.",
            "Our model outperforms the model described in Ju et al. (2018) and Sohrab and Miwa (2018) with F-score value on all categories."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Ours",
                "Ju",
                "Soh",
                "F (%)"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_4",
        "paper_id": "D19-1034",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1034table_5",
        "description": "5.2 Performance of Boundary Detection . We conduct experiments on boundary detection to illustrate that our model extract entity boundaries more precisely comparing to Sohrab and Miwa (2018) and Ju et al. (2018). Table 5 shows the results of boundary detection on GENIA test dataset. Our model locates entities more accurately with a higher recall value (76.9%) than the comparing methods. It gives a reason why our model outperforms other state-of-the-art methods in recall value. We exploit boundary information explicitly and consider the dependencies of boundaries and entity categorical labels with a multitask loss. While in the method of Sohrab and Miwa (2018), candidate entity regions are classified individually.",
        "sentences": [
            "5.2 Performance of Boundary Detection .",
            "We conduct experiments on boundary detection to illustrate that our model extract entity boundaries more precisely comparing to Sohrab and Miwa (2018) and Ju et al. (2018).",
            "Table 5 shows the results of boundary detection on GENIA test dataset.",
            "Our model locates entities more accurately with a higher recall value (76.9%) than the comparing methods.",
            "It gives a reason why our model outperforms other state-of-the-art methods in recall value.",
            "We exploit boundary information explicitly and consider the dependencies of boundaries and entity categorical labels with a multitask loss.",
            "While in the method of Sohrab and Miwa (2018), candidate entity regions are classified individually."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "Boundary Detection"
            ],
            [
                "Our model(softmax)",
                "Ju et al. (2018)",
                "Sohrab and Miwa (2018)"
            ],
            [
                "Our model(softmax)"
            ],
            null,
            null
        ],
        "n_sentence": 7.0,
        "table_id": "table_5",
        "paper_id": "D19-1034",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1034table_7",
        "description": "5.4 Performance of Multitask Learning . Table 7 shows the performance of our pipeline model and multitask model on GENIA development set and test set. For pipeline model, we train the boundary detection module and entity categorical label prediction module separately. Our multitask model has a higher F value both in development set and test set.",
        "sentences": [
            "5.4 Performance of Multitask Learning .",
            "Table 7 shows the performance of our pipeline model and multitask model on GENIA development set and test set.",
            "For pipeline model, we train the boundary detection module and entity categorical label prediction module separately.",
            "Our multitask model has a higher F value both in development set and test set."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "Pipeline",
                "Multitask",
                "Development Set",
                "Test Set"
            ],
            [
                "Pipeline"
            ],
            [
                "Multitask",
                "F (%)",
                "Development Set",
                "Test Set"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_7",
        "paper_id": "D19-1034",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1037table_3",
        "description": "From Figures 5(a) and 5(b), we can see that the CCL based models have further improvements in terms of PR-curves compared with PCNN+ATT/ONE+SelfAtt. The P@N results in Table 3 indicate that CCL further improves the model's performance when compared to PCNN+ATT/ONE+SelfAtt as well.",
        "sentences": [
            "From Figures 5(a) and 5(b), we can see that the CCL based models have further improvements in terms of PR-curves compared with PCNN+ATT/ONE+SelfAtt.",
            "The P@N results in Table 3 indicate that CCL further improves the model's performance when compared to PCNN+ATT/ONE+SelfAtt as well."
        ],
        "class_sentence": [
            0,
            1
        ],
        "header_mention": [
            null,
            [
                "PCNN+ATT",
                "PCNN+ONE+SelAtt",
                "[NetMax+SelfAtt]+CCL-CT",
                "[NetAtt+SelfAtt]+CCL-CT"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_3",
        "paper_id": "D19-1037",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1040table_3",
        "description": "5.2 Results . Table 3 shows the performance of our models on the EntEval tasks. Our findings are detailed below: . Pretrained CWRs (ELMo, BERT) perform the best on EntEval overall, indicating that they capture knowledge about entities in contextual mentions or as entity descriptions. BERT performs poorly on entity similarity and relatedness tasks. Since this task is zero-shot, it validates the recommended setting of finetuning BERT (Devlin et al., 2018) on downstream tasks, while the embedding of the [CLS] token does not necessarily capture the semantics of the entity. BERT Large is better than BERT Base on average, showing large improvements in ERT and NED. To perform well at ERT, a model must either glean particular relationships from pairs of lengthy entity descriptions or else leverage knowledge from pretraining about the entities considered. Relatedly, performance on NED is expected to increase with both the ability to extract knowledge from descriptions and by starting with increased knowledge from pretraining. The Large model appears to be handling these capabilities better than the Base model. EntELMo improves over the EntELMo baseline (trained without the hyperlinking loss) on some tasks but suffers on others. The hyperlink-based training helps on CERP, EFP, ET, and NED. Since the hyperlink loss is closely-associated to the NED problem, it is unsurprising that NED performance is improved. Overall, we believe that hyperlink-based training benefits contextualized entity representations but does not benefit descriptive entity representations (see, for example, the drop of nearly 2 points on ESR, which is based solely on descriptive representations). This pattern may be due to the difficulty of using descriptive entity representations to reconstruct their appearing context.",
        "sentences": [
            "5.2 Results .",
            "Table 3 shows the performance of our models on the EntEval tasks.",
            "Our findings are detailed below: .",
            "Pretrained CWRs (ELMo, BERT) perform the best on EntEval overall, indicating that they capture knowledge about entities in contextual mentions or as entity descriptions.",
            "BERT performs poorly on entity similarity and relatedness tasks.",
            "Since this task is zero-shot, it validates the recommended setting of finetuning BERT (Devlin et al., 2018) on downstream tasks, while the embedding of the [CLS] token does not necessarily capture the semantics of the entity.",
            "BERT Large is better than BERT Base on average, showing large improvements in ERT and NED.",
            "To perform well at ERT, a model must either glean particular relationships from pairs of lengthy entity descriptions or else leverage knowledge from pretraining about the entities considered.",
            "Relatedly, performance on NED is expected to increase with both the ability to extract knowledge from descriptions and by starting with increased knowledge from pretraining.",
            "The Large model appears to be handling these capabilities better than the Base model.",
            "EntELMo improves over the EntELMo baseline (trained without the hyperlinking loss) on some tasks but suffers on others. The hyperlink-based training helps on CERP, EFP, ET, and NED.",
            "Since the hyperlink loss is closely-associated to the NED problem, it is unsurprising that NED performance is improved. Overall, we believe that hyperlink-based training benefits contextualized entity representations but does not benefit descriptive entity representations (see, for example, the drop of nearly 2 points on ESR, which is based solely on descriptive representations).",
            "This pattern may be due to the difficulty of using descriptive entity representations to reconstruct their appearing context."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            1,
            2,
            1,
            2,
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "ELMo",
                "BERT Base",
                "BERT Large"
            ],
            [
                "BERT Base",
                "BERT Large"
            ],
            null,
            [
                "BERT Large",
                "BERT Base",
                "ERT",
                "NED"
            ],
            [
                "ERT"
            ],
            [
                "NED"
            ],
            [
                "BERT Large",
                "BERT Base"
            ],
            [
                "CERP",
                "EFP",
                "ET",
                "NED",
                "EntELMo",
                "EntELMo baseline"
            ],
            [
                "NED",
                "ESR"
            ],
            null
        ],
        "n_sentence": 13.0,
        "table_id": "table_3",
        "paper_id": "D19-1040",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1041table_4",
        "description": "In Table 4 we further show the breakdown performances for each positive relation on TB-Dense. The breakdown on MATRES is shown in Table 10 in the appendix. BEFORE, AFTER and VAGUE are the three dominant label classes in TB-Dense. We observe that the linguistic rule-based model, CAEVO, tends to have a more evenly spread-out performance, whereas our neural network-based models are more likely to have concentrated predictions due to the imbalance of the training sample across different label classes.",
        "sentences": [
            "In Table 4 we further show the breakdown performances for each positive relation on TB-Dense.",
            "The breakdown on MATRES is shown in Table 10 in the appendix.",
            "BEFORE, AFTER and VAGUE are the three dominant label classes in TB-Dense.",
            "We observe that the linguistic rule-based model, CAEVO, tends to have a more evenly spread-out performance, whereas our neural network-based models are more likely to have concentrated predictions due to the imbalance of the training sample across different label classes."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "B",
                "A",
                "V"
            ],
            [
                "CAEVO"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_4",
        "paper_id": "D19-1041",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1043table_1",
        "description": "4.3 Results and Discussions . Table 1 reports the results of our model on different datasets comparing with the widely used text classification methods and state-of-the-art approaches. We can have the following observations. Our HCapsNet achieves the best results on 5 out of 6 datasets, which verifies the effectiveness of our model. In particular, HCapsNet outperforms vanilla capsule network Capsule-B[Yang et al., 2018] by a remarkable margin, which only utilizes the dynamic routing mechanism without hyperplane projecting.",
        "sentences": [
            "4.3 Results and Discussions .",
            "Table 1 reports the results of our model on different datasets comparing with the widely used text classification methods and state-of-the-art approaches.",
            "We can have the following observations.",
            "Our HCapsNet achieves the best results on 5 out of 6 datasets, which verifies the effectiveness of our model.",
            "In particular, HCapsNet outperforms vanilla capsule network Capsule-B[Yang et al., 2018] by a remarkable margin, which only utilizes the dynamic routing mechanism without hyperplane projecting."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "HCapsNet"
            ],
            null,
            [
                "HCapsNet"
            ],
            [
                "HCapsNet",
                "Capsule-B [Yang et al. 2018]"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_1",
        "paper_id": "D19-1043",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1056table_5",
        "description": "The bottom part of Table 5 shows the scores when restricting the evaluation to sentences with score greater than equal 10. We observed that this threshold is a good trade-off in both the amount of kept sentences (above the threshold) and average BLEU score increase (presumably sentence quality).",
        "sentences": [
            "The bottom part of Table 5 shows the scores when restricting the evaluation to sentences with score greater than equal 10.",
            "We observed that this threshold is a good trade-off in both the amount of kept sentences (above the threshold) and average BLEU score increase (presumably sentence quality)."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "XL-GloVe [greater than equal 10]",
                "XL-BERT [greater than equal 10]"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_5",
        "paper_id": "D19-1056",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1057table_5",
        "description": "Firstly, results in Table 5 show that with little dependency information (DEP), LISA performs better, while incorporating richer syntactic knowledge (DEP&REL or DEP&RELPATH), three methods achieve similar performance. Overall, RELAWE achieves best results given enough syntactic knowledge.",
        "sentences": [
            "Firstly, results in Table 5 show that with little dependency information (DEP), LISA performs better, while incorporating richer syntactic knowledge (DEP&REL or DEP&RELPATH), three methods achieve similar performance.",
            "Overall, RELAWE achieves best results given enough syntactic knowledge."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "DEP",
                "LISA",
                "DEP&REL",
                "DEP&RELPATH",
                "INPUT",
                "RELAWE"
            ],
            [
                "RELAWE"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_5",
        "paper_id": "D19-1057",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1057table_7",
        "description": "Table 7 shows that our OPEN model achieves more than 3 points of f1-score than the stateand RELAWE with DEPof-the-art PATH&RELPATH achieves in both CLOSED and OPEN settings. Notice that our best CLOSED model can almost perform as well as the state-of-the-art model while the latter utilizes pretrained word embeddings. Besides, performance gap between three models under OPEN setting is very small. It indicates that the representation ability of BERT is so powerful and may contains rich syntactic information. At last, the GOLD result is much higher than the other models, indicating that there is still large space for improvement for this task.",
        "sentences": [
            "Table 7 shows that our OPEN model achieves more than 3 points of f1-score than the stateand RELAWE with DEPof-the-art PATH&RELPATH achieves in both CLOSED and OPEN settings.",
            "Notice that our best CLOSED model can almost perform as well as the state-of-the-art model while the latter utilizes pretrained word embeddings.",
            "Besides, performance gap between three models under OPEN setting is very small.",
            "It indicates that the representation ability of BERT is so powerful and may contains rich syntactic information.",
            "At last, the GOLD result is much higher than the other models, indicating that there is still large space for improvement for this task."
        ],
        "class_sentence": [
            1,
            2,
            1,
            2,
            1
        ],
        "header_mention": [
            [
                "Open",
                "Closed",
                "RELAWE(DEPPATH&RELPATH) + BERT"
            ],
            [
                "RELAWE(DEPPATH&RELPATH) + BERT",
                "Closed"
            ],
            [
                "Open"
            ],
            null,
            [
                "GOLD"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_7",
        "paper_id": "D19-1057",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1061table_8",
        "description": "Regarding interest in the possessee, all models but the majority baseline (including logistic regression) obtain similar F1s (0.58\u20130.59). While there is certainly room for improvement, the current results lead to the conclusion that a few keywords are sufficient to obtain 0.58 F1: neither images nor word embeddings bring improvements. Temporal Anchors. Table 8 presents results obtained with the neural network when predicting temporal anchors. The image components are beneficial with all anchors, especially before (F1:0.47 vs. 0.59, +25%) and after (0.55 vs. 0.67, +22%), and to a lesser degree during (0.48 vs. 0.52; 8%). F1 scores are higher for yes label than no label across all temporal anchors.",
        "sentences": [
            "Regarding interest in the possessee, all models but the majority baseline (including logistic regression) obtain similar F1s (0.58\u20130.59).",
            "While there is certainly room for improvement, the current results lead to the conclusion that a few keywords are sufficient to obtain 0.58 F1: neither images nor word embeddings bring improvements.",
            "Temporal Anchors.",
            "Table 8 presents results obtained with the neural network when predicting temporal anchors.",
            "The image components are beneficial with all anchors, especially before (F1:0.47 vs. 0.59, +25%) and after (0.55 vs. 0.67, +22%), and to a lesser degree during (0.48 vs. 0.52; 8%).",
            "F1 scores are higher for yes label than no label across all temporal anchors."
        ],
        "class_sentence": [
            0,
            0,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            [
                "NN, only text",
                "NN, text + img",
                "Before",
                "After",
                "During"
            ],
            [
                "F1",
                "yes",
                "no"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_8",
        "paper_id": "D19-1061",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1063table_6",
        "description": "Does the proposed imitation learning algorithm achieve its goals?. The curiosity-encouraging training objective is proposed to prevent the agent from making the same mistakes at previously encountered situations. Table 6 shows that training with the curiosity-encouraging objective reduces the chance of the agent looping and making the same decisions repeatedly. As a result, its success rate is greatly boosted (+4.33% on TEST UNSEENALL) over no curiosity-encouraging.",
        "sentences": [
            "Does the proposed imitation learning algorithm achieve its goals?.",
            "The curiosity-encouraging training objective is proposed to prevent the agent from making the same mistakes at previously encountered situations.",
            "Table 6 shows that training with the curiosity-encouraging objective reduces the chance of the agent looping and making the same decisions repeatedly.",
            "As a result, its success rate is greatly boosted (+4.33% on TEST UNSEENALL) over no curiosity-encouraging."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Our model (alpha = 1)",
                "Nav. mistake repeat (%)",
                "Help-request repeat (%)"
            ],
            [
                "Our model (alpha = 1)",
                "Our model (alpha = 0)"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_6",
        "paper_id": "D19-1063",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1068table_2",
        "description": "Exploring Lexical Mapping Method. To explore our lexical mapping method, we compare the performance of several variant systems retrieving a different number of candidates (ranging from 1 to 5) and the embedding-projection method (Embedding proj). Note the system retrieving only one candidate actually takes the nearest Chinese neighbor as the word translation. The lexical mapping in it is still context-independent. Table 2 summarizes the results. From the results, we observe that even though both of CL Trans (1 cand.) and Embedding proj are content-independent mapping methods, the former outperforms the latter by a margin (+3.2% on F1). This implies that the embedding-projection method might suffer from the misalignment in the shared embedding space, and enforcing a word-to-word alignment (as in CL Trans (1 cand.)) could alleviate this problem to some extent. Retrieving more translation candidates could consistently improve Recall. But when too many candidates (e.g., 5) are added, the Precision drops, which harms the overall F1 measure.",
        "sentences": [
            "Exploring Lexical Mapping Method.",
            "To explore our lexical mapping method, we compare the performance of several variant systems retrieving a different number of candidates (ranging from 1 to 5) and the embedding-projection method (Embedding proj).",
            "Note the system retrieving only one candidate actually takes the nearest Chinese neighbor as the word translation.",
            "The lexical mapping in it is still context-independent.",
            "Table 2 summarizes the results.",
            "From the results, we observe that even though both of CL Trans (1 cand.) and Embedding proj are content-independent mapping methods, the former outperforms the latter by a margin (+3.2% on F1).",
            "This implies that the embedding-projection method might suffer from the misalignment in the shared embedding space, and enforcing a word-to-word alignment (as in CL Trans (1 cand.)) could alleviate this problem to some extent.",
            "Retrieving more translation candidates could consistently improve Recall.",
            "But when too many candidates (e.g., 5) are added, the Precision drops, which harms the overall F1 measure."
        ],
        "class_sentence": [
            2,
            1,
            2,
            2,
            1,
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "CL Trans (1 cand.)",
                "CL Trans (2 cand.)",
                "CL Trans (3 cand.)",
                "CL Trans (4 cand.)",
                "CL Trans (5 cand.)"
            ],
            null,
            null,
            null,
            [
                "CL Trans (1 cand.)",
                "F1"
            ],
            [
                "CL Trans (1 cand.)"
            ],
            [
                "Rec."
            ],
            [
                "CL Trans (5 cand.)",
                "Pre.",
                "F1"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_2",
        "paper_id": "D19-1068",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1070table_2",
        "description": "Table 2 compares the performance of the discussed models against the baselines, evaluating per-step entity prediction performance. Using the ground truth about ingredient's state, we also report the uncombined (UR) and combined (CR) recalls, which are per-timestep ingredient recall distinguished by whether the ingredient is explicitly mentioned (uncombined) or part of a mixture (combined). Note that Exact Match and First Occ baselines represent high-precision and high-recall regimes for this task, respectively.",
        "sentences": [
            "Table 2 compares the performance of the discussed models against the baselines, evaluating per-step entity prediction performance.",
            "Using the ground truth about ingredient's state, we also report the uncombined (UR) and combined (CR) recalls, which are per-timestep ingredient recall distinguished by whether the ingredient is explicitly mentioned (uncombined) or part of a mixture (combined).",
            "Note that Exact Match and First Occ baselines represent high-precision and high-recall regimes for this task, respectively."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "UR",
                "CR"
            ],
            [
                "Exact Match",
                "First Occ",
                "P",
                "R"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_2",
        "paper_id": "D19-1070",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1070table_9",
        "description": "Table 9 presents these ablation studies. We only observe a minor performance drop from 84.59 to 82.71 (accuracy) when other ingredients are removed entirely. Removing verbs dropped the performance to 79.08 and further omitting both leads to 77.79. This shows the models dependence on verb semantics over tracking the other ingredients.",
        "sentences": [
            "Table 9 presents these ablation studies.",
            "We only observe a minor performance drop from 84.59 to 82.71 (accuracy) when other ingredients are removed entirely.",
            "Removing verbs dropped the performance to 79.08 and further omitting both leads to 77.79.",
            "This shows the models dependence on verb semantics over tracking the other ingredients."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Accuracy",
                "w/o Other Ingredients"
            ],
            [
                "Accuracy",
                "w/o Verbs",
                "w/o Verbs & Other Ingredients"
            ],
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_9",
        "paper_id": "D19-1070",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1076table_1",
        "description": "4.3 Results. Table 1 shows the average of precision over the 10 random splits @k = 1, 5 and 10. The bold values are statistically significant results between LLMap and the MUSE supervised method. The RGP column refers to our model without the piecewise mapping, which we discuss later in this section. In all cases, except Czech (CS) and Bulgarian (BG) @1 where MUSE has a slight edge over LLMap, our method achieved higher precision on average over 10-fold cross-validation than the MUSE algorithm. In the majority of the cases the improvements are statistically significant. We can see the most significant improvements (over 8%) are observed in Japanese (JA) language and Chinese (ZH). The other languages mostly see between 1%-3% improvement in the precision. The average gain in precision @10 sits at 3.7%.",
        "sentences": [
            "4.3 Results.",
            "Table 1 shows the average of precision over the 10 random splits @k = 1, 5 and 10.",
            "The bold values are statistically significant results between LLMap and the MUSE supervised method.",
            "The RGP column refers to our model without the piecewise mapping, which we discuss later in this section.",
            "In all cases, except Czech (CS) and Bulgarian (BG) @1 where MUSE has a slight edge over LLMap, our method achieved higher precision on average over 10-fold cross-validation than the MUSE algorithm.",
            "In the majority of the cases the improvements are statistically significant. We can see the most significant improvements (over 8%) are observed in Japanese (JA) language and Chinese (ZH).",
            "The other languages mostly see between 1%-3% improvement in the precision.",
            "The average gain in precision @10 sits at 3.7%."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "P@1",
                "P@5",
                "P@10"
            ],
            [
                "LLMap",
                "MUSE"
            ],
            [
                "RGP"
            ],
            [
                "LLMap",
                "MUSE",
                "Language",
                "Czech (CS)",
                "Bulgarian (BG)"
            ],
            [
                "Japanese (JA)",
                "Chinese (ZH)"
            ],
            [
                "Language"
            ],
            [
                "P@10"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_1",
        "paper_id": "D19-1076",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1076table_2",
        "description": "Table 2 shows the precision@5 for the pre-split dictionaries. In all 14 languages the LLMap outperforms the MUSE algorithm for recovering both all senses and any sense of a word with significant gains in Chinese and Japanese. These results are consistent with the more comprehensive crossvalidation settings. Note that the any sense recovery is on average higher than all senses, pointing to the same fact the model is better at creating a better neighborhood around words where at least one sense of a word can be recovered.",
        "sentences": [
            "Table 2 shows the precision@5 for the pre-split dictionaries.",
            "In all 14 languages the LLMap outperforms the MUSE algorithm for recovering both all senses and any sense of a word with significant gains in Chinese and Japanese.",
            "These results are consistent with the more comprehensive crossvalidation settings.",
            "Note that the any sense recovery is on average higher than all senses, pointing to the same fact the model is better at creating a better neighborhood around words where at least one sense of a word can be recovered."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "LLMap",
                "All Senses",
                "Any Sense",
                "ZH",
                "JA"
            ],
            null,
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "D19-1076",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1081table_6",
        "description": "6.1 The amount of training data . Table 6 provides BLEU and consistency scores for the DocRepair model trained on different amount of data. We see that even when using a dataset of moderate size (e.g., 5m fragments) we can achieve performance comparable to the model trained on a large amount of data (30m fragments). Moreover, we notice that deixis scores are less sensitive to the amount of training data than lexical cohesion and ellipsis scores. The reason might be that, as we observed in our previous work (Voita et al., 2019), inconsistencies in translations due to the presence of deictic words and phrases are more frequent in this dataset than other types of inconsistencies. Also, as we show in Section 7, this is the phenomenon the model learns faster in training.",
        "sentences": [
            "6.1 The amount of training data .",
            "Table 6 provides BLEU and consistency scores for the DocRepair model trained on different amount of data.",
            "We see that even when using a dataset of moderate size (e.g., 5m fragments) we can achieve performance comparable to the model trained on a large amount of data (30m fragments).",
            "Moreover, we notice that deixis scores are less sensitive to the amount of training data than lexical cohesion and ellipsis scores.",
            "The reason might be that, as we observed in our previous work (Voita et al., 2019), inconsistencies in translations due to the presence of deictic words and phrases are more frequent in this dataset than other types of inconsistencies.",
            "Also, as we show in Section 7, this is the phenomenon the model learns faster in training."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "BLEU"
            ],
            [
                "5m",
                "30m"
            ],
            [
                "deixis",
                "lex. c.",
                "ellipsis"
            ],
            null,
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_6",
        "paper_id": "D19-1081",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1083table_4",
        "description": "Surprisingly, training deep Transformers with both DS-Init and MAtt improves not only running efficiency but also translation quality (by 0.2 BLEU), compared with DS-Init alone. To get an improved understanding, we analyze model performance on both training and development set. Results in Table 4 show that models with DS-Init yield the best perplexity on both training and development set, and those with T2T achieve the best BLEU on the training set. However, DSInit+MAtt performs best in terms of BLEU on the development set. This indicates that the success of DS-Init+MAtt comes from its better generalization rather than better fitting training data.",
        "sentences": [
            "Surprisingly, training deep Transformers with both DS-Init and MAtt improves not only running efficiency but also translation quality (by 0.2 BLEU), compared with DS-Init alone.",
            "To get an improved understanding, we analyze model performance on both training and development set.",
            "Results in Table 4 show that models with DS-Init yield the best perplexity on both training and development set, and those with T2T achieve the best BLEU on the training set.",
            "However, DSInit+MAtt performs best in terms of BLEU on the development set.",
            "This indicates that the success of DS-Init+MAtt comes from its better generalization rather than better fitting training data."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "15",
                "BLEU",
                "13"
            ],
            null,
            [
                "13",
                "PPL",
                "Train",
                "Dev",
                "BLEU",
                "12",
                "14"
            ],
            [
                "15",
                "BLEU"
            ],
            [
                "15"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_4",
        "paper_id": "D19-1083",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1084table_4",
        "description": "5.1 Results & Analysis. Table 4 shows that while NER systems trained on projected data do categorically worse than an NER system trained on gold-standard data, the higherquality alignments obtained from DiscAlign lead to a major improvement in F1 when compared to FastAlign.",
        "sentences": [
            "5.1 Results & Analysis.",
            "Table 4 shows that while NER systems trained on projected data do categorically worse than an NER system trained on gold-standard data, the higherquality alignments obtained from DiscAlign lead to a major improvement in F1 when compared to FastAlign."
        ],
        "class_sentence": [
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "DiscAlign",
                "FastAlign"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_4",
        "paper_id": "D19-1084",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1085table_3",
        "description": "4.3 Results on Japanese->English Task. Table 3 lists the results. We compare our models and the best external ZP prediction approach. As seen, our models also significantly improve translation performance, demonstrating the effectiveness and universality of the proposed approach. This improvement on Japanese->English translation is lower than that on Chinese->English, showing that ZP prediction and translation are more challenging for Japanese. The reason may be two folds: 1) Japanese language has a larger number of pronoun variations borrowed from archaism, which leads to more difficulties in learning ZPs; 2) Japanese language is subject-objectverb (SOV) while English has subject-verb-object (SVO) structure, and this poses difficulties for ZP annotation via alignment method.",
        "sentences": [
            "4.3 Results on Japanese->English Task.",
            "Table 3 lists the results.",
            "We compare our models and the best external ZP prediction approach.",
            "As seen, our models also significantly improve translation performance, demonstrating the effectiveness and universality of the proposed approach.",
            "This improvement on Japanese->English translation is lower than that on Chinese->English, showing that ZP prediction and translation are more challenging for Japanese.",
            "The reason may be two folds: 1) Japanese language has a larger number of pronoun variations borrowed from archaism, which leads to more difficulties in learning ZPs; 2) Japanese language is subject-objectverb (SOV) while English has subject-verb-object (SVO) structure, and this poses difficulties for ZP annotation via alignment method."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "Joint Model",
                "+ Discourse-Level Context",
                "External ZP Prediction"
            ],
            [
                "Joint Model",
                "+ Discourse-Level Context"
            ],
            null,
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_3",
        "paper_id": "D19-1085",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1092table_4",
        "description": "5.5 Comparison with Previous Work . We compare our method with previous work in the literature. Table 4 shows the results, where the UAS values are reported. Our model denoted by This refers to the model of Src + Mix. Note that these models are not directly comparable due to the setting and baseline parser differences. The first block shows several models by directly transferring gold-standard source treebank knowledge into the target side, including the models of Guo15 (Guo et al., 2015), Guo16 (Guo et al., 2016b) and TA16 (Tiedemann and Agic\u00b4, 2016). Our model gives the best performance with one exception on the German language. One possible reason may be that TA16 has exploited multiple sources of treebanks besides English.",
        "sentences": [
            "5.5 Comparison with Previous Work .",
            "We compare our method with previous work in the literature.",
            "Table 4 shows the results, where the UAS values are reported.",
            "Our model denoted by This refers to the model of Src + Mix.",
            "Note that these models are not directly comparable due to the setting and baseline parser differences.",
            "The first block shows several models by directly transferring gold-standard source treebank knowledge into the target side, including the models of Guo15 (Guo et al., 2015), Guo16 (Guo et al., 2016b) and TA16 (Tiedemann and Agic\u00b4, 2016).",
            "Our model gives the best performance with one exception on the German language.",
            "One possible reason may be that TA16 has exploited multiple sources of treebanks besides English."
        ],
        "class_sentence": [
            2,
            2,
            1,
            2,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "This"
            ],
            null,
            [
                "Guo15",
                "Guo16",
                "TA16"
            ],
            [
                "This",
                " DE"
            ],
            [
                " DE"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_4",
        "paper_id": "D19-1092",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1093table_2",
        "description": "Results on English Penn Treebank. Table 2 presents the results on English Penn Treebank. StackPtr (paper) refer to the results reported by Ma et al.(2018), and StackPtr (code) is our run of their code in identical settings as ours. Our model H-PtrNet-PST (Gate) outperforms the baseline by 0.09 and 0.08 in terms of UAS and LAS, respectively. Performance of H-PtrNet-PST (SGate) is close to that of H-PtrNet-PST (Gate), though we see slight improvement. We also test H-PtrNetPS (Gate), the model with parent and sibling connections only, which further improves the performance to 96.09 and 95.03 in UAS and LAS.",
        "sentences": [
            "Results on English Penn Treebank.",
            "Table 2 presents the results on English Penn Treebank.",
            "StackPtr (paper) refer to the results reported by Ma et al.(2018), and StackPtr (code) is our run of their code in identical settings as ours.",
            "Our model H-PtrNet-PST (Gate) outperforms the baseline by 0.09 and 0.08 in terms of UAS and LAS, respectively.",
            "Performance of H-PtrNet-PST (SGate) is close to that of H-PtrNet-PST (Gate), though we see slight improvement.",
            "We also test H-PtrNetPS (Gate), the model with parent and sibling connections only, which further improves the performance to 96.09 and 95.03 in UAS and LAS."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "StackPtr (paper)",
                "StackPtr (code)"
            ],
            [
                " H-PtrNet-PST (Gate)",
                "UAS",
                " LAS"
            ],
            [
                " H-PtrNet-PST (SGate)",
                " H-PtrNet-PST (Gate)"
            ],
            [
                " H-PtrNet-PS (Gate)",
                "UAS",
                " LAS"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_2",
        "paper_id": "D19-1093",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1094table_4",
        "description": "Table 4 presents the results of our experiments (without ELMo) on Chinese, German, and Spanish. Although we have not performed detailed parameter selection in these languages (i.e., we used the same parameters as in English), our model achieves state-of-the-art performance across all three languages.",
        "sentences": [
            "Table 4 presents the results of our experiments (without ELMo) on Chinese, German, and Spanish.",
            "Although we have not performed detailed parameter selection in these languages (i.e., we used the same parameters as in English), our model achieves state-of-the-art performance across all three languages."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "Chinese",
                "German",
                "Spanish"
            ],
            [
                "Ours (supervised training)",
                "Ours (with CVT)",
                "Ours(with CVT)",
                "Chinese",
                "German",
                "Spanish"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_4",
        "paper_id": "D19-1094",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1102table_3",
        "description": "We employ two baselines: a monolingual model (\u0e22\u0e073.1) and a cross-lingual model (\u0e22\u0e072.3), both without data augmentation. The monolingual model acts as a simple baseline, to resemble a situation when the target treebank does not have any source treebank (i.e., no available treebanks from related languages). The cross-lingual model serves as a strong baseline, simulating a case when there is a source treebank. We compare both baselines to models trained with MORPH and NONCE augmentation methods. Table 3 reports our results. We see that the cross-lingual training (cross-base) performs better than monolingual models even with augmentation. For the T10 setting, cross-base achieves almost twice as much as the monolingual baseline (mono-base). The benefits of data augmentation are less evident in the cross-lingual setting, but in the T10 scenario, data augmentation still clearly helps. Overall, cross-lingual combined with data augmentation yields the best result.",
        "sentences": [
            "We employ two baselines: a monolingual model (\u0e22\u0e073.1) and a cross-lingual model (\u0e22\u0e072.3), both without data augmentation.",
            "The monolingual model acts as a simple baseline, to resemble a situation when the target treebank does not have any source treebank (i.e., no available treebanks from related languages).",
            "The cross-lingual model serves as a strong baseline, simulating a case when there is a source treebank.",
            "We compare both baselines to models trained with MORPH and NONCE augmentation methods.",
            "Table 3 reports our results.",
            "We see that the cross-lingual training (cross-base) performs better than monolingual models even with augmentation.",
            "For the T10 setting, cross-base achieves almost twice as much as the monolingual baseline (mono-base).",
            "The benefits of data augmentation are less evident in the cross-lingual setting, but in the T10 scenario, data augmentation still clearly helps.",
            "Overall, cross-lingual combined with data augmentation yields the best result."
        ],
        "class_sentence": [
            1,
            2,
            2,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "mono-base",
                "cross-base"
            ],
            [
                "mono-base"
            ],
            [
                "cross-base"
            ],
            [
                "mono-base",
                "cross-base",
                "Morph",
                "Nonce"
            ],
            null,
            [
                "CROSS-LINGUAL",
                "MONOLINGUAL"
            ],
            [
                "T10",
                "CROSS-LINGUAL",
                "cross-base",
                "MONOLINGUAL",
                "mono-base"
            ],
            [
                "T10",
                "CROSS-LINGUAL",
                "Morph",
                "Nonce"
            ],
            [
                "CROSS-LINGUAL",
                "Morph",
                "Nonce"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_3",
        "paper_id": "D19-1102",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1102table_7",
        "description": "5.1 Experimental results . Table 7 reports the LAS performance on the development sets. MORPH augmentation improves performance over the zero-shot baseline and achieves comparable or better LAS with a cross-lingual model trained with pre-trained word embeddings. Next, we look at the effects of transliteration (see Kazakh vs Kazakh (translit.) in Table 7). In the zero-shot experiments, simply mapping both Turkish and Kazakh characters to the Latin alphabet improves accuracy from 12.5 to 21.2 LAS. Cross-lingual training with MORPH further improves performance to 36.7 LAS.",
        "sentences": [
            "5.1 Experimental results .",
            "Table 7 reports the LAS performance on the development sets.",
            "MORPH augmentation improves performance over the zero-shot baseline and achieves comparable or better LAS with a cross-lingual model trained with pre-trained word embeddings.",
            "Next, we look at the effects of transliteration (see Kazakh vs Kazakh (translit.) in Table 7).",
            "In the zero-shot experiments, simply mapping both Turkish and Kazakh characters to the Latin alphabet improves accuracy from 12.5 to 21.2 LAS.",
            "Cross-lingual training with MORPH further improves performance to 36.7 LAS."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                " +Morph",
                "zero-shot",
                "CROSS-LINGUAL"
            ],
            [
                "Kazakh",
                "Kazakh (translit.)"
            ],
            [
                "zero-shot"
            ],
            [
                "CROSS-LINGUAL",
                " +Morph"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_7",
        "paper_id": "D19-1102",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1109table_2",
        "description": "Table 2 shows the full results. Our unsupervised approach achieves a test set F1 score of 78.8, comparable to the 79.4 F1 score found by the supervised prototypical approach. The Factorized and DNN models significantly outperformed our approach with F1 scores of 89.2 and 89.0, respectively. Our grid search found an optimal \u00ce\u00bb value of 1.65 for the Concatenation sentence generation model and 1.55 for the Coherency Ranking model. The Template and Template + Grammar methods found lambda values of 1.20 and 0.95, respectively.",
        "sentences": [
            "Table 2 shows the full results.",
            "Our unsupervised approach achieves a test set F1 score of 78.8, comparable to the 79.4 F1 score found by the supervised prototypical approach.",
            "The Factorized and DNN models significantly outperformed our approach with F1 scores of 89.2 and 89.0, respectively.",
            "Our grid search found an optimal \u00ce\u00bb value of 1.65 for the Concatenation sentence generation model and 1.55 for the Coherency Ranking model.",
            "The Template and Template + Grammar methods found lambda values of 1.20 and 0.95, respectively."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "Unsupervised",
                "COHERENCY RANK",
                "PROTOTYPICAL"
            ],
            [
                "FACTORIZED",
                "DNN"
            ],
            null,
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_2",
        "paper_id": "D19-1109",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1112table_1",
        "description": "3.1 Results . We first use the three meta-learning algorithms with PPS sampling and present in Table 1 the experimental results on the GLUE test set. Generally, the meta-learning algorithms achieve better performance than the strong baseline models, with Reptile performing the best. Since the MT-DNN also uses PPS sampling, the improvements suggest meta-learning algorithms can indeed learn better representations compared with multi-task learning. Reptile outperforming MAML indicates that reptile is a more effective and efficient algorithm compared with MAML in our setting.",
        "sentences": [
            "3.1 Results .",
            "We first use the three meta-learning algorithms with PPS sampling and present in Table 1 the experimental results on the GLUE test set.",
            "Generally, the meta-learning algorithms achieve better performance than the strong baseline models, with Reptile performing the best.",
            "Since the MT-DNN also uses PPS sampling, the improvements suggest meta-learning algorithms can indeed learn better representations compared with multi-task learning.",
            "Reptile outperforming MAML indicates that reptile is a more effective and efficient algorithm compared with MAML in our setting."
        ],
        "class_sentence": [
            0,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Reptile"
            ],
            [
                "MT-DNN"
            ],
            [
                "Reptile",
                "MAML"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_1",
        "paper_id": "D19-1112",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1114table_1",
        "description": "SSE (Nie and Bansal, 2017) is a stacked BiLSTM model with shortcut connections and finetuning of word embeddings. Unlike our setting, where each word is represented by its own hidden state in the final output layer, SSE applies maxpooling over time to the output of the last BiLSTM layer to extract the final sentence feature vector. Based on Table 1, mPWIMseq clearly outperforms SSE on Twitter, PIT-2015, STS-2014, WikiQA, and TrecQA. However, for the SNLI and Quora datasets, SSE slightly exceeds mPWIM by 0.4% and 1.6%, respectively. SNLI and Quora have the largest training data among all the datasets with 550k and 393k training sentence pairs, respectively, which suggests that SSE performs better on larger data beyond a certain threshold. We surmise that as the dataset increases in size, the simplicity of SSE will have more performance advantages.",
        "sentences": [
            "SSE (Nie and Bansal, 2017) is a stacked BiLSTM model with shortcut connections and finetuning of word embeddings.",
            "Unlike our setting, where each word is represented by its own hidden state in the final output layer, SSE applies maxpooling over time to the output of the last BiLSTM layer to extract the final sentence feature vector.",
            "Based on Table 1, mPWIMseq clearly outperforms SSE on Twitter, PIT-2015, STS-2014, WikiQA, and TrecQA.",
            "However, for the SNLI and Quora datasets, SSE slightly exceeds mPWIM by 0.4% and 1.6%, respectively.",
            "SNLI and Quora have the largest training data among all the datasets with 550k and 393k training sentence pairs, respectively, which suggests that SSE performs better on larger data beyond a certain threshold.",
            "We surmise that as the dataset increases in size, the simplicity of SSE will have more performance advantages."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            [
                "SSE"
            ],
            [
                "SSE"
            ],
            [
                "mPWIMseq",
                "SSE",
                "Twitter",
                "PIT-2015",
                "STS-2014",
                "WikiQA",
                "TrecQA"
            ],
            [
                "SSE",
                "mPWIMseq",
                "SNLI",
                "Quora"
            ],
            [
                "SNLI",
                "Quora",
                "SSE"
            ],
            [
                "SSE"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_1",
        "paper_id": "D19-1114",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1122table_7",
        "description": "Because the two subsections \u00e2\u20ac\u201c AMT and GCS \u00e2\u20ac\u201c are slightly different despite being obtained using the same questionnaire (see Table 3), we test whether this difference in\u00ef\u00ac\u201auences the relation extraction models. We evaluate models trained using training data from one source and tested using data from the other source. A robust model should be able to detect and extract the targeted relations even when they appear in sentences of different structure and complexity. This would be re\u00ef\u00ac\u201aected in close results on its own (same as training) or the other subset\u00e2\u20ac\u2122s test data. Table 7 shows the results in terms of accuracy for the various experimental set-ups. The results reflect the difference between the two subsets: the results on the GCS data fluctuate more (between 0.79 and 0.94 accuracy) when the AMT or the GCS data is used for training, while AMT is rather stable (0.86 \u2013 0.89 accuracy). Using all available training data leads to best results on both test subsets, for both simple and complex sentences.",
        "sentences": [
            "Because the two subsections \u00e2\u20ac\u201c AMT and GCS \u00e2\u20ac\u201c are slightly different despite being obtained using the same questionnaire (see Table 3), we test whether this difference in\u00ef\u00ac\u201auences the relation extraction models.",
            "We evaluate models trained using training data from one source and tested using data from the other source.",
            "A robust model should be able to detect and extract the targeted relations even when they appear in sentences of different structure and complexity.",
            "This would be re\u00ef\u00ac\u201aected in close results on its own (same as training) or the other subset\u00e2\u20ac\u2122s test data.",
            "Table 7 shows the results in terms of accuracy for the various experimental set-ups.",
            "The results reflect the difference between the two subsets: the results on the GCS data fluctuate more (between 0.79 and 0.94 accuracy) when the AMT or the GCS data is used for training, while AMT is rather stable (0.86 \u2013 0.89 accuracy).",
            "Using all available training data leads to best results on both test subsets, for both simple and complex sentences."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null,
            [
                " GCS training data AMT",
                " GCS training data GCS",
                " GCS training data AMT + GCS",
                " AMT training data AMT",
                " AMT training data GCS",
                " AMT training data AMT + GCS"
            ],
            [
                " Simple",
                " Complex",
                " AMT & GCS training data AMT + GCS"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_7",
        "paper_id": "D19-1122",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1126table_2",
        "description": "3.2 Main Results. Table 2 show the F1 score of slot filling performance comparison results on ATIS dataset. The results show that ProgModel consistently outperforms AttRNN, where the improvement gain is up to 4.24% in ATIS. As expected, ProgModel continuously improves performance with moreand more new batches of training data, even though it is only trained on new data at each batch. Among all competitors, FT-Cp-AttRNN achieves the closest performance to ProgModel by using much larger model size (shown in Section 3.4). In comparison, both FT-AttRNN and FT-Lr-AttRNN frequently suffer from catastrophic forgetting. The values in pink show that the performance of FTAttRNN and FT-Cp-AttRNN drops up to 3.82% and 5.38% respectively. As a result, their F1 scores are significantly reduced in the end. At last, we observe that ProgModel is quite close to upper bound performance (Note that this is only for reference rather than comparison since upper bound performance assumes the availability of all training data while ProgModel does not).",
        "sentences": [
            "3.2 Main Results.",
            "Table 2 show the F1 score of slot filling performance comparison results on ATIS dataset.",
            "The results show that ProgModel consistently outperforms AttRNN, where the improvement gain is up to 4.24% in ATIS.",
            "As expected, ProgModel continuously improves performance with moreand more new batches of training data, even though it is only trained on new data at each batch.",
            "Among all competitors, FT-Cp-AttRNN achieves the closest performance to ProgModel by using much larger model size (shown in Section 3.4).",
            "In comparison, both FT-AttRNN and FT-Lr-AttRNN frequently suffer from catastrophic forgetting.",
            "The values in pink show that the performance of FTAttRNN and FT-Cp-AttRNN drops up to 3.82% and 5.38% respectively.",
            "As a result, their F1 scores are significantly reduced in the end.",
            "At last, we observe that ProgModel is quite close to upper bound performance (Note that this is only for reference rather than comparison since upper bound performance assumes the availability of all training data while ProgModel does not)."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "t-ProgModel",
                "c-ProgModel",
                "AttRNN (upper bound)",
                " Batch 0"
            ],
            [
                "t-ProgModel",
                "c-ProgModel",
                " Batch 0",
                "Batch 1",
                "Batch 2",
                "Batch 3",
                "Batch 4"
            ],
            [
                "FT-Cp-AttRNN",
                "t-ProgModel",
                "c-ProgModel"
            ],
            [
                "FT-AttRNN",
                "FT-Lr-AttRNN"
            ],
            [
                "FT-Cp-AttRNN"
            ],
            [
                "FT-Cp-AttRNN",
                " Batch 0",
                "Batch 1",
                "Batch 2",
                "Batch 3",
                "Batch 4"
            ],
            [
                "t-ProgModel",
                "c-ProgModel",
                "AttRNN (upper bound)"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_2",
        "paper_id": "D19-1126",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1131table_2",
        "description": "4 Results . 4.1 Results with oos-train . Table 2 presents results for all models across the four variations of the dataset. First, BERT is consistently the best approach for in-scope, followed by MLP. Second, out-of-scope query performance is much lower than in-scope across all methods. Training on less data (Small and Imbalanced) yields models that perform slightly worse on in-scope queries. The trend is mostly the opposite when evaluating out-of-scope, where recall increases under the Small and Imbalanced training conditions. Under these two conditions, the size of the in-scope training set was decreased, while the number of out-of-scope training queries remained constant. This indicates that out-of-scope performance can be increased by increasing the relative number of out-of-scope training queries. We do just that in the OOS+ setting\u2014where the models were trained on the full training set as well as 150 additional out-of-scope queries\u2014and see that performance on out-of-scope increases substantially, yet still remains low relative to in-scope accuracy.",
        "sentences": [
            "4 Results .",
            "4.1 Results with oos-train .",
            "Table 2 presents results for all models across the four variations of the dataset.",
            "First, BERT is consistently the best approach for in-scope, followed by MLP.",
            "Second, out-of-scope query performance is much lower than in-scope across all methods.",
            "Training on less data (Small and Imbalanced) yields models that perform slightly worse on in-scope queries.",
            "The trend is mostly the opposite when evaluating out-of-scope, where recall increases under the Small and Imbalanced training conditions.",
            "Under these two conditions, the size of the in-scope training set was decreased, while the number of out-of-scope training queries remained constant.",
            "This indicates that out-of-scope performance can be increased by increasing the relative number of out-of-scope training queries.",
            "We do just that in the OOS+ setting\u2014where the models were trained on the full training set as well as 150 additional out-of-scope queries\u2014and see that performance on out-of-scope increases substantially, yet still remains low relative to in-scope accuracy."
        ],
        "class_sentence": [
            0,
            2,
            1,
            1,
            1,
            1,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "oos-train BERT",
                "oos-threshold BERT"
            ],
            [
                "Out-Of-Scope Recall",
                "In-Scope Accuracy"
            ],
            [
                "In-Scope Accuracy",
                "Small",
                " Imbal"
            ],
            [
                "Out-Of-Scope Recall",
                "Small",
                " Imbal"
            ],
            null,
            null,
            null
        ],
        "n_sentence": 10.0,
        "table_id": "table_2",
        "paper_id": "D19-1131",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1131table_3",
        "description": "4.3 Results with oos-binary. Table 3 compares classifier performance using the oos-binary scheme. In-scope accuracy suffers for all models using the undersampling scheme when compared to training on the full dataset using the oos-train and oos-threshold approaches shown in Table 2. However, out-of-scope recall improves compared to oos-train on Full but not OOS+. Augmenting the out-of-scope training set appears to help improve both in-scope and out-of-scope performance compared to undersampling, but outof-scope performance remains weak.",
        "sentences": [
            "4.3 Results with oos-binary.",
            "Table 3 compares classifier performance using the oos-binary scheme.",
            "In-scope accuracy suffers for all models using the undersampling scheme when compared to training on the full dataset using the oos-train and oos-threshold approaches shown in Table 2.",
            "However, out-of-scope recall improves compared to oos-train on Full but not OOS+.",
            "Augmenting the out-of-scope training set appears to help improve both in-scope and out-of-scope performance compared to undersampling, but outof-scope performance remains weak."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "In-Scope Accuracy",
                "Out-of-Scope Recall"
            ],
            [
                "Out-of-Scope Recall"
            ],
            [
                "Out-of-Scope Recall",
                "wiki aug"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_3",
        "paper_id": "D19-1131",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1132table_1",
        "description": "5 Results and Analysis . Automatic Results: Table 1 shows that all data augmentation approaches (last 3 rows) improve statistically significantly (p < 0.01) 10 over the strongest baseline VHRED (w/ attention). Moreover, our input-agnostic AutoAugment is statistically significantly (p < 0.01) better (on Activity and Entity F1) than the strong manual-policy Alloperations model, while the input-aware model is stat.signif.(p < 0.01) better on Activity F1.11 .",
        "sentences": [
            "5 Results and Analysis .",
            "Automatic Results: Table 1 shows that all data augmentation approaches (last 3 rows) improve statistically significantly (p < 0.01) 10 over the strongest baseline VHRED (w/ attention).",
            "Moreover, our input-agnostic AutoAugment is statistically significantly (p < 0.01) better (on Activity and Entity F1) than the strong manual-policy Alloperations model, while the input-aware model is stat.signif.(p < 0.01) better on Activity F1.11 ."
        ],
        "class_sentence": [
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "All-operations",
                "Input-aware",
                "Input-agnostic",
                "VHRED (w/ attn.)"
            ],
            [
                "Input-agnostic",
                "Activity F1",
                "Entity F1"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_1",
        "paper_id": "D19-1132",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1140table_2",
        "description": "4 Results and Discussion. Table 2 shows our classification results, presenting the F1 scores obtained by the different MT-based approaches in the two training conditions. When NMT is trained on 100% of the parallel data, for both languages Reinforce produces translations that lead to classification improvements over those produced by the Generic model (+0.5 De-En, +0.8 It-En). Although the scores are considerably better than those obtained by the Original classifiers (+9.3 De-En, +7.2 It-En), the gap with respect to the English classifier is still quite large (-1.4 De-En and -2.3 It-En).",
        "sentences": [
            "4 Results and Discussion.",
            "Table 2 shows our classification results, presenting the F1 scores obtained by the different MT-based approaches in the two training conditions.",
            "When NMT is trained on 100% of the parallel data, for both languages Reinforce produces translations that lead to classification improvements over those produced by the Generic model (+0.5 De-En, +0.8 It-En).",
            "Although the scores are considerably better than those obtained by the Original classifiers (+9.3 De-En, +7.2 It-En), the gap with respect to the English classifier is still quite large (-1.4 De-En and -2.3 It-En)."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "100%",
                "Generic",
                "Reinforce"
            ],
            [
                "Original",
                "English"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "D19-1140",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1151table_2",
        "description": "Comparison to Prior Work: Table 2 shows the performance of previous models trained on the same data. For Arabic, both A-TCN and BiLSTM provide significantly better performance than MADAMIRA (Pasha et al., 2014), which is a morphological disambiguation tool for Arabic. The performance of Zalmout and Habash (2017)\u2019s model falls in between BiLSTM and ATCN. As opposed to our character-based models, both previous models use other morphological features along with a language model to rank all possible diacritic choices. We believe that this additional semantic and morphological information help their models perform better on OOV words. For Vietnamese, when we re-train Naplava et al. \u00b4 (2018) model on the same sample discussed in Section 4, both A-TCN and BiLSTM provide significantly better results. Naplava et al. \u00b4 (2018) also use BiLSTM but with different parameter settings and different dataset preparation. For Yoruba, both character-based architectures provide lower performance than Orife (2018)\u2019s model. However, Orife (2018) uses seq2seq modeling which generate diacritized sentences that are not of the same length as the input and can generate words not present in the original sentence (hallucinations). This is unpleasant behaviour for diacritization especially if used in text-to-speech applications.",
        "sentences": [
            "Comparison to Prior Work: Table 2 shows the performance of previous models trained on the same data.",
            "For Arabic, both A-TCN and BiLSTM provide significantly better performance than MADAMIRA (Pasha et al., 2014), which is a morphological disambiguation tool for Arabic.",
            "The performance of Zalmout and Habash (2017)\u2019s model falls in between BiLSTM and ATCN.",
            "As opposed to our character-based models, both previous models use other morphological features along with a language model to rank all possible diacritic choices.",
            "We believe that this additional semantic and morphological information help their models perform better on OOV words.",
            "For Vietnamese, when we re-train Naplava et al. \u00b4 (2018) model on the same sample discussed in Section 4, both A-TCN and BiLSTM provide significantly better results.",
            "Naplava et al. \u00b4 (2018) also use BiLSTM but with different parameter settings and different dataset preparation.",
            "For Yoruba, both character-based architectures provide lower performance than Orife (2018)\u2019s model.",
            "However, Orife (2018) uses seq2seq modeling which generate diacritized sentences that are not of the same length as the input and can generate words not present in the original sentence (hallucinations).",
            "This is unpleasant behaviour for diacritization especially if used in text-to-speech applications."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            2,
            1,
            2,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "Arabic",
                "A-TCN",
                "BiLSTM",
                "Pasha et al. (2014)"
            ],
            [
                "Zalmout and Habash (2017)",
                "BiLSTM",
                "A-TCN"
            ],
            [
                "Pasha et al. (2014)",
                "Zalmout and Habash (2017)"
            ],
            null,
            [
                "Vietnamese",
                "Naplava et al. (2018)",
                "A-TCN",
                "BiLSTM"
            ],
            null,
            [
                "Yoruba",
                "BiLSTM",
                "A-TCN",
                "Orife (2018)"
            ],
            [
                "Orife (2018)"
            ],
            null
        ],
        "n_sentence": 10.0,
        "table_id": "table_2",
        "paper_id": "D19-1151",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1168table_3",
        "description": "4.2 Different Language Pairs. In this paper, we aim to propose a robust document context extraction model. To achieve this goal, we perform experiments on different language pairs to further illustrate the effectiveness of our proposed HM-GDC model. Table 3 shows the performance of our model on German-English document-level translation and the baseline here refers to the Transformer model. For clarity, we only use the German-English document-level parallel corpus to train these two models without pretraining. From the results, our proposed HMGDC model can help improve the Transformer model on German-English document-level translation by 0.90 BLEU points. The experimental results further validate the robustness of our model in different language pairs.",
        "sentences": [
            "4.2 Different Language Pairs.",
            "In this paper, we aim to propose a robust document context extraction model.",
            "To achieve this goal, we perform experiments on different language pairs to further illustrate the effectiveness of our proposed HM-GDC model.",
            "Table 3 shows the performance of our model on German-English document-level translation and the baseline here refers to the Transformer model.",
            "For clarity, we only use the German-English document-level parallel corpus to train these two models without pretraining.",
            "From the results, our proposed HMGDC model can help improve the Transformer model on German-English document-level translation by 0.90 BLEU points.",
            "The experimental results further validate the robustness of our model in different language pairs."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "Baseline",
                "Ours"
            ],
            null,
            [
                "Ours"
            ],
            null
        ],
        "n_sentence": 7.0,
        "table_id": "table_3",
        "paper_id": "D19-1168",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1170table_1",
        "description": "Table 1 shows the performance of our model and other competitive approaches on the development and test sets. MTMSN outperforms all existing approaches by a large margin, and creates new state-of-the-art results by achieving an EM score of 75.85 and a F1 score of 79.88 on the test set. Since our best model utilizes BERTLARGE as encoder, we therefore compare MTMSNLARGE with the NABERTLARGE baseline. As we can see, our model obtains 12.07/13.19 absolute gain of EM/F1 over the baseline, demonstrating the effectiveness of our approach. However, as the human achieves 95.98 F1 on the test set, our results suggest that there is still room for improvement.",
        "sentences": [
            "Table 1 shows the performance of our model and other competitive approaches on the development and test sets.",
            "MTMSN outperforms all existing approaches by a large margin, and creates new state-of-the-art results by achieving an EM score of 75.85 and a F1 score of 79.88 on the test set.",
            "Since our best model utilizes BERTLARGE as encoder, we therefore compare MTMSNLARGE with the NABERTLARGE baseline.",
            "As we can see, our model obtains 12.07/13.19 absolute gain of EM/F1 over the baseline, demonstrating the effectiveness of our approach.",
            "However, as the human achieves 95.98 F1 on the test set, our results suggest that there is still room for improvement."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "EM",
                "F1",
                "Test",
                "MTMSNLARGE"
            ],
            [
                "MTMSNLARGE",
                "NABERTLARGE"
            ],
            [
                "MTMSNLARGE",
                "NABERTLARGE",
                "EM",
                "F1",
                "Test"
            ],
            [
                "Human Performance (Dua et al., 2019)",
                "Heuristic Baseline (Dua et al., 2019)",
                "BERTBASE (Devlin et al., 2019)",
                "NAQANet (Dua et al., 2019)",
                "F1",
                "Test"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_1",
        "paper_id": "D19-1170",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1170table_4",
        "description": "Performance breakdown. We now provide a quantitative analysis by showing performance breakdown on the development set. Table 4 shows that our gains mainly come from the most frequent number type, which requires various types of symbolic, discrete reasoning operations. Moreover, significant improvements are also obtained in the multi-span category, where the F1 score increases by more than 40 points. This result further proves the validity of our multi-span extraction method.",
        "sentences": [
            "Performance breakdown.",
            "We now provide a quantitative analysis by showing performance breakdown on the development set.",
            "Table 4 shows that our gains mainly come from the most frequent number type, which requires various types of symbolic, discrete reasoning operations.",
            "Moreover, significant improvements are also obtained in the multi-span category, where the F1 score increases by more than 40 points.",
            "This result further proves the validity of our multi-span extraction method."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Number",
                "Metric"
            ],
            [
                "Multi Span",
                "F1"
            ],
            [
                "Multi Span"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_4",
        "paper_id": "D19-1170",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1171table_5",
        "description": "The results are given in Table 5 where we report the accuracy (P@1), averaged over the five datasets. Interestingly, we do not observe large differences between supervised training and WS-TB for both models when they use the same number of positive training instances (ranging from 2.8k to 5.8k). Thus, using title-body information instead of question-answer pairs to train models without direct answer supervision is feasible and effective. Further, when we use all available title-body pairs, the BiLSTM model substantially improves by 5pp, which is only slightly worse than COALA (which was designed for smaller training sets). We hypothesize that one reason is that BiLSTM can learn improved representations with the additional data. Further, title-body pairs have a higher overlap than question-answer pairs (see \u0e22\u0e076) which provides a stronger training signal to the siamese network. These results demonstrate that our work can have broader impact to cQA, e.g., to train models on other tasks beyond duplicate question detection.",
        "sentences": [
            "The results are given in Table 5 where we report the accuracy (P@1), averaged over the five datasets.",
            "Interestingly, we do not observe large differences between supervised training and WS-TB for both models when they use the same number of positive training instances (ranging from 2.8k to 5.8k).",
            "Thus, using title-body information instead of question-answer pairs to train models without direct answer supervision is feasible and effective.",
            "Further, when we use all available title-body pairs, the BiLSTM model substantially improves by 5pp, which is only slightly worse than COALA (which was designed for smaller training sets).",
            "We hypothesize that one reason is that BiLSTM can learn improved representations with the additional data.",
            "Further, title-body pairs have a higher overlap than question-answer pairs (see \u0e22\u0e076) which provides a stronger training signal to the siamese network.",
            "These results demonstrate that our work can have broader impact to cQA, e.g., to train models on other tasks beyond duplicate question detection."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "Supervised",
                "WS-TB"
            ],
            null,
            [
                "BiLSTM",
                "COALA"
            ],
            [
                "BiLSTM"
            ],
            null,
            [
                "BiLSTM"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_5",
        "paper_id": "D19-1171",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1172table_8",
        "description": "Clarification Question Generation. Table 8 shows that Seq2Seq achieves low BLEU scores, which indicates its tendency to generate irrelevant text. Transformer achieves higher performance than Seq2Seq. Our proposed coarse-to-fine model demonstrates a new state of the art, improving the current highest baseline result by 3.35 and 0.60 BLEU scores, respectively.",
        "sentences": [
            "Clarification Question Generation.",
            "Table 8 shows that Seq2Seq achieves low BLEU scores, which indicates its tendency to generate irrelevant text.",
            "Transformer achieves higher performance than Seq2Seq.",
            "Our proposed coarse-to-fine model demonstrates a new state of the art, improving the current highest baseline result by 3.35 and 0.60 BLEU scores, respectively."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Seq2Seq"
            ],
            [
                "Seq2Seq",
                "Transformer"
            ],
            [
                "The proposed Model"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_8",
        "paper_id": "D19-1172",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1184table_2",
        "description": "The results in Table 2 demonstrate the strong performance gains obtained with MGT. With L = 5 granularities, MGT outperforms a similarly sized ensemble of dual encoders. These results demonstrate that explicitly enforcing the policy that makes models learn multiple granularities of representation improves the representative power and performance on next utterance retrieval.",
        "sentences": [
            "The results in Table 2 demonstrate the strong performance gains obtained with MGT.",
            "With L = 5 granularities, MGT outperforms a similarly sized ensemble of dual encoders.",
            "These results demonstrate that explicitly enforcing the policy that makes models learn multiple granularities of representation improves the representative power and performance on next utterance retrieval."
        ],
        "class_sentence": [
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "Multi-Granularity (5)"
            ],
            [
                "Multi-Granularity (5)"
            ],
            null
        ],
        "n_sentence": 3.0,
        "table_id": "table_2",
        "paper_id": "D19-1184",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1184table_5",
        "description": "The results shown in Table 5 demonstrate that MGT results in more general representations of language, thereby facilitating better transfer. However, there is room for improvement when comparing to models fine-tuned on the downstream task. This suggests that additional measures can be taken to improve the representative power of these models.",
        "sentences": [
            "The results shown in Table 5 demonstrate that MGT results in more general representations of language, thereby facilitating better transfer.",
            "However, there is room for improvement when comparing to models fine-tuned on the downstream task.",
            "This suggests that additional measures can be taken to improve the representative power of these models."
        ],
        "class_sentence": [
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "Multi-Granularity (5)"
            ],
            [
                "Fine-tuned"
            ],
            null
        ],
        "n_sentence": 3.0,
        "table_id": "table_5",
        "paper_id": "D19-1184",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1184table_6",
        "description": "The results in Table 6 demonstrate that MGT learns general representations which effectively transfer to downstream tasks, especially more difficult tasks such as dialog act prediction. Finetuning the latent representations learned by MGT, results in improved performance on dialog act prediction.",
        "sentences": [
            "The results in Table 6 demonstrate that MGT learns general representations which effectively transfer to downstream tasks, especially more difficult tasks such as dialog act prediction.",
            "Finetuning the latent representations learned by MGT, results in improved performance on dialog act prediction."
        ],
        "class_sentence": [
            1,
            2
        ],
        "header_mention": [
            [
                "Multi-Granularity (5)"
            ],
            [
                "Multi-Granularity (5)"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_6",
        "paper_id": "D19-1184",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1188table_1",
        "description": "4.4 Overall Performance. Table 1 lists the performance of our system and the comparison systems. CVAE and LAED inject SEQ2SEQ with stochastic latent variable, resulting in more informative responses and better performance on Distinct-{1, 2, 3}. TA-SEQ2SEQ incorporates SEQ2SEQ with the outsourcing topic information from LDA. It is not surprising that it performs much better on the response relevance (BLEU, Average, Greedy, Extrema), while its improvements on the informativeness are limited. DOM-SEQ2SEQ builds multiple domain-specific encoder-decoders. It gains improvements on both the relevance metrics and informativeness metrics. In general, with both the context-aware and topic-aware parameterization, our model outperforms all the competitive baselines in terms of the response relevance and informativeness.",
        "sentences": [
            "4.4 Overall Performance.",
            "Table 1 lists the performance of our system and the comparison systems.",
            "CVAE and LAED inject SEQ2SEQ with stochastic latent variable, resulting in more informative responses and better performance on Distinct-{1, 2, 3}.",
            "TA-SEQ2SEQ incorporates SEQ2SEQ with the outsourcing topic information from LDA.",
            "It is not surprising that it performs much better on the response relevance (BLEU, Average, Greedy, Extrema), while its improvements on the informativeness are limited.",
            "DOM-SEQ2SEQ builds multiple domain-specific encoder-decoders.",
            "It gains improvements on both the relevance metrics and informativeness metrics.",
            "In general, with both the context-aware and topic-aware parameterization, our model outperforms all the competitive baselines in terms of the response relevance and informativeness."
        ],
        "class_sentence": [
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "CVAE",
                "LAED",
                "SEQ2SEQ"
            ],
            [
                "TA-SEQ2SEQ",
                "SEQ2SEQ"
            ],
            [
                "TA-SEQ2SEQ"
            ],
            [
                "DOM-SEQ2SEQ"
            ],
            [
                "DOM-SEQ2SEQ"
            ],
            null
        ],
        "n_sentence": 8.0,
        "table_id": "table_1",
        "paper_id": "D19-1188",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1192table_1",
        "description": "Table 1 shows the experiment result, which indicates that our rewriting method outperforms heuristic methods. Moreover, a 54.2 BLEU-4 score means that the rewritten sentences are very similar to the human references. CRN-RL has a higher score than CRN-Pre-train on BLEU4, it proves reinforcement learning promotes our model effectively.",
        "sentences": [
            "Table 1 shows the experiment result, which indicates that our rewriting method outperforms heuristic methods.",
            "Moreover, a 54.2 BLEU-4 score means that the rewritten sentences are very similar to the human references.",
            "CRN-RL has a higher score than CRN-Pre-train on BLEU4, it proves reinforcement learning promotes our model effectively."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "CRN",
                "CRN + RL",
                "Last Utterance",
                "Last Utterance + Keyword",
                "Last Utterance + Context"
            ],
            [
                "BLEU-4"
            ],
            [
                "CRN + RL",
                "CRN",
                "BLEU-4"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_1",
        "paper_id": "D19-1192",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1193table_2",
        "description": "6.4 Experimental Results. Table 2 presents the evaluation results of our reproduced IMN model (Gu et al., 2019) and previous methods on PERSONA-CHAT dataset without using personas. It can be seen that the IMN model outperformed other models on this dataset by a margin larger than 28.9% in terms of hits@1. As introduced above, our proposed models for personalized response selection were all built on IMN.",
        "sentences": [
            "6.4 Experimental Results.",
            "Table 2 presents the evaluation results of our reproduced IMN model (Gu et al., 2019) and previous methods on PERSONA-CHAT dataset without using personas.",
            "It can be seen that the IMN model outperformed other models on this dataset by a margin larger than 28.9% in terms of hits@1.",
            "As introduced above, our proposed models for personalized response selection were all built on IMN."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "IMN",
                "IR baseline",
                "Starspace",
                "Profile",
                "KV Profile"
            ],
            [
                "IMN",
                "IR baseline",
                "Starspace",
                "Profile",
                "KV Profile",
                "hits@1"
            ],
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "D19-1193",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1193table_3",
        "description": "Table 3 presents the evaluation results of our proposed and previous methods on PERSONACHAT under various persona configurations. The t-test shows that the differences between our proposed models, i.e., IMNutt and DIM, and the baseline model, i.e. IMNctx, were both statistically significant with p-value < 0.01. We can see that the fine-grained persona fusion at the utterance level rendered a hits@1 improvement of 2.4% and an MRR improvement of 1.9% by comparing IMNctx and IMNutr conditioned on original self personas. The DIM model outperformed its baseline IMNctx by a margin of 14.5% in terms of hits@1 and 10.5% in terms of MRR. Compared with the FT-PC model (Mazare et al. \u00c2\u00b4 , 2018) which was first pretrained using a large-scale corpus and then fine-tuned on the PERSONA-CHAT dataset, the DIM model outperformed it by a margin of 10.0% in terms of hits@1 conditioned on revised self personas. Another advantage of DIM is that it was trained in an end-to-end mode without pretraining and using any external knowledge. Lastly, the DIM model outperforms previous models by margins larger than 27.7% in terms of hits@1 conditioned on original self personas.",
        "sentences": [
            "Table 3 presents the evaluation results of our proposed and previous methods on PERSONACHAT under various persona configurations.",
            "The t-test shows that the differences between our proposed models, i.e., IMNutt and DIM, and the baseline model, i.e.",
            "IMNctx, were both statistically significant with p-value < 0.01.",
            "We can see that the fine-grained persona fusion at the utterance level rendered a hits@1 improvement of 2.4% and an MRR improvement of 1.9% by comparing IMNctx and IMNutr conditioned on original self personas.",
            "The DIM model outperformed its baseline IMNctx by a margin of 14.5% in terms of hits@1 and 10.5% in terms of MRR.",
            "Compared with the FT-PC model (Mazare et al. \u00c2\u00b4 , 2018) which was first pretrained using a large-scale corpus and then fine-tuned on the PERSONA-CHAT dataset, the DIM model outperformed it by a margin of 10.0% in terms of hits@1 conditioned on revised self personas.",
            "Another advantage of DIM is that it was trained in an end-to-end mode without pretraining and using any external knowledge.",
            "Lastly, the DIM model outperforms previous models by margins larger than 27.7% in terms of hits@1 conditioned on original self personas."
        ],
        "class_sentence": [
            1,
            2,
            2,
            1,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "IMNctx",
                "IMNutr",
                "Self Persona",
                "Original",
                "hits@1"
            ],
            [
                "DIM",
                "IMNctx",
                "hits@1"
            ],
            [
                "DIM",
                "FT-PC",
                "Self Persona",
                "Revised",
                "hits@1"
            ],
            null,
            [
                "DIM",
                "KV Profile",
                "Self Persona",
                "Original",
                "hits@1"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_3",
        "paper_id": "D19-1193",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1194table_6",
        "description": "Table 6 presents the BLEU-2 scores (as recommended in the prior work (Liu et al., 2016)), perplexity (PPL), and distinct scores. The results show that all models have similar levels of BLEU2 and PPL, while Qadpt+multi has slightly better distinct scores. The results suggest the same claim as Liu et al.(2016) that BLEU scores are not suitable for dialogue generation.",
        "sentences": [
            "Table 6 presents the BLEU-2 scores (as recommended in the prior work (Liu et al., 2016)), perplexity (PPL), and distinct scores.",
            "The results show that all models have similar levels of BLEU2 and PPL, while Qadpt+multi has slightly better distinct scores.",
            "The results suggest the same claim as Liu et al.(2016) that BLEU scores are not suitable for dialogue generation."
        ],
        "class_sentence": [
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "BLEU",
                "PPL",
                "dist-1",
                "dist-2",
                "dist-3",
                "dist-4"
            ],
            [
                "BLEU",
                "PPL",
                "Qadpt + multi"
            ],
            [
                "BLEU"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_6",
        "paper_id": "D19-1194",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1197table_2",
        "description": "5.4 Evaluation Results. Table 2 report the results of automatic evaluation on sentiment response generation task. We can see that S2S-Temp outperforms all baseline models in terms of all metrics, and the improvements are statistically significant (t-test with p-value< 0.01). The results demonstrate that when only limited pairs are available, S2STemp can effectively leverage unpaired data to enhance the quality of response generation. Although lacking fine-grained check, from the comparison among S2S-Temp-None, S2S-Temp-50%, and S2S-Temp, we can conclude that the performance of S2S-Temp improves with more unpaired data. Moreover, without unpaired data, our model is even worse than CVAE since the structured templates cannot be accurately estimated from such a few data, and as long as half of the unpaired data are available, the model outperforms the baseline models on most metrics. The results further verified the important role the unpaired data plays in learning of a response generation model from low resources. S2S-Temp is better than S2S-TempMLE, indicating that the adversarial learning approach can indeed enhance the relevance of responses regarding to messages.",
        "sentences": [
            "5.4 Evaluation Results.",
            "Table 2 report the results of automatic evaluation on sentiment response generation task.",
            "We can see that S2S-Temp outperforms all baseline models in terms of all metrics, and the improvements are statistically significant (t-test with p-value< 0.01).",
            "The results demonstrate that when only limited pairs are available, S2STemp can effectively leverage unpaired data to enhance the quality of response generation.",
            "Although lacking fine-grained check, from the comparison among S2S-Temp-None, S2S-Temp-50%, and S2S-Temp, we can conclude that the performance of S2S-Temp improves with more unpaired data.",
            "Moreover, without unpaired data, our model is even worse than CVAE since the structured templates cannot be accurately estimated from such a few data, and as long as half of the unpaired data are available, the model outperforms the baseline models on most metrics.",
            "The results further verified the important role the unpaired data plays in learning of a response generation model from low resources.",
            "S2S-Temp is better than S2S-TempMLE, indicating that the adversarial learning approach can indeed enhance the relevance of responses regarding to messages."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "S2S-Temp"
            ],
            [
                "S2S-Temp"
            ],
            [
                "S2S-Temp-None",
                "S2S-Temp-50%",
                "S2S-Temp"
            ],
            [
                "S2S-Temp-None",
                "CVAE"
            ],
            null,
            [
                "S2S-Temp",
                "S2S-Temp-MLE"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_2",
        "paper_id": "D19-1197",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1199table_4",
        "description": "Table 4 shows the consistency between human inference and model predictions. W2W also outperforms the baselines with a larger margin on longer conversation scenarios, which is consistent with the phenomenon of automatic evaluation. The advantage on unlabeled data of our W2W model demonstrates the superiority for detecting the latent speaker-addressee structure unspecified in the conversation stream, and that it could help find out the relationship between and across users in the session.",
        "sentences": [
            "Table 4 shows the consistency between human inference and model predictions.",
            "W2W also outperforms the baselines with a larger margin on longer conversation scenarios, which is consistent with the phenomenon of automatic evaluation.",
            "The advantage on unlabeled data of our W2W model demonstrates the superiority for detecting the latent speaker-addressee structure unspecified in the conversation stream, and that it could help find out the relationship between and across users in the session."
        ],
        "class_sentence": [
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "W2W"
            ],
            [
                "W2W"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_4",
        "paper_id": "D19-1199",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1201table_2",
        "description": "Incorporating personalization in the conditional GMD prior is more effective than combing personalization in decoder. As shown in Table 2, Persona model only achieves comparable results with Seq2Seq in terms of BLEU scores and BOW scores. For PersonaWAE and DiaWAE-GMD, incorporating personalizations in both decoder and the latent space yields a performance improvement. For the BLEU-Recall, which PersonaWAE does not outperform than DiaWAE-GMD, a possible explanation for this might be that PersonaWAE model the personalization information and generation may be more limited.",
        "sentences": [
            "Incorporating personalization in the conditional GMD prior is more effective than combing personalization in decoder.",
            "As shown in Table 2, Persona model only achieves comparable results with Seq2Seq in terms of BLEU scores and BOW scores.",
            "For PersonaWAE and DiaWAE-GMD, incorporating personalizations in both decoder and the latent space yields a performance improvement.",
            "For the BLEU-Recall, which PersonaWAE does not outperform than DiaWAE-GMD, a possible explanation for this might be that PersonaWAE model the personalization information and generation may be more limited."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Persona",
                "BLEU",
                "Embedding Metrics"
            ],
            [
                "PersonaWAE",
                "DiaWAE-GMD",
                "Embedding Metrics"
            ],
            [
                "PersonaWAE",
                "DiaWAE-GMD",
                "Recall"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "D19-1201",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1205table_2",
        "description": "Crossway vs Response-DA/Context-DA: Although the dialogue acts have been shown to be useful for the response selection task, existing work has only used the dialogue acts of the context. Whereas, in our experiments, we have found that the model that uses the dialogue acts of both context and response outperforms the models that use the dialogue acts of either context or response. To further analyze the results, we perform an ablation study and show the results of using the dialogue acts of context, response and of both. In Table 2, we report the MRR numbers of several models that use the dialogue acts in different settings. More specifically, we show how the following models i.e., Siamese with actual dialogue act (Siamese-ADA), Siamese with predicted dialogue acts in single-task setting (Siamese-PDA-ST) and Siamese with predicted dialogue act in multi-task setting (SiamesePDA-MT) perform when they are given the dialogue acts of only context (Context-DA), dialogue acts of only response (Response-DA), and dialogue acts of both (Crossway). Results in Table 2 indicate that the Crossway always outperforms the Context-DA or the Response-DA, for both datasets. For DailyDialog dataset, Context-DA performs better than Response-DA for all three models, whereas in the SwDA dataset, ResponseDA does a relatively better job than contextDA (two out of three models). Despite their different behavior for different datasets, when we combine Response-DA and Context-DA in a Crossway fashion, it outperforms the both, giving the best of both worlds. This performance improvement of the Crossway over context-DA and response-DA can also be attributed to the Crossway model, there are four similarities that play a role, i.e. context-response, ContextDAResponseDA, ContextDA-response and ContextresponseDA, graphically depicted in Figure 2. So, in the case of erroneous prediction of either of context DA or response DA, it shall only corrupt two of the four similarities, still leaving two other similarities that can provide strong clues to the underlying model about the correct response belonging to the context.",
        "sentences": [
            "Crossway vs Response-DA/Context-DA: Although the dialogue acts have been shown to be useful for the response selection task, existing work has only used the dialogue acts of the context.",
            "Whereas, in our experiments, we have found that the model that uses the dialogue acts of both context and response outperforms the models that use the dialogue acts of either context or response.",
            "To further analyze the results, we perform an ablation study and show the results of using the dialogue acts of context, response and of both.",
            "In Table 2, we report the MRR numbers of several models that use the dialogue acts in different settings.",
            "More specifically, we show how the following models i.e., Siamese with actual dialogue act (Siamese-ADA), Siamese with predicted dialogue acts in single-task setting (Siamese-PDA-ST) and Siamese with predicted dialogue act in multi-task setting (SiamesePDA-MT) perform when they are given the dialogue acts of only context (Context-DA), dialogue acts of only response (Response-DA), and dialogue acts of both (Crossway).",
            "Results in Table 2 indicate that the Crossway always outperforms the Context-DA or the Response-DA, for both datasets.",
            "For DailyDialog dataset, Context-DA performs better than Response-DA for all three models, whereas in the SwDA dataset, ResponseDA does a relatively better job than contextDA (two out of three models).",
            "Despite their different behavior for different datasets, when we combine Response-DA and Context-DA in a Crossway fashion, it outperforms the both, giving the best of both worlds.",
            "This performance improvement of the Crossway over context-DA and response-DA can also be attributed to the Crossway model, there are four similarities that play a role, i.e. context-response, ContextDAResponseDA, ContextDA-response and ContextresponseDA, graphically depicted in Figure 2.",
            "So, in the case of erroneous prediction of either of context DA or response DA, it shall only corrupt two of the four similarities, still leaving two other similarities that can provide strong clues to the underlying model about the correct response belonging to the context."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            1,
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            [
                "Siamese-ADA (Kumar et al., 2018)",
                "Siamese-PDA-ST",
                "Context-DA (Zhao et al., 2017)",
                "Response-DA",
                "Crossway"
            ],
            [
                "Context-DA (Zhao et al., 2017)",
                "Response-DA"
            ],
            [
                "Context-DA (Zhao et al., 2017)",
                "Response-DA"
            ],
            [
                "Context-DA (Zhao et al., 2017)",
                "Response-DA",
                "Crossway"
            ],
            null,
            null
        ],
        "n_sentence": 10.0,
        "table_id": "table_2",
        "paper_id": "D19-1205",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1208table_2",
        "description": "Table 2 shows the comparison with SelfRetrieval. For a fair comparison with them, we replace the cross entropy loss from our loss with the policy gradient method [Rennie et al., 2017] to directly optimize our model with CIDEr score as in [Liu et al., 2018]. As our baseline model (denoted as Baseline), we train a model only with policy gradient method without the proposed GAN model. When only using the 100% paired MS COCO dataset (denoted as w/o unlabeled), our model already shows improved performance over Self-Retrieval. Moreover, when adding Unlabeled-COCO images (denoted as with unlabeled), our model performs favorably against Self-Retrieval in all the metrics. The results suggest that our method is also advantageous in the semi-supervised setup.",
        "sentences": [
            "Table 2 shows the comparison with SelfRetrieval.",
            "For a fair comparison with them, we replace the cross entropy loss from our loss with the policy gradient method [Rennie et al., 2017] to directly optimize our model with CIDEr score as in [Liu et al., 2018].",
            "As our baseline model (denoted as Baseline), we train a model only with policy gradient method without the proposed GAN model.",
            "When only using the 100% paired MS COCO dataset (denoted as w/o unlabeled), our model already shows improved performance over Self-Retrieval.",
            "Moreover, when adding Unlabeled-COCO images (denoted as with unlabeled), our model performs favorably against Self-Retrieval in all the metrics.",
            "The results suggest that our method is also advantageous in the semi-supervised setup."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "Ours (w/o unlabeled)"
            ],
            [
                "Ours (with unlabeled)"
            ],
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_2",
        "paper_id": "D19-1208",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1211table_4",
        "description": "6 Results and Discussion. The results of our experiments are presented in Table 4. Results demonstrate that both context and punchline information are important as C-MFN outperforms C-MFN (P) and C-MFN (C) models. Punchline is the most important component for detecting humor as the performance of C-MFN (P) is significantly higher than C-MFN (C). Models that use all modalities (T+A+V) outperform models that use only one or two modalities (T, T+A, T+V, A+V). Between text (T) and nonverbal behaviors (A+V), text shows to be the most important modality. Most of the cases, both modalities of visual and acoustic improve the performance of text alone (T+V, T+A). Based on the above observations, each neural component of the C-MFN model is useful in improving the prediction of humor. The results also indicate that modeling humor from a multimodal perspective yields successful results. Furthermore, both context and punchline are important in understanding humor. The highest accuracy achieved by Random Forest baseline (after hyper parameter tuning & using same folds as C-MFN) was 57.78%, which is higher than random baseline but lower than CMFN (65.23%). In addition, C-MFN achieves higher accuracy than similar notable unimodal previous work (58.9%) where only punchline and textual information were used (Chen and Lee, 2017) . The human performance 5 on the URFUNNY dataset is 82.5%.",
        "sentences": [
            "6 Results and Discussion.",
            "The results of our experiments are presented in Table 4.",
            "Results demonstrate that both context and punchline information are important as C-MFN outperforms C-MFN (P) and C-MFN (C) models.",
            "Punchline is the most important component for detecting humor as the performance of C-MFN (P) is significantly higher than C-MFN (C).",
            "Models that use all modalities (T+A+V) outperform models that use only one or two modalities (T, T+A, T+V, A+V).",
            "Between text (T) and nonverbal behaviors (A+V), text shows to be the most important modality.",
            "Most of the cases, both modalities of visual and acoustic improve the performance of text alone (T+V, T+A).",
            "Based on the above observations, each neural component of the C-MFN model is useful in improving the prediction of humor.",
            "The results also indicate that modeling humor from a multimodal perspective yields successful results.",
            "Furthermore, both context and punchline are important in understanding humor.",
            "The highest accuracy achieved by Random Forest baseline (after hyper parameter tuning & using same folds as C-MFN) was 57.78%, which is higher than random baseline but lower than CMFN (65.23%).",
            "In addition, C-MFN achieves higher accuracy than similar notable unimodal previous work (58.9%) where only punchline and textual information were used (Chen and Lee, 2017) .",
            "The human performance 5 on the URFUNNY dataset is 82.5%."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "C-MFN",
                "C-MFN (C)",
                "C-MFN (P)"
            ],
            [
                "C-MFN (P)",
                "C-MFN (C)"
            ],
            [
                "T+A+V",
                "T",
                "T+A",
                "T+V",
                "A+V"
            ],
            [
                "T",
                "A+V"
            ],
            [
                "T",
                "T+V",
                "T+A"
            ],
            [
                "C-MFN"
            ],
            null,
            null,
            [
                "C-MFN"
            ],
            [
                "C-MFN"
            ],
            null
        ],
        "n_sentence": 13.0,
        "table_id": "table_4",
        "paper_id": "D19-1211",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1213table_2",
        "description": "4.5 Quantitative Analysis In Table 2 and Table 3, we compare our ASGN+LNA model with the state-of-the-art models on the Youtube2Text and MSR-VTT datasets. Following the operation of (Gan et al., 2016; Pasunuru and Bansal, 2017), ASGN+LNA is the average ensemble of 5 ASGN+LNA (RL) models trained with different initializations. From the results, our method achieves the competitive performance on the two datasets. Compared with the other interpretable improvement methods (Dong et al., 2017; Wu et al., 2018), interpretability of our neural network is explicitly improved, and the performance of our model is more competitive.",
        "sentences": [
            "4.5 Quantitative Analysis In Table 2 and Table 3, we compare our ASGN+LNA model with the state-of-the-art models on the Youtube2Text and MSR-VTT datasets.",
            "Following the operation of (Gan et al., 2016; Pasunuru and Bansal, 2017), ASGN+LNA is the average ensemble of 5 ASGN+LNA (RL) models trained with different initializations.",
            "From the results, our method achieves the competitive performance on the two datasets.",
            "Compared with the other interpretable improvement methods (Dong et al., 2017; Wu et al., 2018), interpretability of our neural network is explicitly improved, and the performance of our model is more competitive."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "ASGN+LNA"
            ],
            [
                "ASGN+LNA"
            ],
            [
                "ASGN+LNA"
            ],
            [
                "ASGN+LNA"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "D19-1213",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1214table_3",
        "description": "Table 3 gives the result of the comparison experiment. From the result of gate-mechanism row, we can observe that without the Stack-Propagation learning and simply using the gate-mechanism to incorporate the intent information, the slot filling (F1) performance drops significantly, which demonstrates that directly leverage the intent information with Stack-Propagation can improve the slot filling performance effectively than using the gate mechanism. Besides, we can see that the intent detection (Acc) and overall accuracy (Acc) decrease a lot. We attribute it to the fact that the bad slot filling performance harms the intent detection and the whole sentence semantic performance due to the joint learning scheme. Besides, from the pipeline model row of Table 3, we can see that without shared encoder, the performance on all metrics declines significantly. This shows that Stack-Propagation model can learn the correlation knowledge which may promote each other and ease the error propagation effectively.",
        "sentences": [
            "Table 3 gives the result of the comparison experiment.",
            "From the result of gate-mechanism row, we can observe that without the Stack-Propagation learning and simply using the gate-mechanism to incorporate the intent information, the slot filling (F1) performance drops significantly, which demonstrates that directly leverage the intent information with Stack-Propagation can improve the slot filling performance effectively than using the gate mechanism.",
            "Besides, we can see that the intent detection (Acc) and overall accuracy (Acc) decrease a lot.",
            "We attribute it to the fact that the bad slot filling performance harms the intent detection and the whole sentence semantic performance due to the joint learning scheme.",
            "Besides, from the pipeline model row of Table 3, we can see that without shared encoder, the performance on all metrics declines significantly.",
            "This shows that Stack-Propagation model can learn the correlation knowledge which may promote each other and ease the error propagation effectively."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "gate-mechanism",
                "Our model",
                "Slot (F1)"
            ],
            [
                "Intent (Acc)",
                "Overall (Acc)"
            ],
            [
                "Slot (F1)",
                "Intent (Acc)"
            ],
            [
                "pipelined model"
            ],
            [
                "Our model"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_3",
        "paper_id": "D19-1214",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1217table_1",
        "description": "Table 1 shows the experimental results of answer generation on TACoS-MultiLevel and YoutubeClip datasets, and Table 2 shows the question generation results on same datasets. Our method (RICT) outperforms all above models in almost all metrics. This fact shows the effectiveness of our overall network architecture. And we find that the image dialog models perform better than video QA models in answer generation.",
        "sentences": [
            "Table 1 shows the experimental results of answer generation on TACoS-MultiLevel and YoutubeClip datasets, and Table 2 shows the question generation results on same datasets.",
            "Our method (RICT) outperforms all above models in almost all metrics.",
            "This fact shows the effectiveness of our overall network architecture.",
            "And we find that the image dialog models perform better than video QA models in answer generation."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "RICT (ours)"
            ],
            [
                "RICT (ours)"
            ],
            [
                "TACoS-MultiLevel",
                "YoutubeClip"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_1",
        "paper_id": "D19-1217",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1217table_2",
        "description": "Table 1 shows the experimental results of answer generation on TACoS-MultiLevel and YoutubeClip datasets, and Table 2 shows the question generation results on same datasets. Our method (RICT) outperforms all above models in almost all metrics. This fact shows the effectiveness of our overall network architecture. And we find that the image dialog models perform better than video QA models in answer generation, but worse in question generation on both datasets. This might indicate that for these two datasets, the answer generation is more dependent on dialog, and question generation is more dependent on video content.",
        "sentences": [
            "Table 1 shows the experimental results of answer generation on TACoS-MultiLevel and YoutubeClip datasets, and Table 2 shows the question generation results on same datasets.",
            "Our method (RICT) outperforms all above models in almost all metrics.",
            "This fact shows the effectiveness of our overall network architecture.",
            "And we find that the image dialog models perform better than video QA models in answer generation, but worse in question generation on both datasets.",
            "This might indicate that for these two datasets, the answer generation is more dependent on dialog, and question generation is more dependent on video content."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "RICT (ours)"
            ],
            [
                "RICT (ours)"
            ],
            [
                "TACoS-MultiLevel",
                "YoutubeClip"
            ],
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_2",
        "paper_id": "D19-1217",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1230table_2",
        "description": "4.2 Results. Table 2 shows the performance comparison of various negative focus detection models. We can see that all of contextual attention based models (row 7-9 in Table 2) achieve better perfomances than existing methods (row 1-3 in Table 2) and models without contextual attention (row 4-6 in Table 2). In addition, both word-level and topiclevel contextual attention based models outperform the state-of-the-art system (WTGM in Table 2) with about 2% accuracy gain at least. The results demonstrate the effectiveness of these two types of contextual attention mechanisms. Moreover, to better quantify the contribution of the different attention mechanisms of our approach, we also conduct several attention variants. Comparing the three types of attention mechanisms (row 7-9 in Table 2), the topic-level attention based model achieves the best performance. We also observe that the word-level attention based model also achieves comparative performance with the topic-level one. However, when combining the two types of attention mechanisms, the performance declines. It indicates that both the word-level attention mechanism and the topiclevel one can capture the contextual information effectively, but applying such information repeatedly might lead to feature redundancy. In addition to the methods that take advantage of the contextual features in adjacent sentences, we also compare the performances of different frameworks for negative focus detection (row 4-6 in Table 2), which only apply the features in current sentence. The results indicate that the BiLSTM-CRF framework is a better fit for encoding order information and long-range context dependency for such sequence labeling task.",
        "sentences": [
            "4.2 Results.",
            "Table 2 shows the performance comparison of various negative focus detection models.",
            "We can see that all of contextual attention based models (row 7-9 in Table 2) achieve better perfomances than existing methods (row 1-3 in Table 2) and models without contextual attention (row 4-6 in Table 2).",
            "In addition, both word-level and topiclevel contextual attention based models outperform the state-of-the-art system (WTGM in Table 2) with about 2% accuracy gain at least.",
            "The results demonstrate the effectiveness of these two types of contextual attention mechanisms.",
            "Moreover, to better quantify the contribution of the different attention mechanisms of our approach, we also conduct several attention variants.",
            "Comparing the three types of attention mechanisms (row 7-9 in Table 2), the topic-level attention based model achieves the best performance.",
            "We also observe that the word-level attention based model also achieves comparative performance with the topic-level one.",
            "However, when combining the two types of attention mechanisms, the performance declines.",
            "It indicates that both the word-level attention mechanism and the topiclevel one can capture the contextual information effectively, but applying such information repeatedly might lead to feature redundancy.",
            "In addition to the methods that take advantage of the contextual features in adjacent sentences, we also compare the performances of different frameworks for negative focus detection (row 4-6 in Table 2), which only apply the features in current sentence.",
            "The results indicate that the BiLSTM-CRF framework is a better fit for encoding order information and long-range context dependency for such sequence labeling task."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "CLaC (Rosenberg (2012))",
                "FOC-DET (Blanco (2013))",
                "WTGM (Zou (2015))",
                "LSTM",
                "BiLSTM",
                "BiLSTM-CRF",
                "W-Att BiLSTM-CRF",
                "T-Att BiLSTM-CRF",
                "WT-Att BiLSTM-CRF"
            ],
            [
                "WT-Att BiLSTM-CRF",
                "WTGM (Zou (2015))"
            ],
            [
                "WT-Att BiLSTM-CRF"
            ],
            null,
            [
                "W-Att BiLSTM-CRF",
                "T-Att BiLSTM-CRF",
                "WT-Att BiLSTM-CRF"
            ],
            [
                "W-Att BiLSTM-CRF",
                "T-Att BiLSTM-CRF"
            ],
            [
                "WT-Att BiLSTM-CRF"
            ],
            null,
            [
                "LSTM",
                "BiLSTM",
                "BiLSTM-CRF"
            ],
            [
                "BiLSTM-CRF"
            ]
        ],
        "n_sentence": 12.0,
        "table_id": "table_2",
        "paper_id": "D19-1230",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1230table_7",
        "description": "Impact of Pre-trained Word Embedding. To compare the impacts of different pre-trained word embeddings for negative focus detection task, we attempt to employ other pre-trained word embeddings. Table 7 show the performances of the T-Att BiLSTM-CRF model with different pre-trained word embeddings, including Senna, Glove, Word2vec, and BERT. We can see that ELMo achieves the best performance, but the performance gaps between different pre-trained word embeddings and different dimensions are not significant.",
        "sentences": [
            "Impact of Pre-trained Word Embedding.",
            "To compare the impacts of different pre-trained word embeddings for negative focus detection task, we attempt to employ other pre-trained word embeddings.",
            "Table 7 show the performances of the T-Att BiLSTM-CRF model with different pre-trained word embeddings, including Senna, Glove, Word2vec, and BERT.",
            "We can see that ELMo achieves the best performance, but the performance gaps between different pre-trained word embeddings and different dimensions are not significant."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Word Embedding",
                "Senna",
                "Glove",
                "Word2vec",
                "BERT"
            ],
            [
                "ELMo",
                "Word Embedding"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_7",
        "paper_id": "D19-1230",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1231table_3",
        "description": "5.3 Results on Local Discrimination . Table 3 shows the results in accuracy on the \u0093local\u0094\u009d discrimination task. From the table, we see that existing models including our global model perform poorly compared to our proposed local models. They are likely to fail in distinguishing the text segments that are locally coherent and penalize them unfairly. One of the possible explanations of this phenomenon can be found in the nature of the global model. These models (except L&H) are designed to make a decision at a global level, thus they are likely to penalize locally coherent segments of a text. This observation is further bolstered by the performance of our local coherence models, which show higher sensitivity in discriminating locally coherent texts and achieve significantly higher accuracy compared to the baseline models and our global model.",
        "sentences": [
            "5.3 Results on Local Discrimination .",
            "Table 3 shows the results in accuracy on the \u0093local\u0094\u009d discrimination task.",
            "From the table, we see that existing models including our global model perform poorly compared to our proposed local models.",
            "They are likely to fail in distinguishing the text segments that are locally coherent and penalize them unfairly.",
            "One of the possible explanations of this phenomenon can be found in the nature of the global model.",
            "These models (except L&H) are designed to make a decision at a global level, thus they are likely to penalize locally coherent segments of a text.",
            "This observation is further bolstered by the performance of our local coherence models, which show higher sensitivity in discriminating locally coherent texts and achieve significantly higher accuracy compared to the baseline models and our global model."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "Our Local Model"
            ],
            [
                "Lex. Neural Grid (M&J)",
                "Dist. sentence (L&H)",
                "Our Full Model",
                "Our Global Model",
                "Our Local Model"
            ],
            null,
            null,
            null,
            [
                "Our Local Model",
                "Our Global Model",
                "Lex. Neural Grid (M&J)",
                "Dist. sentence (L&H)"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_3",
        "paper_id": "D19-1231",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1233table_4",
        "description": "5.4.2 Parsing Performance . Table 4 shows RST-DT test set labelled attachment metrics for various parsers. Our model outperforms all of the published neural models that do not use additional training data in Morey et al. (2017)'s replication study on all of the metrics. On span accuracy (S), we outperform all of the other parsers except for Feng and Hirst (2014a)'s graph CRF model. On spans with nuclearity (N), the equivalent of the unlabelled attachment score for discourse dependencies, we outperform all of the parsers in the study. We perform competitively on spans with relations (R), and we outperform all of the published parsers that do not use additional data on spans with nuclearity and relations (F). Our model also outperforms the discriminative baseline using the same features and implementation on all metrics by between 1.9% and 2.7%.",
        "sentences": [
            "5.4.2 Parsing Performance .",
            "Table 4 shows RST-DT test set labelled attachment metrics for various parsers.",
            "Our model outperforms all of the published neural models that do not use additional training data in Morey et al. (2017)'s replication study on all of the metrics.",
            "On span accuracy (S), we outperform all of the other parsers except for Feng and Hirst (2014a)'s graph CRF model.",
            "On spans with nuclearity (N), the equivalent of the unlabelled attachment score for discourse dependencies, we outperform all of the parsers in the study.",
            "We perform competitively on spans with relations (R), and we outperform all of the published parsers that do not use additional data on spans with nuclearity and relations (F).",
            "Our model also outperforms the discriminative baseline using the same features and implementation on all metrics by between 1.9% and 2.7%."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Generative Model"
            ],
            [
                "S",
                "Generative Model",
                "Feng and Hirst (2014a)"
            ],
            [
                "N",
                "Generative Model"
            ],
            [
                "R",
                "Generative Model",
                "F"
            ],
            [
                "Generative Model"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_4",
        "paper_id": "D19-1233",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1234table_2",
        "description": "As seen in Table 2 on STAC test data, GEN dramatically outperformed our deep learning baselines, BiLSTM, BERT, and BERT + LogReg* architectures on gold labels, as well as the LAST baseline, which attaches every DU in a dialogue to the DU directly preceding it. In addition, stand alone GEN also outperformed all the coupled Snorkel models, in which GEN is combined with an added discriminative step, by up to a 30 point improvement in F1 score (GEN vs. GEN+BiLSTM). We did not expect this, given that adding a discriminative model in Snorkel is meant to generalize, and hence improve, what GEN learns.",
        "sentences": [
            "As seen in Table 2 on STAC test data, GEN dramatically outperformed our deep learning baselines, BiLSTM, BERT, and BERT + LogReg* architectures on gold labels, as well as the LAST baseline, which attaches every DU in a dialogue to the DU directly preceding it.",
            "In addition, stand alone GEN also outperformed all the coupled Snorkel models, in which GEN is combined with an added discriminative step, by up to a 30 point improvement in F1 score (GEN vs. GEN+BiLSTM).",
            "We did not expect this, given that adding a discriminative model in Snorkel is meant to generalize, and hence improve, what GEN learns."
        ],
        "class_sentence": [
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "GEN",
                "BiLSTM on Gold labels",
                "GEN + Disc (BiLSTM)",
                "BERT on Gold labels",
                "BERT+LogReg* on Gold labels"
            ],
            [
                "GEN",
                "GENERATIVE STAND ALONE",
                "SNORKEL PIPELINE",
                "GEN + Disc (BiLSTM)",
                "F1 score"
            ],
            [
                "SNORKEL PIPELINE",
                "GEN"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_2",
        "paper_id": "D19-1234",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1239table_4",
        "description": "5 Results and Discussions 5.1 Comparison with ADR Detectors . In the first experiment, we compare our approach (i.e.trained with 100% of the labeled training data with 1 sample generated for each sample in the LC) against different ADR detector techniques described in Section 4.3. Table 4 reports precision, and recall and F1-measure, of all the baselines in comparison to proposed approach CRF+VAE in Twitter and Reddit dataset. We make the following observations: QuickUMLS is outperformed by all the other methods. The result shows that dictionary based approaches are not able to cover concepts that do not have a reference in UMLS dictionary, and produce false positives by labeling irrelevant words such as \u201cmaybe\u201d, \u201cenergy\u201d, \u201ccondition\u201d, \u201cillness\u201d, or \u201cworse\u201d as positive.",
        "sentences": [
            "5 Results and Discussions 5.1 Comparison with ADR Detectors .",
            "In the first experiment, we compare our approach (i.e.trained with 100% of the labeled training data with 1 sample generated for each sample in the LC) against different ADR detector techniques described in Section 4.3.",
            "Table 4 reports precision, and recall and F1-measure, of all the baselines in comparison to proposed approach CRF+VAE in Twitter and Reddit dataset.",
            "We make the following observations: QuickUMLS is outperformed by all the other methods.",
            "The result shows that dictionary based approaches are not able to cover concepts that do not have a reference in UMLS dictionary, and produce false positives by labeling irrelevant words such as \u201cmaybe\u201d, \u201cenergy\u201d, \u201ccondition\u201d, \u201cillness\u201d, or \u201cworse\u201d as positive."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "CRF+VAE",
                "Precision",
                "Recall",
                "Fscore"
            ],
            [
                "QuickUMLS"
            ],
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_4",
        "paper_id": "D19-1239",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1247table_6",
        "description": "4.7.1 Without Pre-trained KB Embeddings. Pre-trained KB embeddings may provide rich structured relational information among entities. However, it heavily relies on large-scale triplets, which is time and resource-intensive. To investigate the effectiveness of pre-trained KB embedding for KBQG, we report the performance of KBQG whether using pre-trained KB embeddings by simply applying TransE. Table 6 shows that the performance of KBQG is degraded without TransE embeddings. In comparison, Elsahar et al. (2018) obtain obvious degradation on all metrics while there is only a slight decline in our model. We believe that it may owe to the contextaugmented fact encoder since our model drops to 40.87 on the BLEU4 score without contextaugmented fact encoder and transE embeddings.",
        "sentences": [
            "4.7.1 Without Pre-trained KB Embeddings.",
            "Pre-trained KB embeddings may provide rich structured relational information among entities.",
            "However, it heavily relies on large-scale triplets, which is time and resource-intensive.",
            "To investigate the effectiveness of pre-trained KB embedding for KBQG, we report the performance of KBQG whether using pre-trained KB embeddings by simply applying TransE.",
            "Table 6 shows that the performance of KBQG is degraded without TransE embeddings.",
            "In comparison, Elsahar et al. (2018) obtain obvious degradation on all metrics while there is only a slight decline in our model.",
            "We believe that it may owe to the contextaugmented fact encoder since our model drops to 40.87 on the BLEU4 score without contextaugmented fact encoder and transE embeddings."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "TransE"
            ],
            [
                "TransE"
            ],
            [
                "Our Model ans loss",
                "Elsahar et al. (2018)"
            ],
            [
                "Our Model ans loss",
                "BLEU4"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_6",
        "paper_id": "D19-1247",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1252table_3",
        "description": "Second, Multi-language fine-tuning is helpful to find the relation between languages, we will analyze it below. Table 3 show it can bring a significant boost in cross-lingual language understanding performance. With the help of Multi-language finetuning, Unicoder is been improved by 1.6% of accuracy on XNLI and 3.3% on XQA. In Table 3, we proved that Multi-language Fine-tuning with 15 languages is better than TRANSLATE-TRAIN who only fine-tune on 1 language. In this sub-section, we try more setting to analysis the relation between language number and fine-tuning performance.",
        "sentences": [
            "Second, Multi-language fine-tuning is helpful to find the relation between languages, we will analyze it below.",
            "Table 3 show it can bring a significant boost in cross-lingual language understanding performance.",
            "With the help of Multi-language finetuning, Unicoder is been improved by 1.6% of accuracy on XNLI and 3.3% on XQA.",
            "In Table 3, we proved that Multi-language Fine-tuning with 15 languages is better than TRANSLATE-TRAIN who only fine-tune on 1 language.",
            "In this sub-section, we try more setting to analysis the relation between language number and fine-tuning performance."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "Multi-language Fine-tuning",
                "Unicoder"
            ],
            [
                "Multi-language Fine-tuning",
                "Machine translate at training (TRANSLATE-TRAIN)"
            ],
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_3",
        "paper_id": "D19-1252",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1254table_2",
        "description": "Table 2 summarizes the experimental results. We observe that the proposed method consistently outperforms SAN and the SynNet+SAN model on all datasets. in the SQuAD \u2192 NewsQA setting, where the sourcedomain dataset is SQuAD and the target-domain dataset is NewsQA, AdaMRC achieves 38.46% and 54.20% in terms of EM and F1 scores, outperforming the pre-trained SAN by 1.78% (EM) and 1.41% (F1), respectively, as well as surpassing SynNet by 3.27% (EM) and 4.59% (F1), respectively. Similar improvements are also observed in NewsQA \u2192 SQuAD, SQuAD \u2192 MS MARCO and MS MARCO \u2192 SQuAD settings, which demonstrates the effectiveness of the proposed model.",
        "sentences": [
            "Table 2 summarizes the experimental results.",
            "We observe that the proposed method consistently outperforms SAN and the SynNet+SAN model on all datasets.",
            "in the SQuAD \u2192 NewsQA setting, where the sourcedomain dataset is SQuAD and the target-domain dataset is NewsQA, AdaMRC achieves 38.46% and 54.20% in terms of EM and F1 scores, outperforming the pre-trained SAN by 1.78% (EM) and 1.41% (F1), respectively, as well as surpassing SynNet by 3.27% (EM) and 4.59% (F1), respectively.",
            "Similar improvements are also observed in NewsQA \u2192 SQuAD, SQuAD \u2192 MS MARCO and MS MARCO \u2192 SQuAD settings, which demonstrates the effectiveness of the proposed model."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "AdaMRC",
                "AdaMRC with GT questions"
            ],
            [
                "SQuAD \u2192 NewsQA",
                "AdaMRC",
                "EM",
                "F1",
                "SAN",
                "SynNet + SAN"
            ],
            [
                "AdaMRC",
                "EM",
                "F1",
                "NewsQA \u2192 SQuAD",
                "SQuAD \u2192 MS MARCO",
                "MS MARCO \u2192 SQuAD"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "D19-1254",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1255table_3",
        "description": "Table 3 reports the human evaluation scores of KEAG and state-of-the-art answer generation models. The KEAG model surpasses all the others in generating correct answers syntactically and substantively. In terms of syntactic correctness, KEAG and MHPGM both perform well thanks to their architectures of composing answer text and integrating knowledge. On the other hand, KEAG significantly outperforms all compared models in generating substantively correct answers, which demonstrates its power in exploiting external knowledge.",
        "sentences": [
            "Table 3 reports the human evaluation scores of KEAG and state-of-the-art answer generation models.",
            "The KEAG model surpasses all the others in generating correct answers syntactically and substantively.",
            "In terms of syntactic correctness, KEAG and MHPGM both perform well thanks to their architectures of composing answer text and integrating knowledge.",
            "On the other hand, KEAG significantly outperforms all compared models in generating substantively correct answers, which demonstrates its power in exploiting external knowledge."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            [
                "KEAG",
                "gQA",
                "gQA w/ KBLSTM",
                "gQA w/ CRWE",
                "MHPGM"
            ],
            [
                "KEAG",
                "Correct"
            ],
            [
                "KEAG",
                "MHPGM"
            ],
            [
                "KEAG",
                "Correct"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "D19-1255",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1256table_3",
        "description": "Second, we verified the model performance on  the ARC test sets in order to check how the model  generalizes on unseen data and to compare it with  other  top  models  in  the ARC  public  leaderboard  (https://leaderboard.allenai.org/arc/subm issions/public). A  summary  of  the  results  is  reported in Table 3 and Table 4. In both cases, our  Attentive  Ranker  model  outperforms  the  current  state-of-the-art  (SOTA)  approach  proving  that,  indeed,  performing  a  semantic  ranking  is  very  effective  for  QA  systems.",
        "sentences": [
            "Second, we verified the model performance on  the ARC test sets in order to check how the model  generalizes on unseen data and to compare it with  other  top  models  in  the ARC  public  leaderboard  (https://leaderboard.allenai.org/arc/subm issions/public).",
            "A  summary  of  the  results  is  reported in Table 3 and Table 4.",
            "In both cases, our  Attentive  Ranker  model  outperforms  the  current  state-of-the-art  (SOTA)  approach  proving  that,  indeed,  performing  a  semantic  ranking  is  very  effective  for  QA  systems."
        ],
        "class_sentence": [
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Attentive Ranker (ours)",
                "Reading Strategies (previous SOTA)"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_3",
        "paper_id": "D19-1256",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1256table_6",
        "description": "Table 6 shows that using documents ranked by  our  attentive  neural  network  always  leads  to  a  performance  increase  in  downstream  models,  compared  to  TF-IDF. On  the  validation  set,  the  improvement is considerably higher (+7.79) due to  a  possible  over-fitting  of  the  hyperparameters  during the Attentive Ranker's training.",
        "sentences": [
            "Table 6 shows that using documents ranked by  our  attentive  neural  network  always  leads  to  a  performance  increase  in  downstream  models,  compared  to  TF-IDF.",
            "On  the  validation  set,  the  improvement is considerably higher (+7.79) due to  a  possible  over-fitting  of  the  hyperparameters  during the Attentive Ranker's training."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "Ours",
                "TF-IDF"
            ],
            null
        ],
        "n_sentence": 2.0,
        "table_id": "table_6",
        "paper_id": "D19-1256",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1259table_4",
        "description": "5 Experiments . 5.1 Human Performance . Human performance is measured during the annotation: As shown in Algorithm 1, annotations of annotator 1 and annotator 2 are used to calculate reasoning-free and reasoning-required human performance, respectively, against the discussed ground truth labels. Human performance on the test set of PQA-L is shown in Table 4. We only test single-annotator performance due to limited resources. Kwiatkowski et al. (2019) show that an ensemble of annotators perform significantly better than single-annotator, so the results reported in Table 4 are the lower bounds of human performance. Under reasoning-free setting where the annotator can see the conclusions, a single human achieves 90.4% accuracy and 84.2% macroF1.",
        "sentences": [
            "5 Experiments .",
            "5.1 Human Performance .",
            "Human performance is measured during the annotation: As shown in Algorithm 1, annotations of annotator 1 and annotator 2 are used to calculate reasoning-free and reasoning-required human performance, respectively, against the discussed ground truth labels.",
            "Human performance on the test set of PQA-L is shown in Table 4.",
            "We only test single-annotator performance due to limited resources.",
            "Kwiatkowski et al. (2019) show that an ensemble of annotators perform significantly better than single-annotator, so the results reported in Table 4 are the lower bounds of human performance.",
            "Under reasoning-free setting where the annotator can see the conclusions, a single human achieves 90.4% accuracy and 84.2% macroF1."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null,
            null,
            [
                "Reasoning-Free",
                "Accuracy (%)",
                "Macro-F1 (%)"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_4",
        "paper_id": "D19-1259",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1268table_2",
        "description": "4.4 Results Table 2 shows the performances of different methods on the link prediction task according to various metrics. Numbers in bold mean the best results among all methods and the underlined ones mean the second best. The evaluation results of baselines are from their original work, and \u201d-\u201d in the table means there is no reported results in prior work. Note that we implement ProjE and PTransE on WN18 using the public codes. From Table 2 we could observe that: (1) PTransE performs better than its basic model TransE, and RPE outperforms its original method TransR. This indicates that additional information from relation paths between entity pairs is helpful for link prediction. Note that OPTransE outperforms baselines which do not take relation paths into consideration in most cases. These results demonstrate the effectiveness of OPTransE to take advantage of the path features in the KG. (2) OPTransE performs better than previous pathbased models like RTransE, PTransE, PaSKoGE and RPE on all metrics. This implies that the order of relations in paths is of great importance for reasoning, and learning representations of ordered relation paths can significantly improve the accuracy of link prediction. Moreover, the proposed pooling strategy which aims to extract nonlinear features from different relation paths also contributes to the improvements of performance.",
        "sentences": [
            "4.4 Results Table 2 shows the performances of different methods on the link prediction task according to various metrics.",
            "Numbers in bold mean the best results among all methods and the underlined ones mean the second best.",
            "The evaluation results of baselines are from their original work, and \u201d-\u201d in the table means there is no reported results in prior work.",
            "Note that we implement ProjE and PTransE on WN18 using the public codes.",
            "From Table 2 we could observe that: (1) PTransE performs better than its basic model TransE, and RPE outperforms its original method TransR.",
            "This indicates that additional information from relation paths between entity pairs is helpful for link prediction.",
            "Note that OPTransE outperforms baselines which do not take relation paths into consideration in most cases.",
            "These results demonstrate the effectiveness of OPTransE to take advantage of the path features in the KG.",
            "(2) OPTransE performs better than previous pathbased models like RTransE, PTransE, PaSKoGE and RPE on all metrics.",
            "This implies that the order of relations in paths is of great importance for reasoning, and learning representations of ordered relation paths can significantly improve the accuracy of link prediction.",
            "Moreover, the proposed pooling strategy which aims to extract nonlinear features from different relation paths also contributes to the improvements of performance."
        ],
        "class_sentence": [
            1,
            2,
            2,
            2,
            1,
            2,
            2,
            2,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            [
                "PTransE (ADD, 2-step)",
                "PTransE (MUL, 2-step)",
                "PTransE (ADD, 3-step)",
                "OPTransE",
                "TransE",
                "RPE (ACOM)",
                "RPE (MCOM)",
                "TransR"
            ],
            null,
            [
                "OPTransE"
            ],
            [
                "OPTransE"
            ],
            [
                "OPTransE",
                "RTransE",
                "PTransE (ADD, 2-step)",
                "PTransE (MUL, 2-step)",
                "PTransE (ADD, 3-step)",
                "PaSKoGE",
                "RPE (ACOM)",
                "RPE (MCOM)"
            ],
            null,
            null
        ],
        "n_sentence": 11.0,
        "table_id": "table_2",
        "paper_id": "D19-1268",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1272table_2",
        "description": "Table 2 shows overall average results by model. As seen in Table 2, both SMERTI variations achieve higher STES and outperform the other models overall, with the WordNet models performing the worst. SMERTI excels especially on fluency and content similarity. The transformer variation achieves slightly higher SLOR, while the RNN variation achieves slightly higher CSS. These results correspond well with our automatic evaluation results in Table 2. We look at the Pearson correlation values between RE Match, Fluency, and Sentiment Preservation with CSS, SLOR, and SPA, respectively. These are 0.9952, 0.9327, and 0.8768, respectively, demonstrating that our automatic metrics are highly effective and correspond well with human ratings.",
        "sentences": [
            "Table 2 shows overall average results by model.",
            "As seen in Table 2, both SMERTI variations achieve higher STES and outperform the other models overall, with the WordNet models performing the worst.",
            "SMERTI excels especially on fluency and content similarity.",
            "The transformer variation achieves slightly higher SLOR, while the RNN variation achieves slightly higher CSS.",
            "These results correspond well with our automatic evaluation results in Table 2.",
            "We look at the Pearson correlation values between RE Match, Fluency, and Sentiment Preservation with CSS, SLOR, and SPA, respectively.",
            "These are 0.9952, 0.9327, and 0.8768, respectively, demonstrating that our automatic metrics are highly effective and correspond well with human ratings."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            0,
            0,
            0
        ],
        "header_mention": [
            null,
            [
                "SMERTI-Transformer",
                "SMERTI-RNN",
                "STES"
            ],
            [
                "SMERTI-Transformer",
                "SMERTI-RNN"
            ],
            [
                "SMERTI-Transformer",
                "SLOR",
                "SMERTI-RNN",
                "CSS"
            ],
            null,
            null,
            null
        ],
        "n_sentence": 7.0,
        "table_id": "table_2",
        "paper_id": "D19-1272",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1278table_2",
        "description": "5 Results System comparison Table 2 summarizes our results on the PMB gold data (v.2.1.0, test set). We compare our graph decoder against the system of van Noord et al. (2018) and our implementation of a seq2seq model, enhanced with a copy mechanism. Overall, we see that our graph decoder outperforms both models. Moreover, it reduces the number of illformed representations without any specific constraints or post-processing in order to ensure the well-formedness of the semantics of the output.",
        "sentences": [
            "5 Results System comparison Table 2 summarizes our results on the PMB gold data (v.2.1.0, test set).",
            "We compare our graph decoder against the system of van Noord et al. (2018) and our implementation of a seq2seq model, enhanced with a copy mechanism.",
            "Overall, we see that our graph decoder outperforms both models.",
            "Moreover, it reduces the number of illformed representations without any specific constraints or post-processing in order to ensure the well-formedness of the semantics of the output."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "van Noord et al. (2018)",
                "seq2seq+copy"
            ],
            [
                "van Noord et al. (2018)",
                "seq2seq+copy",
                "seq2graph"
            ],
            [
                "seq2graph"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "D19-1278",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1282table_1",
        "description": "We conduct the experiments with our in-house splits to investigate whether our KA GNE T can also work well on other universal language encoders (GPT and BERT-BASE), particularly with different fractions of the dataset (say 10%, 50%, 100% of the training data). Table 1 shows that our KA GNE T-based methods using fixed pre-trained language encoders outperform fine-tuning themselves in all settings. Furthermore, we find that the improvements in a small data situation (10%) is relatively limited, and we believe an important future research direction is thus few-shot learning for commonsense reasoning.",
        "sentences": [
            "We conduct the experiments with our in-house splits to investigate whether our KA GNE T can also work well on other universal language encoders (GPT and BERT-BASE), particularly with different fractions of the dataset (say 10%, 50%, 100% of the training data).",
            "Table 1 shows that our KA GNE T-based methods using fixed pre-trained language encoders outperform fine-tuning themselves in all settings.",
            "Furthermore, we find that the improvements in a small data situation (10%) is relatively limited, and we believe an important future research direction is thus few-shot learning for commonsense reasoning."
        ],
        "class_sentence": [
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "GPT-KAGNET",
                "BERT-BASE-KAGNET",
                "BERT-LARGE-KAGNET",
                "GPT-FINETUNING",
                "BERT-BASE-FINETUNING",
                "BERT-LARGE-FINETUNING"
            ],
            [
                "10(%) of Ihtrain"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_1",
        "paper_id": "D19-1282",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1284table_4",
        "description": "Table 4 shows training method significantly outperforms all the weaklysupervised learning algorithms, including 10% gain over the previous state of the art. These results indicate that precomputing a solution set and training a model through hard updates play a significant role to the performance. Given that our method does not require SQL executions at training time (unlike MAPO), it provides a simpler, more effective and time-efficient strategy. Comparing to previous models with full supervision, our results are still on par and outperform most of the published results.",
        "sentences": [
            "Table 4 shows training method significantly outperforms all the weaklysupervised learning algorithms, including 10% gain over the previous state of the art.",
            "These results indicate that precomputing a solution set and training a model through hard updates play a significant role to the performance.",
            "Given that our method does not require SQL executions at training time (unlike MAPO), it provides a simpler, more effective and time-efficient strategy.",
            "Comparing to previous models with full supervision, our results are still on par and outperform most of the published results."
        ],
        "class_sentence": [
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            [
                "Fully-supervised setting",
                "Weakly-supervised setting"
            ],
            null,
            null,
            [
                "Ours",
                "Fully-supervised setting"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_4",
        "paper_id": "D19-1284",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1291table_3",
        "description": "Intra-turn Relations . We report the results of our binary classification task in Table 3 in terms of Precision, Recall and F-score for the \u201ctrue\u201d class, i.e., when a relation is present. We report results given both gold premises and predicted premises (using our best model from 5.1). Our best results are obtained from ensembling the RST classifier with BERT fine-tuned on IMHO+context, for statistically significant (p < 0.001) improvement over all other models. We obtain comparable performance to previous work on relation prediction in other argumentative datasets (Niculae et al., 2017; Morio and Fujita, 2018).",
        "sentences": [
            "Intra-turn Relations .",
            "We report the results of our binary classification task in Table 3 in terms of Precision, Recall and F-score for the \u201ctrue\u201d class, i.e., when a relation is present.",
            "We report results given both gold premises and predicted premises (using our best model from 5.1).",
            "Our best results are obtained from ensembling the RST classifier with BERT fine-tuned on IMHO+context, for statistically significant (p < 0.001) improvement over all other models.",
            "We obtain comparable performance to previous work on relation prediction in other argumentative datasets (Niculae et al., 2017; Morio and Fujita, 2018)."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Precision",
                "Recall",
                "F-Score"
            ],
            [
                "Gold",
                "Pred"
            ],
            [
                " + RST Ensemble"
            ],
            [
                "IMHO Context Fine-Tuned BERT",
                "Morio and Fujita (2018)"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_3",
        "paper_id": "D19-1291",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1294table_2",
        "description": "Table 2 presents results for our binary classification task. The BERT classifier has higher F1-score and precision than other classifiers. The BERTSA model after three epochs only achieves an F1 score of 0.491, which confirms the difference between sentiment analysis and our good/bad task, i.e., even if the segment has positive sentiment, it might be not suitable as a justification.",
        "sentences": [
            "Table 2 presents results for our binary classification task.",
            "The BERT classifier has higher F1-score and precision than other classifiers.",
            "The BERTSA model after three epochs only achieves an F1 score of 0.491, which confirms the difference between sentiment analysis and our good/bad task, i.e., even if the segment has positive sentiment, it might be not suitable as a justification."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "BERT",
                "F1",
                "Precision"
            ],
            [
                "BERT-SA (three epoch)",
                "F1"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_2",
        "paper_id": "D19-1294",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1296table_1",
        "description": "Results . Table 1 lists the accuracy (Acc), precision (Prec), recall (Rec), and F1 of the compared methods. The SECTION method achieved 100% precision, which indicates that our technique of exploiting causality-describing sections in Wikipedia could accurately extract causalities. As the method\u2019s recall indicates, however, it covered only a small portion of our target causalities. The INFOBOX method also achieved 100% precision, though its coverage was also quite limited. The RELATED method exhibited the highest recall, but the precision was unacceptably low for the subsequent manual labor that would be required to construct the CKB. The ORACLE RE method achieved 100% precision by design. Its recall was rather low because 67.3% of the entity pairs in the data (\u00a73.1) consisted of entities that did NOT co-occur in a sentence. This means that most RE methods that work sentence-wise will miss a large portion of causalities, regardless of their accuracy. Finally, PROP achieved the best F1 score, though its recall still had room for improvement. The fact that PROP outperformed the baselines, especially ORACLE RE, clearly shows the effectiveness of our method.",
        "sentences": [
            "Results .",
            "Table 1 lists the accuracy (Acc), precision (Prec), recall (Rec), and F1 of the compared methods.",
            "The SECTION method achieved 100% precision, which indicates that our technique of exploiting causality-describing sections in Wikipedia could accurately extract causalities.",
            "As the method\u2019s recall indicates, however, it covered only a small portion of our target causalities.",
            "The INFOBOX method also achieved 100% precision, though its coverage was also quite limited.",
            "The RELATED method exhibited the highest recall, but the precision was unacceptably low for the subsequent manual labor that would be required to construct the CKB.",
            "The ORACLE RE method achieved 100% precision by design. Its recall was rather low because 67.3% of the entity pairs in the data (\u00a73.1) consisted of entities that did NOT co-occur in a sentence.",
            "This means that most RE methods that work sentence-wise will miss a large portion of causalities, regardless of their accuracy.",
            "Finally, PROP achieved the best F1 score, though its recall still had room for improvement.",
            "The fact that PROP outperformed the baselines, especially ORACLE RE, clearly shows the effectiveness of our method."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Acc",
                "Prec",
                "Rec",
                "F1"
            ],
            [
                "SECTION",
                "Prec"
            ],
            null,
            [
                "INFOBOX",
                "Prec"
            ],
            [
                "RELATED"
            ],
            [
                "O RACLE RE",
                "Prec"
            ],
            [
                "O RACLE RE"
            ],
            [
                "PROP",
                "F1"
            ],
            [
                "PROP",
                "O RACLE RE"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_1",
        "paper_id": "D19-1296",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1298table_2",
        "description": "The performance of all models on arXiv and Pubmed is shown in Table 2 and Table 3, respectively. Follow the work (Kedzie et al., 2018), we use the approximate randomization as the statistical significance test method (Riezler and Maxwell, 2005) with a Bonferroni correction for multiple comparisons, at the confidence level 0.01 (p < 0.01). As we can see in these tables, on both datasets, the neural extractive models outperforms the traditional extractive models on informativeness (ROUGE-1,2) by a wide margin, but results are mixed on ROUGE-L. Presumably, this is due to the neural training process, which relies on a goal standard based on ROUGE-1. Exploring other training schemes and/or a combination of traditional and neural approaches is left as future work. Similarly, the neural extractive models also dominate the neural abstractive models on ROUGE-1,2, but these abstractive models tend to have the highest ROUGE-L scores, possibly because they are trained directly on gold standard abstract summaries. Compared with other neural extractive models, our models (both with attentive context and concatenation decoder) have better performances on all three ROUGE scores, as well as METEOR. In particular, the improvements over the Baseline model show that a combination of local and global contextual information does help to identify the most important sentences (more on this in the next section). Interestingly, just the Baseline model already achieves a slightly better performance than previous works; possibly because the auto-regressive approach used in those models is even more detrimental for long documents.",
        "sentences": [
            "The performance of all models on arXiv and Pubmed is shown in Table 2 and Table 3, respectively.",
            "Follow the work (Kedzie et al., 2018), we use the approximate randomization as the statistical significance test method (Riezler and Maxwell, 2005) with a Bonferroni correction for multiple comparisons, at the confidence level 0.01 (p < 0.01).",
            "As we can see in these tables, on both datasets, the neural extractive models outperforms the traditional extractive models on informativeness (ROUGE-1,2) by a wide margin, but results are mixed on ROUGE-L.",
            "Presumably, this is due to the neural training process, which relies on a goal standard based on ROUGE-1.",
            "Exploring other training schemes and/or a combination of traditional and neural approaches is left as future work.",
            "Similarly, the neural extractive models also dominate the neural abstractive models on ROUGE-1,2, but these abstractive models tend to have the highest ROUGE-L scores, possibly because they are trained directly on gold standard abstract summaries.",
            "Compared with other neural extractive models, our models (both with attentive context and concatenation decoder) have better performances on all three ROUGE scores, as well as METEOR.",
            "In particular, the improvements over the Baseline model show that a combination of local and global contextual information does help to identify the most important sentences (more on this in the next section).",
            "Interestingly, just the Baseline model already achieves a slightly better performance than previous works; possibly because the auto-regressive approach used in those models is even more detrimental for long documents."
        ],
        "class_sentence": [
            2,
            2,
            1,
            2,
            2,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Baseline",
                "Cheng & Lapata",
                "SummaRuNNer",
                "Ours-attentive context",
                "Ours-concat",
                "SumBasic*",
                "LSA*",
                "LexRank*",
                "ROUGE-1",
                "ROUGE-2",
                "ROUGE-L"
            ],
            [
                "ROUGE-1"
            ],
            null,
            [
                "Baseline",
                "Cheng & Lapata",
                "SummaRuNNer",
                "Ours-attentive context",
                "Ours-concat",
                "ROUGE-1",
                "ROUGE-2",
                "ROUGE-L",
                "Attn-Seq2Seq*",
                "Pntr-Gen-Seq2Seq*",
                "Discourse-aware*"
            ],
            [
                "Baseline",
                "Cheng & Lapata",
                "SummaRuNNer",
                "Ours-attentive context",
                "Ours-concat",
                "ROUGE-1",
                "ROUGE-2",
                "ROUGE-L",
                "METEOR"
            ],
            [
                "Baseline"
            ],
            [
                "Baseline"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_2",
        "paper_id": "D19-1298",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1300table_1",
        "description": "4 Experimental Results . 4.1 Quantitative Analysis . We first report the ROUGE metrics on the combined CNN/DailyMail test sets in Table 1 and the separate results in Table 2. We can get several observations from these two tables. Firstly, our model generally performs the best and even surpasses 42 on ROUGE-1 score on the combined CNN/DailyMail dataset. It also shows better results on the separate datasets. We argue that global and local features from rough reading can help extract summaries by capturing deep contextual relations, and the designed structure in careful reading makes it more flexible in selecting sentence sets. Hence a two-stage framework based on the human\u2019s reading cognition is more appropriate for extractive summarization. Secondly, directly optimizing the evaluation metric by combining cross-entropy loss with rewards may improve the extractive results. RLbased methods, Refresh (Narayan et al., 2018) and RNES (Wu and Hu, 2018), perform better than the sequence labeling methods like SummaRuNNer (Nallapati et al., 2017). BANDITSUM (Dong et al., 2018) generally performs better than the other baselines, and it reports that framing the extractive summarization based on contextual bandit is more suitable than sequential labeling setting and also has more search space than other RLbased methods (Narayan et al., 2018; Yao et al., 2018; Wu and Hu, 2018).",
        "sentences": [
            "4 Experimental Results .",
            "4.1 Quantitative Analysis .",
            "We first report the ROUGE metrics on the combined CNN/DailyMail test sets in Table 1 and the separate results in Table 2.",
            "We can get several observations from these two tables.",
            "Firstly, our model generally performs the best and even surpasses 42 on ROUGE-1 score on the combined CNN/DailyMail dataset.",
            "It also shows better results on the separate datasets.",
            "We argue that global and local features from rough reading can help extract summaries by capturing deep contextual relations, and the designed structure in careful reading makes it more flexible in selecting sentence sets.",
            "Hence a two-stage framework based on the human\u2019s reading cognition is more appropriate for extractive summarization.",
            "Secondly, directly optimizing the evaluation metric by combining cross-entropy loss with rewards may improve the extractive results.",
            "RLbased methods, Refresh (Narayan et al., 2018) and RNES (Wu and Hu, 2018), perform better than the sequence labeling methods like SummaRuNNer (Nallapati et al., 2017).",
            "BANDITSUM (Dong et al., 2018) generally performs better than the other baselines, and it reports that framing the extractive summarization based on contextual bandit is more suitable than sequential labeling setting and also has more search space than other RLbased methods (Narayan et al., 2018; Yao et al., 2018; Wu and Hu, 2018)."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1,
            2,
            2,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                " ROUGE"
            ],
            null,
            [
                "HER",
                "R1"
            ],
            null,
            null,
            null,
            null,
            [
                "Refresh",
                "RNES"
            ],
            [
                "BANDITSUM"
            ]
        ],
        "n_sentence": 11.0,
        "table_id": "table_1",
        "paper_id": "D19-1300",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1300table_3",
        "description": "4.2 Ablation Test. Next, we conduct ablation test by removing the modules of the proposed HER step by step. Firstly, we replace the automatic termination mechanism with a fixed extracting strategy that always selects three sentences for every document and we present the model as HER-3. Based on HER-3, we also remove bandit policy, local net, general net gradually, and denote them as HER-3 w/o policy, HER3 w/o policy & local net and HER-3 w/o policy & rough reading individually. The results are reported in Table 3 and it proves the effectiveness of each proposed module. Firstly, HER constructed with an automatic termination mechanism is more \u00ef\u00ac\u201aexible and reliable in extracting various numbers of sentences varying different documents. Secondly, HER use (cid:15)-greedy to select sentences in order to raise the exploration chances on discovering important but easily ignored information. Thirdly, general cognition from rough reading process is useful in extractive summarizarion.",
        "sentences": [
            "4.2 Ablation Test.",
            "Next, we conduct ablation test by removing the modules of the proposed HER step by step.",
            "Firstly, we replace the automatic termination mechanism with a fixed extracting strategy that always selects three sentences for every document and we present the model as HER-3.",
            "Based on HER-3, we also remove bandit policy, local net, general net gradually, and denote them as HER-3 w/o policy, HER3 w/o policy & local net and HER-3 w/o policy & rough reading individually.",
            "The results are reported in Table 3 and it proves the effectiveness of each proposed module.",
            "Firstly, HER constructed with an automatic termination mechanism is more \u00ef\u00ac\u201aexible and reliable in extracting various numbers of sentences varying different documents.",
            "Secondly, HER use (cid:15)-greedy to select sentences in order to raise the exploration chances on discovering important but easily ignored information.",
            "Thirdly, general cognition from rough reading process is useful in extractive summarizarion."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "HER"
            ],
            [
                "HER-3"
            ],
            [
                "HER-3",
                "HER-3 w/o policy",
                "HER-3 w/o policy&L",
                "HER-3 w/o policy&F"
            ],
            null,
            [
                "HER"
            ],
            null,
            null
        ],
        "n_sentence": 8.0,
        "table_id": "table_3",
        "paper_id": "D19-1300",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1301table_1",
        "description": "4.3 Results. 4.3.1 English Results. The experimental results on the English evaluation sets are listed in Table 1. We report the full-length F-1 scores of ROUGE-1 (R-1), ROUGE2 (R-2), and ROUGE-L (R-L) on the evaluation set of the annotated Gigaword, while report the recall-based scores of the R-1, R-2, and R-L on the evaluation set of DUC2004 to follow the setting of the previous works. The results of our works are shown at the bottom of Table 1. The performances of the related works are reported in the upper part of Table 1 for comparison. ABS and ABS+ are the pioneer works of using neural models for abstractive text summarization. RAS-Elman extends ABS/ABS+ with attentive CNN encoder, wordslvt5k-1sent uses large vocabulary and linguistic features such as POS and NER tags. RNNMRT, Actor-Critic, StructuredLoss are sequence-level training methods to overcome the problem of the usual teacher-forcing methods. DRGD uses recurrent latent random model to improve summarization quality. FactAware generates summary words conditioned on both the source text and the fact descriptions extracted from OpenIE or dependencies. Besides the above RNN-based related works, CNN-based architectures of ConvS2S and ConvS2SReinforceTopic are included for comparison. Table 1 shows that we build a strong baseline using Transformer alone which obtains the state-of-the-art performance on Gigaword evaluation set, and obtains comparable performance to the state-of-the-art on DUC2004. When we introduce the contrastive attention mechanism into Transformer, it significantly improves the performance of Transformer, and greatly advances the state-of-the-art on both Gigaword evaluation set and DUC2004, as shown in the row of \u201cTransformer+Contrastive Attention\u201d.",
        "sentences": [
            "4.3 Results.",
            "4.3.1 English Results.",
            "The experimental results on the English evaluation sets are listed in Table 1.",
            "We report the full-length F-1 scores of ROUGE-1 (R-1), ROUGE2 (R-2), and ROUGE-L (R-L) on the evaluation set of the annotated Gigaword, while report the recall-based scores of the R-1, R-2, and R-L on the evaluation set of DUC2004 to follow the setting of the previous works.",
            "The results of our works are shown at the bottom of Table 1.",
            "The performances of the related works are reported in the upper part of Table 1 for comparison.",
            "ABS and ABS+ are the pioneer works of using neural models for abstractive text summarization.",
            "RAS-Elman extends ABS/ABS+ with attentive CNN encoder, wordslvt5k-1sent uses large vocabulary and linguistic features such as POS and NER tags.",
            "RNNMRT, Actor-Critic, StructuredLoss are sequence-level training methods to overcome the problem of the usual teacher-forcing methods.",
            "DRGD uses recurrent latent random model to improve summarization quality.",
            "FactAware generates summary words conditioned on both the source text and the fact descriptions extracted from OpenIE or dependencies.",
            "Besides the above RNN-based related works, CNN-based architectures of ConvS2S and ConvS2SReinforceTopic are included for comparison.",
            "Table 1 shows that we build a strong baseline using Transformer alone which obtains the state-of-the-art performance on Gigaword evaluation set, and obtains comparable performance to the state-of-the-art on DUC2004.",
            "When we introduce the contrastive attention mechanism into Transformer, it significantly improves the performance of Transformer, and greatly advances the state-of-the-art on both Gigaword evaluation set and DUC2004, as shown in the row of \u201cTransformer+Contrastive Attention\u201d."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1,
            1,
            1,
            2,
            2,
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "R-1",
                "R-2",
                "R-L"
            ],
            [
                "Transformer+ContrastiveAttention"
            ],
            null,
            [
                "ABS (Rush et al. 2015)",
                "ABS+ (Rush et al. 2015)"
            ],
            [
                "RAS-Elman (Chopra et al. 2016)"
            ],
            [
                "RNNMRT (Ayana et al. 2016)",
                "Actor-Critic (Li et al. 2018)",
                "StructuredLoss (Edunov et al. 2018)"
            ],
            [
                "DRGD (Li et al. 2017)"
            ],
            [
                "FactAware (Cao et al. 2018)"
            ],
            [
                "ConvS2SReinforceTopic (Wang et al. 2018)",
                "ConvS2S (Gehring et al. 2017)"
            ],
            [
                "Transformer"
            ],
            [
                "Transformer+ContrastiveAttention"
            ]
        ],
        "n_sentence": 14.0,
        "table_id": "table_1",
        "paper_id": "D19-1301",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1301table_2",
        "description": "4.3.2 Chinese Results. Table 2 presents the evaluation results on LCSTS. The upper rows list the performances of the related works, the bottom rows list the performances of our Transformer baseline and the integration of the contrastive attention mechanism into Transformer. We only take character sequences as source-summary pairs and evaluate the performance based on reference characters for strict comparison to the related works. Table 2 shows that Transformer also sets a strong baseline on LCSTS that surpasses the performances of the previous works. When Transformer is equipped with our proposed contrastive attention mechanism, the performance is significantly improved and drastically advances the state-of-the-art on LCSTS.",
        "sentences": [
            "4.3.2 Chinese Results.",
            "Table 2 presents the evaluation results on LCSTS.",
            "The upper rows list the performances of the related works, the bottom rows list the performances of our Transformer baseline and the integration of the contrastive attention mechanism into Transformer.",
            "We only take character sequences as source-summary pairs and evaluate the performance based on reference characters for strict comparison to the related works.",
            "Table 2 shows that Transformer also sets a strong baseline on LCSTS that surpasses the performances of the previous works.",
            "When Transformer is equipped with our proposed contrastive attention mechanism, the performance is significantly improved and drastically advances the state-of-the-art on LCSTS."
        ],
        "class_sentence": [
            2,
            1,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Transformer"
            ],
            null,
            [
                "Transformer"
            ],
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_2",
        "paper_id": "D19-1301",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1307table_4",
        "description": "Table 4 presents the human evaluation results. Summaries generated by NeuralTD receives significantly higher human evaluation scores than those by Refresh and ExtAbsRL. Also, the average human rating for Refresh is significantly higher than ExtAbsRL.",
        "sentences": [
            "Table 4 presents the human evaluation results.",
            "Summaries generated by NeuralTD receives significantly higher human evaluation scores than those by Refresh and ExtAbsRL.",
            "Also, the average human rating for Refresh is significantly higher than ExtAbsRL."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Ours",
                " Refresh",
                " ExtAbsRL"
            ],
            [
                "Avg. Human Rating",
                " Refresh",
                " ExtAbsRL"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_4",
        "paper_id": "D19-1307",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1307table_5",
        "description": "7.3 Abstractive Summarisation. Table 5 compares the ROUGE scores of using different rewards to train the extractor in ExtAbsRL (the abstractor is pre-trained, and is applied to rephrase the extracted sentences). Again, when ROUGE is used as rewards, the generated summaries have higher ROUGE scores. We randomly sampled 20 documents from the test set in CNN/DailyMail and asked three users to rate the quality of the two summaries generated with different rewards. We asked the users to consider not only the informativeness and conciseness of summaries, but also their grammaticality and faithfulness (whether the information in the summary is consistent with that in the original news). It is clear from Table 5 that using the learned reward helps the RL-based system generate summaries with significantly higher human ratings. Furthermore, we note that the overall human ratings for the abstractive summaries are lower than the extractive summaries (compared to Table 4). Qualitative analysis suggests that the poor overall rating may be caused by occasional information inconsistencies between a summary and its source text: for instance, a sentence in the source article reads \u201cafter Mayweather was almost two hours late for his workout , Pacquiao has promised to be on time\u201d, but the generated summary outputs \u201cMayweather has promised to be on time for the fight\u201d. High redundancy is another reason for the low human ratings: ExtAbsRL generates six summaries with redundant sentences when applying ROUGE-L as reward, while the number drops to two when the learned reward is applied.",
        "sentences": [
            "7.3 Abstractive Summarisation.",
            "Table 5 compares the ROUGE scores of using different rewards to train the extractor in ExtAbsRL (the abstractor is pre-trained, and is applied to rephrase the extracted sentences).",
            "Again, when ROUGE is used as rewards, the generated summaries have higher ROUGE scores.",
            "We randomly sampled 20 documents from the test set in CNN/DailyMail and asked three users to rate the quality of the two summaries generated with different rewards.",
            "We asked the users to consider not only the informativeness and conciseness of summaries, but also their grammaticality and faithfulness (whether the information in the summary is consistent with that in the original news).",
            "It is clear from Table 5 that using the learned reward helps the RL-based system generate summaries with significantly higher human ratings.",
            "Furthermore, we note that the overall human ratings for the abstractive summaries are lower than the extractive summaries (compared to Table 4).",
            "Qualitative analysis suggests that the poor overall rating may be caused by occasional information inconsistencies between a summary and its source text: for instance, a sentence in the source article reads \u201cafter Mayweather was almost two hours late for his workout , Pacquiao has promised to be on time\u201d, but the generated summary outputs \u201cMayweather has promised to be on time for the fight\u201d.",
            "High redundancy is another reason for the low human ratings: ExtAbsRL generates six summaries with redundant sentences when applying ROUGE-L as reward, while the number drops to two when the learned reward is applied."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            2,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "Learned (ours)",
                "R-1",
                " R-2",
                " R-L"
            ],
            null,
            null,
            [
                " Human",
                "R-L (original)"
            ],
            [
                " Human"
            ],
            null,
            null
        ],
        "n_sentence": 9.0,
        "table_id": "table_5",
        "paper_id": "D19-1307",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1308table_4",
        "description": "Comparison with State-of-the-art . Table 4 compares the performance of SELECTOR with the state-of-the-art bottom-up content selection of Gehrmann et al. (2018) in abstractive summarization. SELECTOR passes focus embeddings at the decoding step, whereas the bottom-up selection method only uses the masked words for the copy mechanism. We set K, the number of mixtures of SELECTOR, to 1 to directly compare it with the previous work (Bottom-Up (Gehrmann et al., 2018)). We observe that SELECTOR not only outperforms Bottom-Up in every metric, but also achieves a new state-of-the-art ROUGE-1 and ROUGE-L on CNN-DM. Moreover, our method scores state-of-the-art BLEU-4 in question generation on SQuAD (Table 1).",
        "sentences": [
            "Comparison with State-of-the-art .",
            "Table 4 compares the performance of SELECTOR with the state-of-the-art bottom-up content selection of Gehrmann et al. (2018) in abstractive summarization.",
            "SELECTOR passes focus embeddings at the decoding step, whereas the bottom-up selection method only uses the masked words for the copy mechanism.",
            "We set K, the number of mixtures of SELECTOR, to 1 to directly compare it with the previous work (Bottom-Up (Gehrmann et al., 2018)).",
            "We observe that SELECTOR not only outperforms Bottom-Up in every metric, but also achieves a new state-of-the-art ROUGE-1 and ROUGE-L on CNN-DM.",
            "Moreover, our method scores state-of-the-art BLEU-4 in question generation on SQuAD (Table 1)."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            1,
            0
        ],
        "header_mention": [
            null,
            [
                "SELECTOR & 10-Beam PG (Ours)"
            ],
            [
                "SELECTOR & 10-Beam PG (Ours)"
            ],
            [
                "SELECTOR & 10-Beam PG (Ours)",
                "Bottom-Up (Gehrmann et al., 2018)"
            ],
            [
                "SELECTOR & 10-Beam PG (Ours)",
                "Bottom-Up (Gehrmann et al., 2018)",
                " R-1",
                " R-L"
            ],
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_4",
        "paper_id": "D19-1308",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1308table_5",
        "description": "Efficient Training . Table 5 shows that SELECTOR trains up to 3.7 times faster than mixture decoder (Shen et al., 2019). Training time of mixture decoder linearly increases with the number of decoders, while parallel focus inference of SELECTOR makes additional training time negligible.",
        "sentences": [
            "Efficient Training .",
            "Table 5 shows that SELECTOR trains up to 3.7 times faster than mixture decoder (Shen et al., 2019).",
            "Training time of mixture decoder linearly increases with the number of decoders, while parallel focus inference of SELECTOR makes additional training time negligible."
        ],
        "class_sentence": [
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "SELECTOR (Ours)",
                "3-M. Decoder",
                "5-M. Decoder"
            ],
            [
                "3-M. Decoder",
                "5-M. Decoder",
                "SELECTOR (Ours)"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_5",
        "paper_id": "D19-1308",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1312table_4",
        "description": "6.2 Seen Entities vs. Unseen Entities. In the evaluation, we also distinguished the results for seen and unseen entities. We trained the model on a training set contains 64,353 referring expressions and evaluate the model on a test set with 3,591 referring expressions related to seen entities and 2,955 expressions related to unseen entities. Table 4 shows the evaluation results. From Table 4, it is easy to see that the model performs better when generating referring for seen entities. Among four referring expression types, the accuracy of description type drops dramatically for unseen entities, from 48.72% to 20.54%. This is probably due to the fact that compared with name and pronoun, description type is often hard to identify and more flexible. For instance, one of the gold-standard descriptions in the test set is the comic character , amazing-man. The model\u2019s generation for this referring expression is amazingman.",
        "sentences": [
            "6.2 Seen Entities vs. Unseen Entities.",
            "In the evaluation, we also distinguished the results for seen and unseen entities.",
            "We trained the model on a training set contains 64,353 referring expressions and evaluate the model on a test set with 3,591 referring expressions related to seen entities and 2,955 expressions related to unseen entities.",
            "Table 4 shows the evaluation results.",
            "From Table 4, it is easy to see that the model performs better when generating referring for seen entities.",
            "Among four referring expression types, the accuracy of description type drops dramatically for unseen entities, from 48.72% to 20.54%.",
            "This is probably due to the fact that compared with name and pronoun, description type is often hard to identify and more flexible.",
            "For instance, one of the gold-standard descriptions in the test set is the comic character , amazing-man.",
            "The model\u2019s generation for this referring expression is amazingman."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            [
                "Seen",
                "Unseen"
            ],
            [
                "Seen",
                "Unseen"
            ],
            [
                "Seen",
                "Unseen"
            ],
            null,
            [
                "Seen"
            ],
            [
                "Seen",
                "Unseen",
                "description"
            ],
            [
                "name",
                "pronoun",
                "description"
            ],
            null,
            null
        ],
        "n_sentence": 9.0,
        "table_id": "table_4",
        "paper_id": "D19-1312",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1313table_3",
        "description": "Table 3 shows scores on Twitter dataset. Our model achieves the best BLEU score and outperforms all baselines for METEOR. For diversity, our model again performs considerably better than other models. Since part of sentence pairs in Twitter dataset may be mislabelled by the labeling algorithm, it is harder to generate high-quality paraphrases on Twitter. Paraphrases generated by our model have not only more diverse expression but also better quality compared to those generated by other models.",
        "sentences": [
            "Table 3 shows scores on Twitter dataset.",
            "Our model achieves the best BLEU score and outperforms all baselines for METEOR.",
            "For diversity, our model again performs considerably better than other models.",
            "Since part of sentence pairs in Twitter dataset may be mislabelled by the labeling algorithm, it is harder to generate high-quality paraphrases on Twitter.",
            "Paraphrases generated by our model have not only more diverse expression but also better quality compared to those generated by other models."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "ours",
                "BLEU2",
                "BLEU3",
                "BLEU4",
                "self-BLEU2",
                "self-BLEU3",
                "self-BLEU4",
                "METEOR"
            ],
            [
                "ours",
                "Twitter (Diversity)"
            ],
            [
                "Twitter (Quality)"
            ],
            [
                "ours",
                "Twitter (Diversity)",
                "Twitter (Quality)"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_3",
        "paper_id": "D19-1313",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1313table_4",
        "description": "For human evaluation, we randomly select 100 input sentences from the test set of each dataset and get the generated results of different models for these inputs. We follow the human evaluation guideline in (Li et al., 2018). The sentence pairs are scored for two aspects of the generated results: fluency (whether the generated sentence is fluent and grammatical) and consistency (the meaning similarity between the input sentence and the generated sentence). Each output is given two ratings, scaling from 1 to 5, on fluency and consistency separately. An input and outputs generated for the input from different systems form a test group. The outputs from different systems in the test group are shuffled. The test groups are randomly assigned to annotators. Every test group is evaluated by two annotators and scores for each output are averaged. The agreement between annotators is moderate (kappa=0.54). The final score for each system is the average score for all outputs. Table 4 shows our model achieves better scores in both meaning similarity and fluency than baseline models.",
        "sentences": [
            "For human evaluation, we randomly select 100 input sentences from the test set of each dataset and get the generated results of different models for these inputs.",
            "We follow the human evaluation guideline in (Li et al., 2018).",
            "The sentence pairs are scored for two aspects of the generated results: fluency (whether the generated sentence is fluent and grammatical) and consistency (the meaning similarity between the input sentence and the generated sentence).",
            "Each output is given two ratings, scaling from 1 to 5, on fluency and consistency separately.",
            "An input and outputs generated for the input from different systems form a test group.",
            "The outputs from different systems in the test group are shuffled.",
            "The test groups are randomly assigned to annotators.",
            "Every test group is evaluated by two annotators and scores for each output are averaged.",
            "The agreement between annotators is moderate (kappa=0.54).",
            "The final score for each system is the average score for all outputs.",
            "Table 4 shows our model achieves better scores in both meaning similarity and fluency than baseline models."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Fluency",
                "Consistency"
            ],
            [
                "Fluency",
                "Consistency"
            ],
            null,
            null,
            null,
            null,
            null,
            null,
            [
                "ours",
                "Fluency",
                "Consistency"
            ]
        ],
        "n_sentence": 11.0,
        "table_id": "table_4",
        "paper_id": "D19-1313",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1315table_1",
        "description": "First, we conduct a human evaluation to measure the consistency among three outputs. Table 1 shows the evaluation results with their consistency. The scores are the percentage of articles evaluated as consistent by a majority of workers. We consider all three outputs as consistent if all pairs of outputs are consistent. The results indicate that the proposed method improves the consistency of the three generated outputs 1.",
        "sentences": [
            "First, we conduct a human evaluation to measure the consistency among three outputs.",
            "Table 1 shows the evaluation results with their consistency.",
            "The scores are the percentage of articles evaluated as consistent by a majority of workers.",
            "We consider all three outputs as consistent if all pairs of outputs are consistent.",
            "The results indicate that the proposed method improves the consistency of the three generated outputs 1."
        ],
        "class_sentence": [
            2,
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            [
                "Proposed",
                "3 Outputs"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_1",
        "paper_id": "D19-1315",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1315table_2",
        "description": "Second, to evaluate the quality of the generated headline, we implement a human evaluation to measure the adequacy and \ufb02uency. Table 2 shows the evaluation result along with the adequacy and \ufb02uency. The proposed method improves the adequacy by 0.42pt and the occupation adequacy by 0.44pt. Proposed method can generate more adequate outputs, particularly for the occupation.",
        "sentences": [
            "Second, to evaluate the quality of the generated headline, we implement a human evaluation to measure the adequacy and \ufb02uency.",
            "Table 2 shows the evaluation result along with the adequacy and \ufb02uency.",
            "The proposed method improves the adequacy by 0.42pt and the occupation adequacy by 0.44pt.",
            "Proposed method can generate more adequate outputs, particularly for the occupation."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Adequacy",
                " Fluency"
            ],
            [
                "Adequacy",
                " Occupation Adequacy"
            ],
            [
                "Proposed"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "D19-1315",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1315table_3",
        "description": "Automatic evaluation of job advertisement corpus. We implement an automatic evaluation using the ROUGE metrics (Lin, 2004) and accuracy. We conduct the experiment ten times, and calculate the average score. Table 3 shows the effect of the proposed methods: multi-task learning (MTL), scheduling strategy (SD) and hierarchical consistency loss (HCL). From this result, the proposed method (MTL + SD + HCL) achieves the best score on all three tasks. MTL and HCL improve for all three tasks, and SD improves the score of the headline generation. Automatic evaluation of the CNN-DM dataset.",
        "sentences": [
            "Automatic evaluation of job advertisement corpus.",
            "We implement an automatic evaluation using the ROUGE metrics (Lin, 2004) and accuracy.",
            "We conduct the experiment ten times, and calculate the average score.",
            "Table 3 shows the effect of the proposed methods: multi-task learning (MTL), scheduling strategy (SD) and hierarchical consistency loss (HCL).",
            "From this result, the proposed method (MTL + SD + HCL) achieves the best score on all three tasks.",
            "MTL and HCL improve for all three tasks, and SD improves the score of the headline generation.",
            "Automatic evaluation of the CNN-DM dataset."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            1,
            1,
            0
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "Proposed (MTL + SD + Hierarchical Consistency Loss (HCL))"
            ],
            [
                "Proposed (MTL + SD + Hierarchical Consistency Loss (HCL))"
            ],
            [
                "Proposed (MTL + SD + Hierarchical Consistency Loss (HCL))"
            ],
            null
        ],
        "n_sentence": 7.0,
        "table_id": "table_3",
        "paper_id": "D19-1315",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1315table_4",
        "description": "Automatic evaluation of the CNN-DM dataset. Table 4 shows the results of the CNN and DailyMail datasets, respectively. For both datasets, the proposed method improves the ROUGE scores of the summarization and headline generation.",
        "sentences": [
            "Automatic evaluation of the CNN-DM dataset.",
            "Table 4 shows the results of the CNN and DailyMail datasets, respectively.",
            "For both datasets, the proposed method improves the ROUGE scores of the summarization and headline generation."
        ],
        "class_sentence": [
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "CNN",
                "DailyMail"
            ],
            [
                "Proposed (MTL + SD + HCL)"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_4",
        "paper_id": "D19-1315",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1316table_2",
        "description": "6.2 Results. Table 2 shows the results. It turns out that the simple application of the sequence-to-sequence model does not work well at all on this task; note that it is provided with the information about which sentence should have a feedback comment (i.e., tested on only the sentences having feedback comments). Nevertheless, its performance is very poor. This suggests that it requires modifications to achieve better performance with the sequence-to-sequence framework. Case frame-based successfully generates feedback comments in some cases. However, its recall is quite low. In contrast, the neural retrieval-based method achieves a far better performance in recall, achieving a precision comparable to that of case frame-based. At the same time, Table 2 shows that there is still room for improvement. Subsect. 7.1 will investigate the generation results to reveal what has been solved by the methods.",
        "sentences": [
            "6.2 Results.",
            "Table 2 shows the results.",
            "It turns out that the simple application of the sequence-to-sequence model does not work well at all on this task; note that it is provided with the information about which sentence should have a feedback comment (i.e., tested on only the sentences having feedback comments).",
            "Nevertheless, its performance is very poor.",
            "This suggests that it requires modifications to achieve better performance with the sequence-to-sequence framework.",
            "Case frame-based successfully generates feedback comments in some cases.",
            "However, its recall is quite low. In contrast, the neural retrieval-based method achieves a far better performance in recall, achieving a precision comparable to that of case frame-based.",
            "At the same time, Table 2 shows that there is still room for improvement.",
            "Subsect. 7.1 will investigate the generation results to reveal what has been solved by the methods."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Seq2seq"
            ],
            [
                "Seq2seq"
            ],
            null,
            [
                "Case frame-based"
            ],
            [
                "Case frame-based",
                "Recall"
            ],
            [
                "Retrieval-based",
                "Recall",
                "Precision"
            ],
            null
        ],
        "n_sentence": 9.0,
        "table_id": "table_2",
        "paper_id": "D19-1316",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1317table_4",
        "description": "4.1 Main Results . Table 4 shows automatic evaluation results for our model and baselines (copied from their papers). Our proposed model which combines structured answer-relevant relations and unstructured sentences achieves significant improvements over proximity-based answer-aware models (Zhou et al., 2017; Sun et al., 2018) on both dataset splits. Presumably, our structured answer-relevant relation is a generalization of the context explored by the proximity-based methods because they can only capture short dependencies around answer fragments while our extractions can capture both short and long dependencies given the answer fragments. Moreover, our proposed framework is a general one to jointly leverage structured relations and unstructured sentences. All compared baseline models which only consider unstructured sentences can be further enhanced under our framework.",
        "sentences": [
            "4.1 Main Results .",
            "Table 4 shows automatic evaluation results for our model and baselines (copied from their papers).",
            "Our proposed model which combines structured answer-relevant relations and unstructured sentences achieves significant improvements over proximity-based answer-aware models (Zhou et al., 2017; Sun et al., 2018) on both dataset splits.",
            "Presumably, our structured answer-relevant relation is a generalization of the context explored by the proximity-based methods because they can only capture short dependencies around answer fragments while our extractions can capture both short and long dependencies given the answer fragments.",
            "Moreover, our proposed framework is a general one to jointly leverage structured relations and unstructured sentences.",
            "All compared baseline models which only consider unstructured sentences can be further enhanced under our framework."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "Our model"
            ],
            [
                "Our model",
                "s2s+MP+GSA (Zhao et al., 2018)",
                "Hybrid model (Sun et al., 2018)"
            ],
            null,
            [
                "Our model"
            ],
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_4",
        "paper_id": "D19-1317",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1321table_5",
        "description": "Table 5 shows the experimental results. Our model outperforms baselines in terms of coverage and diversity; it manages to use more given ingredients and generates more diversified cooking steps. We also found that Checklist / Link-S2S produces the general phrase \u201call ingredients\u201d in 14.9% / 24.5% of the generated recipes, while CVAE / Pointer-S2S / PHVM produce the phrase in 7.8% / 6.3% / 5.0% of recipes respectively.",
        "sentences": [
            "Table 5 shows the experimental results.",
            "Our model outperforms baselines in terms of coverage and diversity; it manages to use more given ingredients and generates more diversified cooking steps.",
            "We also found that Checklist / Link-S2S produces the general phrase \u201call ingredients\u201d in 14.9% / 24.5% of the generated recipes, while CVAE / Pointer-S2S / PHVM produce the phrase in 7.8% / 6.3% / 5.0% of recipes respectively."
        ],
        "class_sentence": [
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "PHVM (ours)",
                "Coverage (%)"
            ],
            [
                "Checklist",
                "Link-S2S",
                "CVAE",
                "Pointer-S2S",
                "PHVM (ours)"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_5",
        "paper_id": "D19-1321",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1324table_5",
        "description": "We also report the results on the full CNNDM and NYT although they are less compressable. Table 5 shows the experimental results on these datasets. Our models still yield strong performance compared to baselines and past work on the CNNDMdataset. The EXTRACTION model achieves comparable results to past successful extractive approaches on CNNDM and JECS improves on this across the datasets. In some cases, our model slightly underperforms on ROUGE-2. One possible reason is that we remove stop words when constructing our oracles, which could underestimate the importance of bigrams containing stopwords for evaluation. Finally, we note that our compressive approach substantially outperforms the compression-augmented LatSum model.",
        "sentences": [
            "We also report the results on the full CNNDM and NYT although they are less compressable.",
            "Table 5 shows the experimental results on these datasets.",
            "Our models still yield strong performance compared to baselines and past work on the CNNDMdataset.",
            "The EXTRACTION model achieves comparable results to past successful extractive approaches on CNNDM and JECS improves on this across the datasets.",
            "In some cases, our model slightly underperforms on ROUGE-2.",
            "One possible reason is that we remove stop words when constructing our oracles, which could underestimate the importance of bigrams containing stopwords for evaluation.",
            "Finally, we note that our compressive approach substantially outperforms the compression-augmented LatSum model."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Lead",
                "LEADDEDUP",
                "LEADCOMP"
            ],
            [
                "EXTRACTION",
                "JECS"
            ],
            [
                "Lead",
                "LEADDEDUP",
                "LEADCOMP",
                "R-2"
            ],
            null,
            null
        ],
        "n_sentence": 7.0,
        "table_id": "table_5",
        "paper_id": "D19-1324",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1330table_2",
        "description": "5.1 Results on IWSLT . Table 2 shows the main translation results of En\u00a8Zh/Ja and En\u00a8De/Fr on IWSLT datasets. We also conduct a typical one-to-many translation adopting Johnson et al. (2017) method on Transformer as our another baseline model, referred to Multi. Compared with Indiv, we can see that Multi achieves better results on all cases, which can be attributed to that the encoder can be enhanced by extra training data from the other language pair. As for our proposed method, the synchronous translation method performs significantly better than both Indiv and Multi baseline methods, and it can achieve the improvements up to 2.75 BLEU points (19.31 vs. 16.56) on En_Ja.",
        "sentences": [
            "5.1 Results on IWSLT .",
            "Table 2 shows the main translation results of En\u00a8Zh/Ja and En\u00a8De/Fr on IWSLT datasets.",
            "We also conduct a typical one-to-many translation adopting Johnson et al. (2017) method on Transformer as our another baseline model, referred to Multi.",
            "Compared with Indiv, we can see that Multi achieves better results on all cases, which can be attributed to that the encoder can be enhanced by extra training data from the other language pair.",
            "As for our proposed method, the synchronous translation method performs significantly better than both Indiv and Multi baseline methods, and it can achieve the improvements up to 2.75 BLEU points (19.31 vs. 16.56) on En_Ja."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "En-Zh/Ja"
            ],
            [
                "Multi"
            ],
            [
                "Indiv",
                "Multi"
            ],
            [
                "SyncTrans",
                "Indiv",
                "Multi",
                "En-Ja"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_2",
        "paper_id": "D19-1330",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1334table_3",
        "description": "5.4 Robustness Analysis . We can use different frequency thresholds K to select few-shot relations. In this section, we will study the impact of K on the performance of our model. In our experiments, some triples will be removed until every few-shot relation has only K triples. We do link prediction experiments on FB15K-237 and use ConvE as our reward function. The final results are shown in Table 3. K = max means we use the whole datasets in Table 2 and do not remove any triples. From Table 3 we can see our model is robust to K and outperforms MultiHop in every case.",
        "sentences": [
            "5.4 Robustness Analysis .",
            "We can use different frequency thresholds K to select few-shot relations.",
            "In this section, we will study the impact of K on the performance of our model.",
            "In our experiments, some triples will be removed until every few-shot relation has only K triples.",
            "We do link prediction experiments on FB15K-237 and use ConvE as our reward function.",
            "The final results are shown in Table 3.",
            "K = max means we use the whole datasets in Table 2 and do not remove any triples.",
            "From Table 3 we can see our model is robust to K and outperforms MultiHop in every case."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            2,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null,
            null,
            [
                "K = max"
            ],
            [
                " Meta-KGR",
                "K = 1",
                "K = 5",
                "K = 10",
                "K = max",
                "MultiHop"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_3",
        "paper_id": "D19-1334",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1336table_2",
        "description": "3.5 Results . Table 2 show the results of human evaluation. We find that: 1) Pun-GAN achieves the best ambiguity score. 2) Compared with CLM+JD which is actually the same as our pre-trained generator, Pun-GAN has a large improvement in unusualness. 3) Pun-GAN can generate more diverse sentence with different tokens and words. This phenomenon accords with previous work of GANs (Wang and Wan, 2018).",
        "sentences": [
            "3.5 Results .",
            "Table 2 show the results of human evaluation.",
            "We find that: 1) Pun-GAN achieves the best ambiguity score.",
            "2) Compared with CLM+JD which is actually the same as our pre-trained generator, Pun-GAN has a large improvement in unusualness.",
            "3) Pun-GAN can generate more diverse sentence with different tokens and words.",
            "This phenomenon accords with previous work of GANs (Wang and Wan, 2018)."
        ],
        "class_sentence": [
            0,
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "Pun-GAN",
                "Ambiguity"
            ],
            [
                "Pun-GAN",
                "CLM+JD (Yu et al., 2018)",
                "Fluency"
            ],
            [
                "Pun-GAN"
            ],
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_2",
        "paper_id": "D19-1336",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1342table_2",
        "description": "Table 2 shows that D-AT-GRU model outperforms all baseline methods. Given that AT-LSTM (Wang et al., 2016) has strong correlation to our base model (AT-GRU), their work can be categorized as a baseline to our model. In contrast, the results prove that the additional components are helpful to recognize con\ufb02ict opinions. We also compare our model to the recently proposed GCAE (Xue and Li, 2018), which is based on gated CNN. D-AT-GRU performs competitively with GCAE overall and significantly better on con\ufb02ict category.",
        "sentences": [
            "Table 2 shows that D-AT-GRU model outperforms all baseline methods.",
            "Given that AT-LSTM (Wang et al., 2016) has strong correlation to our base model (AT-GRU), their work can be categorized as a baseline to our model.",
            "In contrast, the results prove that the additional components are helpful to recognize con\ufb02ict opinions.",
            "We also compare our model to the recently proposed GCAE (Xue and Li, 2018), which is based on gated CNN.",
            "D-AT-GRU performs competitively with GCAE overall and significantly better on con\ufb02ict category."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "D-AT-GRU"
            ],
            [
                "AT-LSTM",
                "AT-GRU"
            ],
            [
                "D-AT-GRU",
                "Conflict"
            ],
            [
                "GCAE"
            ],
            [
                "D-AT-GRU",
                "GCAE"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_2",
        "paper_id": "D19-1342",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1345table_2",
        "description": "3.3 Experimental. Results Table 2 reports the results of our models against other baseline methods. We can see that our model can achieve the state-of-the-art result. We  note  that  the  results  of  graph-based  models  are  better  than  traditional  models  like  CNN, LSTM, and fastText. That is likely due to the characteristics of the graph structure. Graph structure allows a different number of neighbor nodes to exist, which enables word nodes to learn more accurate representations through different collocations. Besides,  the  relationship  between  words  can  be recorded in the edge weights and shared globally. These are all impossible for traditional models.",
        "sentences": [
            "3.3 Experimental.",
            "Results Table 2 reports the results of our models against other baseline methods.",
            "We can see that our model can achieve the state-of-the-art result.",
            "We  note  that  the  results  of  graph-based  models  are  better  than  traditional  models  like  CNN, LSTM, and fastText.",
            "That is likely due to the characteristics of the graph structure. Graph structure allows a different number of neighbor nodes to exist, which enables word nodes to learn more accurate representations through different collocations.",
            "Besides,  the  relationship  between  words  can  be recorded in the edge weights and shared globally.",
            "These are all impossible for traditional models."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "Our Model*"
            ],
            [
                "Our Model*"
            ],
            [
                "Graph-CNN",
                "CNN*",
                "LSTM*",
                "fastText*"
            ],
            null,
            null,
            null
        ],
        "n_sentence": 7.0,
        "table_id": "table_2",
        "paper_id": "D19-1345",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1350table_1",
        "description": "In our experiments, the models are evaluated based on the perplexity (PPL, lower is better) and topic coherence measure (Cv) based on external corpus (R\u00a8oder et al., 2015) (higher is better). The results with 30 and 50 topics are shown in Table 1. LDA is a conventional topic model, while all the other models are neural topic models. It can be observed from Table 1 that NVDM and NGTM achieve better perplexities compared to LDA. However, in terms of topic coherence measure, NVDM and NGTM perform slightly worse than LDA. A similar observation has been reported in (Card et al., 2017). Scholar achieves better coherence compared to other neural models. Nevertheless, after using reinforcement learning based on the topic coherence scores in our proposed model, VTMRL outperforms all the other models on the topic coherence measure by a large margin. RL could activate words which are semantically related to topics regardless of their occurrence frequency. The inclusion of some rare words would impact the models\u2019 predictive probabilities. As such, we observe worse perplexity results for models trained with RL-based vocabulary compared to frequency-based vocabulary in 20 Newsgroups, though the converse is true for NIPS. Nevertheless, the coherence scores improve for all the models with RL-based vocabulary.",
        "sentences": [
            "In our experiments, the models are evaluated based on the perplexity (PPL, lower is better) and topic coherence measure (Cv) based on external corpus (R\u00a8oder et al., 2015) (higher is better).",
            "The results with 30 and 50 topics are shown in Table 1.",
            "LDA is a conventional topic model, while all the other models are neural topic models.",
            "It can be observed from Table 1 that NVDM and NGTM achieve better perplexities compared to LDA.",
            "However, in terms of topic coherence measure, NVDM and NGTM perform slightly worse than LDA.",
            "A similar observation has been reported in (Card et al., 2017).",
            "Scholar achieves better coherence compared to other neural models.",
            "Nevertheless, after using reinforcement learning based on the topic coherence scores in our proposed model, VTMRL outperforms all the other models on the topic coherence measure by a large margin.",
            "RL could activate words which are semantically related to topics regardless of their occurrence frequency.",
            "The inclusion of some rare words would impact the models\u2019 predictive probabilities.",
            "As such, we observe worse perplexity results for models trained with RL-based vocabulary compared to frequency-based vocabulary in 20 Newsgroups, though the converse is true for NIPS.",
            "Nevertheless, the coherence scores improve for all the models with RL-based vocabulary."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            1,
            1,
            1,
            1,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "PPL",
                " Cv"
            ],
            [
                "#Topics = 30, frequency-based vocab.",
                "#Topics = 30, RL-based vocab.",
                "#Topics = 50, frequency-based vocab.",
                "#Topics = 50, RL-based vocab."
            ],
            [
                "LDA"
            ],
            [
                "NVDM",
                "NGTM",
                "PPL",
                "LDA"
            ],
            [
                " Cv",
                "NVDM",
                "NGTM",
                "LDA"
            ],
            null,
            [
                "Scholar",
                "NVDM",
                "NGTM"
            ],
            [
                "VTMRL"
            ],
            [
                "VTMRL"
            ],
            null,
            [
                "PPL",
                "#Topics = 30, RL-based vocab.",
                "#Topics = 50, RL-based vocab.",
                "#Topics = 30, frequency-based vocab.",
                "#Topics = 50, frequency-based vocab.",
                "20News",
                "NIPS"
            ],
            [
                "#Topics = 30, RL-based vocab.",
                "#Topics = 50, RL-based vocab.",
                " Cv"
            ]
        ],
        "n_sentence": 12.0,
        "table_id": "table_1",
        "paper_id": "D19-1350",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1359table_3",
        "description": "Table 3 compares UKB + SyntagNet against the best supervised English WSD systems (Yuan et al., 2016; Melacci et al., 2018; Uslu et al., 2018). None of the differences across datasets between the best performing supervised system and SyntagNet is statistically significant according to chi-square test (p < 0.01), meaning that SyntagNet enables knowledge-based WSD to rival current supervised approaches.",
        "sentences": [
            "Table 3 compares UKB + SyntagNet against the best supervised English WSD systems (Yuan et al., 2016; Melacci et al., 2018; Uslu et al., 2018).",
            "None of the differences across datasets between the best performing supervised system and SyntagNet is statistically significant according to chi-square test (p < 0.01), meaning that SyntagNet enables knowledge-based WSD to rival current supervised approaches."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "UKB+SyntagNet"
            ],
            [
                "UKB+SyntagNet"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_3",
        "paper_id": "D19-1359",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1368table_2",
        "description": "Discussion. It could be seen in Table 2 that JoBi ComplEx outperforms both ComplEx and Dist-Mult on all three standard datasets, on all the metrics we consider. For Hits@1, JoBi Complex out-performs baseline ComplEx by 4% on FB15K-237, 6.4% on FB15K and 5.6% on YAGO3-10. Moreover, results in Table 2 demonstrate that JoBi improves performance on DistMult and SimplE. It should be noted that on FB15K-237, all JoBi models outperform all the baseline models, regardless of the base model used.",
        "sentences": [
            "Discussion.",
            "It could be seen in Table 2 that JoBi ComplEx outperforms both ComplEx and Dist-Mult on all three standard datasets, on all the metrics we consider.",
            "For Hits@1, JoBi Complex out-performs baseline ComplEx by 4% on FB15K-237, 6.4% on FB15K and 5.6% on YAGO3-10.",
            "Moreover, results in Table 2 demonstrate that JoBi improves performance on DistMult and SimplE.",
            "It should be noted that on FB15K-237, all JoBi models outperform all the baseline models, regardless of the base model used."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "JoBi ComplEx",
                "ComplEx",
                "DistMult"
            ],
            [
                "h@1",
                "JoBi ComplEx",
                "ComplEx",
                "FB15K-237",
                "FB15K",
                "YAGO3-10"
            ],
            [
                "JoBi SimplE",
                "JoBi DistMult",
                "JoBi ComplEx",
                "DistMult",
                "SimplE"
            ],
            [
                "FB15K-237"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_2",
        "paper_id": "D19-1368",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1368table_6",
        "description": "In Table 6 it can be seen that Joint on its own gives a slight performance boost over the baseline, and BiasedNeg performs slightly under the baseline on all measures. However, combining our two techniques in JoBi gives 5.6% points improvement on hits@1. This suggests that biased negative sampling increases the efficacy of joint training greatly, but is not very effective on its own.",
        "sentences": [
            "In Table 6 it can be seen that Joint on its own gives a slight performance boost over the baseline, and BiasedNeg performs slightly under the baseline on all measures.",
            "However, combining our two techniques in JoBi gives 5.6% points improvement on hits@1.",
            "This suggests that biased negative sampling increases the efficacy of joint training greatly, but is not very effective on its own."
        ],
        "class_sentence": [
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "Joint",
                "Baseline",
                "BiasedNeg"
            ],
            [
                "JoBi",
                "h@1"
            ],
            null
        ],
        "n_sentence": 3.0,
        "table_id": "table_6",
        "paper_id": "D19-1368",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1370table_2",
        "description": "2.3 Autoencoder-based Initialization . Based on the observations above we hypothesize that VAEs might benefit from initialization with an non-collapsed encoder, trained via an AE objective. Intuitively, if the encoder is providing useful information from the beginning of training, the decoder is more likely to make use of the latent code. In Table 2 we show the results of exploring this hypothesis on PTB. Even with encoder pretraining, we see that posterior collapse occurs immediately after beginning to update both encoder and decoder using the full ELBO objective. This indicates that the gradients of ELBO point towards a collapsed local optimum, even with biased initialization. When pretraining is combined with annealing, PPL improves substantially. However, the pretraining and anneal combination only has 2 active units and has small KL value \u00e2\u20ac\u201c the latent representation is likely unsatisfactory. We speculate that this is because the annealing schedule eventually returns to the full ELBO objective which guides learning towards a (nearly) collapsed latent space. In the next section, we present an alternate approach using the KL thresholding / free bits method.",
        "sentences": [
            "2.3 Autoencoder-based Initialization .",
            "Based on the observations above we hypothesize that VAEs might benefit from initialization with an non-collapsed encoder, trained via an AE objective.",
            "Intuitively, if the encoder is providing useful information from the beginning of training, the decoder is more likely to make use of the latent code.",
            "In Table 2 we show the results of exploring this hypothesis on PTB.",
            "Even with encoder pretraining, we see that posterior collapse occurs immediately after beginning to update both encoder and decoder using the full ELBO objective.",
            "This indicates that the gradients of ELBO point towards a collapsed local optimum, even with biased initialization.",
            "When pretraining is combined with annealing, PPL improves substantially.",
            "However, the pretraining and anneal combination only has 2 active units and has small KL value \u00e2\u20ac\u201c the latent representation is likely unsatisfactory.",
            "We speculate that this is because the annealing schedule eventually returns to the full ELBO objective which guides learning towards a (nearly) collapsed latent space.",
            "In the next section, we present an alternate approach using the KL thresholding / free bits method."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            1,
            2,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            [
                "VAE",
                "+ pretrain",
                "-ELBO"
            ],
            [
                "-ELBO"
            ],
            [
                "+ pretrain + anneal",
                "PPL#"
            ],
            [
                "+ pretrain + anneal",
                "AU",
                "KL"
            ],
            [
                "+ pretrain + anneal",
                "-ELBO"
            ],
            [
                "KL"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_2",
        "paper_id": "D19-1370",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1372table_3",
        "description": "The results on the Pun of the Day dataset are shown in Table 3 above. It shows an accuracy of 93 percent, close to 4 percent greater accuracy than the best CNN model proposed. Although the CNN model used a variety of techniques to extract the best features from the dataset, we see that the self-attention layers found even greater success in pulling out the crucial features.",
        "sentences": [
            "The results on the Pun of the Day dataset are shown in Table 3 above.",
            "It shows an accuracy of 93 percent, close to 4 percent greater accuracy than the best CNN model proposed.",
            "Although the CNN model used a variety of techniques to extract the best features from the dataset, we see that the self-attention layers found even greater success in pulling out the crucial features."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "CNN+F+HN",
                "Transformer"
            ],
            [
                "CNN"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_3",
        "paper_id": "D19-1372",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1374table_1",
        "description": "The results in Table 1 make it clear that the BERT-based  model  for  each  task  is  a  solid  win  over  a Meta-LSTM model in both the per-language and multilingual settings. However, the number of parameters of the BERT model is very large (179M parameters), making deploying memory intensive and inference slow: 230ms on an Intel Xeon CPU. Our goal is to produce a model fast enough to run on a single CPU while maintaining the modeling capability of the large model on our tasks.",
        "sentences": [
            "The results in Table 1 make it clear that the BERT-based  model  for  each  task  is  a  solid  win  over  a Meta-LSTM model in both the per-language and multilingual settings.",
            "However, the number of parameters of the BERT model is very large (179M parameters), making deploying memory intensive and inference slow: 230ms on an Intel Xeon CPU.",
            "Our goal is to produce a model fast enough to run on a single CPU while maintaining the modeling capability of the large model on our tasks."
        ],
        "class_sentence": [
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "BERT",
                "Meta-LSTM"
            ],
            [
                "BERT"
            ],
            null
        ],
        "n_sentence": 3.0,
        "table_id": "table_1",
        "paper_id": "D19-1374",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1376table_3",
        "description": "In    addition    to    PRPN,    we    compare  to DIORA (Drozdov et al., 2019), which uses an inside-outside dynamic program in an autoencoder. Table 3 shows the F1 results. PaLM outperforms the right branching baseline, but is not as accurate as the other models. This indicates that the type of syntactic trees learned by it, albeit useful to the LM component, do not correspond well to PTB-like syntactic trees.",
        "sentences": [
            "In    addition    to    PRPN,    we    compare  to DIORA (Drozdov et al., 2019), which uses an inside-outside dynamic program in an autoencoder.",
            "Table 3 shows the F1 results.",
            "PaLM outperforms the right branching baseline, but is not as accurate as the other models.",
            "This indicates that the type of syntactic trees learned by it, albeit useful to the LM component, do not correspond well to PTB-like syntactic trees."
        ],
        "class_sentence": [
            0,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Unlabeled F1"
            ],
            [
                "zPaLM-U",
                "Right Branching"
            ],
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "D19-1376",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1379table_3",
        "description": "Comparison  between  unsupervised  domain adaptation and transduction. In unsupervised domain adaptation,  target domain unlabeled data (the texts whose domain is the same as that of a test set) is used for adaptation. Although the domain is identical between target domain data and a test set, their word distributions are somewhat different. In transductive learning, because an unlabeled test set can be used for training, it is possible to adapt LMs directly to the word distributions of the test set.  Here, we investigate whether adapting LMs directly to each test set is more effective than adapting LMs to each target domain unlabeled data. Similarly to our transductive method shown in Figure 1, we first train LMs on the largescale unlabeled corpus (the 1B word benchmark corpus) and then fine-tune them on the unlabeled target domain data8. In addition, we control the sizes of the target domain unlabeled data and test sets. That is, we use the same number of sentences in the unlabeled data of each target domain as in each test set. Table 3 shows the F1 scores averaged across all the target domains. The transductive models (T) consistently outperformed the domain-adapted models (CU). This demonstrates that adapting LMs directly to test sets is more effective than adapting them to target domain unlabeled data.",
        "sentences": [
            "Comparison  between  unsupervised  domain adaptation and transduction.",
            "In unsupervised domain adaptation,  target domain unlabeled data (the texts whose domain is the same as that of a test set) is used for adaptation.",
            "Although the domain is identical between target domain data and a test set, their word distributions are somewhat different.",
            "In transductive learning, because an unlabeled test set can be used for training, it is possible to adapt LMs directly to the word distributions of the test set.",
            " Here, we investigate whether adapting LMs directly to each test set is more effective than adapting LMs to each target domain unlabeled data.",
            "Similarly to our transductive method shown in Figure 1, we first train LMs on the largescale unlabeled corpus (the 1B word benchmark corpus) and then fine-tune them on the unlabeled target domain data8.",
            "In addition, we control the sizes of the target domain unlabeled data and test sets.",
            "That is, we use the same number of sentences in the unlabeled data of each target domain as in each test set.",
            "Table 3 shows the F1 scores averaged across all the target domains.",
            "The transductive models (T) consistently outperformed the domain-adapted models (CU).",
            "This demonstrates that adapting LMs directly to test sets is more effective than adapting them to target domain unlabeled data."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            [
                "T",
                "CU"
            ],
            [
                "T",
                "CU"
            ]
        ],
        "n_sentence": 11.0,
        "table_id": "table_3",
        "paper_id": "D19-1379",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1379table_4",
        "description": "Combination of unsupervised domain adaptation and transduction. In real-world situations, large-scale unlabeled data of target domains is sometimes available. In such cases, LMs can be trained on both the target domain unlabeled data and the test sets. Here, we investigate the effectiveness of using both datasets. Table 4 shows the F1 scores averaged across all the target domains. Fine-tuning the LMs on the target domain unlabeled data as well as each test set (U + T) showed better performance than fine-tuning them only on the target domain unlabeled data (U). This combination of tranduction with unsupervised domain adaptation further improves performance.",
        "sentences": [
            "Combination of unsupervised domain adaptation and transduction.",
            "In real-world situations, large-scale unlabeled data of target domains is sometimes available.",
            "In such cases, LMs can be trained on both the target domain unlabeled data and the test sets.",
            "Here, we investigate the effectiveness of using both datasets.",
            "Table 4 shows the F1 scores averaged across all the target domains.",
            "Fine-tuning the LMs on the target domain unlabeled data as well as each test set (U + T) showed better performance than fine-tuning them only on the target domain unlabeled data (U).",
            "This combination of tranduction with unsupervised domain adaptation further improves performance."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null,
            [
                "U + T",
                "U"
            ],
            null
        ],
        "n_sentence": 7.0,
        "table_id": "table_4",
        "paper_id": "D19-1379",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1379table_5",
        "description": "Effects in standard benchmarks. Some studies indicated that when promising new techniques are only evaluated on very basic models, determining how much (if any) improvement will carry over to stronger models can be difficult (Denkowski and Neubig, 2017; Suzuki et al., 2018). Motivated by such  studies,  we  provide  the  results  in  standard benchmark settings. For syntactic chunking,  we use the CoNLL-2000 dataset (Sang and Buchholz, 2000) and follow the standard experimental protocol (Hashimoto et al., 2017). For SRL, we use the CoNLL-2005 (Carreras and M`arquez, 2005) and CoNLL-2012 datasets (Pradhan et al., 2012) and follow the standard experimental protocol (Ouchi et al., 2018). Table 5 shows the F1 scores of our models and those of existing models. The results of the baseline model were comparable with those of the state-of-the-art models, and the transductive model consistently outperformed the baseline model. Note that we cannot fairly compare the transductive and existing models due to the difference in settings. These results, however, demonstrate that transductive LM fine-tuning improves state-of-the-art chunking and SRL models.",
        "sentences": [
            "Effects in standard benchmarks.",
            "Some studies indicated that when promising new techniques are only evaluated on very basic models, determining how much (if any) improvement will carry over to stronger models can be difficult (Denkowski and Neubig, 2017; Suzuki et al., 2018).",
            "Motivated by such  studies,  we  provide  the  results  in  standard benchmark settings.",
            "For syntactic chunking,  we use the CoNLL-2000 dataset (Sang and Buchholz, 2000) and follow the standard experimental protocol (Hashimoto et al., 2017).",
            "For SRL, we use the CoNLL-2005 (Carreras and M`arquez, 2005) and CoNLL-2012 datasets (Pradhan et al., 2012) and follow the standard experimental protocol (Ouchi et al., 2018).",
            "Table 5 shows the F1 scores of our models and those of existing models.",
            "The results of the baseline model were comparable with those of the state-of-the-art models, and the transductive model consistently outperformed the baseline model.",
            "Note that we cannot fairly compare the transductive and existing models due to the difference in settings.",
            "These results, however, demonstrate that transductive LM fine-tuning improves state-of-the-art chunking and SRL models."
        ],
        "class_sentence": [
            0,
            0,
            0,
            2,
            2,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null,
            null,
            [
                "BASE",
                "TRANS"
            ],
            null,
            [
                "TRANS",
                "BASE"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_5",
        "paper_id": "D19-1379",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1380table_4",
        "description": "For fair comparison, we use the same sentiment and text classification datasets, the SST-5, 20 newsgroups (20-NG) and Reuters-8 (R-8), as those used in Kayal and Tsatsaronis (2019). We also evaluate using the same pre-trained word embedding, framework and approaches as described in their work. Table 4 shows the best results for the various models as reported in Kayal and Tsatsaronis (2019), in addition to the best performance of our model denoted as ck. Note that the DCT-based model, DCT*, described in Kayal and Tsatsaronis (2019) performed relatively poorly in all tasks, while our model achieved close to state-of-the-art performance in both the 20-NG and R-8 tasks. Our model outperformed EignSent on all tasks and generally performed better than or on par with p-means, ELMo, BERT, and EigenSent\u2295Avg on both the 20-NG and R-8. On the other hand, both EigenSent\u2295Avg and ELMo performed better than all other models on SST-5.",
        "sentences": [
            "For fair comparison, we use the same sentiment and text classification datasets, the SST-5, 20 newsgroups (20-NG) and Reuters-8 (R-8), as those used in Kayal and Tsatsaronis (2019).",
            "We also evaluate using the same pre-trained word embedding, framework and approaches as described in their work.",
            "Table 4 shows the best results for the various models as reported in Kayal and Tsatsaronis (2019), in addition to the best performance of our model denoted as ck.",
            "Note that the DCT-based model, DCT*, described in Kayal and Tsatsaronis (2019) performed relatively poorly in all tasks, while our model achieved close to state-of-the-art performance in both the 20-NG and R-8 tasks.",
            "Our model outperformed EignSent on all tasks and generally performed better than or on par with p-means, ELMo, BERT, and EigenSent\u2295Avg on both the 20-NG and R-8.",
            "On the other hand, both EigenSent\u2295Avg and ELMo performed better than all other models on SST-5."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "ck"
            ],
            [
                "DCT*",
                "ck",
                "20-NG",
                "R-8"
            ],
            [
                "ck",
                "p-means",
                "ELMo",
                "BERT",
                "20-NG",
                "R-8"
            ],
            [
                "ELMo",
                "SST-5"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_4",
        "paper_id": "D19-1380",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1381table_1",
        "description": "Table 1 shows the event detection performance of the models on the test set. Our model achieves performance comparable to the state-of-the-art TEES event detection module without the use of any syntactic and hand-engineered features, suggesting it can be applied to other domains with no need for feature engineering. We validated it to have no significant statistical difference with the TEES model (the Approximate Randomisation test (Yeh, 2000; Noreen, 1989)).",
        "sentences": [
            "Table 1 shows the event detection performance of the models on the test set.",
            "Our model achieves performance comparable to the state-of-the-art TEES event detection module without the use of any syntactic and hand-engineered features, suggesting it can be applied to other domains with no need for feature engineering.",
            "We validated it to have no significant statistical difference with the TEES model (the Approximate Randomisation test (Yeh, 2000; Noreen, 1989))."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "TEES"
            ],
            [
                "TEES"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_1",
        "paper_id": "D19-1381",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1381table_3",
        "description": "Table 3 shows the number of classifications (or action scoring function calls in our model) performed by each model with the corresponding actual running time. SBNN performs fewer classifications and in less time than TEES, implying it is more computationally efficient.",
        "sentences": [
            "Table 3 shows the number of classifications (or action scoring function calls in our model) performed by each model with the corresponding actual running time.",
            "SBNN performs fewer classifications and in less time than TEES, implying it is more computationally efficient."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "SBNN k = 8",
                "TEES"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_3",
        "paper_id": "D19-1381",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1383table_4",
        "description": "Table 4 summarizes results on CSPUB-SUM. Following   Collins   et   al.   (2017)   we take the top   10 predicted sentences   as   the summary  and  use   ROUGE-L scores for evaluation. It   is   clear   that   our   approach   outperforms BERT+TRANSFORMER. The BERT+TRANSFORMER+CRF baseline  is  not  included here because, as mentioned in section 3, we train our model to predict ROUGE, not binary labels as in Collins et al. (2017). As in Collins et al. (2017), we  found  the  ABSTRACT-ROUGE feature  to  be useful. Our  model  augmented  with  this  feature slightly outperforms  Collins et al. (2017)\u2019s model, which is a relatively complex ensemble model and uses a number of carefully engineered features for the task. Our model is a single model with only one added feature.",
        "sentences": [
            "Table 4 summarizes results on CSPUB-SUM.",
            "Following   Collins   et   al.   (2017)   we take the top   10 predicted sentences   as   the summary  and  use   ROUGE-L scores for evaluation.",
            "It   is   clear   that   our   approach   outperforms BERT+TRANSFORMER.",
            "The BERT+TRANSFORMER+CRF baseline  is  not  included here because, as mentioned in section 3, we train our model to predict ROUGE, not binary labels as in Collins et al. (2017).",
            "As in Collins et al. (2017), we  found  the  ABSTRACT-ROUGE feature  to  be useful.",
            "Our  model  augmented  with  this  feature slightly outperforms  Collins et al. (2017)\u2019s model, which is a relatively complex ensemble model and uses a number of carefully engineered features for the task.",
            "Our model is a single model with only one added feature."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "ROUGE-L"
            ],
            [
                "Our model",
                "BERT +Transformer"
            ],
            null,
            [
                "Our model + ABSTRACTROUGE"
            ],
            [
                "Our model + ABSTRACTROUGE",
                "SAF + F Ens (Collins et al., 2017)"
            ],
            null
        ],
        "n_sentence": 7.0,
        "table_id": "table_4",
        "paper_id": "D19-1383",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1387table_3",
        "description": "Table 3 presents results on the NYT dataset. Following the evaluation protocol in Durrett et al. (2016), we use limited-length ROUGE Recall, where predicted summaries are truncated to the length of the gold summaries. Again, we report the performance of the ORACLE upper bound and LEAD-3 baseline. The second block in the table contains previously proposed extractive models as well as our own Transformer baseline. COMPRESS (Durrett et al., 2016) is an ILP-based model which combines compression and anaphoricity constraints. The third block includes abstractive models from the literature, and our Transformer baseline. BERT-based models are shown in the fourth block. Again, we observe that they outperform previously proposed approaches. On this dataset, abstractive BERT models generally perform better compared to BERTSUMEXT, almost approaching ORACLE performance.",
        "sentences": [
            "Table 3 presents results on the NYT dataset.",
            "Following the evaluation protocol in Durrett et al. (2016), we use limited-length ROUGE Recall, where predicted summaries are truncated to the length of the gold summaries.",
            "Again, we report the performance of the ORACLE upper bound and LEAD-3 baseline.",
            "The second block in the table contains previously proposed extractive models as well as our own Transformer baseline.",
            "COMPRESS (Durrett et al., 2016) is an ILP-based model which combines compression and anaphoricity constraints.",
            "The third block includes abstractive models from the literature, and our Transformer baseline.",
            "BERT-based models are shown in the fourth block.",
            "Again, we observe that they outperform previously proposed approaches.",
            "On this dataset, abstractive BERT models generally perform better compared to BERTSUMEXT, almost approaching ORACLE performance."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "ORACLE",
                "LEAD-3"
            ],
            [
                "Extractive",
                "TransformerEXT"
            ],
            [
                "COMPRESS (Durrett et al. 2016)"
            ],
            [
                "Abstractive",
                "TransformerABS"
            ],
            [
                "BERT-based"
            ],
            [
                "BERT-based"
            ],
            [
                "BERTSUMEXTABS",
                "BERTSUMEXT",
                "ORACLE"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_3",
        "paper_id": "D19-1387",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1388table_4",
        "description": "For the human evaluation, we asked annotators to rate each summary according to its consistency and fluency. The rating score ranges from 1 to 3, with 3 being the best. Table 4 lists the average scores of each model, showing that PESG outperforms the other baseline models in both fluency and consistency. The kappa statistics are 0.33 and 0.29 for fluency and consistency respectively, and that indicates the moderate agreement between annotators. To prove the significance of these results, we also conduct the paired student t-test between our model and Re 3 Sum (row with shaded background). We obtain a p-value of 2 x 10^(-7) and 9 x 10^(-12) for fluency and consistency, respectively.",
        "sentences": [
            "For the human evaluation, we asked annotators to rate each summary according to its consistency and fluency.",
            "The rating score ranges from 1 to 3, with 3 being the best.",
            "Table 4 lists the average scores of each model, showing that PESG outperforms the other baseline models in both fluency and consistency.",
            "The kappa statistics are 0.33 and 0.29 for fluency and consistency respectively, and that indicates the moderate agreement between annotators.",
            "To prove the significance of these results, we also conduct the paired student t-test between our model and Re 3 Sum (row with shaded background).",
            "We obtain a p-value of 2 x 10^(-7) and 9 x 10^(-12) for fluency and consistency, respectively."
        ],
        "class_sentence": [
            2,
            2,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "PESG",
                "Fluency",
                "Consistency"
            ],
            [
                "Fluency",
                "Consistency"
            ],
            null,
            [
                "Fluency",
                "Consistency"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_4",
        "paper_id": "D19-1388",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1399table_6",
        "description": "4.2 Additional Experiments CoNLL-2003 English Table 6 shows the performance on the CoNLL-2003 English dataset. The dependencies are predicted from Spacy (Honnibal and Montani, 2017). With the contextualized word representations, DGLSTM-CRF outperforms BiLSTM-CRF with 0.2 points in F1 (p < 0.09). The improvement is not significant due to the relatively lower equality of the dependency trees. To further study the effect of the dependencies, we modified the predicted dependencies to ensure each entity form a subtree in the complete dataset. Such modification improves the F1 to 92.7, which is significantly better (p < 0.05) than the BiLSTM-CRF.",
        "sentences": [
            "4.2 Additional Experiments CoNLL-2003 English Table 6 shows the performance on the CoNLL-2003 English dataset.",
            "The dependencies are predicted from Spacy (Honnibal and Montani, 2017).",
            "With the contextualized word representations, DGLSTM-CRF outperforms BiLSTM-CRF with 0.2 points in F1 (p < 0.09).",
            "The improvement is not significant due to the relatively lower equality of the dependency trees.",
            "To further study the effect of the dependencies, we modified the predicted dependencies to ensure each entity form a subtree in the complete dataset.",
            "Such modification improves the F1 to 92.7, which is significantly better (p < 0.05) than the BiLSTM-CRF."
        ],
        "class_sentence": [
            1,
            2,
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "DGLSTM-CRF + ELMo (L = 2)",
                "BiLSTM-CRF + ELMo (L = 2)"
            ],
            null,
            null,
            [
                "DGLSTM-CRF + ELMo (L = 2)"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_6",
        "paper_id": "D19-1399",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1400table_4",
        "description": "Effect of Document Representation. Table 4 considers the effect of the document representation component of the transformation discussed in Section 3.2.3 across the tasks, datasets, and translation methods. The first result column shows the performance of a simple unweighted average of the word vectors. The second result column shows a document-frequency-based weighting according to Equation 1. The third result column shows the performance of a weighting scheme that takes the language of origin into consideration, based on Equation 4. Examining the table, we observe that using a non-uniform weighting scheme generally gives improved performance over the naive unweighted baseline. The effect of the document representation method is significant when paired with the NN translation approach. Thus, the translation and document representation components of the compound transformation are complementary in the sense that when translation is of high quality, a naive document representation suffices. Conversely, when translation quality is sub-optimal, the choice of document representation can significantly impact performance.",
        "sentences": [
            "Effect of Document Representation.",
            "Table 4 considers the effect of the document representation component of the transformation discussed in Section 3.2.3 across the tasks, datasets, and translation methods.",
            "The first result column shows the performance of a simple unweighted average of the word vectors.",
            "The second result column shows a document-frequency-based weighting according to Equation 1.",
            "The third result column shows the performance of a weighting scheme that takes the language of origin into consideration, based on Equation 4.",
            "Examining the table, we observe that using a non-uniform weighting scheme generally gives improved performance over the naive unweighted baseline.",
            "The effect of the document representation method is significant when paired with the NN translation approach.",
            "Thus, the translation and document representation components of the compound transformation are complementary in the sense that when translation is of high quality, a naive document representation suffices.",
            "Conversely, when translation quality is sub-optimal, the choice of document representation can significantly impact performance."
        ],
        "class_sentence": [
            2,
            1,
            2,
            2,
            2,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "AVG"
            ],
            [
                "IDF"
            ],
            [
                "LANG"
            ],
            [
                "LANG",
                "IDF",
                "AVG"
            ],
            null,
            null,
            null
        ],
        "n_sentence": 9.0,
        "table_id": "table_4",
        "paper_id": "D19-1400",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1402table_2",
        "description": "4.1 STC: Comparison with On-Device Work. We compare our on-device model against state-of-cdfczxybvgthe-art on-device short text classification approach SGNN (Ravi and Kozareva, 2018). SGNN was evaluated only on the SWDA and MRDA dialog act tasks (Ravi and Kozareva, 2018) and reached state-of-the-art performance against prior non-ondevice neural approaches (Khanpour et al., 2016). We directly compare performance on the same SWDA and MRDA datasets. As shown in Table 2 ProSeqo reaches +5.3% improvement for SWDA and +3.4% accuracy improvement for MRDA. Since we want to compare on-device performance on a wider spectrum of tasks and datasets, we re-implemented SGNN with the same parameters (Ravi and Kozareva, 2018). We run experiments on ATIS and SNIPS intent prediction tasks and reported results in Table 2 in italic to indicate that this is a re-implementation of (Ravi and Kozareva, 2018) and previously these ondevice results were not reported. As shown in Table 2, SGNN consistently performs well on dialog act and intent prediction tasks. Overall, ProSeqo reached +8.9% accuracy improvements on ATIS and +4.5% accuracy improvements on SNIPS compared to SGNN. This shows that ProSeqo\u2019s recurrent dynamic projections learn more powerful representations than the static SGNN ones, this also leads to significant performance improvements on multiple tasks.",
        "sentences": [
            "4.1 STC: Comparison with On-Device Work.",
            "We compare our on-device model against state-of-cdfczxybvgthe-art on-device short text classification approach SGNN (Ravi and Kozareva, 2018).",
            "SGNN was evaluated only on the SWDA and MRDA dialog act tasks (Ravi and Kozareva, 2018) and reached state-of-the-art performance against prior non-ondevice neural approaches (Khanpour et al., 2016).",
            "We directly compare performance on the same SWDA and MRDA datasets.",
            "As shown in Table 2 ProSeqo reaches +5.3% improvement for SWDA and +3.4% accuracy improvement for MRDA.",
            "Since we want to compare on-device performance on a wider spectrum of tasks and datasets, we re-implemented SGNN with the same parameters (Ravi and Kozareva, 2018).",
            "We run experiments on ATIS and SNIPS intent prediction tasks and reported results in Table 2 in italic to indicate that this is a re-implementation of (Ravi and Kozareva, 2018) and previously these ondevice results were not reported.",
            "As shown in Table 2, SGNN consistently performs well on dialog act and intent prediction tasks.",
            "Overall, ProSeqo reached +8.9% accuracy improvements on ATIS and +4.5% accuracy improvements on SNIPS compared to SGNN.",
            "This shows that ProSeqo\u2019s recurrent dynamic projections learn more powerful representations than the static SGNN ones, this also leads to significant performance improvements on multiple tasks."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            1,
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "ProSeqo (our on-device model)",
                "SGNN(Ravi and Kozareva, 2018)(on-device)"
            ],
            [
                "SGNN(Ravi and Kozareva, 2018)(on-device)",
                "SWDA",
                "MRDA"
            ],
            [
                "SWDA",
                "MRDA"
            ],
            [
                "ProSeqo (our on-device model)",
                "SWDA",
                "MRDA",
                "SGNN(Ravi and Kozareva, 2018)(on-device)"
            ],
            [
                "SGNN(Ravi and Kozareva, 2018)(on-device)"
            ],
            [
                "ATIS",
                "SNIPS"
            ],
            [
                "SGNN(Ravi and Kozareva, 2018)(on-device)"
            ],
            [
                "ProSeqo (our on-device model)",
                "ATIS",
                "SNIPS",
                "SGNN(Ravi and Kozareva, 2018)(on-device)"
            ],
            [
                "ProSeqo (our on-device model)",
                "SGNN(Ravi and Kozareva, 2018)(on-device)"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_2",
        "paper_id": "D19-1402",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1402table_3",
        "description": "5 LDC: Long Document Classification Results This section focuses on long document classification results. Table 3 shows the results on three tasks and datasets (AG, Y!A, AMZN). Overall, ProSeqo significantly improved upon the ondevice neural model SGNN (Ravi and Kozareva, 2018) with +23% to +35.9% accuracy. ProSeqo also reached comparable performance to prior non-on-device neural LSTMs and character CNNs approaches (Zhang et al., 2015; Bui et al., 2018).",
        "sentences": [
            "5 LDC: Long Document Classification Results This section focuses on long document classification results.",
            "Table 3 shows the results on three tasks and datasets (AG, Y!A, AMZN).",
            "Overall, ProSeqo significantly improved upon the ondevice neural model SGNN (Ravi and Kozareva, 2018) with +23% to +35.9% accuracy.",
            "ProSeqo also reached comparable performance to prior non-on-device neural LSTMs and character CNNs approaches (Zhang et al., 2015; Bui et al., 2018)."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "AG",
                "Y!A",
                "AMZN"
            ],
            [
                "ProSeqo (our on-device model)",
                "SGNN (Ravi and Kozareva 2018)(on-device)"
            ],
            [
                "ProSeqo (our on-device model)",
                "LSTM-full (Zhang et al. 2015)",
                "CharCNNLargeWithThesau. (Zhang et al. 2015)"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "D19-1402",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1418table_3",
        "description": "Table 3 shows the results. The learned-mixin method was highly effective, boosting performance on VQA-CP by about 9 points, and the entropy regularizer can increase this by another 3 points, significantly surpassing prior work. For the learned-mixin ensemble, we find g(x i) is strongly correlated with the bias\u2019s expected accuracy, with a spearmanr correlation of 0.77 on the test data. Qualitative examples (Figure 2) further suggest the model increases g(x i) when it knows if can rely on the bias-only model.",
        "sentences": [
            "Table 3 shows the results.",
            "The learned-mixin method was highly effective, boosting performance on VQA-CP by about 9 points, and the entropy regularizer can increase this by another 3 points, significantly surpassing prior work.",
            "For the learned-mixin ensemble, we find g(x i) is strongly correlated with the bias\u2019s expected accuracy, with a spearmanr correlation of 0.77 on the test data.",
            "Qualitative examples (Figure 2) further suggest the model increases g(x i) when it knows if can rely on the bias-only model."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "Learned-Mixin",
                "None"
            ],
            [
                "Learned-Mixin"
            ],
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "D19-1418",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1420table_5",
        "description": "Table 5 shows the performance of subjective evaluations. Looking at the first column of the table, our model is better in confusing human, which gives a higher rate in selecting \"UNK\". It confirms that the three-player introspective model selects more comprehensive rationales and leave less informative texts unattended. Furthermore, the results also show that human evaluators offer worse sentiment predictions on the proposed approach, which is also desired and expected.",
        "sentences": [
            "Table 5 shows the performance of subjective evaluations.",
            "Looking at the first column of the table, our model is better in confusing human, which gives a higher rate in selecting \"UNK\".",
            "It confirms that the three-player introspective model selects more comprehensive rationales and leave less informative texts unattended.",
            "Furthermore, the results also show that human evaluators offer worse sentiment predictions on the proposed approach, which is also desired and expected."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "Intros+minimax",
                "%UNK"
            ],
            [
                "Intros+minimax"
            ],
            [
                "Intros+minimax",
                "Acc"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_5",
        "paper_id": "D19-1420",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1422table_6",
        "description": "Table 6 compares our model with topperforming methods reported in the literature. In particular, Huang et al. (2015) use BiLSTM-CRF. Ma and Hovy (2016), Liu et al. (2017) and Yang et al. (2018) explore character level representations on BiLSTM-CRF. Zhang et al. (2018c) use S-LSTM-CRF, a graph recurrent network encoder. Yasunaga et al. (2018) demonstrate that adversarial training can improve the tagging accuracy. Xin et al. (2018) proposed a compositional character-to-word model combined with LSTMCRF. BiLSTM-LAN gives highly competitive result on WSJ without training on external data.",
        "sentences": [
            "Table 6 compares our model with topperforming methods reported in the literature.",
            "In particular, Huang et al. (2015) use BiLSTM-CRF.",
            "Ma and Hovy (2016), Liu et al. (2017) and Yang et al. (2018) explore character level representations on BiLSTM-CRF.",
            "Zhang et al. (2018c) use S-LSTM-CRF, a graph recurrent network encoder.",
            "Yasunaga et al. (2018) demonstrate that adversarial training can improve the tagging accuracy.",
            "Xin et al. (2018) proposed a compositional character-to-word model combined with LSTMCRF.",
            "BiLSTM-LAN gives highly competitive result on WSJ without training on external data."
        ],
        "class_sentence": [
            1,
            2,
            2,
            2,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "Huang et al. (2015)"
            ],
            [
                "Ma and Hovy (2016)",
                "Liu et al. (2017)",
                "Yang et al. (2018)"
            ],
            [
                "Zhang et al. (2018c)"
            ],
            [
                "Yasunaga et al. (2018)"
            ],
            [
                "Xin et al. (2018)"
            ],
            [
                "BiLSTM-LAN"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_6",
        "paper_id": "D19-1422",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1426table_1",
        "description": "20 users were asked to interact with the learned LiD policy to teach a chosen email-classification task. For each task, the system asked a sequence of 10 questions, and the human teacher's responses were incorporated into the system to update the classification model. The users were also asked to teach another task with questions asked through a random policy. Table 1 shows the average cumulative reward for humans interacting with LiD vs a random policy for this experiment. We note that LiD leads to better performance on average. This trend is the same as in the simulated analysis, although we note that the learning is slower with real teachers than in the simulated setting on the same tasks, and the gain in performance is substantially smaller. A contributing reason for this is likely annotator bias (Geva et al., 2019), since in the simulated testing scenarios, the teacher's explanations can often likely come from a small set of turkers whose language explanations for teaching other tasks were used for training the learner's semantic parsing model. We note that the learned policy was rated by human users as more natural than a random policy on a Likert scale (with range 1-5).",
        "sentences": [
            "20 users were asked to interact with the learned LiD policy to teach a chosen email-classification task.",
            "For each task, the system asked a sequence of 10 questions, and the human teacher's responses were incorporated into the system to update the classification model.",
            "The users were also asked to teach another task with questions asked through a random policy.",
            "Table 1 shows the average cumulative reward for humans interacting with LiD vs a random policy for this experiment.",
            "We note that LiD leads to better performance on average.",
            "This trend is the same as in the simulated analysis, although we note that the learning is slower with real teachers than in the simulated setting on the same tasks, and the gain in performance is substantially smaller.",
            "A contributing reason for this is likely annotator bias (Geva et al., 2019), since in the simulated testing scenarios, the teacher's explanations can often likely come from a small set of turkers whose language explanations for teaching other tasks were used for training the learner's semantic parsing model.",
            "We note that the learned policy was rated by human users as more natural than a random policy on a Likert scale (with range 1-5)."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "LiD",
                "Random",
                "Avg. Reward"
            ],
            [
                "LiD",
                "Avg. Reward"
            ],
            [
                "Avg. Reward"
            ],
            null,
            [
                "LiD",
                "Natural",
                "Random"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_1",
        "paper_id": "D19-1426",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1429table_4",
        "description": "The results in Table 4 show that both the basicKD method and fine-grained methods achieve performance improvements through domain adaptation. Compared with the basicKD method, FGKF behaves better (+1.1% F and +2.8% Roov v.s. takes multilevel relevance discrepancies into account. The sample-q method performs better than the domainq method, which shows the domain feature is better represented at the sample level, not at the domain level.",
        "sentences": [
            "The results in Table 4 show that both the basicKD method and fine-grained methods achieve performance improvements through domain adaptation.",
            "Compared with the basicKD method, FGKF behaves better (+1.1% F and +2.8% Roov v.s. takes multilevel relevance discrepancies into account.",
            "The sample-q method performs better than the domainq method, which shows the domain feature is better represented at the sample level, not at the domain level."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "BasicKD",
                "FGKF"
            ],
            [
                "BasicKD",
                "FGKF"
            ],
            [
                "sampDomain-q a samp",
                "elemDomain-q a elem",
                "multiDomain-q a multi",
                "Sample-q a samp",
                "elemSample-q a elem",
                "multiSample-q a multi"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_4",
        "paper_id": "D19-1429",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1431table_5",
        "description": "Table 5 shows that removing gradient meta decreases 29.3% and 15% on two dataset settings, and further removing relation meta continuous decreases the performance with 55% and 72% compared to the standard results. Thus both relation meta and gradient meta contribute significantly and relation meta contributes more than gradient meta. Without gradient meta and relation meta, there is no relation-specific meta information transferred in the model and it almost doesn't work. This also illustrates that relation-specific meta information is important and effective for few-shot link prediction task.",
        "sentences": [
            "Table 5 shows that removing gradient meta decreases 29.3% and 15% on two dataset settings, and further removing relation meta continuous decreases the performance with 55% and 72% compared to the standard results.",
            "Thus both relation meta and gradient meta contribute significantly and relation meta contributes more than gradient meta.",
            "Without gradient meta and relation meta, there is no relation-specific meta information transferred in the model and it almost doesn't work.",
            "This also illustrates that relation-specific meta information is important and effective for few-shot link prediction task."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                " -g",
                " -g -r"
            ],
            [
                " -g -r"
            ],
            [
                " -g -r"
            ],
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_5",
        "paper_id": "D19-1431",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1437table_1",
        "description": "Table 1 provides the BLEU scores of FlowSeq with argmax decoding, together with baselines with purely non-autoregressive decoding methods that generate output sequence in one parallel pass. The first block lists results of models trained on raw data, while the second block are results using knowledge distillation. Without using knowledge distillation, FlowSeq base model achieves significant improvement (more than 9 BLEU points) over CMLM-base and LV NAR. It demonstrates the effectiveness of FlowSeq on modeling the complex interdependence in target languages.",
        "sentences": [
            "Table 1 provides the BLEU scores of FlowSeq with argmax decoding, together with baselines with purely non-autoregressive decoding methods that generate output sequence in one parallel pass.",
            "The first block lists results of models trained on raw data, while the second block are results using knowledge distillation.",
            "Without using knowledge distillation, FlowSeq base model achieves significant improvement (more than 9 BLEU points) over CMLM-base and LV NAR.",
            "It demonstrates the effectiveness of FlowSeq on modeling the complex interdependence in target languages."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "FlowSeq-base",
                "FlowSeq-large"
            ],
            null,
            [
                "FlowSeq-base",
                "CMLM-base",
                "LV NAR"
            ],
            [
                "FlowSeq-base",
                "FlowSeq-large"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_1",
        "paper_id": "D19-1437",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1457table_1",
        "description": "Table 1 reports results of all models on the new dependency task. XPAD significantly outperforms the strongest baselines, ProGlobal and ProStruct, by more than 3 points F1. XPAD has much higher precision than ProGlobal with similar recall, suggesting that XPAD dependency-aware decoder helps it select more accurate dependencies. Compared with ProStruct, it yields more than 11.6 points improvement on recall. As XPAD adds a novel dependency layer on top of the ProStruct architecture, we note that all these gains come exclusively from the dependency layer.",
        "sentences": [
            "Table 1 reports results of all models on the new dependency task.",
            "XPAD significantly outperforms the strongest baselines, ProGlobal and ProStruct, by more than 3 points F1.",
            "XPAD has much higher precision than ProGlobal with similar recall, suggesting that XPAD dependency-aware decoder helps it select more accurate dependencies.",
            "Compared with ProStruct, it yields more than 11.6 points improvement on recall.",
            "As XPAD adds a novel dependency layer on top of the ProStruct architecture, we note that all these gains come exclusively from the dependency layer."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "XPAD",
                "ProGlobal",
                "ProStruct"
            ],
            null,
            [
                "XPAD",
                "ProStruct"
            ],
            [
                "XPAD"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_1",
        "paper_id": "D19-1457",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1461table_2",
        "description": "Experiments . We compare the two afore mentioned models with (Khatri et al., 2018) who conducted their experiments with a BiLSTM with GloVe pre-trained word vectors (Pennington et al., 2014). Results are listed in Table 2 and we compare them using the weighted-F1, i.e. the sum of F1 score of each class weighted by their frequency in the dataset. We also report the F1 of the OFFENSIVE-class which is the metric we favor within this work, although we report both. (Note that throughout the paper, the notation F1 is always referring to OFFENSIVE-class F1.). Indeed, in the case of an imbalanced dataset such as Wikipedia Toxic Comments where most samples are SAFE, the weighted-F1 is closer to the F1 score of the SAFE class while we focus on detecting OFFENSIVE content. Our BERT-based model outperforms the method from Khatri et al. (2018); throughout the rest of the paper, we use the BERTbased architecture in our experiments. In particular, we used this baseline trained on WTC to bootstrap our approach, to be described subsequently.",
        "sentences": [
            "Experiments .",
            "We compare the two afore mentioned models with (Khatri et al., 2018) who conducted their experiments with a BiLSTM with GloVe pre-trained word vectors (Pennington et al., 2014).",
            "Results are listed in Table 2 and we compare them using the weighted-F1, i.e. the sum of F1 score of each class weighted by their frequency in the dataset.",
            "We also report the F1 of the OFFENSIVE-class which is the metric we favor within this work, although we report both.",
            "(Note that throughout the paper, the notation F1 is always referring to OFFENSIVE-class F1.).",
            "Indeed, in the case of an imbalanced dataset such as Wikipedia Toxic Comments where most samples are SAFE, the weighted-F1 is closer to the F1 score of the SAFE class while we focus on detecting OFFENSIVE content.",
            "Our BERT-based model outperforms the method from Khatri et al. (2018); throughout the rest of the paper, we use the BERTbased architecture in our experiments.",
            "In particular, we used this baseline trained on WTC to bootstrap our approach, to be described subsequently."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "fastText",
                "BERT-based",
                "(Khatri et al. 2018)"
            ],
            [
                "Weighted F1"
            ],
            [
                "OFFENSIVE F1"
            ],
            [
                "OFFENSIVE F1"
            ],
            [
                "Weighted F1",
                "OFFENSIVE F1"
            ],
            [
                "BERT-based",
                "(Khatri et al. 2018)"
            ],
            null
        ],
        "n_sentence": 8.0,
        "table_id": "table_2",
        "paper_id": "D19-1461",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1463table_3",
        "description": "Table 3 shows results of models on bAbI tasks. HMNs and Mem2Seq adopt one hop attention only and note that all results are the best performance of each model in 100 epochs. HMNs achieved the best results on most tasks except T5. HMNs-CFO also outperforms the other models. This demonstrates that both training multiple distributions over heterogeneous information and employment of context-aware memory benefit the end-to-end dialogue system. The improvements in per-dialogue accuracy on out-of-vocabulary tests are even more significant. Figure 4 shows the changes of HMNs and HMNs-CFO's total loss across time. HMNs learns significantly faster.",
        "sentences": [
            "Table 3 shows results of models on bAbI tasks.",
            "HMNs and Mem2Seq adopt one hop attention only and note that all results are the best performance of each model in 100 epochs.",
            "HMNs achieved the best results on most tasks except T5.",
            "HMNs-CFO also outperforms the other models.",
            "This demonstrates that both training multiple distributions over heterogeneous information and employment of context-aware memory benefit the end-to-end dialogue system.",
            "The improvements in per-dialogue accuracy on out-of-vocabulary tests are even more significant.",
            "Figure 4 shows the changes of HMNs and HMNs-CFO's total loss across time.",
            "HMNs learns significantly faster."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "HMNs-CFO",
                "Mem2Seq"
            ],
            [
                "HMNs-CFO",
                "Task",
                "T5"
            ],
            [
                "HMNs-CFO"
            ],
            null,
            [
                "Per-dialog accuracy",
                "T3-OOV",
                "T4-OOV",
                "T5-OOV"
            ],
            [
                "HMNs-CFO"
            ],
            [
                "HMNs-CFO"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_3",
        "paper_id": "D19-1463",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1463table_4",
        "description": "Table 4 shows our model gets the best F1 score on dataset DSTC 2, while SEQ2SEQ with attention gets the best BLEU result.",
        "sentences": [
            "Table 4 shows our model gets the best F1 score on dataset DSTC 2, while SEQ2SEQ with attention gets the best BLEU result."
        ],
        "class_sentence": [
            1
        ],
        "header_mention": [
            [
                "Our model",
                "F1",
                "BLEU"
            ]
        ],
        "n_sentence": 1.0,
        "table_id": "table_4",
        "paper_id": "D19-1463",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1467table_2",
        "description": "Single-task Settings . Table 2 shows our experimental results of ALSC in single-task settings. Firstly, we observe that by introducing attention regularizations (either Rs or Ro), most of our proposed methods outperform their counterparts. Particularly, AT-CAN-Rs and AT-CAN-Ro outperform AT-LSTM in all results; ATAE-CANRs and ATAE-CAN-Ro also outperform ATAELSTM in 15 of 16 results. in the Rest15 dataset, ATAE-CAN-Ro outperforms ATAE-LSTM by up to 5.39% of accuracy and 6.46% of the F1 score in the 3-way classification. Secondly, regularization Ro achieves better performance improvement than Rs in all results. This is because Ro includes both orthogonal and sparse regularizations for non-overlapping multiaspect sentences. Thirdly, our approaches, especially ATAE-CAN-Ro, outperform the state-ofthe-art baseline model GCAE. Finally, the LSTM method outputs the worst results in all cases, because it can not distinguish different aspects.",
        "sentences": [
            "Single-task Settings .",
            "Table 2 shows our experimental results of ALSC in single-task settings.",
            "Firstly, we observe that by introducing attention regularizations (either Rs or Ro), most of our proposed methods outperform their counterparts.",
            "Particularly, AT-CAN-Rs and AT-CAN-Ro outperform AT-LSTM in all results; ATAE-CANRs and ATAE-CAN-Ro also outperform ATAELSTM in 15 of 16 results.",
            "in the Rest15 dataset, ATAE-CAN-Ro outperforms ATAE-LSTM by up to 5.39% of accuracy and 6.46% of the F1 score in the 3-way classification.",
            "Secondly, regularization Ro achieves better performance improvement than Rs in all results.",
            "This is because Ro includes both orthogonal and sparse regularizations for non-overlapping multiaspect sentences.",
            "Thirdly, our approaches, especially ATAE-CAN-Ro, outperform the state-ofthe-art baseline model GCAE.",
            "Finally, the LSTM method outputs the worst results in all cases, because it can not distinguish different aspects."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "AT-CAN-Rs",
                "AT-CAN-Ro",
                "ATAE-CAN-Rs",
                "ATAE-CAN-Ro"
            ],
            [
                "AT-CAN-Rs",
                "AT-CAN-Ro",
                "AT-LSTM",
                "ATAE-CAN-Rs",
                "ATAE-CAN-Ro",
                "ATAE-LSTM"
            ],
            [
                "Rest15",
                "ATAE-CAN-Ro",
                "ATAE-LSTM",
                "Acc",
                "F1"
            ],
            [
                "ATAE-CAN-Rs",
                "ATAE-CAN-Ro"
            ],
            [
                "ATAE-CAN-Ro"
            ],
            [
                "ATAE-CAN-Ro",
                "GCAE"
            ],
            [
                "LSTM"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_2",
        "paper_id": "D19-1467",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1467table_3",
        "description": "Multi-task Settings Table 3 shows experimental results of ALSC in multi-task settings. We first observe that the overall results in multi-task settings outperform the ones in single-task settings, which demonstrates the effectiveness of multi-task learning by introducing the auxiliary ACD task to help the ALSC task. Second, in almost all cases, applying attention regularizations to both tasks gains more performance improvement than only to the ALSC task, which shows that our attention regularization approach can be extended to different tasks which involving aspect level attention weights, and works well in multi-task settings. For example, for the Binary classification in the Rest15 dataset, M-AT-LASTM outperforms ATLSTM by 3,57% of accuracy and 4,96% of the F1 score, and M-CAN-2Ro further outperforms MAT-LSTM by 3:28% of accuracy and 4,0% of the F1 score.",
        "sentences": [
            "Multi-task Settings Table 3 shows experimental results of ALSC in multi-task settings.",
            "We first observe that the overall results in multi-task settings outperform the ones in single-task settings, which demonstrates the effectiveness of multi-task learning by introducing the auxiliary ACD task to help the ALSC task.",
            "Second, in almost all cases, applying attention regularizations to both tasks gains more performance improvement than only to the ALSC task, which shows that our attention regularization approach can be extended to different tasks which involving aspect level attention weights, and works well in multi-task settings.",
            "For example, for the Binary classification in the Rest15 dataset, M-AT-LASTM outperforms ATLSTM by 3,57% of accuracy and 4,96% of the F1 score, and M-CAN-2Ro further outperforms MAT-LSTM by 3:28% of accuracy and 4,0% of the F1 score."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "Binary",
                "Rest15",
                "M-AT-LSTM",
                "Acc",
                "F1",
                "M-CAN-2Ro"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "D19-1467",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1467table_4",
        "description": "Table 4 shows the results of the ACD task in multi-task settings. Our proposed regularization terms can also improve the performance of ACD. Regularization Ro achieves the best performance in almost all metrics.",
        "sentences": [
            "Table 4 shows the results of the ACD task in multi-task settings.",
            "Our proposed regularization terms can also improve the performance of ACD.",
            "Regularization Ro achieves the best performance in almost all metrics."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "M-CAN-2Rs",
                "M-CAN-2Ro"
            ],
            [
                "M-CAN-2Ro"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_4",
        "paper_id": "D19-1467",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1470table_3",
        "description": "We first compare the effects of varying interaction modeling methods (see Section 3.3) on conversation recommendation. Table 3 displays their results on development set. In comparison, we consider BiLSTM over turn sequence (only chronological order encoded and henceforth BiLSTM), GLSTM (state number g = 6), GCN (layer number set to 3) without BiLSTM-encoded temporal representations (henceforth GCN (W/O BiLSTM)), and the full GCN described in Section 3.3 (henceforth GCN (With BiLSTM) and layer number set to 1). The above hyper-parameters are tuned based on the training loss. From the results, we find that BiLSTM exhibits the worst results for not encoding replying relations. Its difference from others are larger on Reddit attributed to the rich replying structure therein (as shown in Figure 4(b)). The best performance is achieved for GCN (With BiLSTM), with relatively less training time. This shows the effectiveness and efficiency to explore the order of turns with BiLSTM and the user interactions with GCN. In the later analysis, we will only discuss our model that exploits GCN (With BiLSTM) for interaction modeling.",
        "sentences": [
            "We first compare the effects of varying interaction modeling methods (see Section 3.3) on conversation recommendation.",
            "Table 3 displays their results on development set.",
            "In comparison, we consider BiLSTM over turn sequence (only chronological order encoded and henceforth BiLSTM), GLSTM (state number g = 6), GCN (layer number set to 3) without BiLSTM-encoded temporal representations (henceforth GCN (W/O BiLSTM)), and the full GCN described in Section 3.3 (henceforth GCN (With BiLSTM) and layer number set to 1).",
            "The above hyper-parameters are tuned based on the training loss.",
            "From the results, we find that BiLSTM exhibits the worst results for not encoding replying relations.",
            "Its difference from others are larger on Reddit attributed to the rich replying structure therein (as shown in Figure 4(b)).",
            "The best performance is achieved for GCN (With BiLSTM), with relatively less training time.",
            "This shows the effectiveness and efficiency to explore the order of turns with BiLSTM and the user interactions with GCN.",
            "In the later analysis, we will only discuss our model that exploits GCN (With BiLSTM) for interaction modeling."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            1,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "BiLSTM",
                "GLSTM",
                "GCN (W/O BiLSTM)",
                "GCN (With BiLSTM)"
            ],
            null,
            [
                "BiLSTM",
                "MAP"
            ],
            [
                "Reddit"
            ],
            [
                "GCN (With BiLSTM)",
                "MAP",
                "Train Time"
            ],
            [
                "GCN (With BiLSTM)"
            ],
            [
                "GCN (With BiLSTM)"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_3",
        "paper_id": "D19-1470",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1470table_4",
        "description": "5.2 Comparisons with Previous Work . Main Results. Table 4 shows the conversation recommendation results with baselines and state of the arts. Our model exhibits the best results on both datasets, significantly outperforming all the comparison models. It indicates the usefulness to encode user interactions for conversation recommendation. Particularly, CONVMF is able to encode turns' temporal orders yet ignores how they reply with each other in conversation history. It is outperformed by our model, showing the benefit to capture users' replying patterns for predicting what conversations will draw their engagement.",
        "sentences": [
            "5.2 Comparisons with Previous Work .",
            "Main Results.",
            "Table 4 shows the conversation recommendation results with baselines and state of the arts.",
            "Our model exhibits the best results on both datasets, significantly outperforming all the comparison models.",
            "It indicates the usefulness to encode user interactions for conversation recommendation.",
            "Particularly, CONVMF is able to encode turns' temporal orders yet ignores how they reply with each other in conversation history.",
            "It is outperformed by our model, showing the benefit to capture users' replying patterns for predicting what conversations will draw their engagement."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "OURS"
            ],
            null,
            [
                "CONVMF"
            ],
            [
                "OURS"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_4",
        "paper_id": "D19-1470",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1485table_2",
        "description": "Performance Comparison Table 2 shows the results of different methods for rumor stance classification. Clearly, the macro-averaged F1 of Conversational-GCN is better than all baselines. Especially, our method shows the effectiveness of determining denying stance, while other methods can not give any correct prediction for denying class (the FD scores of them are equal to zero). Further, Conversational-GCN also achieves higher F1 score for querying stance (FQ). Identifying denying and querying stances correctly is crucial for veracity prediction because they play the role of indicators for f alse and unverif ied rumors respectively (see Figure 2). Meanwhile, the classimbalanced problem of data makes this a challenge. Conversational-GCN effectively encodes structural context for each tweet via aggregating information from its neighbors, learning powerful stance features without feature engineering. It is also more computationally efficient than sequential and temporal based methods. The information aggregations for all tweets in a conversation are worked in parallel and thus the running time is not sensitive to conversation\u2019s depth.",
        "sentences": [
            "Performance Comparison Table 2 shows the results of different methods for rumor stance classification.",
            "Clearly, the macro-averaged F1 of Conversational-GCN is better than all baselines.",
            "Especially, our method shows the effectiveness of determining denying stance, while other methods can not give any correct prediction for denying class (the FD scores of them are equal to zero).",
            "Further, Conversational-GCN also achieves higher F1 score for querying stance (FQ).",
            "Identifying denying and querying stances correctly is crucial for veracity prediction because they play the role of indicators for f alse and unverif ied rumors respectively (see Figure 2).",
            "Meanwhile, the classimbalanced problem of data makes this a challenge.",
            "Conversational-GCN effectively encodes structural context for each tweet via aggregating information from its neighbors, learning powerful stance features without feature engineering.",
            "It is also more computationally efficient than sequential and temporal based methods.",
            "The information aggregations for all tweets in a conversation are worked in parallel and thus the running time is not sensitive to conversation\u2019s depth."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            2,
            2,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "Macro-F1",
                "Conversational-GCN (Ours, L = 2)"
            ],
            [
                "Conversational-GCN (Ours, L = 2)"
            ],
            [
                "Conversational-GCN (Ours, L = 2)"
            ],
            null,
            null,
            [
                "Conversational-GCN (Ours, L = 2)"
            ],
            null,
            null
        ],
        "n_sentence": 9.0,
        "table_id": "table_2",
        "paper_id": "D19-1485",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1485table_3",
        "description": "Performance Comparison Table 3 shows the comparisons of different methods. By comparing single-task methods, Hierarchical GCN-RNN performs better than TD-RvNN, which indicates that our hierarchical framework can effectively model conversation structures to learn high-quality tweet representations. The recursive operation in TD-RvNN is performed in a fixed direction and runs over all tweets, thus may not obtain enough useful information. Moreover, the training speed of Hierarchical GCN-RNN is significantly faster than TD-RvNN: in the condition of batch-wise optimization for training one step over a batch containing 32 conversations, our method takes only 0.18 seconds, while TD-RvNN takes 5.02 seconds.",
        "sentences": [
            "Performance Comparison Table 3 shows the comparisons of different methods.",
            "By comparing single-task methods, Hierarchical GCN-RNN performs better than TD-RvNN, which indicates that our hierarchical framework can effectively model conversation structures to learn high-quality tweet representations.",
            "The recursive operation in TD-RvNN is performed in a fixed direction and runs over all tweets, thus may not obtain enough useful information.",
            "Moreover, the training speed of Hierarchical GCN-RNN is significantly faster than TD-RvNN: in the condition of batch-wise optimization for training one step over a batch containing 32 conversations, our method takes only 0.18 seconds, while TD-RvNN takes 5.02 seconds."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Hierarchical GCN-RNN (Ours)",
                "TD-RvNN (Ma et al., 2018b)"
            ],
            [
                "TD-RvNN (Ma et al., 2018b)"
            ],
            [
                "Hierarchical GCN-RNN (Ours)",
                "TD-RvNN (Ma et al., 2018b)"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "D19-1485",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1488table_2",
        "description": "Table 2 shows the classification accuracy of different methods on 6 benchmark datasets. We can see that our methods significantly outperform all the baselines by a large margin, which shows the effectiveness of our proposed method on semisupervised short text classification. The traditional method SVMs based on the human-designed features, achieve better performance than the deep models with random initialization, i.e., CNN-rand and LSTM-rand in most cases. While CNN-pretrain and LSTM-pretrain using the pre-trained vectors achieve significant improvements and outperform SVMs. Our model HGAT consistently outperforms all the state-ofthe-art models by a large margin, which shows the effectiveness of our proposed method.",
        "sentences": [
            "Table 2 shows the classification accuracy of different methods on 6 benchmark datasets.",
            "We can see that our methods significantly outperform all the baselines by a large margin, which shows the effectiveness of our proposed method on semisupervised short text classification.",
            "The traditional method SVMs based on the human-designed features, achieve better performance than the deep models with random initialization, i.e., CNN-rand and LSTM-rand in most cases.",
            "While CNN-pretrain and LSTM-pretrain using the pre-trained vectors achieve significant improvements and outperform SVMs.",
            "Our model HGAT consistently outperforms all the state-ofthe-art models by a large margin, which shows the effectiveness of our proposed method."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "Dataset"
            ],
            [
                "HGAT"
            ],
            [
                "SVM +TFIDF",
                "SVM +LDACNN",
                "CNN -rand",
                "LSTM -rand"
            ],
            [
                "CNN -pretrain",
                "LSTM -pretrain",
                "SVM +TFIDF",
                "SVM +LDACNN"
            ],
            [
                "HGAT"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_2",
        "paper_id": "D19-1488",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1491table_7",
        "description": "We report the results on the full BENCHLS dataset in the upper half of Table 7. In the lower half, we compare our results on the test set of 464 instances to those running the Paetzold and Specia (2016a) system (P&S) on the same test splits. Since the P&S system was trained on half of BENCHLS we cannot run it on the full dataset. Table 7 shows that ranking with S+ C works best according to all measures and across both sets.",
        "sentences": [
            "We report the results on the full BENCHLS dataset in the upper half of Table 7.",
            "In the lower half, we compare our results on the test set of 464 instances to those running the Paetzold and Specia (2016a) system (P&S) on the same test splits.",
            "Since the P&S system was trained on half of BENCHLS we cannot run it on the full dataset.",
            "Table 7 shows that ranking with S+ C works best according to all measures and across both sets."
        ],
        "class_sentence": [
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            [
                "BENCHLS"
            ],
            [
                "P&S"
            ],
            [
                "P&S",
                "BENCHLS"
            ],
            [
                "S+C"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_7",
        "paper_id": "D19-1491",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1496table_3",
        "description": "4.2 Experimental Results Performance on identifying perpetuated tokens. Table 3 shows the performance of DISP and SC in discriminating perturbations. Compared to SC, DISP has an absolute improvement by 35% and 46% on SST-2 and IMDb in terms of F1score, respectively. It also proves that the context information is essential when discriminating the perturbations. An interesting observation is that SC has high recall but low precision scores for character-level attacks because it is eager to correct misspellings while most of its corrections are not perturbations. Conversely, DISP has more balances of recall and precision scores since it is optimized to discriminate the perturbed tokens. For the word-level attacks, SC shows similar low performance on both random and embed attacks while DISP behaves much better. Moreover, DISP works better on the random attack because the embeddings of the original tokens tend to have noticeably greater Euclidean distances to randomlypicked tokens than the distances to other tokens.",
        "sentences": [
            "4.2 Experimental Results Performance on identifying perpetuated tokens.",
            "Table 3 shows the performance of DISP and SC in discriminating perturbations.",
            "Compared to SC, DISP has an absolute improvement by 35% and 46% on SST-2 and IMDb in terms of F1score, respectively.",
            "It also proves that the context information is essential when discriminating the perturbations.",
            "An interesting observation is that SC has high recall but low precision scores for character-level attacks because it is eager to correct misspellings while most of its corrections are not perturbations.",
            "Conversely, DISP has more balances of recall and precision scores since it is optimized to discriminate the perturbed tokens.",
            "For the word-level attacks, SC shows similar low performance on both random and embed attacks while DISP behaves much better.",
            "Moreover, DISP works better on the random attack because the embeddings of the original tokens tend to have noticeably greater Euclidean distances to randomlypicked tokens than the distances to other tokens."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "DISP",
                "SC"
            ],
            [
                "SC",
                "DISP",
                "SST-2",
                "IMDb",
                "F1"
            ],
            null,
            [
                "SC",
                "Recall",
                "Precision",
                "Character-level Attacks"
            ],
            [
                "DISP",
                "Recall",
                "Precision"
            ],
            [
                "SC",
                "DISP",
                "Random",
                "Embed"
            ],
            [
                "DISP",
                "Random"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_3",
        "paper_id": "D19-1496",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1498table_1",
        "description": "4 Results Table 1 depicts the performance of our proposed model on the CDR test set, in comparison with the state-of-the-art. We directly compare our model with models that do not incorporate external knowledge. Verga et al. (2018) and Nguyen and Verspoor (2018) consider a single pair per document, while Gu et al. (2017) develops separate models for intra- and inter-sentence pairs. As it can be observed, the proposed model outperforms the state-of-the-art in CDR dataset by 1.3 percentage points of overall performance. We also show the methods that take advantage of syntactic dependency tools. Li et al. (2016b) uses cotraining with additional unlabeled training data. Our model performs significantly better on intraand inter-sentential pairs, even compared to most of the models with external knowledge, except for Li et al. (2016b).",
        "sentences": [
            "4 Results Table 1 depicts the performance of our proposed model on the CDR test set, in comparison with the state-of-the-art.",
            "We directly compare our model with models that do not incorporate external knowledge. Verga et al. (2018) and Nguyen and Verspoor (2018) consider a single pair per document, while Gu et al. (2017) develops separate models for intra- and inter-sentence pairs.",
            "As it can be observed, the proposed model outperforms the state-of-the-art in CDR dataset by 1.3 percentage points of overall performance.",
            "We also show the methods that take advantage of syntactic dependency tools.",
            "Li et al. (2016b) uses cotraining with additional unlabeled training data.",
            "Our model performs significantly better on intraand inter-sentential pairs, even compared to most of the models with external knowledge, except for Li et al. (2016b)."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "Verga et al. (2018)",
                "Nguyen and Verspoor (2018)",
                "Gu et al. (2017)",
                "EoG"
            ],
            [
                "EoG"
            ],
            null,
            [
                "Li et al. (2016b)"
            ],
            [
                "EoG"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_1",
        "paper_id": "D19-1498",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1499table_3",
        "description": "9 Results and Analysis Evaluation Results. Table 3 presents the evaluation results of automatic metrics on the models. It can be seen that the BLEU scores and GLEU scores of the semi-supervised models on almost all the datasets are better than the baseline S2S model. This result indicates that the model benefits from the nonparallel data in terms of content preservation. One interesting thing is that the overall BLEU scores on the ancient poems and modern Chinese datasets are lower than other datasets. This result may be explained by the fact that the edit distance between formal and informal texts are smaller than between ancient poems and modern Chinese texts. Therefore, it is more challenging for model to preserve the content meaning when transferring between ancient poems and modern Chinese text. Among three semi-supervised models, CPLS model achieves the greatest improvement, verifying the effectiveness of the projection functions. However, the gain of CPLS model in the aspect of style accuracy is not that significant. A possible explanation may be the bias of the style classifier. Take the transfer task from ancient poems to modern Chinese text for example. We observe that the classifier tends to classify short sentences into ancient poems as length is an obvious feature. We analyse the sentences generated by S2S model and by the CPLS model, and the statistics show that the average length of the text generated by S2S model is shorter, which may lead to the bias of the style classifier. Therefore, we also adopt human evaluation to alleviate this issue.",
        "sentences": [
            "9 Results and Analysis Evaluation Results.",
            "Table 3 presents the evaluation results of automatic metrics on the models.",
            "It can be seen that the BLEU scores and GLEU scores of the semi-supervised models on almost all the datasets are better than the baseline S2S model.",
            "This result indicates that the model benefits from the nonparallel data in terms of content preservation.",
            "One interesting thing is that the overall BLEU scores on the ancient poems and modern Chinese datasets are lower than other datasets.",
            "This result may be explained by the fact that the edit distance between formal and informal texts are smaller than between ancient poems and modern Chinese texts.",
            "Therefore, it is more challenging for model to preserve the content meaning when transferring between ancient poems and modern Chinese text.",
            "Among three semi-supervised models, CPLS model achieves the greatest improvement, verifying the effectiveness of the projection functions.",
            "However, the gain of CPLS model in the aspect of style accuracy is not that significant.",
            "A possible explanation may be the bias of the style classifier.",
            "Take the transfer task from ancient poems to modern Chinese text for example.",
            "We observe that the classifier tends to classify short sentences into ancient poems as length is an obvious feature.",
            "We analyse the sentences generated by S2S model and by the CPLS model, and the statistics show that the average length of the text generated by S2S model is shorter, which may lead to the bias of the style classifier.",
            "Therefore, we also adopt human evaluation to alleviate this issue."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            1,
            2,
            2,
            1,
            1,
            2,
            2,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "BLEU",
                "GLEU",
                "S2S"
            ],
            null,
            [
                "BLEU",
                "to Anc.P",
                "to M.zh"
            ],
            [
                "to Anc.P",
                "to M.zh"
            ],
            [
                "to Anc.P",
                "to M.zh"
            ],
            [
                "CPLS"
            ],
            [
                "CPLS",
                "Acc"
            ],
            null,
            null,
            null,
            [
                "S2S",
                "CPLS"
            ],
            null
        ],
        "n_sentence": 14.0,
        "table_id": "table_3",
        "paper_id": "D19-1499",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1499table_4",
        "description": "Table 4 compares the human evaluation results of S2S model and CPLS model on all the datasets, which are calculated by the average score of the human annotations. As shown in the Table 4, the CPLS model outperforms the S2S model in the aspects of the content preservation and style strength, and is on par in terms of fluency.",
        "sentences": [
            "Table 4 compares the human evaluation results of S2S model and CPLS model on all the datasets, which are calculated by the average score of the human annotations.",
            "As shown in the Table 4, the CPLS model outperforms the S2S model in the aspects of the content preservation and style strength, and is on par in terms of fluency."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "S2S",
                "CPLS"
            ],
            [
                "CPLS",
                "S2S",
                "Content",
                "Style",
                "Fluency"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_4",
        "paper_id": "D19-1499",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1505table_4",
        "description": "5.2.2 Edit Anchoring. Table 4 shows the results for Edit Anchoring. Our method, CmntEdit-EA, outperforms the best baseline method, Gated-RNN, by 5.5% on F1 and 6.9% on accuracy. The improvements over all the baselines are statistically significant at a p-value of 0.01. The baseline classifiers including PassiveAggressive, Random Forest and Adaboost have high accuracies, but low F1 scores. This is because of the imbalance between positive and negative samples in our data. Specifically, the number of negative samples is 4 times greater than the number of positive samples when the size of the candidate set is 5 - and even greater when it is 10. Therefore, the baseline classifiers tend to naively predict a negative label, which artificially boosts precision to the detriment of recall. In fact, Adaboost actually outperforms our models on accuracy when the candidate set size is 10, but yields a much lower F1 score.",
        "sentences": [
            "5.2.2 Edit Anchoring.",
            "Table 4 shows the results for Edit Anchoring.",
            "Our method, CmntEdit-EA, outperforms the best baseline method, Gated-RNN, by 5.5% on F1 and 6.9% on accuracy.",
            "The improvements over all the baselines are statistically significant at a p-value of 0.01.",
            "The baseline classifiers including PassiveAggressive, Random Forest and Adaboost have high accuracies, but low F1 scores.",
            "This is because of the imbalance between positive and negative samples in our data.",
            "Specifically, the number of negative samples is 4 times greater than the number of positive samples when the size of the candidate set is 5 - and even greater when it is 10.",
            "Therefore, the baseline classifiers tend to naively predict a negative label, which artificially boosts precision to the detriment of recall.",
            "In fact, Adaboost actually outperforms our models on accuracy when the candidate set size is 10, but yields a much lower F1 score."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            2,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "CmntEdit-EA",
                "Gated RNN",
                "F1",
                "Acc"
            ],
            null,
            [
                "Passive-Aggr",
                "RandForest",
                "Adaboost",
                "Acc",
                "F1"
            ],
            null,
            null,
            null,
            [
                "Adaboost",
                "Acc",
                "CmntEdit-EA",
                "CmntEdit-MT",
                "F1"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_4",
        "paper_id": "D19-1505",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1506table_3",
        "description": "We train a PRADO model variant with 8-bit quantization as described in (Jacob et al., 2018). This procedure simulates the quantization process during  training  by  nudging  the  weights  and  activations towards a grid of discrete levels (2N levels, where N=8 is the number of bits). We estimate the activation ranges for each training batch and use exponential moving average to smooth the quantization ranges across training steps. (Jacob et al., 2018)  noted  that  by  training  with  quantization, they  reached  similar  accuracy  with  8-bit  models as floating point ones on several image classification and object detection data sets. For text classification, we observed that training with quantization  significantly  improves  accuracy  as  shown in Table 3. We believe that this is due to the improved regularization as quantization has the highest  impact  on  Yelp. This  dataset  has  relatively few training samples per class (see Table 1) which causes the model to overfit the training data and regularization provided by the operation that simulates quantization during training helps it generalize better. Furthermore, the model size of 8-bit quantized PRADO models is equal to the number of parameters. Figure 3 shows that PRADO can reach the performance reported in the Table 3 with model size of less than 200 Kilobytes. PRADO starts getting competitive results on the same data sets with tiny model size as low as 25 Kilobytes.",
        "sentences": [
            "We train a PRADO model variant with 8-bit quantization as described in (Jacob et al., 2018).",
            "This procedure simulates the quantization process during  training  by  nudging  the  weights  and  activations towards a grid of discrete levels (2N levels, where N=8 is the number of bits).",
            "We estimate the activation ranges for each training batch and use exponential moving average to smooth the quantization ranges across training steps.",
            "(Jacob et al., 2018)  noted  that  by  training  with  quantization, they  reached  similar  accuracy  with  8-bit  models as floating point ones on several image classification and object detection data sets.",
            "For text classification, we observed that training with quantization  significantly  improves  accuracy  as  shown in Table 3.",
            "We believe that this is due to the improved regularization as quantization has the highest  impact  on  Yelp.",
            "This  dataset  has  relatively few training samples per class (see Table 1) which causes the model to overfit the training data and regularization provided by the operation that simulates quantization during training helps it generalize better.",
            "Furthermore, the model size of 8-bit quantized PRADO models is equal to the number of parameters.",
            "Figure 3 shows that PRADO can reach the performance reported in the Table 3 with model size of less than 200 Kilobytes.",
            "PRADO starts getting competitive results on the same data sets with tiny model size as low as 25 Kilobytes."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            1,
            1,
            2,
            2,
            2,
            2
        ],
        "header_mention": [
            [
                "PRADO 8-bit Quantized"
            ],
            null,
            null,
            null,
            [
                "PRADO 8-bit Quantized"
            ],
            [
                "PRADO 8-bit Quantized",
                "Yelp"
            ],
            [
                "Yelp"
            ],
            [
                "PRADO 8-bit Quantized"
            ],
            [
                "PRADO"
            ],
            [
                "PRADO"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_3",
        "paper_id": "D19-1506",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1510table_2",
        "description": "Comparison against Baselines. Table 2 lists the results for the DfWiki dataset. We obtain new SOTA results with LASERTAGGERAR, outperforming the previous SOTA 7-layer Transformer model from Geva et al. (2019) by 2.7% Exact score and 1.0% SARI score. We also find that the pretrained SEQ2SEQBERT model yields nearly as good performance, demonstrating the effectiveness of unsupervised pretraining for generation tasks. The performance of the tagger is impaired significantly when leaving out the SWAP tag due to the model's inability to reconstruct 10.5% of the training set.",
        "sentences": [
            "Comparison against Baselines.",
            "Table 2 lists the results for the DfWiki dataset.",
            "We obtain new SOTA results with LASERTAGGERAR, outperforming the previous SOTA 7-layer Transformer model from Geva et al. (2019) by 2.7% Exact score and 1.0% SARI score.",
            "We also find that the pretrained SEQ2SEQBERT model yields nearly as good performance, demonstrating the effectiveness of unsupervised pretraining for generation tasks.",
            "The performance of the tagger is impaired significantly when leaving out the SWAP tag due to the model's inability to reconstruct 10.5% of the training set."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "LASERTAGGERAR",
                "Transformer (Geva et al., 2019)",
                "Exact",
                " SARI"
            ],
            [
                "SEQ2SEQBERT"
            ],
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_2",
        "paper_id": "D19-1510",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1510table_5",
        "description": "Table 5 compares our taggers against two baselines. Again, the tagging approach clearly outperforms the BERT-based seq2seq model, here by being more than seven times as accurate in the prediction of corrections. This can be accounted to the seq2seq model's much richer generation capacity, which the model can not properly tune to the task at hand given the small amount of training data. The tagging approach on the other hand is naturally suited to this kind of problem.",
        "sentences": [
            "Table 5 compares our taggers against two baselines.",
            "Again, the tagging approach clearly outperforms the BERT-based seq2seq model, here by being more than seven times as accurate in the prediction of corrections.",
            "This can be accounted to the seq2seq model's much richer generation capacity, which the model can not properly tune to the task at hand given the small amount of training data.",
            "The tagging approach on the other hand is naturally suited to this kind of problem."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "SEQ2SEQBERT",
                "LASERTAGGER AR",
                "LASERTAGGER FF"
            ],
            [
                "SEQ2SEQBERT"
            ],
            [
                "LASERTAGGER AR",
                "LASERTAGGER FF"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_5",
        "paper_id": "D19-1510",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1512table_5",
        "description": "4.5 Discussions.  Ablation study: We compare the full model of DeepCom with the following variants: (1) No Reading: the entire reading network is replaced by a TF-IDF based keyword extractor, and top 40 keywords (tuned on validation sets) are fed to the generation network; (2) No Prediction: the prediction layer of the reading network is removed, and thus the entire V is used in the generation network; and (3) No Sampling: we directly use the model pre-trained by maximizing Objective (12). Table 5 reports the results on automatic metrics. We can see that all variants suffer from performance drop and No Reading is the worst among the three variants. Thus, we can conclude that (1) span prediction cannot be simply replaced by TF-IDF based keyword extraction, as the former is based on a deep comprehension of news articles and calibrated in the end-to-end learning process; (2) even with sophisticated representations, one cannot directly feed the entire article to the generation network, as comment generation is vulnerable to the noise in the article; and (3) pre-training is useful, but optimizing the lower bound of the true objective is still beneficial.",
        "sentences": [
            "4.5 Discussions.",
            " Ablation study: We compare the full model of DeepCom with the following variants: (1) No Reading: the entire reading network is replaced by a TF-IDF based keyword extractor, and top 40 keywords (tuned on validation sets) are fed to the generation network; (2) No Prediction: the prediction layer of the reading network is removed, and thus the entire V is used in the generation network; and (3) No Sampling: we directly use the model pre-trained by maximizing Objective (12).",
            "Table 5 reports the results on automatic metrics.",
            "We can see that all variants suffer from performance drop and No Reading is the worst among the three variants.",
            "Thus, we can conclude that (1) span prediction cannot be simply replaced by TF-IDF based keyword extraction, as the former is based on a deep comprehension of news articles and calibrated in the end-to-end learning process; (2) even with sophisticated representations, one cannot directly feed the entire article to the generation network, as comment generation is vulnerable to the noise in the article; and (3) pre-training is useful, but optimizing the lower bound of the true objective is still beneficial."
        ],
        "class_sentence": [
            0,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "No Reading"
            ],
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_5",
        "paper_id": "D19-1512",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1515table_1",
        "description": "Table 1 shows the performance of previous structured prediction models, current state-of-theart models, our baseline models and the SoftLabel Chain CRF model. For a fair comparison with BAN (Kim et al., 2018), we also report result of the hard-label baseline with GloVe (Pennington et al., 2014) embeddings, while we obtain 0.33% higher result with ELMo. Training a non-CRF model on soft-label target distributions improves  accuracy  by  a  further  2.08%. On  top of that, Soft-Label Chain CRF improves accuracy by another 0.40%, which shows the effectiveness of treating phrase grounding as a sequence labeling task and using CRFs to capture entity dependencies. We  also  observe  that  the  Hard-Label Chain CRF outperforms the hard-label baseline by a mere margin of 0.05%, so our conjecture is that using chain CRFs works well only with a suitable choice of training regime.  Soft-Label Chain CRF gives an overall improvement of 2.48% over the hard-label  baseline;  it  significantly  outperforms previous  structured  prediction  models  including Structured Matching (Wang et al., 2016), Phrase-Region  CCA  (Plummer  et  al.,  2017a)  and  QRC Net (Chen et al., 2017b), and surpasses the state-of-the-art BAN (Kim et al., 2018) and DDPN (Yu et al., 2018b) models by a margin of 5.00% and about 1.4%, respectively.",
        "sentences": [
            "Table 1 shows the performance of previous structured prediction models, current state-of-theart models, our baseline models and the SoftLabel Chain CRF model.",
            "For a fair comparison with BAN (Kim et al., 2018), we also report result of the hard-label baseline with GloVe (Pennington et al., 2014) embeddings, while we obtain 0.33% higher result with ELMo.",
            "Training a non-CRF model on soft-label target distributions improves  accuracy  by  a  further  2.08%.",
            "On  top of that, Soft-Label Chain CRF improves accuracy by another 0.40%, which shows the effectiveness of treating phrase grounding as a sequence labeling task and using CRFs to capture entity dependencies.",
            "We  also  observe  that  the  Hard-Label Chain CRF outperforms the hard-label baseline by a mere margin of 0.05%, so our conjecture is that using chain CRFs works well only with a suitable choice of training regime.",
            " Soft-Label Chain CRF gives an overall improvement of 2.48% over the hard-label  baseline;  it  significantly  outperforms previous  structured  prediction  models  including Structured Matching (Wang et al., 2016), Phrase-Region  CCA  (Plummer  et  al.,  2017a)  and  QRC Net (Chen et al., 2017b), and surpasses the state-of-the-art BAN (Kim et al., 2018) and DDPN (Yu et al., 2018b) models by a margin of 5.00% and about 1.4%, respectively."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "BAN (Kim et al. 2018)"
            ],
            [
                "Soft-Label (SL)",
                "Hard-Label (HL)"
            ],
            [
                "Soft-Label Chain CRF (SL-CCRF)",
                "Soft-Label (SL)"
            ],
            [
                "Hard-Label Chain CRF (HL-CCRF)",
                "Hard-Label (HL)"
            ],
            [
                "Soft-Label Chain CRF (SL-CCRF)",
                "Hard-Label (HL)",
                "Structured Matching (Wang et al. 2016)",
                "Phrase-Region CCA (Plummer et al. 2017a)",
                "QRC Net (Chen et al. 2017b)",
                "BAN (Kim et al. 2018)",
                "DDPN (Yu et al. 2018b)"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_1",
        "paper_id": "D19-1515",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1521table_7",
        "description": "6.2    Ablation Study. Table 7 shows ablation results on BLING-KPE\u2019s variations. Each variation removes a component and keeps all others unchanged. ELMo Embedding. We first verify the effectiveness of using ELMo embedding by replacing ELMo with the WordPiece token embedding (Wu et  al.,  2016). The  accuracy  of  this  variation  is much  lower  than  the  accuracy  of  the  full  model and others. The result is shown in the first row of Table 7. The context-aware word embedding is a necessary component of BLING-KPE. Network  Architecture. The second part of Table 7 studies the contribution of Transformer and position embedding. Transformer contributes significantly to Query Prediction; with a lot of training data, the self-attention layers capture the global contexts between n-grams. But on OpenKP, its effectiveness is mostly observed on the first position. The position embedding barely helps, since real-world web pages are often not one text sequence. Beyond Language Understanding. As shown in the second part of Table 7, both visual features and search pretraining contribute significantly to BLING-KPE\u2019s  effectiveness. Without  either  of them, the accuracy drops significantly. Visual features even help on Query Prediction, though users issued the click queries and clicked on the documents before seeing its full page. The crucial role of ELMo embeddings confirm the  benefits  of  bringing  background  knowledge and general language understanding, in the format of pre-trained contextual embedding, in keyphrase extraction. The importance of visual features and search weak supervisions confirms the benefits of going beyond language understanding in modeling real-world web documents.",
        "sentences": [
            "6.2    Ablation Study.",
            "Table 7 shows ablation results on BLING-KPE\u2019s variations.",
            "Each variation removes a component and keeps all others unchanged.",
            "ELMo Embedding.",
            "We first verify the effectiveness of using ELMo embedding by replacing ELMo with the WordPiece token embedding (Wu et  al.,  2016).",
            "The  accuracy  of  this  variation  is much  lower  than  the  accuracy  of  the  full  model and others.",
            "The result is shown in the first row of Table 7.",
            "The context-aware word embedding is a necessary component of BLING-KPE.",
            "Network  Architecture.",
            "The second part of Table 7 studies the contribution of Transformer and position embedding.",
            "Transformer contributes significantly to Query Prediction; with a lot of training data, the self-attention layers capture the global contexts between n-grams.",
            "But on OpenKP, its effectiveness is mostly observed on the first position.",
            "The position embedding barely helps, since real-world web pages are often not one text sequence.",
            "Beyond Language Understanding.",
            "As shown in the second part of Table 7, both visual features and search pretraining contribute significantly to BLING-KPE\u2019s  effectiveness.",
            "Without  either  of them, the accuracy drops significantly.",
            "Visual features even help on Query Prediction, though users issued the click queries and clicked on the documents before seeing its full page.",
            "The crucial role of ELMo embeddings confirm the  benefits  of  bringing  background  knowledge and general language understanding, in the format of pre-trained contextual embedding, in keyphrase extraction.",
            "The importance of visual features and search weak supervisions confirms the benefits of going beyond language understanding in modeling real-world web documents."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            1,
            1,
            1,
            1,
            2,
            1,
            2,
            2,
            1,
            2,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            [
                "No ELMo"
            ],
            [
                "No ELMo"
            ],
            null,
            null,
            null,
            [
                "No Transformer",
                "No Position"
            ],
            [
                "No Transformer"
            ],
            null,
            [
                "No Position"
            ],
            null,
            [
                "No Visual",
                "No Pretraining"
            ],
            [
                "No Visual",
                "No Pretraining"
            ],
            [
                "No Visual"
            ],
            [
                "No ELMo"
            ],
            null
        ],
        "n_sentence": 19.0,
        "table_id": "table_7",
        "paper_id": "D19-1521",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1524table_6",
        "description": "As Table 6 shows, our SciResREC framework outperforms the two baselines. An ablation test suggests that each feature component of our model contributes to the final performance, which indicates the information of role and function are helpful for understanding the scientific resources. And we can observe that the feature of 2-nd category role label has the largest impact on performance indicating that capturing fine-grained role types is important for recognizing specific resources.",
        "sentences": [
            "As Table 6 shows, our SciResREC framework outperforms the two baselines.",
            "An ablation test suggests that each feature component of our model contributes to the final performance, which indicates the information of role and function are helpful for understanding the scientific resources.",
            "And we can observe that the feature of 2-nd category role label has the largest impact on performance indicating that capturing fine-grained role types is important for recognizing specific resources."
        ],
        "class_sentence": [
            1,
            2,
            1
        ],
        "header_mention": [
            [
                "SciResREC",
                "RF (BoW+TFIDF)",
                "RF (N-grams+TFIDF)"
            ],
            null,
            [
                "-Role 2nd feature"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_6",
        "paper_id": "D19-1524",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1526table_2",
        "description": "We evaluate the performance of supervised hashing in this section. Table 2 shows the performances of different supervised hashing models on three datasets under different lengths of hashing codes. We observe that all of the VAE-based generative hashing models (i.e VDSH, NASH, GMSH and BMSH) exhibit better performance, demonstrating the effectiveness of generative models on the task of semantic hashing. It can be also seen that BMSH-S achieves the best performance, suggesting that the advantages of Bernoulli mixture priors can also be extended to the supervised scenarios.",
        "sentences": [
            "We evaluate the performance of supervised hashing in this section.",
            "Table 2 shows the performances of different supervised hashing models on three datasets under different lengths of hashing codes.",
            "We observe that all of the VAE-based generative hashing models (i.e VDSH, NASH, GMSH and BMSH) exhibit better performance, demonstrating the effectiveness of generative models on the task of semantic hashing.",
            "It can be also seen that BMSH-S achieves the best performance, suggesting that the advantages of Bernoulli mixture priors can also be extended to the supervised scenarios."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "VDSH-S",
                "NASH-DN-S",
                "GMSH-S",
                "BMSH-S"
            ],
            [
                "BMSH-S"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "D19-1526",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1530table_2",
        "description": "Word similarity . Table 2 reports the SimLex-999 Spearman rank-order correlation coefficients rs (all are significant, p < 0.01). Surprisingly, the WED40 and 70 methods outperform the unmitigated embedding, although the difference in result is small (0.386 and 0.395 vs. 0.385 on Gigaword, 0.371 and 0.367 vs. 0.368 on Wikipedia). nWED70, on the other hand, performs worse than the unmitigated embedding (0.384 vs. 0.385 on Gigaword, 0.367 vs. 0.368 on Wikipedia). CDA and CDS methods do not match the quality of the unmitigated space, but once again the difference is small. It should be noted that since SimLex-999 was pro-duced by human raters, it will reflect the humanbiases these methods were designed to remove, so worse performance might result from successful bias mitigation.",
        "sentences": [
            "Word similarity .",
            "Table 2 reports the SimLex-999 Spearman rank-order correlation coefficients rs (all are significant, p < 0.01).",
            "Surprisingly, the WED40 and 70 methods outperform the unmitigated embedding, although the difference in result is small (0.386 and 0.395 vs. 0.385 on Gigaword, 0.371 and 0.367 vs. 0.368 on Wikipedia).",
            "nWED70, on the other hand, performs worse than the unmitigated embedding (0.384 vs. 0.385 on Gigaword, 0.367 vs. 0.368 on Wikipedia).",
            "CDA and CDS methods do not match the quality of the unmitigated space, but once again the difference is small.",
            "It should be noted that since SimLex-999 was pro-duced by human raters, it will reflect the humanbiases these methods were designed to remove, so worse performance might result from successful bias mitigation."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "rs"
            ],
            [
                "WED40",
                "WED70",
                "none",
                "Gigaword",
                "Wikipedia"
            ],
            [
                "nWED70",
                "none",
                "Gigaword",
                "Wikipedia"
            ],
            [
                "CDA",
                "gCDA",
                "nCDA",
                "gCDS",
                "nCDS"
            ],
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_2",
        "paper_id": "D19-1530",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1533table_2",
        "description": "Table 2 shows our WSD results in F1 measure. It is shown in the table that with the nearest neighbor matching model, BERT outperforms context2vec and ELMo. This shows the effectiveness of BERT\u2019s pre-trained contextualized word representation. When we include surrounding sentences, one to the left and one to the right, we get improved F1 scores consistently. We also show that linear projection to the sense output vector further improves WSD performance, by which our best results are achieved. While BERT has been shown to outperform other pre-trained contextualized word representations through the nearest neighbor matching experiments, its potential can be maximized through linear projection to the sense output vector. It is worthwhile to note that our more advanced linear projection, by means of layer weighting (\u00a74.2.2 and gated linear unit (\u00a74.2.3) gives the best F1 scores on all test sets. All our BERT WSD systems outperform glossenhanced neural WSD, which has the best overall score among all prior systems.",
        "sentences": [
            "Table 2 shows our WSD results in F1 measure.",
            "It is shown in the table that with the nearest neighbor matching model, BERT outperforms context2vec and ELMo.",
            "This shows the effectiveness of BERT\u2019s pre-trained contextualized word representation.",
            "When we include surrounding sentences, one to the left and one to the right, we get improved F1 scores consistently.",
            "We also show that linear projection to the sense output vector further improves WSD performance, by which our best results are achieved.",
            "While BERT has been shown to outperform other pre-trained contextualized word representations through the nearest neighbor matching experiments, its potential can be maximized through linear projection to the sense output vector.",
            "It is worthwhile to note that our more advanced linear projection, by means of layer weighting (\u00a74.2.2 and gated linear unit (\u00a74.2.3) gives the best F1 scores on all test sets.",
            "All our BERT WSD systems outperform glossenhanced neural WSD, which has the best overall score among all prior systems."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "BERT nearest neighbor (ours)",
                "context2vec (Melamud et al., 2016)",
                "ELMo (Peters et al., 2018)"
            ],
            null,
            [
                "BERT nearest neighbor (ours)",
                "1nn (1sent+1sur)"
            ],
            [
                "BERT linear projection (ours)"
            ],
            null,
            [
                "BERT linear projection (ours)",
                "LW (1sent)",
                "LW (1sent+1sur)",
                "GLU+LW (1sent)",
                "GLU+LW (1sent+1sur)",
                "GLU (1sent)",
                "GLU (1sent+1sur)"
            ],
            [
                "BERT nearest neighbor (ours)",
                "BERT linear projection (ours)"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_2",
        "paper_id": "D19-1533",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1535table_1",
        "description": "Answer Level. Table 1 shows AnsAcc results of competitive baselines on the test set. Compared with them, STAR achieves the highest, 65.05%, which demonstrates its superiority. Meanwhile, it verifies the feasibility of follow-up query analysis in cooperating with context-independent semantic parsing. Compared with CONCAT, our approach boosts over 39.81% on COARSE2FINE for the capability of context-dependent semantic parsing. Query Level. Table 1 also shows SymAcc and BLEU of different methods on the dev and test sets. As observed, STAR significantly outperforms all baselines, demonstrating its effectiveness. For example, STAR achieves an absolute improvement of 8.03% BLEU over the state-ofthe-art baseline FANDA on testing. Moreover, the rewriting-based baselines, even the simplest CONCAT, perform better than the generation-based ones. It suggests that the idea of rewriting is more reasonable for the task, where precedent and follow-up queries are of full utilization.",
        "sentences": [
            "Answer Level.",
            "Table 1 shows AnsAcc results of competitive baselines on the test set.",
            "Compared with them, STAR achieves the highest, 65.05%, which demonstrates its superiority.",
            "Meanwhile, it verifies the feasibility of follow-up query analysis in cooperating with context-independent semantic parsing.",
            "Compared with CONCAT, our approach boosts over 39.81% on COARSE2FINE for the capability of context-dependent semantic parsing.",
            "Query Level.",
            "Table 1 also shows SymAcc and BLEU of different methods on the dev and test sets.",
            "As observed, STAR significantly outperforms all baselines, demonstrating its effectiveness.",
            "For example, STAR achieves an absolute improvement of 8.03% BLEU over the state-ofthe-art baseline FANDA on testing.",
            "Moreover, the rewriting-based baselines, even the simplest CONCAT, perform better than the generation-based ones.",
            "It suggests that the idea of rewriting is more reasonable for the task, where precedent and follow-up queries are of full utilization."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            1,
            2,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "AnsAcc (%)"
            ],
            [
                "STAR"
            ],
            null,
            [
                "CONCAT"
            ],
            null,
            [
                "SymAcc (%)",
                "BLEU (%)"
            ],
            [
                "STAR"
            ],
            [
                "STAR",
                "BLEU (%)",
                "Test"
            ],
            [
                "CONCAT"
            ],
            null
        ],
        "n_sentence": 11.0,
        "table_id": "table_1",
        "paper_id": "D19-1535",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1538table_3",
        "description": "Table 3 presents all test results on seven languages of CoNLL-2009 datasets. So far, the best previously reported results of Catalan, Japanese and Spanish are still from CoNLL-2009 shared task. Compared with previous methods, our baseline yields strong performance on all datasets except German. Especially for Catalan, Czech, Japanese and Spanish, our baseline performs better than existing methods with a large margin of 3.5% F1 on average. Nevertheless, applying our argument pruning to the strong syntax-agnostic baseline can still boost the model performance, which demonstrates the effectiveness of proposed method. On the other hand, it indicates that syntax is generally beneficial to multiple languages, and can enhance the multilingual SRL performance with effective syntactic integration.",
        "sentences": [
            "Table 3 presents all test results on seven languages of CoNLL-2009 datasets.",
            "So far, the best previously reported results of Catalan, Japanese and Spanish are still from CoNLL-2009 shared task.",
            "Compared with previous methods, our baseline yields strong performance on all datasets except German.",
            "Especially for Catalan, Czech, Japanese and Spanish, our baseline performs better than existing methods with a large margin of 3.5% F1 on average.",
            "Nevertheless, applying our argument pruning to the strong syntax-agnostic baseline can still boost the model performance, which demonstrates the effectiveness of proposed method.",
            "On the other hand, it indicates that syntax is generally beneficial to multiple languages, and can enhance the multilingual SRL performance with effective syntactic integration."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Catalan",
                "Japanese",
                "Spanish"
            ],
            [
                "Our baseline",
                "German"
            ],
            [
                "Catalan",
                "Czech",
                "Japanese",
                "Spanish",
                "Our baseline"
            ],
            null,
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_3",
        "paper_id": "D19-1538",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1539table_3",
        "description": "Table 3 shows the results, with comparison to previous published ELMoBASE results (Peters et al., 2018) and the BERT models. Both of our stacking methods outperform the previous state of the art, but fine tuning gives the biggest gain.",
        "sentences": [
            "Table 3 shows the results, with comparison to previous published ELMoBASE results (Peters et al., 2018) and the BERT models.",
            "Both of our stacking methods outperform the previous state of the art, but fine tuning gives the biggest gain."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "ELMoBASE",
                "BERTBASE",
                "BERTLARGE"
            ],
            [
                "CNN Large + ELMo",
                "CNN Large + fine-tune"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_3",
        "paper_id": "D19-1539",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1539table_4",
        "description": "6.2.2 Constituency Parsing. We also report parseval F1 for Penn Treebank constituency parsing. We adopted the current state-ofthe-art architecture (Kitaev and Klein, 2018). We again used grid search for learning rates and number of layers in parsing encoder, and used 8E-04 for language model finetuning, 8E-03 for the parsing model parameters, and two layers for encoder. Table 4 shows the results. Here, fine tuning is required to achieve gains over the previous state of the art, which used ELMo embeddings.",
        "sentences": [
            "6.2.2 Constituency Parsing.",
            "We also report parseval F1 for Penn Treebank constituency parsing.",
            "We adopted the current state-ofthe-art architecture (Kitaev and Klein, 2018).",
            "We again used grid search for learning rates and number of layers in parsing encoder, and used 8E-04 for language model finetuning, 8E-03 for the parsing model parameters, and two layers for encoder.",
            "Table 4 shows the results.",
            "Here, fine tuning is required to achieve gains over the previous state of the art, which used ELMo embeddings."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "dev F1",
                "test F1"
            ],
            null,
            null,
            null,
            [
                "CNN Large + fine-tune",
                "CNN Large + ELMo"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_4",
        "paper_id": "D19-1539",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1539table_5",
        "description": "Table 5 shows that the cloze loss performs significantly better than the bilm loss and that combining the two loss types does not improve over the cloze loss by itself. We conjecture that individual left and right context prediction tasks are too different from center word prediction and that their learning signals are not complementary enough.",
        "sentences": [
            "Table 5 shows that the cloze loss performs significantly better than the bilm loss and that combining the two loss types does not improve over the cloze loss by itself.",
            "We conjecture that individual left and right context prediction tasks are too different from center word prediction and that their learning signals are not complementary enough."
        ],
        "class_sentence": [
            1,
            2
        ],
        "header_mention": [
            [
                "cloze",
                "bilm",
                "cloze + bilm"
            ],
            null
        ],
        "n_sentence": 2.0,
        "table_id": "table_5",
        "paper_id": "D19-1539",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1540table_2",
        "description": "4 Results. Our main results on the TrecQA, TwitterURL, and Quora datasets are shown in Table 2 and results on TREC Microblog 2013\u00e2\u20ac\u201c2014 are shown in Table 3. The best numbers for each dataset (besides BERT) are bolded. We compare to three variants of our HCAN model: (1) only relevance matching signals (RM), (2) only semantic matching signals (SM), and (3) the complete model (HCAN). In these experiments, we use the deep encoder. From Table 2, we can see that on all three datasets, relevance matching (RM) achieves significantly higher effectiveness than semantic matching (SM). It beats other competitive baselines (InferSent, DecAtt and ESIM) by a large margin on the TrecQA dataset, and is still comparable to those baselines on TwitterURL and Quora. This finding suggests that soft term matching signals alone are fairly effective for many textual similarity modeling tasks. However, SM performs much worse on TrecQA and TwitterURL, while the gap between SM and RM is reduced on Quora. By combining SM and RM signals, we observe consistent effectiveness gains in HCAN across all three datasets, establishing new state-of-the-art (non-BERT) results on TrecQA.",
        "sentences": [
            "4 Results.",
            "Our main results on the TrecQA, TwitterURL, and Quora datasets are shown in Table 2 and results on TREC Microblog 2013\u00e2\u20ac\u201c2014 are shown in Table 3.",
            "The best numbers for each dataset (besides BERT) are bolded.",
            "We compare to three variants of our HCAN model: (1) only relevance matching signals (RM), (2) only semantic matching signals (SM), and (3) the complete model (HCAN).",
            "In these experiments, we use the deep encoder.",
            "From Table 2, we can see that on all three datasets, relevance matching (RM) achieves significantly higher effectiveness than semantic matching (SM).",
            "It beats other competitive baselines (InferSent, DecAtt and ESIM) by a large margin on the TrecQA dataset, and is still comparable to those baselines on TwitterURL and Quora.",
            "This finding suggests that soft term matching signals alone are fairly effective for many textual similarity modeling tasks.",
            "However, SM performs much worse on TrecQA and TwitterURL, while the gap between SM and RM is reduced on Quora.",
            "By combining SM and RM signals, we observe consistent effectiveness gains in HCAN across all three datasets, establishing new state-of-the-art (non-BERT) results on TrecQA."
        ],
        "class_sentence": [
            2,
            1,
            2,
            2,
            2,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null,
            [
                "TrecQA",
                "TwitterURL",
                "Quora",
                "RM",
                "SM"
            ],
            [
                "Our Approach",
                "InferSent",
                "DecAtt",
                "TwitterURL",
                "Quora"
            ],
            null,
            [
                "SM",
                "TrecQA",
                "TwitterURL",
                "RM",
                "Quora"
            ],
            [
                "SM",
                "RM",
                "HCAN",
                "TrecQA",
                "TwitterURL",
                "Quora"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_2",
        "paper_id": "D19-1540",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1541table_1",
        "description": "4.3 Main Results. Results of Syntax-aware Methods. Table 1 shows the results of these syntax-aware methods on CPB1.0 dataset. First, the first line shows the results of our baseline model, which only employs the word embeddings and char representations as the inputs of the basic SRL model. Second, the Tree-GRU method only achieves 80.06 F1 score on the test data, which even didn\u00e2\u20ac\u2122t catch up with the baseline model. We think this is caused by the relatively low accuracy in Chinese dependency parsing. Third, the FIR approach outperforms the baseline by 2.17 F1 score on the test data, demonstrating the effectiveness of introducing fixed implicit syntactic representations. Forth, the HPS strategy achieves more significant performance by 83.51 F1 score. Finally, our proposed framework achieves the best performance of 83.91 F1 score among these methods, outperforming the baseline by 3.43 F1 score. All the improvements are statistically significant (p < 0.0001). From these experimental results, we can conclude that: 1) the quality of syntax has a crucial impact on the methods which depend on the systematic dependency trees, like Tree-GRU, 2) the implicit syntactic features have the potential to improve the down-stream NLP tasks, and 3) learning the syntactic features with the main task performs better than extract them from a fixed dependency parser.",
        "sentences": [
            "4.3 Main Results.",
            "Results of Syntax-aware Methods.",
            "Table 1 shows the results of these syntax-aware methods on CPB1.0 dataset.",
            "First, the first line shows the results of our baseline model, which only employs the word embeddings and char representations as the inputs of the basic SRL model.",
            "Second, the Tree-GRU method only achieves 80.06 F1 score on the test data, which even didn\u00e2\u20ac\u2122t catch up with the baseline model.",
            "We think this is caused by the relatively low accuracy in Chinese dependency parsing.",
            "Third, the FIR approach outperforms the baseline by 2.17 F1 score on the test data, demonstrating the effectiveness of introducing fixed implicit syntactic representations.",
            "Forth, the HPS strategy achieves more significant performance by 83.51 F1 score.",
            "Finally, our proposed framework achieves the best performance of 83.91 F1 score among these methods, outperforming the baseline by 3.43 F1 score.",
            "All the improvements are statistically significant (p < 0.0001).",
            "From these experimental results, we can conclude that: 1) the quality of syntax has a crucial impact on the methods which depend on the systematic dependency trees, like Tree-GRU, 2) the implicit syntactic features have the potential to improve the down-stream NLP tasks, and 3) learning the syntactic features with the main task performs better than extract them from a fixed dependency parser."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1,
            2,
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "Baseline"
            ],
            [
                "Baseline + Dep (Tree-GRU)",
                "Baseline",
                "F1",
                "Test"
            ],
            null,
            [
                "Baseline + Dep (FIR)",
                "Baseline",
                "F1",
                "Test"
            ],
            [
                "Baseline + Dep (HPS)",
                "F1",
                "Test"
            ],
            [
                "Baseline + Dep (IIR)",
                "F1",
                "Test",
                "Baseline"
            ],
            null,
            null
        ],
        "n_sentence": 11.0,
        "table_id": "table_1",
        "paper_id": "D19-1541",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1541table_2",
        "description": "Results on CPB1.0. Table 2 shows the results of our baseline model and proposed framework using external dependency trees on CPB1.0, as well as the corresponding results when adding BERT representations. It is clear that adding dependency trees into the baseline SRL model can effectively improve the performance (p < 0.0001), no matter whether employ the BERT representations or not. Especially, our proposed framework (IIR) consistently outperforms the hard parameter sharing strategy. So we only report the results of our proposed framework in later experiments. Our final results outperforms the best previous model (Xia et al., 2017) by 7.87 and 4.24 F1 scores with BERT representations or not, respectively.",
        "sentences": [
            "Results on CPB1.0.",
            "Table 2 shows the results of our baseline model and proposed framework using external dependency trees on CPB1.0, as well as the corresponding results when adding BERT representations.",
            "It is clear that adding dependency trees into the baseline SRL model can effectively improve the performance (p < 0.0001), no matter whether employ the BERT representations or not.",
            "Especially, our proposed framework (IIR) consistently outperforms the hard parameter sharing strategy.",
            "So we only report the results of our proposed framework in later experiments.",
            "Our final results outperforms the best previous model (Xia et al., 2017) by 7.87 and 4.24 F1 scores with BERT representations or not, respectively."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Baseline + Dep (HPS)"
            ],
            [
                "Baseline + Dep (IIR)",
                "Baseline + BERT + Dep (IIR)",
                "Baseline + Dep (HPS)",
                "Baseline + BERT + Dep (HPS)"
            ],
            null,
            [
                "Ours",
                "Xia et al. (2017)"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_2",
        "paper_id": "D19-1541",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1541table_4",
        "description": "Results on CoNLL-2009. Table 4 shows the results of our framework and comparison with previous works on the CoNLL-2009 Chinese test data. Our baseline achieves nearly the same performance with Cai et al. (2018), which is an endto-end neural model that consists of BiLSTM encoder and biaffine scorer. Our proposed framework outperforms the best reported result (Cai et al., 2018) by 0.8 F1 score and brings a significant improvement (p < 0.0001) of 0.9 F1 score over our baseline model. Our experimental result boosts to 88.5 F1 score when the framework is enhanced with BERT representations. However, compared with the results in the settings without BERT, the improvement is fairly small (88.53 - 88.47 = 0.06 F1 score, p > 0.1) 8 of the proposed framework, which we will discuss in Section 5.3.",
        "sentences": [
            "Results on CoNLL-2009.",
            "Table 4 shows the results of our framework and comparison with previous works on the CoNLL-2009 Chinese test data.",
            "Our baseline achieves nearly the same performance with Cai et al. (2018), which is an endto-end neural model that consists of BiLSTM encoder and biaffine scorer.",
            "Our proposed framework outperforms the best reported result (Cai et al., 2018) by 0.8 F1 score and brings a significant improvement (p < 0.0001) of 0.9 F1 score over our baseline model.",
            "Our experimental result boosts to 88.5 F1 score when the framework is enhanced with BERT representations.",
            "However, compared with the results in the settings without BERT, the improvement is fairly small (88.53 - 88.47 = 0.06 F1 score, p > 0.1) 8 of the proposed framework, which we will discuss in Section 5.3."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Ours",
                "Previous Works"
            ],
            [
                "Baseline",
                "Cai et al. (2018)"
            ],
            [
                "F1",
                "Cai et al. (2018)",
                "Baseline + Dep (IIR)"
            ],
            [
                "F1",
                "Baseline + BERT + Dep (IIR)"
            ],
            [
                "F1",
                "P",
                "Baseline + BERT",
                "Baseline + BERT + Dep (IIR)"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_4",
        "paper_id": "D19-1541",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1542table_3",
        "description": "6.1 Effect on Semantic Equivalence Assessment Tasks. Table 3 shows fine-tuning results on GLUE; our model, denoted as Transfer Fine-Tuning, is compared against BERT-base and BERT-large. The first set of columns shows the results of semantic equivalence assessment tasks. Our model outperformed BERT-base on MRPC (+0.9 points) and STS-B (+2.7 points). Furthermore, it outperformed even BERT-large by 0.6 points on MRPC and by 1.4 points on STS-B, despite BERT-large having 3.1 times more parameters than our model. Devlin et al. (2019) described that the nextsentence prediction task in BERT\u00e2\u20ac\u2122s pre-training aims to train a model that understands sentence relations. Herein, we argue that such relations are effective at generating representations broadly transferable to various NLP tasks, but are too generic to generate representations for semantic equivalence assessment tasks. Our method allows semantic relations between sentences and phrases that are directly useful for this class of tasks to be learned. These results support hypothesis H1, indicating that our approach is more effective than blindly enlarging the model size. A smaller model size is desirable for practical applications. We have also applied our method on the BERT-large model, but its performance was not much improved to warrant the larger model size. Further investigation regarding pre-trained model sizes is our future work.",
        "sentences": [
            "6.1 Effect on Semantic Equivalence Assessment Tasks.",
            "Table 3 shows fine-tuning results on GLUE; our model, denoted as Transfer Fine-Tuning, is compared against BERT-base and BERT-large.",
            "The first set of columns shows the results of semantic equivalence assessment tasks.",
            "Our model outperformed BERT-base on MRPC (+0.9 points) and STS-B (+2.7 points).",
            "Furthermore, it outperformed even BERT-large by 0.6 points on MRPC and by 1.4 points on STS-B, despite BERT-large having 3.1 times more parameters than our model.",
            "Devlin et al. (2019) described that the nextsentence prediction task in BERT\u00e2\u20ac\u2122s pre-training aims to train a model that understands sentence relations.",
            "Herein, we argue that such relations are effective at generating representations broadly transferable to various NLP tasks, but are too generic to generate representations for semantic equivalence assessment tasks.",
            "Our method allows semantic relations between sentences and phrases that are directly useful for this class of tasks to be learned.",
            "These results support hypothesis H1, indicating that our approach is more effective than blindly enlarging the model size.",
            "A smaller model size is desirable for practical applications.",
            "We have also applied our method on the BERT-large model, but its performance was not much improved to warrant the larger model size.",
            "Further investigation regarding pre-trained model sizes is our future work."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            2,
            2,
            2,
            2,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "Transfer Fine-Tuning",
                "BERT-base",
                "BERT-large"
            ],
            null,
            [
                "Transfer Fine-Tuning",
                "BERT-base",
                "MRPC",
                "STS-B"
            ],
            [
                "Transfer Fine-Tuning",
                "BERT-large",
                "MRPC",
                "STS-B"
            ],
            null,
            null,
            [
                "Transfer Fine-Tuning"
            ],
            null,
            null,
            [
                "Transfer Fine-Tuning",
                "BERT-large"
            ],
            null
        ],
        "n_sentence": 12.0,
        "table_id": "table_3",
        "paper_id": "D19-1542",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1543table_2",
        "description": "On Spider, the performance is evaluated by exact-match accuracy on different difficulty levels of SQL queries, i.e., easy, medium, hard and extra hard. (Yu et al., 2018c). Table 2 shows the results. First, the overall accuracy can be improved by 3.2% and 2.5% respectively. Furthermore, performances on medium, hard and extra hard SQL queries achieve more improvement than that on easy SQL queries, indicating that our approach is more helpful for solving complicated cases.",
        "sentences": [
            "On Spider, the performance is evaluated by exact-match accuracy on different difficulty levels of SQL queries, i.e., easy, medium, hard and extra hard. (Yu et al., 2018c).",
            "Table 2 shows the results.",
            "First, the overall accuracy can be improved by 3.2% and 2.5% respectively.",
            "Furthermore, performances on medium, hard and extra hard SQL queries achieve more improvement than that on easy SQL queries, indicating that our approach is more helpful for solving complicated cases."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "SyntaxSQLNet + DAE",
                "SyntaxSQLNetAug + DAE"
            ],
            [
                "Medium (%)",
                "Hard (%)",
                "Extra Hard (%)",
                "Easy (%)"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "D19-1543",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1543table_5",
        "description": "Table 5 shows that DAE significantly outperforms TypeSQL and AnnotatedSeq2Seq on all the evaluation metrics. First, for ACCSC, DAE outperforms TypeSQL and AnotatedSeq2Seq by 16% and 3.5% on test data; for ACCOC, DAE outperforms TypeSQL and AnnotatedSeq2Seq by 0.8% and 28% on test data. Moreover, DAE can achieve around 86% for ACCCE, while other methods fail to recognize cells when the table content is not available due to the privacy problem.",
        "sentences": [
            "Table 5 shows that DAE significantly outperforms TypeSQL and AnnotatedSeq2Seq on all the evaluation metrics.",
            "First, for ACCSC, DAE outperforms TypeSQL and AnotatedSeq2Seq by 16% and 3.5% on test data; for ACCOC, DAE outperforms TypeSQL and AnnotatedSeq2Seq by 0.8% and 28% on test data.",
            "Moreover, DAE can achieve around 86% for ACCCE, while other methods fail to recognize cells when the table content is not available due to the privacy problem."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "DAE",
                "TypeSQL (Yu et al., 2018a)",
                "AnnotatedSeq2Seq (Wang et al., 2018b)"
            ],
            [
                "DAE",
                "TypeSQL (Yu et al., 2018a)",
                "ACCOC",
                "AnnotatedSeq2Seq (Wang et al., 2018b)"
            ],
            [
                "ACCCE",
                "DAE"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_5",
        "paper_id": "D19-1543",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1544table_5",
        "description": "Table 5 gives the results of the proposed CapsuleNet SRL (with global node) on the in-domain test sets of all languages from CoNLL-2009. As shown in Table 5, the proposed model consistently outperforms the non-refinement baseline model and achieves state-of-the-art performance on Catalan (Ca), Czech (Cz), English (En), Japanese (Jp) and Spanish (Es). Interestingly, the effectiveness of the refinement method does not seem to be dependent on the dataset size: the improvements on the smallest (Japanese) and the largest datasets (English) are among the largest.",
        "sentences": [
            "Table 5 gives the results of the proposed CapsuleNet SRL (with global node) on the in-domain test sets of all languages from CoNLL-2009.",
            "As shown in Table 5, the proposed model consistently outperforms the non-refinement baseline model and achieves state-of-the-art performance on Catalan (Ca), Czech (Cz), English (En), Japanese (Jp) and Spanish (Es).",
            "Interestingly, the effectiveness of the refinement method does not seem to be dependent on the dataset size: the improvements on the smallest (Japanese) and the largest datasets (English) are among the largest."
        ],
        "class_sentence": [
            1,
            2,
            1
        ],
        "header_mention": [
            [
                "CapsuleNet SRL (This Work)"
            ],
            [
                "CapsuleNet SRL (This Work)",
                "Baseline Model",
                "Ca",
                "Cz",
                "En",
                "Es"
            ],
            null
        ],
        "n_sentence": 3.0,
        "table_id": "table_5",
        "paper_id": "D19-1544",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1545table_1",
        "description": "7 Results and Discussion. Table 1 presents exact match and BLEU scores on the original CONCODE train/validation/test split. Iyer-Simp yields a large improvement of 3.9 EM and 2.2 BLEU over the best model of Iyer et al. (2018), while also being significantly faster (27 hours for 30 training epochs as compared to 40 hours). Using a reduced BPE vocabulary makes the model memory efficient, which allows us to use a larger batch size that in turn speeds up training. Furthermore, using 200 code idioms further improves BLEU by 2.2% while maintaining comparable EM accuracy. Using the top-200 idioms results in a target AST compression of more than 50%, which results in fewer decoder RNN steps being performed. This reduces training time further by more than 50%, from 27 hours to 13 hours.",
        "sentences": [
            "7 Results and Discussion.",
            "Table 1 presents exact match and BLEU scores on the original CONCODE train/validation/test split.",
            "Iyer-Simp yields a large improvement of 3.9 EM and 2.2 BLEU over the best model of Iyer et al. (2018), while also being significantly faster (27 hours for 30 training epochs as compared to 40 hours).",
            "Using a reduced BPE vocabulary makes the model memory efficient, which allows us to use a larger batch size that in turn speeds up training.",
            "Furthermore, using 200 code idioms further improves BLEU by 2.2% while maintaining comparable EM accuracy.",
            "Using the top-200 idioms results in a target AST compression of more than 50%, which results in fewer decoder RNN steps being performed.",
            "This reduces training time further by more than 50%, from 27 hours to 13 hours."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "Exact",
                "BLEU"
            ],
            [
                "Iyer-Simp",
                "Exact",
                "BLEU",
                "Iyer et al. (2018)\u00e2\u20ac\u00a0"
            ],
            null,
            [
                "Iyer-Simp + 200 idioms",
                "BLEU",
                "Exact"
            ],
            [
                "Iyer-Simp + 200 idioms"
            ],
            null
        ],
        "n_sentence": 7.0,
        "table_id": "table_1",
        "paper_id": "D19-1545",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1547table_5",
        "description": "Table 5 shows the results. In all settings, MISP-SQL improves the base parser\u00e2\u20ac\u2122s performance, demonstrating the benefit of involving human interaction. However, we also notice that the gain is not as large as in simulation, especially on SQLova. Through interviews with the human evaluators, we found that the major reason is that they sometimes had difficulties understanding the true intent of some test questions that are ambiguous, vague, or contain entities they are not familiar with. We believe this reflects a general challenge of setting up human evaluation for semantic parsing that is close to the real application setting, and thus set forth the following discussion.",
        "sentences": [
            "Table 5 shows the results.",
            "In all settings, MISP-SQL improves the base parser\u00e2\u20ac\u2122s performance, demonstrating the benefit of involving human interaction.",
            "However, we also notice that the gain is not as large as in simulation, especially on SQLova.",
            "Through interviews with the human evaluators, we found that the major reason is that they sometimes had difficulties understanding the true intent of some test questions that are ambiguous, vague, or contain entities they are not familiar with.",
            "We believe this reflects a general challenge of setting up human evaluation for semantic parsing that is close to the real application setting, and thus set forth the following discussion."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "MISP-SQL (real user)"
            ],
            [
                "MISP-SQL (real user)",
                "SQLova"
            ],
            null,
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_5",
        "paper_id": "D19-1547",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1548table_2",
        "description": "3.2 Experimental Results. We first show the performance of our baseline system. As mentioned before, BPE and sharing vocabulary are two techniques we applied to relieving data sparsity. Table 2 presents the results of the ablation test on the development set of LDC2015E86 by either removing BPE, or vocabulary sharing, or both of them from the baseline system. From the results we can see that BPE and vocabulary sharing are critical to building our baseline system (an improvement from 18.77 to 24.93 in BLEU), revealing the fact that they are two effective ways to address the issue of data sparseness for AMR-to-text generation.",
        "sentences": [
            "3.2 Experimental Results.",
            "We first show the performance of our baseline system.",
            "As mentioned before, BPE and sharing vocabulary are two techniques we applied to relieving data sparsity.",
            "Table 2 presents the results of the ablation test on the development set of LDC2015E86 by either removing BPE, or vocabulary sharing, or both of them from the baseline system.",
            "From the results we can see that BPE and vocabulary sharing are critical to building our baseline system (an improvement from 18.77 to 24.93 in BLEU), revealing the fact that they are two effective ways to address the issue of data sparseness for AMR-to-text generation."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "-BPE",
                "-Share Vocab.",
                "-Both"
            ],
            [
                "-Both",
                "Baseline"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_2",
        "paper_id": "D19-1548",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1548table_3",
        "description": "Table 3 presents the comparison of our approach and related works on the test sets of LDC2015E86 and LDC2017T10. From the results we can see that the Transformer-based baseline outperforms most of graph-to-sequence models and is comparable with the latest work by Guo et al. (2019). The strong performance of the baseline is attributed to the capability of the Transformer to encode global and implicit structural information in AMR graphs. By comparing the five methods of learning graph structure representations, we have the following observations. All of them achieve significant improvements over the baseline: the biggest improvements are 4.16 and 4.39 BLEU scores on LDC2015E86 and LDC2017T10, respectively. Methods using continuous representations (such as SA-based and CNN-based) outperform the methods using discrete representations (such as feature-based). Compared to the baseline, the methods have very limited affect on the sizes of model parameters (see the column of #P (M) in Table 3). Finally, our best-performing models are the best among all the single and supervised models.",
        "sentences": [
            "Table 3 presents the comparison of our approach and related works on the test sets of LDC2015E86 and LDC2017T10.",
            "From the results we can see that the Transformer-based baseline outperforms most of graph-to-sequence models and is comparable with the latest work by Guo et al. (2019).",
            "The strong performance of the baseline is attributed to the capability of the Transformer to encode global and implicit structural information in AMR graphs.",
            "By comparing the five methods of learning graph structure representations, we have the following observations.",
            "All of them achieve significant improvements over the baseline: the biggest improvements are 4.16 and 4.39 BLEU scores on LDC2015E86 and LDC2017T10, respectively.",
            "Methods using continuous representations (such as SA-based and CNN-based) outperform the methods using discrete representations (such as feature-based).",
            "Compared to the baseline, the methods have very limited affect on the sizes of model parameters (see the column of #P (M) in Table 3).",
            "Finally, our best-performing models are the best among all the single and supervised models."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "Our Approach",
                "LDC2015E86",
                "LDC2017T10"
            ],
            [
                "Our Approach",
                "Previous works with single models",
                "Previous works with either ensemble models or unlabelled data or both",
                "Guo et al. (2019)"
            ],
            [
                "Baseline"
            ],
            null,
            [
                "Our Approach",
                "Baseline",
                "BLEU",
                "LDC2015E86",
                "LDC2017T10"
            ],
            [
                "SA-based",
                "CNN-based",
                "feature-based"
            ],
            [
                "Baseline",
                "Our Approach",
                "#P (M)"
            ],
            [
                "SA-based"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_3",
        "paper_id": "D19-1548",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1548table_4",
        "description": "Table 4 compares the performance of our approach with or without modeling structural information of indirectly connected concept pairs. It shows that by modeling structural information of indirectly connected concept pairs, our approach improves the performance on the test set from 29.92 to 31.82 in BLEU scores. It also shows that even without modeling structural information of indirectly connected concept pairs, our approach achieves better performance than the baseline.",
        "sentences": [
            "Table 4 compares the performance of our approach with or without modeling structural information of indirectly connected concept pairs.",
            "It shows that by modeling structural information of indirectly connected concept pairs, our approach improves the performance on the test set from 29.92 to 31.82 in BLEU scores.",
            "It also shows that even without modeling structural information of indirectly connected concept pairs, our approach achieves better performance than the baseline."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "Our approach",
                "No indirectly connected concept pairs"
            ],
            [
                "No indirectly connected concept pairs",
                "Our approach"
            ],
            [
                "Our approach",
                "Baseline"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_4",
        "paper_id": "D19-1548",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1554table_7",
        "description": "4.5 Analysis. What is the effect of each action?. To show the effect of different actions, we take RNN and CNN on SST-2, SST-5, and RT as examples and conduct experiments by dropping one action at a time. As Table 7 shows, only some actions are useful for the robustness improvement. The average performance becomes better after dropping subordinate words under two different settings. Surprisingly, the neighbor words largely contribute to the performance. Without neighbor words, the average accuracies are dropped from 66.17 to 65.38 for RNN, and from 65.63 to 65.22 for CNN. Neighbor words share the same super word, like \u201cfox\u201d and \u201cwolf\u201d. In WordNet, neighbor words usually have similar semantic meanings. By replacing a word with its similar word, the semantic diversity can be largely enhanced. Furthermore, the semantic similarity can reduce the risk of changing the label when modifying the input text. Therefore, it is a good choice to include this relation in models unless the replacement exactly impacts the original label in specific tasks.",
        "sentences": [
            "4.5 Analysis.",
            "What is the effect of each action?.",
            "To show the effect of different actions, we take RNN and CNN on SST-2, SST-5, and RT as examples and conduct experiments by dropping one action at a time.",
            "As Table 7 shows, only some actions are useful for the robustness improvement.",
            "The average performance becomes better after dropping subordinate words under two different settings.",
            "Surprisingly, the neighbor words largely contribute to the performance.",
            "Without neighbor words, the average accuracies are dropped from 66.17 to 65.38 for RNN, and from 65.63 to 65.22 for CNN.",
            "Neighbor words share the same super word, like \u201cfox\u201d and \u201cwolf\u201d.",
            "In WordNet, neighbor words usually have similar semantic meanings.",
            "By replacing a word with its similar word, the semantic diversity can be largely enhanced.",
            "Furthermore, the semantic similarity can reduce the risk of changing the label when modifying the input text.",
            "Therefore, it is a good choice to include this relation in models unless the replacement exactly impacts the original label in specific tasks."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1,
            1,
            1,
            2,
            2,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "all words (RNN)",
                "all words (CNN)",
                "SST-2",
                "SST-5",
                "RT"
            ],
            null,
            [
                "Average"
            ],
            [
                "-neighbor words"
            ],
            [
                "-neighbor words",
                "all words (RNN)",
                "all words (CNN)"
            ],
            [
                "-neighbor words"
            ],
            [
                "-neighbor words"
            ],
            null,
            null,
            null
        ],
        "n_sentence": 12.0,
        "table_id": "table_7",
        "paper_id": "D19-1554",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1555table_1",
        "description": "To assess Trait\u2030\u00db\u00aas effectiveness, we select the hotel and restaurant domains and prepare four review datasets associated with three attributes: author, trip type, and location. HotelUser, HotelType, and HotelLoc are sets of hotel reviews collected from TripAdvisor. HotelUser contains 28165 reviews posted by 202 randomly selected reviewers, each of whom contributes at least 100 hotel reviews. HotelType contains reviews associated with five trip types including business, couple, family, friend, and solo. HotelLoc contains a total of 136446 reviews about seven US cities, split approximately equally. ResUser is a set of restaurant reviews from Yelp Dataset Challenge (2019). It contains 23874 restaurant reviews posted by 144 users, each of whom contributes at least 100 reviews. Table 1 summarizes our datasets. Datasets and source code are available for research purposes (Trait, 2019).",
        "sentences": [
            "To assess Trait\u2030\u00db\u00aas effectiveness, we select the hotel and restaurant domains and prepare four review datasets associated with three attributes: author, trip type, and location.",
            "HotelUser, HotelType, and HotelLoc are sets of hotel reviews collected from TripAdvisor.",
            "HotelUser contains 28165 reviews posted by 202 randomly selected reviewers, each of whom contributes at least 100 hotel reviews.",
            "HotelType contains reviews associated with five trip types including business, couple, family, friend, and solo.",
            "HotelLoc contains a total of 136446 reviews about seven US cities, split approximately equally.",
            "ResUser is a set of restaurant reviews from Yelp Dataset Challenge (2019).",
            "It contains 23874 restaurant reviews posted by 144 users, each of whom contributes at least 100 reviews.",
            "Table 1 summarizes our datasets.",
            "Datasets and source code are available for research purposes (Trait, 2019)."
        ],
        "class_sentence": [
            2,
            2,
            1,
            2,
            1,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "HotelUser",
                "HotelType",
                "HotelLoc"
            ],
            [
                "HotelUser",
                "# of reviews"
            ],
            [
                "HotelType"
            ],
            [
                "HotelLoc",
                "# of reviews"
            ],
            [
                "ResUser"
            ],
            [
                "ResUser",
                "# of reviews"
            ],
            null,
            null
        ],
        "n_sentence": 9.0,
        "table_id": "table_1",
        "paper_id": "D19-1555",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1557table_3",
        "description": "4.4 Results. Table 2 presents results on the LibCon and the balanced (B) and imbalanced (I) Beauty, Book and Music data sets and Table 3 presents results on the SST and MR data sets. The performance metric reported in both tables is accuracy with with additional micro f-scores reported in Table 2. From Table 2, it is observed that on the LibCon data set, where we have a considerable difference in language use between the two groups of users, the adapted BiLSTM and adapted CNN perform much better than the vanilla baselines. Furthermore, our proposed adaptation layer improves the performance of the Vanilla BiLSTM to surpass the performance of fine-tuned BERT.",
        "sentences": [
            "4.4 Results.",
            "Table 2 presents results on the LibCon and the balanced (B) and imbalanced (I) Beauty, Book and Music data sets and Table 3 presents results on the SST and MR data sets.",
            "The performance metric reported in both tables is accuracy with with additional micro f-scores reported in Table 2.",
            "From Table 2, it is observed that on the LibCon data set, where we have a considerable difference in language use between the two groups of users, the adapted BiLSTM and adapted CNN perform much better than the vanilla baselines.",
            "Furthermore, our proposed adaptation layer improves the performance of the Vanilla BiLSTM to surpass the performance of fine-tuned BERT."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "MR",
                "SST"
            ],
            null,
            [
                "Adapted BiLSTM",
                "Adapted CNN",
                "Vanilla CNN",
                "Vanilla BiLSTM"
            ],
            [
                "Adapted BiLSTM",
                "BERT"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_3",
        "paper_id": "D19-1557",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1563table_2",
        "description": "5.1 Main Results . The experimental results on both datasets are shown in Table 2 and Table 3, respectively. RB yields high precision but with low recall. CB has an opposite scenario from RB. A possible reason is that these linguistic-based methods depend on some cue words to identify the emotion cause, different rules or common sense may contain different cue words.",
        "sentences": [
            "5.1 Main Results .",
            "The experimental results on both datasets are shown in Table 2 and Table 3, respectively.",
            "RB yields high precision but with low recall.",
            "CB has an opposite scenario from RB.",
            "A possible reason is that these linguistic-based methods depend on some cue words to identify the emotion cause, different rules or common sense may contain different cue words."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "RB*",
                "P",
                "R"
            ],
            [
                "CB*",
                "RB*"
            ],
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_2",
        "paper_id": "D19-1563",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1565table_6",
        "description": "Table 6 shows that joint learning (BERT-Joint) hurts the performance compared to single-task BERT. However, using additional information from the sentence-level for the token-level classification (BERT-Granularity) yields small improvements. The multi-granularity models outperform all baselines thanks to their higher precision. This shows the effect of the model excluding sentences that it determined to be non-propagandistic from being considered for token-level classification.",
        "sentences": [
            "Table 6 shows that joint learning (BERT-Joint) hurts the performance compared to single-task BERT.",
            "However, using additional information from the sentence-level for the token-level classification (BERT-Granularity) yields small improvements.",
            "The multi-granularity models outperform all baselines thanks to their higher precision.",
            "This shows the effect of the model excluding sentences that it determined to be non-propagandistic from being considered for token-level classification."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "Joint",
                "BERT"
            ],
            [
                "Granu",
                "BERT"
            ],
            [
                "Multi-Granularity",
                "P"
            ],
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_6",
        "paper_id": "D19-1565",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1569table_2",
        "description": "From Table 2 and Figure 4, it is clear that GCN complements the BiLSTM to improve model performance. This means that the BiLSTM can identify opinion words within the context with respect to a specific aspect. However, in some complicated contexts, it might perform poorly. But the GCN can build upon BiLSTM to attend to the correct opinion words by leveraging the dependencies among words.",
        "sentences": [
            "From Table 2 and Figure 4, it is clear that GCN complements the BiLSTM to improve model performance.",
            "This means that the BiLSTM can identify opinion words within the context with respect to a specific aspect.",
            "However, in some complicated contexts, it might perform poorly.",
            "But the GCN can build upon BiLSTM to attend to the correct opinion words by leveraging the dependencies among words."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "ASP-GCN"
            ],
            [
                "ASP-GCN"
            ],
            [
                "ASP-GCN"
            ],
            [
                "ASP-GCN"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "D19-1569",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1570table_1",
        "description": "CTC Table 1 shows the main BLEU results of different methods on the test set. However, we cannot identify the best DA method because their rankings across the four translation tasks vary a bit. To measure the degree of consistency, we use a correlation measure called Kendall's coefficient of concordance (Kendall and Smith, 1939; Mazurek, 2011) to evaluate the correlation of the rankings produced on the four translation tasks (appx.C). The value shows strong consistency (correlation) of different rankings when it is close to 1. We call the correlation value Cross-Task Consistency measure or CTC. The CTC for the BLEU measure is 0.62, which is of weak consistency. This phenomenon might be a result of the intrinsic nature of using a single specific test as a substitute of the whole data population for evaluation. In the next section, we introduce two measures that are more consistent (with close-to-1 CTC value). They in some extent reflect the model generalization and are easy-to-compute as well.",
        "sentences": [
            "CTC Table 1 shows the main BLEU results of different methods on the test set.",
            "However, we cannot identify the best DA method because their rankings across the four translation tasks vary a bit.",
            "To measure the degree of consistency, we use a correlation measure called Kendall's coefficient of concordance (Kendall and Smith, 1939; Mazurek, 2011) to evaluate the correlation of the rankings produced on the four translation tasks (appx.C).",
            "The value shows strong consistency (correlation) of different rankings when it is close to 1.",
            "We call the correlation value Cross-Task Consistency measure or CTC.",
            "The CTC for the BLEU measure is 0.62, which is of weak consistency.",
            "This phenomenon might be a result of the intrinsic nature of using a single specific test as a substitute of the whole data population for evaluation.",
            "In the next section, we introduce two measures that are more consistent (with close-to-1 CTC value).",
            "They in some extent reflect the model generalization and are easy-to-compute as well."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            2,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            null
        ],
        "n_sentence": 9.0,
        "table_id": "table_1",
        "paper_id": "D19-1570",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1571table_1",
        "description": "Next, we evaluate online decoding with a noisy channel setup compared to just a direct model (DIR) as well as an ensemble of two direct models (DIR ENS). Table 1 shows that adding a language model to DIR (DIR+LM) gives a good improvement (Gulcehre et al., 2015) over a single direct model but ensembling two direct models is slightly more effective (DIR ENS). The noisy channel approach (CH+DIR+LM) improves by 1.9 BLEU over DIR on news2017 and by 0.9 BLEU over the ensemble. Without per word scores, accuracy drops because the direct model and the channel model are not balanced and their weight shifts throughout decoding. Our simple approach outperforms strong online ensembles which illustrates the advantage over incremental architectures (Yu et al., 2017) that do not match vanilla seq2seq models by themselves.",
        "sentences": [
            "Next, we evaluate online decoding with a noisy channel setup compared to just a direct model (DIR) as well as an ensemble of two direct models (DIR ENS).",
            "Table 1 shows that adding a language model to DIR (DIR+LM) gives a good improvement (Gulcehre et al., 2015) over a single direct model but ensembling two direct models is slightly more effective (DIR ENS).",
            "The noisy channel approach (CH+DIR+LM) improves by 1.9 BLEU over DIR on news2017 and by 0.9 BLEU over the ensemble.",
            "Without per word scores, accuracy drops because the direct model and the channel model are not balanced and their weight shifts throughout decoding.",
            "Our simple approach outperforms strong online ensembles which illustrates the advantage over incremental architectures (Yu et al., 2017) that do not match vanilla seq2seq models by themselves."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "DIR",
                "DIR ENS"
            ],
            [
                "DIR+LM",
                "DIR ENS"
            ],
            [
                "CH+DIR+LM"
            ],
            [
                " - per word scores",
                "DIR",
                "CH+DIR+LM"
            ],
            [
                "DIR ENS",
                "CH+DIR+LM"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_1",
        "paper_id": "D19-1571",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1576table_3",
        "description": "To measure the statistical significance of the advantage of our method, we performed the nonparametric Friedman's test to support/reject the claim (null hypothesis): there is no difference between the G+I model and the NDMV model in a multilingual setting. Based on the above sample data, the P-value 7.8911 x 10-4 would result in rejection of the claim at the 0.05 significance level, thus showing the significance in our performance gain. In Table 3 we compare our method with recent state-of-the-art approaches on the UD Treebank dataset: Convex-MST (Grave and Elhadad, 2015), LC-DMV (Noji et al., 2016) and D-J (Jiang et al., 2017). For the three approaches we use the results reported by Jiang et al. (2017). Our G+I model performs better than Convex-MST and LC-DMV on average, even though additional priors and delicate biases are integrated into the two methods (e.g, the universal linguistic prior for ConvexMST and the limited center-embedding for LCDMV). Our method also slightly outperforms D-J on average, even though D-J combines ConvexMST and LC-DMV and therefore utilizes even more linguistic prior knowledge.",
        "sentences": [
            "To measure the statistical significance of the advantage of our method, we performed the nonparametric Friedman's test to support/reject the claim (null hypothesis): there is no difference between the G+I model and the NDMV model in a multilingual setting.",
            "Based on the above sample data, the P-value 7.8911 x 10-4 would result in rejection of the claim at the 0.05 significance level, thus showing the significance in our performance gain.",
            "In Table 3 we compare our method with recent state-of-the-art approaches on the UD Treebank dataset: Convex-MST (Grave and Elhadad, 2015), LC-DMV (Noji et al., 2016) and D-J (Jiang et al., 2017).",
            "For the three approaches we use the results reported by Jiang et al. (2017).",
            "Our G+I model performs better than Convex-MST and LC-DMV on average, even though additional priors and delicate biases are integrated into the two methods (e.g, the universal linguistic prior for ConvexMST and the limited center-embedding for LCDMV).",
            "Our method also slightly outperforms D-J on average, even though D-J combines ConvexMST and LC-DMV and therefore utilizes even more linguistic prior knowledge."
        ],
        "class_sentence": [
            0,
            0,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "LC-DMV",
                "D-J"
            ],
            [
                "LC-DMV",
                "D-J"
            ],
            [
                "G+I",
                "LC-DMV"
            ],
            [
                "G+I",
                "D-J"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_3",
        "paper_id": "D19-1576",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1581table_3",
        "description": "4.3 Results and Discussion . Table 3 shows accuracy. As the Random baseline suggests, positive and negative labels were distributed evenly. The Random+Seed baseline made use of the seed lexicon and output the corresponding label (or the reverse of it for negation) if the event's predicate is in the seed lexicon. We can see that the seed lexicon itself had practically no impact on prediction.",
        "sentences": [
            "4.3 Results and Discussion .",
            "Table 3 shows accuracy.",
            "As the Random baseline suggests, positive and negative labels were distributed evenly.",
            "The Random+Seed baseline made use of the seed lexicon and output the corresponding label (or the reverse of it for negation) if the event's predicate is in the seed lexicon.",
            "We can see that the seed lexicon itself had practically no impact on prediction."
        ],
        "class_sentence": [
            2,
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "Acc"
            ],
            [
                "Random"
            ],
            [
                "Random+Seed"
            ],
            [
                "Random+Seed"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_3",
        "paper_id": "D19-1581",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1582table_1",
        "description": "Table 1 presents the performance comparison between different methods. We can see that MOGANED achieves 1.6% and 1.7% improvement on precision and F1-measure, respectively, compared with the best baselines. MOGANED reaches a lower recall than two sequence based methods, JRNN and DEEB-RNN.",
        "sentences": [
            "Table 1 presents the performance comparison between different methods.",
            "We can see that MOGANED achieves 1.6% and 1.7% improvement on precision and F1-measure, respectively, compared with the best baselines.",
            "MOGANED reaches a lower recall than two sequence based methods, JRNN and DEEB-RNN."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "MOGANED",
                "P",
                "F1"
            ],
            [
                "MOGANED",
                "JRNN",
                "DEEB-RNN",
                "R"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_1",
        "paper_id": "D19-1582",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1585table_1",
        "description": "4 Results and Analyses . State-of-the-art Results . Table 1 shows test set F1 on the entity, relation and event extraction tasks. Our framework establishes a new state-of-the-art on all three high-level tasks, and on all subtasks except event argument identification. Relative error reductions range from 0.2 - 27.9% over previous state of the art models. Benefits of Graph Propagation Table 2 shows that Coreference propagation (CorefProp) improves named entity recognition performance across all three domains. The largest gains are on the computer science research abstracts of SciERC, which make frequent use of long-range coreferences, acronyms and abbreviations. CorefProp also improves relation extraction on SciERC.",
        "sentences": [
            "4 Results and Analyses .",
            "State-of-the-art Results .",
            "Table 1 shows test set F1 on the entity, relation and event extraction tasks.",
            "Our framework establishes a new state-of-the-art on all three high-level tasks, and on all subtasks except event argument identification.",
            "Relative error reductions range from 0.2 - 27.9% over previous state of the art models.",
            "Benefits of Graph Propagation Table 2 shows that Coreference propagation (CorefProp) improves named entity recognition performance across all three domains.",
            "The largest gains are on the computer science research abstracts of SciERC, which make frequent use of long-range coreferences, acronyms and abbreviations.",
            "CorefProp also improves relation extraction on SciERC."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1,
            0,
            0,
            0
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "Ours"
            ],
            [
                "Ours",
                "SOTA",
                "D%"
            ],
            null,
            null,
            null
        ],
        "n_sentence": 8.0,
        "table_id": "table_1",
        "paper_id": "D19-1585",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1588table_1",
        "description": "Table 1 shows that BERT-base offers an improvement of 0.9% over the ELMo-based c2fcoref model. Given how gains on coreference resolution have been hard to come by as evidenced by the table, this is still a considerable improvement. However, the magnitude of gains is relatively modest considering BERT's arguably better architecture and many more trainable parameters. This is in sharp contrast to how even the base variant of BERT has very substantially improved the state of the art in other tasks. BERT-large, however, improves c2f-coref by the much larger margin of 3.9%. We also observe that the overlap variant offers no improvement over independent.",
        "sentences": [
            "Table 1 shows that BERT-base offers an improvement of 0.9% over the ELMo-based c2fcoref model.",
            "Given how gains on coreference resolution have been hard to come by as evidenced by the table, this is still a considerable improvement.",
            "However, the magnitude of gains is relatively modest considering BERT's arguably better architecture and many more trainable parameters.",
            "This is in sharp contrast to how even the base variant of BERT has very substantially improved the state of the art in other tasks.",
            "BERT-large, however, improves c2f-coref by the much larger margin of 3.9%.",
            "We also observe that the overlap variant offers no improvement over independent."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "BERT-base + c2f-coref (independent)"
            ],
            null,
            null,
            null,
            [
                "BERT-large + c2f-coref (independent)"
            ],
            [
                "BERT-large + c2f-coref (overlap)"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_1",
        "paper_id": "D19-1588",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1589table_3",
        "description": "Table 3 shows performance comparison among different delta operations: SUBTRACT, ADD, and MLP which is a multi-layer perceptron network. All scores are macro-averaged across datasets. While ADD shows good performance on METEOR, SUBTRACT does on the soft metric (i.e., VecExt), indicating that subtraction can help the model capture the better semantics than the other functions.",
        "sentences": [
            "Table 3 shows performance comparison among different delta operations: SUBTRACT, ADD, and MLP which is a multi-layer perceptron network.",
            "All scores are macro-averaged across datasets.",
            "While ADD shows good performance on METEOR, SUBTRACT does on the soft metric (i.e., VecExt), indicating that subtraction can help the model capture the better semantics than the other functions."
        ],
        "class_sentence": [
            1,
            2,
            1
        ],
        "header_mention": [
            [
                "SUBTRACT",
                "ADD",
                "MLP"
            ],
            null,
            [
                "ADD",
                "M",
                "SUBTRACT",
                "VE"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_3",
        "paper_id": "D19-1589",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1590table_5",
        "description": "in which text fragments embodying background knowledge are concatenated to the input sentence as explained in Section 2. Table 5 shows that ProposedRU+BK improved the average precision over ProposedRU by about 2.5% (i.e., ProposedRU+BK significantly outperformed the state-of-the-art method, MCNN, by about 5%), suggesting that background knowledge in the form of text fragments is still useful, at least in our current experimental setting. However, the usefulness might be lost when a model is appropriately pretrained with a larger amount of texts that covers even more background knowledge.",
        "sentences": [
            "in which text fragments embodying background knowledge are concatenated to the input sentence as explained in Section 2.",
            "Table 5 shows that ProposedRU+BK improved the average precision over ProposedRU by about 2.5% (i.e., ProposedRU+BK significantly outperformed the state-of-the-art method, MCNN, by about 5%), suggesting that background knowledge in the form of text fragments is still useful, at least in our current experimental setting.",
            "However, the usefulness might be lost when a model is appropriately pretrained with a larger amount of texts that covers even more background knowledge."
        ],
        "class_sentence": [
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "ProposedRU+BK",
                "ProposedRU"
            ],
            null
        ],
        "n_sentence": 3.0,
        "table_id": "table_5",
        "paper_id": "D19-1590",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1596table_1",
        "description": "Table 1 shows quantitative results on our LConVQA and CS-ConVQA datasets. We make a number of observations below. The state-of-the-art VQA has low consistency. The baseline VQA system (row a) retains similarly high top-1 accuracy on the ConVQA splits (63.58% on VQAv2 vs 70.34% / 60.03% on LConVQA / CS-ConVQA); however, it achieves only 26.13% perfect consistency on the human generated CS-ConVQA questions. Finetuning is an effective strategy for the synthetic L-ConVQA split. Finetuning on LConVQA train results in 18.43% gains in perfect consistency on L-ConVQA test (row c vs a). This is unsurprising given the templated questions and simple concepts in L-ConVQA; however, perfect consistency is low in absolute terms at 54.68%. Finetuning does not lead to significant gains in consistency for human-generated questions. Finetuning the VQA model on CS-ConVQA (row b) leads to an improvement in consistency of only 0.26%. Likewise, adding L-ConVQA (row c) and extra Visual Genome questions (row e) actually reduces consistency. CTM-based training preserves or improves consistency when leveraging additional data. When we apply CTM to the Finetuned L/CSConVQA model, we improve CS-ConVQA perfect consistency by 1.24% (row d vs c) while modestly improving other metrics. Extending to Visual Genome questions, the CTM augmented model improves perfect consistency in CS-ConVQA by 2.27% over the finetuned model (row f vs e). Interestingly, the CTM modules were never trained with the human-annotated CS-ConVQA questions and yet lead to this improvement on CSConVQA by acting as an intelligent data augmenter/regularizer.",
        "sentences": [
            "Table 1 shows quantitative results on our LConVQA and CS-ConVQA datasets.",
            "We make a number of observations below.",
            "The state-of-the-art VQA has low consistency.",
            "The baseline VQA system (row a) retains similarly high top-1 accuracy on the ConVQA splits (63.58% on VQAv2 vs 70.34% / 60.03% on LConVQA / CS-ConVQA); however, it achieves only 26.13% perfect consistency on the human generated CS-ConVQA questions.",
            "Finetuning is an effective strategy for the synthetic L-ConVQA split.",
            "Finetuning on LConVQA train results in 18.43% gains in perfect consistency on L-ConVQA test (row c vs a).",
            "This is unsurprising given the templated questions and simple concepts in L-ConVQA; however, perfect consistency is low in absolute terms at 54.68%.",
            "Finetuning does not lead to significant gains in consistency for human-generated questions.",
            "Finetuning the VQA model on CS-ConVQA (row b) leads to an improvement in consistency of only 0.26%.",
            "Likewise, adding L-ConVQA (row c) and extra Visual Genome questions (row e) actually reduces consistency.",
            "CTM-based training preserves or improves consistency when leveraging additional data.",
            "When we apply CTM to the Finetuned L/CSConVQA model, we improve CS-ConVQA perfect consistency by 1.24% (row d vs c) while modestly improving other metrics.",
            "Extending to Visual Genome questions, the CTM augmented model improves perfect consistency in CS-ConVQA by 2.27% over the finetuned model (row f vs e).",
            "Interestingly, the CTM modules were never trained with the human-annotated CS-ConVQA questions and yet lead to this improvement on CSConVQA by acting as an intelligent data augmenter/regularizer."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "L-ConVQA",
                "CS-ConVQA"
            ],
            null,
            [
                "L/CS-ConVQA"
            ],
            [
                "VQA2.0",
                "L-ConVQA"
            ],
            [
                "L-ConVQA"
            ],
            [
                "L-ConVQA",
                "L/CS-ConVQA"
            ],
            [
                "L-ConVQA"
            ],
            [
                "b) FineTune",
                "c) FineTune",
                "e) FineTune"
            ],
            [
                "b) FineTune",
                "c) FineTune",
                "e) FineTune",
                "CS-ConVQA"
            ],
            [
                "b) FineTune",
                "c) FineTune",
                "e) FineTune",
                "L/CS-ConVQA",
                "Avg Con",
                "L/CS-ConVQA,VG",
                "f) +CTMvg"
            ],
            [
                "d) +CTM",
                "f) +CTMvg"
            ],
            [
                "CS-ConVQA",
                "d) +CTM",
                "f) +CTMvg"
            ],
            [
                "d) +CTM",
                "f) +CTMvg",
                "CS-ConVQA"
            ],
            [
                "d) +CTM",
                "f) +CTMvg",
                "CS-ConVQA"
            ]
        ],
        "n_sentence": 14.0,
        "table_id": "table_1",
        "paper_id": "D19-1596",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1599table_1",
        "description": "Does explicit inter-sentence matching matter? . Almost all previous state-of-the-art QA and RC models find answers by matching passages with questions, aka inter-sentence matching (Wang and Jiang, 2017; Wang et al., 2016; Seo et al., 2017; Wang et al., 2017; Song et al., 2017). However, BERT model simply concatenates a passage with a question, and differentiates them by separating them with a delimiter token [SEP], and assigning different segment ids for them. Here, we aim to check whether explicit inter-sentence matching still matters for BERT. We employ a shared BERT model to encode a passage and a question individually, and a weighted sum of all BERT layers is used as the final tokenlevel representation for the question or passage, where weights for all BERT layers are trainable parameters. Then the passage and question representations are input into QANet (Yu et al., 2018) to perform inter-sentence matching, and predict the final answer. Model (10) in Table 1 shows the result of jointly training the BERT encoder and the QANet model. The result is very poor, likely because the parameters in BERT are catastrophically forgotten while training the QANet model. To tackle this issue, we fix parameters in BERT, and only update parameters for QANet. The result is listed as model (11). It works better than model (10), but still worse than multi-passage BERT in model (6). We design another model by starting from model (11), and then jointly fine-tuning the BERT encoder and QANet. Model (12) in Table 1 shows the result. It works better than model (11), but still has a big gap with multi-passage BERT in model (6) . Therefore, we conclude that the explicit inter-sentence matching is not helpful for multi-passage BERT. One possible reason is that the multi-head self-attention layers in BERT has already embedded the inter-sentence matching.",
        "sentences": [
            "Does explicit inter-sentence matching matter? .",
            "Almost all previous state-of-the-art QA and RC models find answers by matching passages with questions, aka inter-sentence matching (Wang and Jiang, 2017; Wang et al., 2016; Seo et al., 2017; Wang et al., 2017; Song et al., 2017).",
            "However, BERT model simply concatenates a passage with a question, and differentiates them by separating them with a delimiter token [SEP], and assigning different segment ids for them.",
            "Here, we aim to check whether explicit inter-sentence matching still matters for BERT.",
            "We employ a shared BERT model to encode a passage and a question individually, and a weighted sum of all BERT layers is used as the final tokenlevel representation for the question or passage, where weights for all BERT layers are trainable parameters.",
            "Then the passage and question representations are input into QANet (Yu et al., 2018) to perform inter-sentence matching, and predict the final answer.",
            "Model (10) in Table 1 shows the result of jointly training the BERT encoder and the QANet model.",
            "The result is very poor, likely because the parameters in BERT are catastrophically forgotten while training the QANet model.",
            "To tackle this issue, we fix parameters in BERT, and only update parameters for QANet.",
            "The result is listed as model (11).",
            "It works better than model (10), but still worse than multi-passage BERT in model (6).",
            "We design another model by starting from model (11), and then jointly fine-tuning the BERT encoder and QANet.",
            "Model (12) in Table 1 shows the result.",
            "It works better than model (11), but still has a big gap with multi-passage BERT in model (6) .",
            "Therefore, we conclude that the explicit inter-sentence matching is not helpful for multi-passage BERT.",
            "One possible reason is that the multi-head self-attention layers in BERT has already embedded the inter-sentence matching."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            2,
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "BERT+QANet",
                "BERT+QANet (fix BERT)",
                "BERT+QANet (init. from (11))"
            ],
            [
                "BERT+QANet",
                "BERT+QANet (fix BERT)",
                "BERT+QANet (init. from (11))"
            ],
            [
                "BERT+QANet",
                "BERT+QANet (fix BERT)",
                "BERT+QANet (init. from (11))"
            ],
            [
                "BERT+QANet",
                "BERT+QANet (fix BERT)",
                "BERT+QANet (init. from (11))"
            ],
            [
                "BERT+QANet"
            ],
            [
                "BERT+QANet"
            ],
            [
                "BERT+QANet"
            ],
            [
                "BERT+QANet (fix BERT)"
            ],
            [
                "BERT+QANet",
                "w/ sliding-window"
            ],
            [
                "BERT+QANet (fix BERT)"
            ],
            [
                "BERT+QANet (init. from (11))"
            ],
            [
                "BERT+QANet (fix BERT)",
                "w/ sliding-window"
            ],
            null,
            null
        ],
        "n_sentence": 16.0,
        "table_id": "table_1",
        "paper_id": "D19-1599",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1616table_2",
        "description": "5 Evaluation and Discussion . We evaluate the results for every 10,000 iterations on the dev and test set. The automatic evaluation results based on the dev and test set are shown in Table 2. To evaluate the proposed algorithms, we use ROUGE (Recall-Oriented Understudy for Gisting Evaluation) score, which is a popular metric for text summarization task, and has several variants like ROUGE-N, and ROUGE-L, which measure the overlap of n-grams between the system and reference summary (LIN, 2004). We use ROUGE 1 F1 (R1 F1), ROUGE 2 F1 (R2 F1), and ROUGE L F1 (RL F1) for scoring the generated summary. In addition, we also use the SacreBLEU4 evaluation metric (Post, 2018). In terms of Rouge score model S3 outperforms model S1 but perform worse than model S2.",
        "sentences": [
            "5 Evaluation and Discussion .",
            "We evaluate the results for every 10,000 iterations on the dev and test set.",
            "The automatic evaluation results based on the dev and test set are shown in Table 2.",
            "To evaluate the proposed algorithms, we use ROUGE (Recall-Oriented Understudy for Gisting Evaluation) score, which is a popular metric for text summarization task, and has several variants like ROUGE-N, and ROUGE-L, which measure the overlap of n-grams between the system and reference summary (LIN, 2004).",
            "We use ROUGE 1 F1 (R1 F1), ROUGE 2 F1 (R2 F1), and ROUGE L F1 (RL F1) for scoring the generated summary.",
            "In addition, we also use the SacreBLEU4 evaluation metric (Post, 2018).",
            "In terms of Rouge score model S3 outperforms model S1 but perform worse than model S2."
        ],
        "class_sentence": [
            2,
            2,
            1,
            2,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            [
                "R1_F1",
                "R2_F1",
                "RL_F1"
            ],
            [
                "BLEU"
            ],
            [
                "R1_F1",
                "R2_F1",
                "RL_F1",
                "S3",
                "S1",
                "S2"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_2",
        "paper_id": "D19-1616",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1627table_4",
        "description": "3.2 Word Sense Selection in Context. We further examine if the captured sense-specific cues help word sense disambiguation via Wordin-Context data (WiC) (Pilehvar and CamachoCollados, 2018), in which each instance contains a pair of two contexts sharing a target word, and the task is to decide whether their word senses are the same.3 . To justify that the models are capable of selecting senses encoded in the embeddings, for each pair, our model outputs 10 candidate definitions (top-10 nearest neighbors), and we output TRUE if any definition occurs in both candidate sets, otherwise FALSE. Table 4 shows that the proposed model with contextualized word embeddings outperforms all previous models. We conclude that contextualized word embeddings indeed capture sense-informative cues and our proposed model is capable of interpreting the corresponding senses via definition.",
        "sentences": [
            "3.2 Word Sense Selection in Context.",
            "We further examine if the captured sense-specific cues help word sense disambiguation via Wordin-Context data (WiC) (Pilehvar and CamachoCollados, 2018), in which each instance contains a pair of two contexts sharing a target word, and the task is to decide whether their word senses are the same.3 .",
            "To justify that the models are capable of selecting senses encoded in the embeddings, for each pair, our model outputs 10 candidate definitions (top-10 nearest neighbors), and we output TRUE if any definition occurs in both candidate sets, otherwise FALSE.",
            "Table 4 shows that the proposed model with contextualized word embeddings outperforms all previous models.",
            "We conclude that contextualized word embeddings indeed capture sense-informative cues and our proposed model is capable of interpreting the corresponding senses via definition."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "Proposed (BERT-base)"
            ],
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_4",
        "paper_id": "D19-1627",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1634table_7",
        "description": "Table 7 shows the comparison of copying accuracies between MS UEDIN, COPYNET, and our approach. We find that our approach outperforms the two baselines. However, the copying accuracy of our approach is almost 20% lower than the prediction accuracy (i.e., 65.61% vs. 85.09%), indicating that it is much more challenging to place the copied words in correct positions.",
        "sentences": [
            "Table 7 shows the comparison of copying accuracies between MS UEDIN, COPYNET, and our approach.",
            "We find that our approach outperforms the two baselines.",
            "However, the copying accuracy of our approach is almost 20% lower than the prediction accuracy (i.e., 65.61% vs. 85.09%), indicating that it is much more challenging to place the copied words in correct positions."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "MS UEDIN",
                "COPYNET",
                "Ours"
            ],
            [
                "Ours"
            ],
            [
                "Ours",
                " Accuracy"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_7",
        "paper_id": "D19-1634",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1638table_5",
        "description": "We conducted separate evaluations for two (Set2SingleSeq, pairs of (Set2SingleSeq, Set2MultipleSeq) Set2MultipleSeq+Opt). The results are shown in Tables 5 and 6 respectively. Results shown in Table 5 explains that incorporating neural components for subset selection and content ordering helps in improving informative instruction generation. We observe that conducting content selection multiple times during each time step through content ordering RNN helps in generating a discrete set of instructions (Set2MultipleSeq). Table 6 shows that penalizing redundancy during beam search decoding reduces noise and helps in generating instructions with rich information density. Inter-evaluator agreement for the entire set of human evaluation is reasonably high: Cohen\u00e2\u20ac\u2122s kappa coefficient is 0.79.",
        "sentences": [
            "We conducted separate evaluations for two (Set2SingleSeq, pairs of (Set2SingleSeq, Set2MultipleSeq) Set2MultipleSeq+Opt).",
            "The results are shown in Tables 5 and 6 respectively.",
            "Results shown in Table 5 explains that incorporating neural components for subset selection and content ordering helps in improving informative instruction generation.",
            "We observe that conducting content selection multiple times during each time step through content ordering RNN helps in generating a discrete set of instructions (Set2MultipleSeq).",
            "Table 6 shows that penalizing redundancy during beam search decoding reduces noise and helps in generating instructions with rich information density.",
            "Inter-evaluator agreement for the entire set of human evaluation is reasonably high: Cohen\u00e2\u20ac\u2122s kappa coefficient is 0.79."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            0,
            0
        ],
        "header_mention": [
            [
                "Set2MultipleSeq",
                "Set2MultipleSeq+opt"
            ],
            null,
            [
                "Set2MultipleSeq+opt"
            ],
            [
                "Set2MultipleSeq+opt"
            ],
            null,
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_5",
        "paper_id": "D19-1638",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1647table_2",
        "description": "5.2 Baseline VA Candidates . We cannot determine recall for our approaches, instead we compute precision, recall and f-score based on the baseline dataset (see Section3), which is shown in Table 2 as well as precision of the baseline dataset. The automated approaches using Wikidata and named entity recognition can boost the precision significantly with a moderate loss in recall. The BLSTM approach performs best. Although the loss in recall is higher than with the WD approach, the precision reaches almost 87% and is thus raised to a new level.",
        "sentences": [
            "5.2 Baseline VA Candidates .",
            "We cannot determine recall for our approaches, instead we compute precision, recall and f-score based on the baseline dataset (see Section3), which is shown in Table 2 as well as precision of the baseline dataset.",
            "The automated approaches using Wikidata and named entity recognition can boost the precision significantly with a moderate loss in recall.",
            "The BLSTM approach performs best.",
            "Although the loss in recall is higher than with the WD approach, the precision reaches almost 87% and is thus raised to a new level."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "WD",
                "NER",
                "baseline",
                " prec",
                " rec"
            ],
            [
                "BLSTM"
            ],
            [
                "BLSTM",
                "WD",
                " rec",
                " prec"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_2",
        "paper_id": "D19-1647",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1648table_1",
        "description": "3.2 Experimental Results. Table 1 shows the experimental results. We can see that HanPaNE+P showed the highest accuracy and HanPaNE+P and VE+P, with consideration of paraphrases, showed a higher accuracy than Baseline. In contrast, HanPaNE-P and VE-P, without consideration of paraphrases, did not. The results indicate that the use of paraphrases contributed to improved accuracy. We also conducted the following two types of hypothesis testing. The first one is a McNemar paired test on the labeling disagreements of words assigned by HanPaNE and the others as in (Sha and Pereira, 2003). All the results except for Baseline were significantly different (p <0.01). The second one is a bi-nominal test used in (Sasano and Kurohashi, 2008). For this test, the number of the entities correctly recognized by only HanPaNE and the number of entities correctly recognized by only the other method are counted. Then, based on the assumption that outputs have the binomial distribution, we apply a binomial test. All the results were significantly different for this test (p < 0.05). These results showed that HanPaNE works better than augmented training data.",
        "sentences": [
            "3.2 Experimental Results.",
            "Table 1 shows the experimental results.",
            "We can see that HanPaNE+P showed the highest accuracy and HanPaNE+P and VE+P, with consideration of paraphrases, showed a higher accuracy than Baseline.",
            "In contrast, HanPaNE-P and VE-P, without consideration of paraphrases, did not.",
            "The results indicate that the use of paraphrases contributed to improved accuracy.",
            "We also conducted the following two types of hypothesis testing.",
            "The first one is a McNemar paired test on the labeling disagreements of words assigned by HanPaNE and the others as in (Sha and Pereira, 2003).",
            "All the results except for Baseline were significantly different (p <0.01).",
            "The second one is a bi-nominal test used in (Sasano and Kurohashi, 2008).",
            "For this test, the number of the entities correctly recognized by only HanPaNE and the number of entities correctly recognized by only the other method are counted.",
            "Then, based on the assumption that outputs have the binomial distribution, we apply a binomial test.",
            "All the results were significantly different for this test (p < 0.05). These results showed that HanPaNE works better than augmented training data."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0
        ],
        "header_mention": [
            null,
            null,
            [
                "HanPaNE+P (Proposed)",
                "VE+P",
                "Baseline"
            ],
            [
                "HanPaNE-P",
                "VE-P"
            ],
            [
                "HanPaNE+P (Proposed)",
                "VE+P"
            ],
            null,
            [
                "HanPaNE-P",
                "HanPaNE+P (Proposed)"
            ],
            [
                "Baseline"
            ],
            null,
            [
                "HanPaNE-P",
                "HanPaNE+P (Proposed)"
            ],
            null,
            [
                "HanPaNE-P",
                "HanPaNE+P (Proposed)"
            ]
        ],
        "n_sentence": 12.0,
        "table_id": "table_1",
        "paper_id": "D19-1648",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1653table_4",
        "description": "Results: Overall Performance . Table 4 shows that our proposed sentence-level BLC with structural information performs best regarding macro F1 in either boundary identification or unit classification tasks. The presence of structural features provides an excellent boost in classifying unit types. For the post-level BLC, the model yields a better score in predicting non-EUparts because post-level discrimination can capture the entire post and thus identify irrelevant boundaries, such as supplementary notes, in posts.",
        "sentences": [
            "Results: Overall Performance .",
            "Table 4 shows that our proposed sentence-level BLC with structural information performs best regarding macro F1 in either boundary identification or unit classification tasks.",
            "The presence of structural features provides an excellent boost in classifying unit types.",
            "For the post-level BLC, the model yields a better score in predicting non-EUparts because post-level discrimination can capture the entire post and thus identify irrelevant boundaries, such as supplementary notes, in posts."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "sentence-level + structural",
                " macro",
                " unit identification",
                "unit classification"
            ],
            null,
            [
                "post-level"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_4",
        "paper_id": "D19-1653",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1659table_1",
        "description": "Table 1 shows the results of various models. Our model based on the unified objective of Eq. (4) offers better balanced results compared to its variants. When removing Lsentiment, our model degrades to an input copy-like method, resulting in low classification accuracies but the highest BLEU scores. When removing Lcontent, the BLEU scores drop, indicating that the model cannot maintain a sufficient number of content words. Without Lalignment, we observe a reduction in both accuracy and BLEU on Yelp. However, this tendency is inconsistent on Amazon (i.e., -2.2 accuracy and +0.56 BLEU). When using only Lsentiment, our model falls back to the vanilla encoder-decoder model with a single loss, yielding poorer results on both datasets.",
        "sentences": [
            "Table 1 shows the results of various models.",
            "Our model based on the unified objective of Eq. (4) offers better balanced results compared to its variants.",
            "When removing Lsentiment, our model degrades to an input copy-like method, resulting in low classification accuracies but the highest BLEU scores.",
            "When removing Lcontent, the BLEU scores drop, indicating that the model cannot maintain a sufficient number of content words.",
            "Without Lalignment, we observe a reduction in both accuracy and BLEU on Yelp.",
            "However, this tendency is inconsistent on Amazon (i.e., -2.2 accuracy and +0.56 BLEU).",
            "When using only Lsentiment, our model falls back to the vanilla encoder-decoder model with a single loss, yielding poorer results on both datasets."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "This work"
            ],
            [
                "w/o Lsentiment",
                "Acc",
                "BLEU"
            ],
            [
                "w/o Lcontent",
                "BLEU"
            ],
            [
                "w/o Lalignment",
                "Acc",
                "BLEU",
                "Yelp"
            ],
            [
                "Amazon",
                "Acc",
                "BLEU"
            ],
            [
                "only Lsentiment"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_1",
        "paper_id": "D19-1659",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1665table_1",
        "description": "In Table 1 we report the test set results for all models. The results for the MFTC dataset are averaged across the six discourse domains. Overall, MTL-LP is the best performing multilabel classification method across all the datasets. MTLLP is also better than the best performing model Seq2Seq reported in Sobhani et al. (2019) for the ETC dataset. MTL-XLD improves on the baseline models for the BBC and MFTC datasets, but performs slightly worse than MTL on the ETC dataset. We note that our results for the BBC and MFTC datasets are not directly comparable with previous work on BBC (Simaki et al., 2017) and MFTC (Dehghani et al., 2019), since we consider the full set of labels, whereas previous work removed those that were sparser. Our reimplementation of the logistic regression model of Simaki et al. (2017), as an additional baseline, resulted in poor performance in the BBC dataset (20 in JSS) and we did not consider it further.",
        "sentences": [
            "In Table 1 we report the test set results for all models.",
            "The results for the MFTC dataset are averaged across the six discourse domains.",
            "Overall, MTL-LP is the best performing multilabel classification method across all the datasets.",
            "MTLLP is also better than the best performing model Seq2Seq reported in Sobhani et al. (2019) for the ETC dataset.",
            "MTL-XLD improves on the baseline models for the BBC and MFTC datasets, but performs slightly worse than MTL on the ETC dataset.",
            "We note that our results for the BBC and MFTC datasets are not directly comparable with previous work on BBC (Simaki et al., 2017) and MFTC (Dehghani et al., 2019), since we consider the full set of labels, whereas previous work removed those that were sparser.",
            "Our reimplementation of the logistic regression model of Simaki et al. (2017), as an additional baseline, resulted in poor performance in the BBC dataset (20 in JSS) and we did not consider it further."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "MFTC"
            ],
            [
                "MTL-LP"
            ],
            [
                "Sobhani (Seq2Seq)",
                "MTL-LP"
            ],
            [
                "MTL-XLD",
                "BBC",
                "MFTC",
                "MTL",
                "ETC"
            ],
            [
                "BBC",
                "MFTC"
            ],
            [
                "BBC"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_1",
        "paper_id": "D19-1665",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1667table_3",
        "description": "Table 3 presents the results of the total term prediction. Although our method is not directly trained to make the final prediction, the performance of our model surpasses all baselines, which confirms that the breakdown charge-based analysis can indeed help the total prison term prediction.",
        "sentences": [
            "Table 3 presents the results of the total term prediction.",
            "Although our method is not directly trained to make the final prediction, the performance of our model surpasses all baselines, which confirms that the breakdown charge-based analysis can indeed help the total prison term prediction."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "DGN"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_3",
        "paper_id": "D19-1667",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1111table_2",
        "description": "Table 2 shows the results obtained on testing the final classifier system on the Test subset of SL. We obtain an overall high accuracy of 86.6% at the sentence level. While RESULT and CONCLUSION obtained F-measures above 90%, OBJECTIVE and METHOD reported reasonable F-measures above 80%. DESIGN obtained the lowest precision, recall and F-measure. Overall, the performance we obtain is in the range of other reported results in similar tasks (Guo et al., 2013).",
        "sentences": [
            "Table 2 shows the results obtained on testing the final classifier system on the Test subset of SL.",
            "We obtain an overall high accuracy of 86.6% at the sentence level.",
            "While RESULT and CONCLUSION obtained F-measures above 90%, OBJECTIVE and METHOD reported reasonable F-measures above 80%.",
            "DESIGN obtained the lowest precision, recall and F-measure.",
            "Overall, the performance we obtain is in the range of other reported results in similar tasks (Guo et al., 2013)."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            0
        ],
        "header_mention": [
            null,
            [
                "ALL",
                "Accuracy"
            ],
            [
                "RESULT",
                "CONCLUSION",
                "OBJECTIVE",
                "METHOD",
                "F-measure"
            ],
            [
                "DESIGN",
                "Precision",
                "Recall",
                "F-measure"
            ],
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_2",
        "paper_id": "P16-1111",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1111table_3",
        "description": "Table 3 shows the performance of our model on this task. As expected a topic's label distribution over its entire life-time is very informative with respect to classifying the topic as growing or declining. We achieve a significant improvement over the baselines on the full dataset (32.3% relative improvement over majority prediction), and this trend holds across each field separately. The ratio feature proved to be extremely predictive in this task, i.e.relative increases/decreases in a topic being used in different functional roles are very predictive of the type of its trajectory.",
        "sentences": [
            "Table 3 shows the performance of our model on this task.",
            "As expected a topic's label distribution over its entire life-time is very informative with respect to classifying the topic as growing or declining.",
            "We achieve a significant improvement over the baselines on the full dataset (32.3% relative improvement over majority prediction), and this trend holds across each field separately.",
            "The ratio feature proved to be extremely predictive in this task, i.e.relative increases/decreases in a topic being used in different functional roles are very predictive of the type of its trajectory."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "LR"
            ],
            [
                "LR",
                "ALL",
                "Majority"
            ],
            [
                "LR"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "P16-1111",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1111table_4",
        "description": "Table 4 shows the performance of our model on this task. (The baseline performances are the same as in the classification task). These results show that we can accurately predict whether a topic will grow or decline using only a small amount of data. Moreover, we see that both percentage and delta features are necessary for this task.",
        "sentences": [
            "Table 4 shows the performance of our model on this task.",
            "(The baseline performances are the same as in the classification task).",
            "These results show that we can accurately predict whether a topic will grow or decline using only a small amount of data.",
            "Moreover, we see that both percentage and delta features are necessary for this task."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Accuracy on ALL"
            ],
            [
                "LD-% + LD-delta",
                "LD-% only"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_4",
        "paper_id": "P16-1111",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1112table_1",
        "description": "Table 1 contains results for experiments comparing different composition architectures on the task of error detection. The CRF has the lowest F0.5 score compared to any of the neural models. It memorises frequent error sequences with high precision, but does not generalise sufficiently, resulting in low recall. The ability to condition on the previous label also does not provide much help on this task - there are only two possible labels and the errors are relatively sparse.",
        "sentences": [
            "Table 1 contains results for experiments comparing different composition architectures on the task of error detection.",
            "The CRF has the lowest F0.5 score compared to any of the neural models.",
            "It memorises frequent error sequences with high precision, but does not generalise sufficiently, resulting in low recall.",
            "The ability to condition on the previous label also does not provide much help on this task - there are only two possible labels and the errors are relatively sparse."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "CRF",
                "F0.5"
            ],
            [
                "CRF",
                "P",
                "R"
            ],
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_1",
        "paper_id": "P16-1112",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1112table_2",
        "description": "Table 2 contains results obtained by incrementally adding training data to the Bi-LSTM model. We found that incorporating the NUCLE dataset does not improve performance over using only the FCE-public dataset, which is likely due to the two corpora containing texts with different domains and writing styles. The texts in FCE are written by young intermediate students, in response to prompts eliciting letters, emails and reviews, whereas NUCLE contains mostly argumentative essays written by advanced adult learners. The differences in the datasets offset the benefits from additional training data, and the performance remains roughly the same.",
        "sentences": [
            "Table 2 contains results obtained by incrementally adding training data to the Bi-LSTM model.",
            "We found that incorporating the NUCLE dataset does not improve performance over using only the FCE-public dataset, which is likely due to the two corpora containing texts with different domains and writing styles.",
            "The texts in FCE are written by young intermediate students, in response to prompts eliciting letters, emails and reviews, whereas NUCLE contains mostly argumentative essays written by advanced adult learners.",
            "The differences in the datasets offset the benefits from additional training data, and the performance remains roughly the same."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            [
                "Training Data"
            ],
            [
                "+NUCLE A4",
                "FCE-public"
            ],
            [
                "FCE-public",
                "+NUCLE A4"
            ],
            [
                "Training Data"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "P16-1112",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1113table_7",
        "description": "The results, summarized in Table 7, indicate that PathLSTM performs better than the system by Bjorkelund et al. (2009) in all cases. For German and Chinese, PathLSTM achieves the best overall F1-scores of 80.1% and 79.4%, respectively.",
        "sentences": [
            "The results, summarized in Table 7, indicate that PathLSTM performs better than the system by Bjorkelund et al. (2009) in all cases.",
            "For German and Chinese, PathLSTM achieves the best overall F1-scores of 80.1% and 79.4%, respectively."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "PathLSTM",
                "Bjorkelund et al. (2009)"
            ],
            [
                "PathLSTM",
                "Chinese",
                "German",
                "F1"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_7",
        "paper_id": "P16-1113",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1116table_1",
        "description": "Table 1 shows the overall performance on the blind test set. We compare our results with the JET baseline as well as the Cross-Event, CrossEntity, and joint methods. When adding the event type classifier, in the line titled \u201c+ ET\u201d, we see a significant increase in the three measures over the JET baseline in recall. Although our trigger\u2019s precision is lower than RBPB(JET), it gains 5.2% improvement on the trigger\u2019s F1 measure, 10.6% improvement on argument identification\u2019s F1 measure and 9.7% improvement on argument classification\u2019s F1 measure. Future work may be done to solve these two limitations. The line titled + Regu in Table 1 represents the performance when we only use the regularization method. In Table 1, Compared to the four baseline systems, the argument identification\u2019s F1 measure of \u201c+ Regu\u201d is significantly higher. The complete approach is denoted as \u201cRBPB\u201d in Table 1. Remarkably, our approach performances comparable in trigger classification with the state-of art methods: Cross-Event, Cross-Entity, Joint model, DMCNN and significantly higher than them in argument identification as well as classification although we did not use the cross-document, cross-event information or any global feature.",
        "sentences": [
            "Table 1 shows the overall performance on the blind test set.",
            "We compare our results with the JET baseline as well as the Cross-Event, CrossEntity, and joint methods.",
            "When adding the event type classifier, in the line titled \u201c+ ET\u201d, we see a significant increase in the three measures over the JET baseline in recall.",
            "Although our trigger\u2019s precision is lower than RBPB(JET), it gains 5.2% improvement on the trigger\u2019s F1 measure, 10.6% improvement on argument identification\u2019s F1 measure and 9.7% improvement on argument classification\u2019s F1 measure.",
            "Future work may be done to solve these two limitations.",
            "The line titled + Regu in Table 1 represents the performance when we only use the regularization method.",
            "In Table 1, Compared to the four baseline systems, the argument identification\u2019s F1 measure of \u201c+ Regu\u201d is significantly higher.",
            "The complete approach is denoted as \u201cRBPB\u201d in Table 1.",
            "Remarkably, our approach performances comparable in trigger classification with the state-of art methods: Cross-Event, Cross-Entity, Joint model, DMCNN and significantly higher than them in argument identification as well as classification although we did not use the cross-document, cross-event information or any global feature."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Cross-Event",
                "Cross-Entity",
                "Joint"
            ],
            [
                "JET",
                "RBPB(JET) + ET",
                "R"
            ],
            [
                "RBPB(JET)",
                "Trigger Classification",
                "P",
                "F1"
            ],
            [
                "JET",
                "RBPB(JET)"
            ],
            [
                "RBPB(JET) + Regu"
            ],
            [
                "Cross-Event",
                "Cross-Entity",
                "Joint",
                "DMCNN",
                "RBPB(JET) + Regu",
                "F1"
            ],
            [
                "RBPB(JET)"
            ],
            [
                "Trigger Classification",
                "RBPB(JET)",
                "Cross-Event",
                "Cross-Entity",
                "Joint",
                "DMCNN"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_1",
        "paper_id": "P16-1116",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1116table_2",
        "description": "We test the performance with argument candidates automatically extracted by JET in Table 2, our approach \u201c+ ET\u201d again significantly outperforms the JET baseline. Remarkably, our result is comparable with the Joint model although we only use lexical features. The line titled + Regu in Table 2 represents the performance when we only use the regularization method. In Table 2, the \u201c+ Regu\u201d again gains a higher F1 measure than the JET, Cross-Document, joint model baseline and \u201c+ ET\u201d. The complete approach is denoted as \u201cRBPB\u201d in Table 2. Remarkably, our approach performances comparable in trigger classification with the state-of art methods: Cross-Document, Joint model, and significantly higher than them in argument identification as well as classification although we did not use the cross-document, cross-event information or any global feature.",
        "sentences": [
            "We test the performance with argument candidates automatically extracted by JET in Table 2, our approach \u201c+ ET\u201d again significantly outperforms the JET baseline.",
            "Remarkably, our result is comparable with the Joint model although we only use lexical features.",
            "The line titled + Regu in Table 2 represents the performance when we only use the regularization method.",
            "In Table 2, the \u201c+ Regu\u201d again gains a higher F1 measure than the JET, Cross-Document, joint model baseline and \u201c+ ET\u201d.",
            "The complete approach is denoted as \u201cRBPB\u201d in Table 2.",
            "Remarkably, our approach performances comparable in trigger classification with the state-of art methods: Cross-Document, Joint model, and significantly higher than them in argument identification as well as classification although we did not use the cross-document, cross-event information or any global feature."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "JET",
                "RBPB(JET) + ET"
            ],
            [
                "RBPB(JET) + ET",
                "Joint"
            ],
            [
                "RBPB(JET) + Regu"
            ],
            [
                "RBPB(JET) + Regu",
                "Trigger",
                "F1",
                "Arg id",
                "Arg id+cl",
                "JET",
                "Cross-Document",
                "Joint",
                "RBPB(JET) + ET"
            ],
            [
                "RBPB(JET)"
            ],
            [
                "RBPB(JET)",
                "RBPB(JET) + ET",
                "RBPB(JET) + Regu",
                "RBPB(JET) + ET + Regu",
                "Trigger",
                "F1",
                "Cross-Document",
                "Joint"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_2",
        "paper_id": "P16-1116",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1118table_4",
        "description": "Table 4 summarizes the system performance on newswire and clinical data. We observe that systems that did well on RTE datasets, were mediocre on the clinical dataset. We did not, however, put any effort into adaption of TIE and EDITS to the clinical data. So the mediocre performance on clinical is understandable. It is interesting to see though that ENT did well (comparatively) on both domains. We note that our problem setting is most similar to the RTE-5 entailment search task. Of the 20 runs across eight teams that participated in RTE-5, the median F-Score was 0.30 and the best system (Mirkin et al., 2009) achieved an F-Score of 0.46. EDITS and TIE perform slightly above the median and ENT (with 0.39 F-score) would have ranked third in the challenge. The performance of all systems on the clinical data is noticeably low as compared to the newswire data.",
        "sentences": [
            "Table 4 summarizes the system performance on newswire and clinical data.",
            "We observe that systems that did well on RTE datasets, were mediocre on the clinical dataset.",
            "We did not, however, put any effort into adaption of TIE and EDITS to the clinical data.",
            "So the mediocre performance on clinical is understandable.",
            "It is interesting to see though that ENT did well (comparatively) on both domains.",
            "We note that our problem setting is most similar to the RTE-5 entailment search task.",
            "Of the 20 runs across eight teams that participated in RTE-5, the median F-Score was 0.30 and the best system (Mirkin et al., 2009) achieved an F-Score of 0.46.",
            "EDITS and TIE perform slightly above the median and ENT (with 0.39 F-score) would have ranked third in the challenge.",
            "The performance of all systems on the clinical data is noticeably low as compared to the newswire data."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            1,
            2,
            2,
            2,
            1
        ],
        "header_mention": [
            [
                "Newswire",
                "Clinical"
            ],
            [
                "Clinical"
            ],
            [
                "TIE",
                "EDITS",
                "Clinical"
            ],
            [
                "Clinical"
            ],
            [
                "ENT",
                "Newswire",
                "Clinical"
            ],
            null,
            [
                "F-score"
            ],
            [
                "EDITS",
                "TIE",
                "ENT",
                "F-score"
            ],
            [
                "System",
                "Clinical",
                "Newswire"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_4",
        "paper_id": "P16-1118",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1120table_3",
        "description": "We measured the performance of translation extraction with top N accuracy (ACCN), the number of test words whose top N translation candidates contain a correct translation over the total number of test words (7,930). Table 3 summarizes ACC1 and ACC10 for each model. As can be seen, Cue/Liu(BiSTM) and Cue/Liu(BiSTM+TS) significantly outperform Cue/Liu(BiLDA) (p < 0.01 in the sign test). This indicates that BiSTM and BiSTM+TS improve the performance of translation extraction for both the Cue and Liu methods by assigning more suitable topics.",
        "sentences": [
            "We measured the performance of translation extraction with top N accuracy (ACCN), the number of test words whose top N translation candidates contain a correct translation over the total number of test words (7,930).",
            "Table 3 summarizes ACC1 and ACC10 for each model.",
            "As can be seen, Cue/Liu(BiSTM) and Cue/Liu(BiSTM+TS) significantly outperform Cue/Liu(BiLDA) (p < 0.01 in the sign test).",
            "This indicates that BiSTM and BiSTM+TS improve the performance of translation extraction for both the Cue and Liu methods by assigning more suitable topics."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "ACC1",
                "ACC10"
            ],
            [
                "ACC1",
                "ACC10"
            ],
            [
                "Cue(BiSTM)",
                "Liu(BiSTM)",
                "Cue(BiSTM+TS)",
                "Liu(BiSTM+TS)",
                "Cue(BiLDA)",
                "Liu(BiLDA)"
            ],
            [
                "Cue(BiSTM+TS)",
                "Liu(BiSTM+TS)"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "P16-1120",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1123table_3",
        "description": "Table 3 provides a detailed comparison of our Multi-Level Attention CNN model with previous approaches. We observe that our novel attention-based architecture achieves new state-of-the-art results on this relation classification dataset. Att Input-CNN relies only on the primal attention at the input level, performing standard max-pooling after the convolution layer to generate the network output w O, in which the new objective function is utilized. With Att-Input-CNN, we achieve an F1-score of 87.5%, thus already outperforming not only the original winner of the SemEval task, an SVM-based approach (82.2%), but also the well-known CR-CNN model (84.1%) with a relative improvement of 4.04%, and the newly released DRNNs (85.8%) with a relative improvement of 2.0%, although the latter approach depends on the Stanford parser to obtain dependency parse information. Our full dual attention model Att-Pooling-CNN achieves an even more favorable F1-score of 88%.",
        "sentences": [
            "Table 3 provides a detailed comparison of our Multi-Level Attention CNN model with previous approaches.",
            "We observe that our novel attention-based architecture achieves new state-of-the-art results on this relation classification dataset.",
            "Att Input-CNN relies only on the primal attention at the input level, performing standard max-pooling after the convolution layer to generate the network output w O, in which the new objective function is utilized.",
            "With Att-Input-CNN, we achieve an F1-score of 87.5%, thus already outperforming not only the original winner of the SemEval task, an SVM-based approach (82.2%), but also the well-known CR-CNN model (84.1%) with a relative improvement of 4.04%, and the newly released DRNNs (85.8%) with a relative improvement of 2.0%, although the latter approach depends on the Stanford parser to obtain dependency parse information.",
            "Our full dual attention model Att-Pooling-CNN achieves an even more favorable F1-score of 88%."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Our Architectures"
            ],
            [
                "Att-Input-CNN"
            ],
            [
                "Att-Input-CNN",
                "F1",
                "SVM (Rink and Harabagiu 2010)",
                "CR-CNN (dos Santos et al. 2015)",
                "DRNNs (Xu et al. 2016)"
            ],
            [
                "Att-Pooling-CNN",
                "F1"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_3",
        "paper_id": "P16-1123",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1123table_4",
        "description": "Table 4 provides the experimental results for the two variants of the model given by Eqs.(7) and (8) in Section 3.3. Our main model outperforms the other variants on this dataset, although the variants may still prove useful when applied to other tasks.",
        "sentences": [
            "Table 4 provides the experimental results for the two variants of the model given by Eqs.(7) and (8) in Section 3.3.",
            "Our main model outperforms the other variants on this dataset, although the variants may still prove useful when applied to other tasks."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Att-Input-CNN (Main)",
                "Att-Input-CNN (Variant-1)",
                "Att-Input-CNN (Variant-2)"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_4",
        "paper_id": "P16-1123",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1126table_3",
        "description": "We split the set of 115 policies into subsets of 75 for training and 40 for testing. The number of clusters in the HMM approach8 is set to 100 and the results are shown in Table 3 as means across 10 runs. The standard deviations for these performance figures are generally between 0.01 and 0.05; the one exception is Do Not Track (the least frequent category) with a standard deviation of 0.2. As the table shows, although the HMM does not reach the same performance as SVM, it performs similarly to logistic regression and meets or exceeds its F1-score for five categories.",
        "sentences": [
            "We split the set of 115 policies into subsets of 75 for training and 40 for testing.",
            "The number of clusters in the HMM approach8 is set to 100 and the results are shown in Table 3 as means across 10 runs.",
            "The standard deviations for these performance figures are generally between 0.01 and 0.05; the one exception is Do Not Track (the least frequent category) with a standard deviation of 0.2.",
            "As the table shows, although the HMM does not reach the same performance as SVM, it performs similarly to logistic regression and meets or exceeds its F1-score for five categories."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "HMM"
            ],
            null,
            [
                "HMM",
                "SVM",
                "LR",
                "F"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "P16-1126",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1127table_2",
        "description": "Results on test data Table 2 shows the test results of SPO and our different systems over the seven domains. It can be seen that all of our sequence-based systems are performing better than SPO by a large margin on these tests. When averaging over the seven domains, our \u2018worst\u2019 system DSP scores at 64.7% compared to SPO at 57.1%. We note that these positive results hold despite the fact that DSP has the handicap that it may generate ungrammatical sequences relative to the underlying grammar, which do not lead to interpretable LFs. The LFP and CFP models, with higher performance than DSP, also may generate ungrammatical sequences.",
        "sentences": [
            "Results on test data Table 2 shows the test results of SPO and our different systems over the seven domains.",
            "It can be seen that all of our sequence-based systems are performing better than SPO by a large margin on these tests.",
            "When averaging over the seven domains, our \u2018worst\u2019 system DSP scores at 64.7% compared to SPO at 57.1%.",
            "We note that these positive results hold despite the fact that DSP has the handicap that it may generate ungrammatical sequences relative to the underlying grammar, which do not lead to interpretable LFs.",
            "The LFP and CFP models, with higher performance than DSP, also may generate ungrammatical sequences."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            [
                "SPO",
                "LFP",
                "CFP",
                "DSP",
                "DSP-C",
                "DSP-CL"
            ],
            [
                "LFP",
                "CFP",
                "DSP",
                "DSP-C",
                "DSP-CL",
                "SPO"
            ],
            [
                "DSP",
                "SPO"
            ],
            [
                "DSP"
            ],
            [
                "LFP",
                "CFP",
                "DSP"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_2",
        "paper_id": "P16-1127",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1129table_3",
        "description": "Different groups of features may play different roles in the LTR models. In order to validate the impact of both the traditional features and the novel task-specific features, we conduct experiments with different combinations by removing each group of features respectively. Table 3 shows the results, with w/o denotes experiments without the corresponding group of features. We can observe that both the traditional features and the novel features contribute useful information for learning to rank models.",
        "sentences": [
            "Different groups of features may play different roles in the LTR models.",
            "In order to validate the impact of both the traditional features and the novel task-specific features, we conduct experiments with different combinations by removing each group of features respectively.",
            "Table 3 shows the results, with w/o denotes experiments without the corresponding group of features.",
            "We can observe that both the traditional features and the novel features contribute useful information for learning to rank models."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "RF"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "P16-1129",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1130table_2",
        "description": "Table 2 shows the translation results using bootstrap resampling (Koehn, 2004). Base stands for the baseline system without any. MERS, CSRS, MERS-MINI and CSRS-MINI means the outputs of the baseline system were reranked using features from the MERS, CSRS, MERS-MINI and CSRS-MINI models respectively. Generally, the CSRS model outperformed the MERS model and the CSRS-MINI model outperformed the MERSMINI model on different translation tasks.",
        "sentences": [
            "Table 2 shows the translation results using bootstrap resampling (Koehn, 2004).",
            "Base stands for the baseline system without any.",
            "MERS, CSRS, MERS-MINI and CSRS-MINI means the outputs of the baseline system were reranked using features from the MERS, CSRS, MERS-MINI and CSRS-MINI models respectively.",
            "Generally, the CSRS model outperformed the MERS model and the CSRS-MINI model outperformed the MERSMINI model on different translation tasks."
        ],
        "class_sentence": [
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "Base"
            ],
            [
                "MERS",
                "CSRS",
                "MERS-MINI",
                "CSRS-MINI"
            ],
            [
                "CSRS",
                "MERS",
                "CSRS-MINI",
                "MERS-MINI"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "P16-1130",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1131table_2",
        "description": "We show the results of two of the best proposed parsers: third-order adding (o3-adding) and third-order perceptron (o3-perceptron) methods, and compare with the reported results of some previous work in Table 2. We compare with three categories of models: other Graph-based NN (neural network) models, traditional Graph-based Linear models and Transition-based NN models. For PTB, there have been several different dependency converters which lead to different sets of dependencies and we choose three of the most popular ones for more comprehensive comparisons. Since not all work report results on all of these dependencies, some of the entries might be not available. From the comparison, we see that the proposed parser has output competitive performance for different dependency conversion conventions and treebanks. Compared with traditional graph based linear models, neural models may benefit from better feature representations and more general non-linear transformations. The results and comparisons in Table 2 demonstrate the proposed models can obtain comparable accuracies, which show the effectiveness of combining local and global features through window-based and convolutional neural networks.",
        "sentences": [
            "We show the results of two of the best proposed parsers: third-order adding (o3-adding) and third-order perceptron (o3-perceptron) methods, and compare with the reported results of some previous work in Table 2.",
            "We compare with three categories of models: other Graph-based NN (neural network) models, traditional Graph-based Linear models and Transition-based NN models.",
            "For PTB, there have been several different dependency converters which lead to different sets of dependencies and we choose three of the most popular ones for more comprehensive comparisons.",
            "Since not all work report results on all of these dependencies, some of the entries might be not available.",
            "From the comparison, we see that the proposed parser has output competitive performance for different dependency conversion conventions and treebanks.",
            "Compared with traditional graph based linear models, neural models may benefit from better feature representations and more general non-linear transformations.",
            "The results and comparisons in Table 2 demonstrate the proposed models can obtain comparable accuracies, which show the effectiveness of combining local and global features through window-based and convolutional neural networks."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            1,
            2,
            1
        ],
        "header_mention": [
            [
                "Graph-NN:proposed",
                "o3-adding",
                "o3-perceptron"
            ],
            [
                "Graph-NN:others",
                "Graph-Linear",
                "Transition-NN"
            ],
            null,
            null,
            [
                "Graph-NN:proposed"
            ],
            [
                "Graph-NN:proposed",
                "Graph-Linear"
            ],
            [
                "Graph-NN:proposed"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_2",
        "paper_id": "P16-1131",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1134table_3",
        "description": "As Table 3 shows, external information improve the performance of grSemi-CRFs for both tasks. Compared to text chunking, we can find out that external information plays an extremely important role in NER, which coincides with the general idea that NER is a knowledge-intensive task (Ratinov and Roth, 2009). Another interesting thing is that, Brown clusters generated from NYT corpus performs better on the CONLL 2000 task while those generated from Reuters RCV1 dataset performs better on the CONLL 2003 task. The reason is that the CONLL 2000 dataset is the subset of Wall Street Journal (WSJ) part of the Penn Treebank II Corpus (Marcus et al., 1993) while the CONLL 2003 dataset is a subset of Reuters RCV1 dataset. Maybe the writing styles between NYT and WSJ are more similar than those between RCV1 and WSJ.",
        "sentences": [
            "As Table 3 shows, external information improve the performance of grSemi-CRFs for both tasks.",
            "Compared to text chunking, we can find out that external information plays an extremely important role in NER, which coincides with the general idea that NER is a knowledge-intensive task (Ratinov and Roth, 2009).",
            "Another interesting thing is that, Brown clusters generated from NYT corpus performs better on the CONLL 2000 task while those generated from Reuters RCV1 dataset performs better on the CONLL 2003 task.",
            "The reason is that the CONLL 2000 dataset is the subset of Wall Street Journal (WSJ) part of the Penn Treebank II Corpus (Marcus et al., 1993) while the CONLL 2003 dataset is a subset of Reuters RCV1 dataset.",
            "Maybe the writing styles between NYT and WSJ are more similar than those between RCV1 and WSJ."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            [
                "Input Features"
            ],
            [
                "Input Features"
            ],
            [
                "Brown(NYT)",
                "CONLL 2000",
                "Brown(RCV1)",
                "CONLL 2003"
            ],
            [
                "CONLL 2000",
                "CONLL 2003"
            ],
            [
                "CONLL 2000",
                "CONLL 2003"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_3",
        "paper_id": "P16-1134",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1134table_4",
        "description": "4.4.2 Impact of Vectorial Gating Coefficients. As Table 4 shows, a grSemi-CRF using vectorial gating coefficients (i.e., Eq. (7)) performs better than that using scalar gating coefficients (i.e., Eq. (6)), which provides evidences for the theoretical intuition that vectorial gating coefficients can make a detailed modeling of the combinations of segment-level latent features and thus performs better than scalar gating coefficients.",
        "sentences": [
            "4.4.2 Impact of Vectorial Gating Coefficients.",
            "As Table 4 shows, a grSemi-CRF using vectorial gating coefficients (i.e., Eq. (7)) performs better than that using scalar gating coefficients (i.e., Eq. (6)), which provides evidences for the theoretical intuition that vectorial gating coefficients can make a detailed modeling of the combinations of segment-level latent features and thus performs better than scalar gating coefficients."
        ],
        "class_sentence": [
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "Gating Coefficients",
                "Scalars",
                "Vectors"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_4",
        "paper_id": "P16-1134",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1137table_4",
        "description": "Table 4 shows a runtime comparison of the losses and sampling strategies. We find random sampling to be orders of magnitude faster than the others while also performing the best.",
        "sentences": [
            "Table 4 shows a runtime comparison of the losses and sampling strategies.",
            "We find random sampling to be orders of magnitude faster than the others while also performing the best."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "random"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_4",
        "paper_id": "P16-1137",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1140table_4",
        "description": "To prove that word form could provides informative and explicit cues for grammatical functions, we train another shuffled character-based word representation, which means that the autoencoder inputs shuffled letters and outputs the shuffled letters again. We use the hidden layer of the shuffled autoencoder as the representation for each word. The result in Table 4 shows that now the character-based model cannot perform as well as the original character-based autoencoder representation does, which again proves that the order of the word form is necessary for learning the grammatical function of a word.",
        "sentences": [
            "To prove that word form could provides informative and explicit cues for grammatical functions, we train another shuffled character-based word representation, which means that the autoencoder inputs shuffled letters and outputs the shuffled letters again.",
            "We use the hidden layer of the shuffled autoencoder as the representation for each word.",
            "The result in Table 4 shows that now the character-based model cannot perform as well as the original character-based autoencoder representation does, which again proves that the order of the word form is necessary for learning the grammatical function of a word."
        ],
        "class_sentence": [
            2,
            2,
            1
        ],
        "header_mention": [
            [
                "Shuf."
            ],
            [
                "Shuf."
            ],
            [
                "Raw",
                "Shuf."
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_4",
        "paper_id": "P16-1140",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1140table_5",
        "description": "To explain the behaviour of AE, we calculate the correlation between the bigram character frequency in the words of the training language (e.g.Finnish) and the bigram character frequency in the words of the testing language (e.g. English). Table 5 reveals that phonological knowledge can be transferred if two languages share similar bigram and trigram character frequency distribution. For example, Finnish and English are both Indo-European language.",
        "sentences": [
            "To explain the behaviour of AE, we calculate the correlation between the bigram character frequency in the words of the training language (e.g.Finnish) and the bigram character frequency in the words of the testing language (e.g. English).",
            "Table 5 reveals that phonological knowledge can be transferred if two languages share similar bigram and trigram character frequency distribution.",
            "For example, Finnish and English are both Indo-European language."
        ],
        "class_sentence": [
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "Bigram token overlap.",
                "Trigram token overlap."
            ],
            [
                "Bigram token overlap.",
                "Trigram token overlap."
            ],
            [
                "Finnish",
                "en",
                "shuf en"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_5",
        "paper_id": "P16-1140",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1148table_4",
        "description": "We demonstrate our emotion model prediction quality using 10-fold c.v. on our hashtag emotion dataset and compare it to other existing datasets in Table 4. Our results significantly outperform the existing approaches and are comparable with the state-of-the-art system for Twitter sentiment classification (Mohammad et al., 2013; Zhu et al., 2014).",
        "sentences": [
            "We demonstrate our emotion model prediction quality using 10-fold c.v. on our hashtag emotion dataset and compare it to other existing datasets in Table 4.",
            "Our results significantly outperform the existing approaches and are comparable with the state-of-the-art system for Twitter sentiment classification (Mohammad et al., 2013; Zhu et al., 2014)."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "This work",
                "Wang (2012)",
                "Roberts (2012)",
                "Qadir (2013)",
                "Mohammad (2014)"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_4",
        "paper_id": "P16-1148",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1150table_4",
        "description": "Without any modifications, we use the same SVM and features as described in Section 4.1. Regarding the BLSTM, we only replace the output layer with a linear activation function and optimize mean absolute error loss. Table 4 shows that SVM outperforms BLSTM.",
        "sentences": [
            "Without any modifications, we use the same SVM and features as described in Section 4.1.",
            "Regarding the BLSTM, we only replace the output layer with a linear activation function and optimize mean absolute error loss.",
            "Table 4 shows that SVM outperforms BLSTM."
        ],
        "class_sentence": [
            2,
            2,
            1
        ],
        "header_mention": [
            [
                "SVM"
            ],
            [
                "BLSTM"
            ],
            [
                "SVM",
                "BLSTM"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_4",
        "paper_id": "P16-1150",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1154table_3",
        "description": "It is clear from Table 3 that COPYNET beats the competitor models with big margin. Hu et al.(2015) reports that the performance of a word-based model is inferior to a character-based one. One possible explanation is that a word-based model, even with a much larger vocabulary (50000 words in Hu et al. (2015)), still has a large proportion of OOVs due to the large number of entity names in the summary data and the mistakes in word segmentation. COPYNET, with its ability to handle the OOV words with the copying mechanism, performs however slightly better with the word-based variant.",
        "sentences": [
            "It is clear from Table 3 that COPYNET beats the competitor models with big margin.",
            "Hu et al.(2015) reports that the performance of a word-based model is inferior to a character-based one.",
            "One possible explanation is that a word-based model, even with a much larger vocabulary (50000 words in Hu et al. (2015)), still has a large proportion of OOVs due to the large number of entity names in the summary data and the mistakes in word segmentation.",
            "COPYNET, with its ability to handle the OOV words with the copying mechanism, performs however slightly better with the word-based variant."
        ],
        "class_sentence": [
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            [
                "COPYNET +C",
                "COPYNET +W"
            ],
            null,
            null,
            [
                "COPYNET +C",
                "COPYNET +W"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "P16-1154",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1159table_5",
        "description": "Table 5 shows the results of subjective evaluation. The two human evaluators made close judgements: around 54% of MLE translations are worse than MRE, 23% are equal, and 23% are better.",
        "sentences": [
            "Table 5 shows the results of subjective evaluation.",
            "The two human evaluators made close judgements: around 54% of MLE translations are worse than MRE, 23% are equal, and 23% are better."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "MLE < MRT",
                "MLE = MRT",
                "MLE > MRT"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_5",
        "paper_id": "P16-1159",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1159table_7",
        "description": "Table 7 shows the results on English-French translation. We list existing end-to-end NMT systems that are comparable to our system. All these systems use the same subset of the WMT 2014 training corpus and adopt MLE as the training criterion. They differ in network architectures and vocabulary sizes. Our RNNSEARCH-MLE system achieves a BLEU score comparable to that of Jean et al. (2015). RNNSEARCH-MRT achieves the highest BLEU score in this setting even with a vocabulary size smaller than Luong et al. (2015b) (2014). Note that our approach does not assume specific architectures and can in principle be applied to any NMT systems.",
        "sentences": [
            "Table 7 shows the results on English-French translation.",
            "We list existing end-to-end NMT systems that are comparable to our system.",
            "All these systems use the same subset of the WMT 2014 training corpus and adopt MLE as the training criterion.",
            "They differ in network architectures and vocabulary sizes.",
            "Our RNNSEARCH-MLE system achieves a BLEU score comparable to that of Jean et al. (2015).",
            "RNNSEARCH-MRT achieves the highest BLEU score in this setting even with a vocabulary size smaller than Luong et al. (2015b) (2014).",
            "Note that our approach does not assume specific architectures and can in principle be applied to any NMT systems."
        ],
        "class_sentence": [
            1,
            2,
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "Vocab"
            ],
            [
                "gated RNN with search",
                "Training",
                "MLE",
                "Jean et al. (2015)",
                "BLEU"
            ],
            [
                "gated RNN with search",
                "Training",
                "MRT",
                "BLEU",
                "Vocab",
                "Luong et al. (2015b)"
            ],
            [
                "this work"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_7",
        "paper_id": "P16-1159",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1161table_2",
        "description": "Table 2 shows the obtained results. Statistically significant differences (alpha=0.01) are marked in bold. The source-context model does not help in the small data setting but brings a substantial improvement of 0.7-0.8 BLEU points for the medium and full data settings, which is an encouraging result. Target-side context information allows our model to push the translation quality further: even for the small data setting, it brings a substantial improvement of 0.5 BLEU points and the gain remains significant as the data size increases. Even in the full data setting, target-side features improve the score by roughly 0.2 BLEU points. Our results demonstrate that feature-rich models scale to large data size both in terms of technical feasibility and of translation quality improvements. Target side information seems consistently beneficial, adding further 0.2-0.5 BLEU points on top of the source-context model.",
        "sentences": [
            "Table 2 shows the obtained results.",
            "Statistically significant differences (alpha=0.01) are marked in bold.",
            "The source-context model does not help in the small data setting but brings a substantial improvement of 0.7-0.8 BLEU points for the medium and full data settings, which is an encouraging result.",
            "Target-side context information allows our model to push the translation quality further: even for the small data setting, it brings a substantial improvement of 0.5 BLEU points and the gain remains significant as the data size increases.",
            "Even in the full data setting, target-side features improve the score by roughly 0.2 BLEU points.",
            "Our results demonstrate that feature-rich models scale to large data size both in terms of technical feasibility and of translation quality improvements.",
            "Target side information seems consistently beneficial, adding further 0.2-0.5 BLEU points on top of the source-context model."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "+source",
                "small",
                "medium",
                "full"
            ],
            [
                "+target",
                "small"
            ],
            [
                "+target",
                "full"
            ],
            [
                "+target",
                "+source"
            ],
            [
                "+target",
                "+source"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_2",
        "paper_id": "P16-1161",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1168table_2",
        "description": "Table 2 shows the evaluation metrics for various settings of cross-lingual transfer learning. All values were calculated for Japanese captions generated for test set images. Our proposed model is labeled \u201ctransfer\u201d. As you can see, it outperformed the other two models for every metric. In particular, the CIDEr-D score was about 4% higher than that for the monolingual baseline. The performance of a model trained using the English and Japanese corpora alternately is shown on the line label \u201calternate\u201d. Surprisingly, this model had lower performance than the baseline model.",
        "sentences": [
            "Table 2 shows the evaluation metrics for various settings of cross-lingual transfer learning.",
            "All values were calculated for Japanese captions generated for test set images.",
            "Our proposed model is labeled \u201ctransfer\u201d.",
            "As you can see, it outperformed the other two models for every metric.",
            "In particular, the CIDEr-D score was about 4% higher than that for the monolingual baseline.",
            "The performance of a model trained using the English and Japanese corpora alternately is shown on the line label \u201calternate\u201d.",
            "Surprisingly, this model had lower performance than the baseline model."
        ],
        "class_sentence": [
            1,
            2,
            2,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "transfer"
            ],
            [
                "transfer"
            ],
            [
                "transfer",
                "CIDEr-D",
                "monolingual"
            ],
            [
                "alternate"
            ],
            [
                "alternate",
                "monolingual"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_2",
        "paper_id": "P16-1168",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1170table_3",
        "description": "Table 3 shows the results of testing the state-of-the-art MSR captioning system on the CaptionsBing-5000 dataset as compared to the MS COCO dataset, measured by the standard BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) metrics. The wide gap in the results further confirms that indeed the V QGBing-5000 dataset covers a new class of images. We hope the availability of this new dataset will encourage including more diverse domains for image captioning.",
        "sentences": [
            "Table 3 shows the results of testing the state-of-the-art MSR captioning system on the CaptionsBing-5000 dataset as compared to the MS COCO dataset, measured by the standard BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) metrics.",
            "The wide gap in the results further confirms that indeed the V QGBing-5000 dataset covers a new class of images.",
            "We hope the availability of this new dataset will encourage including more diverse domains for image captioning."
        ],
        "class_sentence": [
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "MS COCO",
                "Bing",
                "BLEU",
                "METEOR"
            ],
            [
                "Bing"
            ],
            null
        ],
        "n_sentence": 3.0,
        "table_id": "table_3",
        "paper_id": "P16-1170",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1173table_5",
        "description": "Table 5 shows the results. To our surprise, both parsers perform very well on the learner corpora despite the fact that it contains a number of grammatical errors and also syntactic tags that are not defined in PTB-II. Their performance is comparable to, or even better than, that on the Penn Treebank (reported in Petrov (2010)).",
        "sentences": [
            "Table 5 shows the results.",
            "To our surprise, both parsers perform very well on the learner corpora despite the fact that it contains a number of grammatical errors and also syntactic tags that are not defined in PTB-II.",
            "Their performance is comparable to, or even better than, that on the Penn Treebank (reported in Petrov (2010))."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Stanford",
                "Charniak-Johnson"
            ],
            [
                "Stanford",
                "Charniak-Johnson",
                "Petrov (2010)"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_5",
        "paper_id": "P16-1173",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1178table_6",
        "description": "As a reference for future research with the proposed corpus, we trained GLM regression models to predict each aspect individually. Table 6 presents the RMSE for each aspect, for two different sets of feature: a standard BoW and the shallow features described previously, as well as the BaselineM. Despite the simplicity of the features, we can see that the aspects can be inferred from the articles. In particular, the model trained on the BoW features achieves an RMSE that is very close to that of the BaselineM, whereas  model trained on the shallow features outperforms all other models.",
        "sentences": [
            "As a reference for future research with the proposed corpus, we trained GLM regression models to predict each aspect individually.",
            "Table 6 presents the RMSE for each aspect, for two different sets of feature: a standard BoW and the shallow features described previously, as well as the BaselineM.",
            "Despite the simplicity of the features, we can see that the aspects can be inferred from the articles.",
            "In particular, the model trained on the BoW features achieves an RMSE that is very close to that of the BaselineM, whereas  model trained on the shallow features outperforms all other models."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "BoW",
                "Shallow",
                "BaselineM"
            ],
            null,
            [
                "BoW",
                "BaselineM",
                "Shallow"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_6",
        "paper_id": "P16-1178",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1181table_3",
        "description": "Table 3 gives the performance of the sentence boundary detectors on test sets. On WSJ all systems are close to 98 and this high number once again affirms that the task of segmenting newspaper-quality text does not leave much space for improvement. Although the parsing models outperform MARMOT, the improvements in F1 are not significant. In contrast, all systems fare considerably worse on WSJ* which confirms that the orthographic clues in newspaper text suffice to segment the sentences properly. Although NOSYNTAX outperforms MARMOT, the difference is not significant. However, when real syntax is used (JOINT) we see a huge improvement in F1, 10 points absolute, which is significantly better than both NOSYNTAX and MARMOT. On Switchboard MARMOT is much lower and both parsing models outperform it significantly. Surprisingly the NOSYNTAX system achieves a very high result beating the baseline significantly by almost 4.5 points. The usage of syntax in the JOINT model raises this gain to 4.8 points.",
        "sentences": [
            "Table 3 gives the performance of the sentence boundary detectors on test sets.",
            "On WSJ all systems are close to 98 and this high number once again affirms that the task of segmenting newspaper-quality text does not leave much space for improvement.",
            "Although the parsing models outperform MARMOT, the improvements in F1 are not significant.",
            "In contrast, all systems fare considerably worse on WSJ* which confirms that the orthographic clues in newspaper text suffice to segment the sentences properly.",
            "Although NOSYNTAX outperforms MARMOT, the difference is not significant.",
            "However, when real syntax is used (JOINT) we see a huge improvement in F1, 10 points absolute, which is significantly better than both NOSYNTAX and MARMOT.",
            "On Switchboard MARMOT is much lower and both parsing models outperform it significantly.",
            "Surprisingly the NOSYNTAX system achieves a very high result beating the baseline significantly by almost 4.5 points.",
            "The usage of syntax in the JOINT model raises this gain to 4.8 points."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "WSJ"
            ],
            [
                "MARMOT"
            ],
            [
                "WSJ*"
            ],
            [
                "NOSYNTAX",
                "MARMOT"
            ],
            [
                "JOINT",
                "NOSYNTAX",
                "MARMOT"
            ],
            [
                "MARMOT",
                "Switchboard"
            ],
            [
                "NOSYNTAX",
                "Switchboard"
            ],
            [
                "JOINT",
                "Switchboard"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_3",
        "paper_id": "P16-1181",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1188table_2",
        "description": "Table 2 shows the results on the RST corpus. Our system is roughly comparable to Tree Knapsack here, and we note that none of the differences in the table are statistically significant. We also observed significant variation between multiple runs on this corpus, with scores changing by 1-2 ROUGE points for slightly different system variants.",
        "sentences": [
            "Table 2 shows the results on the RST corpus.",
            "Our system is roughly comparable to Tree Knapsack here, and we note that none of the differences in the table are statistically significant.",
            "We also observed significant variation between multiple runs on this corpus, with scores changing by 1-2 ROUGE points for slightly different system variants."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Full",
                "Tree Knapsack"
            ],
            [
                "Full",
                "Tree Knapsack",
                "First k words"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_2",
        "paper_id": "P16-1188",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1191table_6",
        "description": "5.2 Supersense Prediction. We evaluate our system on the same Twitter dataset with provided training and development (Tw-R-dev) set and two test sets: Tw-R-eval, reported by Johannsen et al. as RITTER, and Tw-J-eval, reported by Johannsen et al. as INHOUSE. Our results are shown in table 6 and compared to results reported in previous work by Johannsen et al. (2014), with two additional baselines: The SemCor system of Ciaramita and Altun (2006) and the most frequent sense. Our system achieves comparable performance to the best previously used supervised systems, without using any explicit gazetteers.",
        "sentences": [
            "5.2 Supersense Prediction.",
            "We evaluate our system on the same Twitter dataset with provided training and development (Tw-R-dev) set and two test sets: Tw-R-eval, reported by Johannsen et al. as RITTER, and Tw-J-eval, reported by Johannsen et al. as INHOUSE.",
            "Our results are shown in table 6 and compared to results reported in previous work by Johannsen et al. (2014), with two additional baselines: The SemCor system of Ciaramita and Altun (2006) and the most frequent sense.",
            "Our system achieves comparable performance to the best previously used supervised systems, without using any explicit gazetteers."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Tw-R-dev",
                "Tw-R-eval",
                "Tw-J-eval"
            ],
            [
                "Ours Semcor",
                "Ours Twitter",
                "HMM (Johannsen et al. 2014)",
                "(Ciaramita and Altun 2006)\u00e2\u20ac\u00a0",
                "Most frequent sense"
            ],
            [
                "Ours Semcor",
                "Ours Twitter"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_6",
        "paper_id": "P16-1191",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1195table_3",
        "description": "Table 3 shows automatic evaluation metrics for our model and baselines. CODE-NN outperforms all the other methods in terms of METEOR and BLEU-4 score. We attribute this to its ability to perform better content selection, focusing on the more salient parts of the code by using its attention mechanism jointly with its LSTM memory cells. The neural models have better performance on C# than SQL. This is in part because, unlike SQL, C# code contains informative intermediate variable names that are directly related to the objective of the code. On the other hand, SQL is more challenging in that it only has a handful of keywords and functions, and summarization models need to rely on other structural aspects of the code.",
        "sentences": [
            "Table 3 shows automatic evaluation metrics for our model and baselines.",
            "CODE-NN outperforms all the other methods in terms of METEOR and BLEU-4 score.",
            "We attribute this to its ability to perform better content selection, focusing on the more salient parts of the code by using its attention mechanism jointly with its LSTM memory cells.",
            "The neural models have better performance on C# than SQL.",
            "This is in part because, unlike SQL, C# code contains informative intermediate variable names that are directly related to the objective of the code.",
            "On the other hand, SQL is more challenging in that it only has a handful of keywords and functions, and summarization models need to rely on other structural aspects of the code."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "CODE-NN",
                "BLEU-4",
                "METEOR"
            ],
            [
                "CODE-NN"
            ],
            [
                "CODE-NN",
                "C#",
                "SQL"
            ],
            [
                "C#",
                "SQL"
            ],
            [
                "SQL"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_3",
        "paper_id": "P16-1195",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1201table_6",
        "description": "Table 6 presents the results where we measure precision, recall and F1. Compared with ACE-ANN-FN, events from SF and RF hurt the performance. As analyzed in previous section, SF and RF yield quite a few false events, which dramatically hurt the accuracy. Moreover, ACE-SL-FN obtains a score of 70.3% in F1 measure, which outperforms ACE-ANN-FN. This result illustrates the effectiveness of our \u201csame LU\u201d hypothesis. Finally and most importantly, consistent with the results of manual evaluations, ACE-PSL-FN performs the best, which further proves the effectiveness of our proposed approach for event detection in FN.",
        "sentences": [
            "Table 6 presents the results where we measure precision, recall and F1.",
            "Compared with ACE-ANN-FN, events from SF and RF hurt the performance.",
            "As analyzed in previous section, SF and RF yield quite a few false events, which dramatically hurt the accuracy.",
            "Moreover, ACE-SL-FN obtains a score of 70.3% in F1 measure, which outperforms ACE-ANN-FN.",
            "This result illustrates the effectiveness of our \u201csame LU\u201d hypothesis.",
            "Finally and most importantly, consistent with the results of manual evaluations, ACE-PSL-FN performs the best, which further proves the effectiveness of our proposed approach for event detection in FN."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            2,
            1
        ],
        "header_mention": [
            [
                "Pre",
                "Rec",
                "F1"
            ],
            [
                "ACE-ANN-FN",
                "ACE-SF-FN",
                "ACE-RF-FN"
            ],
            [
                "ACE-SF-FN",
                "ACE-RF-FN"
            ],
            [
                "ACE-SL-FN",
                "ACE-ANN-FN"
            ],
            null,
            [
                "ACE-PSL-FN"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_6",
        "paper_id": "P16-1201",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1206table_2",
        "description": "Table 2 shows the different evaluation results with standard metric and our balanced metric. We can see that the proposed evaluation metric generally gives lower and more distinguishable score, compared with the standard metric.",
        "sentences": [
            "Table 2 shows the different evaluation results with standard metric and our balanced metric.",
            "We can see that the proposed evaluation metric generally gives lower and more distinguishable score, compared with the standard metric."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "pb",
                "rb",
                "fb"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_2",
        "paper_id": "P16-1206",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1209table_3",
        "description": "Table 3 shows the results obtained by different RNN models with only character level word embedding features. For the task A (Disease name recognition) Bi-LSTM and NN models gave competitive performance on the test set, while Bi-RNN and Bi-GRU did not perform so well. On the other hand for the task B, there is 2.08% \u00e2\u02c6\u2019 3.8% improved performance (F1-score) shown by RNN models over the NN model again on the test set. Bi-LSTM model obtained F1-score of 59.78% while NN model gave 57.56%. As discussed earlier, task B is difficult than task A as disease category is more likely to be influenced by the words falling outside the context window considered in window based methods. This could be reason for RNN models to perform well over the NN model. This hypothesis will be stronger if we observe similar pattern in our other experiments.",
        "sentences": [
            "Table 3 shows the results obtained by different RNN models with only character level word embedding features.",
            "For the task A (Disease name recognition) Bi-LSTM and NN models gave competitive performance on the test set, while Bi-RNN and Bi-GRU did not perform so well.",
            "On the other hand for the task B, there is 2.08% \u00e2\u02c6\u2019 3.8% improved performance (F1-score) shown by RNN models over the NN model again on the test set.",
            "Bi-LSTM model obtained F1-score of 59.78% while NN model gave 57.56%.",
            "As discussed earlier, task B is difficult than task A as disease category is more likely to be influenced by the words falling outside the context window considered in window based methods.",
            "This could be reason for RNN models to perform well over the NN model.",
            "This hypothesis will be stronger if we observe similar pattern in our other experiments."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "Task A",
                "Bi-LSTM+CE",
                "NN+CE"
            ],
            [
                "Task B",
                "Bi-RNN+CE",
                "NN+CE"
            ],
            [
                "Task B",
                "Bi-LSTM+CE",
                "NN+CE"
            ],
            [
                "Task B",
                "Task A"
            ],
            [
                "Bi-RNN+CE"
            ],
            null
        ],
        "n_sentence": 7.0,
        "table_id": "table_3",
        "paper_id": "P16-1209",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1218table_3",
        "description": "Table 3 lists the performances of our model as well as previous state-of-the-art systems on on PennYM, Penn-SD and CTB5. We compare to conventional state-of-the-art graph-based model (Zhang and McDonald, 2014), conventional state-of-theart transition-based model using beam search (Zhang and Nivre, 2011), transition-based model combining graph-based approach (Bernd Bohnet, 2012) , transition-based neural network model using stack LSTM (Dyer et al., 2015) and transitionbased neural network model using beam search (Weiss et al., 2015). Overall, our model achieves competitive accuracy on all three datasets. Although our model is slightly lower in accuracy than unlimited-order double beam model (Zhang and McDonald, 2014) on Penn-YM and CTB5, our model outperforms their model on Penn-SD. It seems that our model performs better on data sets with larger label sets, given the number of labels used in Penn-SD data set is almost four times more than Penn-YM and CTB5 data sets.",
        "sentences": [
            "Table 3 lists the performances of our model as well as previous state-of-the-art systems on on PennYM, Penn-SD and CTB5.",
            "We compare to conventional state-of-the-art graph-based model (Zhang and McDonald, 2014), conventional state-of-theart transition-based model using beam search (Zhang and Nivre, 2011), transition-based model combining graph-based approach (Bernd Bohnet, 2012) , transition-based neural network model using stack LSTM (Dyer et al., 2015) and transitionbased neural network model using beam search (Weiss et al., 2015).",
            "Overall, our model achieves competitive accuracy on all three datasets.",
            "Although our model is slightly lower in accuracy than unlimited-order double beam model (Zhang and McDonald, 2014) on Penn-YM and CTB5, our model outperforms their model on Penn-SD.",
            "It seems that our model performs better on data sets with larger label sets, given the number of labels used in Penn-SD data set is almost four times more than Penn-YM and CTB5 data sets."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "Penn-YM",
                "Penn-SD",
                "CTB5"
            ],
            [
                "(Zhang and McDonald 2014)",
                "(Zhang and Nivre 2011)",
                "(Bernd Bohnet 2012)",
                "(Dyer et al. 2015)",
                "(Weiss et al. 2015)"
            ],
            [
                "Our basic model + segment",
                "Penn-YM",
                "Penn-SD",
                "CTB5"
            ],
            [
                "Our basic model + segment",
                "(Zhang and McDonald 2014)",
                "Penn-YM",
                "CTB5",
                "Penn-SD"
            ],
            [
                "Our basic model + segment",
                "Penn-SD",
                "Penn-YM",
                "CTB5"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_3",
        "paper_id": "P16-1218",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1218table_4",
        "description": "To make comparison as fair as possible, we let two models have almost the same number parameters. Table 4 lists the UAS of two methods on test set. As we can see, LSTM-Minus shows better performance because our method further incorporates more sentence-level information into our model.",
        "sentences": [
            "To make comparison as fair as possible, we let two models have almost the same number parameters.",
            "Table 4 lists the UAS of two methods on test set.",
            "As we can see, LSTM-Minus shows better performance because our method further incorporates more sentence-level information into our model."
        ],
        "class_sentence": [
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Method"
            ],
            [
                "LSTM-Minus"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_4",
        "paper_id": "P16-1218",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1220table_1",
        "description": "5.3.3 Impact of the Inference on Unstructured Data. As shown in Table 1, when structured inference is augmented with the unstructured inference, we see an improvement of 2.9% (from 44.1% to 47.0%). And when Structured + Joint uses unstructured inference, the performance boosts by 6.2% (from 47.1% to 53.3%) achieving a new state-of-the-art result. For the latter, we manually analyzed the cases in which unstructured inference helps.",
        "sentences": [
            "5.3.3 Impact of the Inference on Unstructured Data.",
            "As shown in Table 1, when structured inference is augmented with the unstructured inference, we see an improvement of 2.9% (from 44.1% to 47.0%).",
            "And when Structured + Joint uses unstructured inference, the performance boosts by 6.2% (from 47.1% to 53.3%) achieving a new state-of-the-art result.",
            "For the latter, we manually analyzed the cases in which unstructured inference helps."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Structured",
                "Structured + Unstructured"
            ],
            [
                "Structured + Joint + Unstructured"
            ],
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_1",
        "paper_id": "P16-1220",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1222table_2",
        "description": "5.4 Performance. In Table 2 we show the overall performance of our proposed NCM system compared with strong competing methods as described above. We see that, for perplexity, BLEU and human judgments, our system outperforms other baseline models.",
        "sentences": [
            "5.4 Performance.",
            "In Table 2 we show the overall performance of our proposed NCM system compared with strong competing methods as described above.",
            "We see that, for perplexity, BLEU and human judgments, our system outperforms other baseline models."
        ],
        "class_sentence": [
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Neural Couplet Machine (NCM)",
                "Perplexity",
                "BLEU",
                "Human Evaluation (Syntactic)",
                "Human Evaluation (Semantic)",
                "Human Evaluation (Overall)"
            ],
            [
                "Neural Couplet Machine (NCM)"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_2",
        "paper_id": "P16-1222",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1223table_2",
        "description": "Table 2 presents our main results. The conventional feature-based classifier obtains 67.9% accuracy on the CNN test set. Not only does this significantly outperform any of the symbolic approaches reported in (Hermann et al., 2015), it also outperforms all the neural network systems from their paper and the best single-system result reported so far from (Hill et al., 2016). This suggests that the task might not be as difficult as suggested, and a simple feature set can cover many of the cases. More dramatically, our single-model neural network surpasses the previous results by a large margin (over 5%), pushing up the state-of-the-art accuracies to 72.4% and 75.8% respectively. Due to resource constraints, we have not had a chance to investigate ensembles of models, which generally can bring further gains, as demonstrated in (Hill et al., 2016) and many other papers.",
        "sentences": [
            "Table 2 presents our main results.",
            "The conventional feature-based classifier obtains 67.9% accuracy on the CNN test set.",
            "Not only does this significantly outperform any of the symbolic approaches reported in (Hermann et al., 2015), it also outperforms all the neural network systems from their paper and the best single-system result reported so far from (Hill et al., 2016).",
            "This suggests that the task might not be as difficult as suggested, and a simple feature set can cover many of the cases.",
            "More dramatically, our single-model neural network surpasses the previous results by a large margin (over 5%), pushing up the state-of-the-art accuracies to 72.4% and 75.8% respectively.",
            "Due to resource constraints, we have not had a chance to investigate ensembles of models, which generally can bring further gains, as demonstrated in (Hill et al., 2016) and many other papers."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Ours: Classifier",
                "CNN",
                "Test"
            ],
            [
                "Ours: Classifier",
                "CNN",
                "Test"
            ],
            null,
            [
                "Ours: Neural net",
                "CNN",
                "Test",
                "Daily Mail"
            ],
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_2",
        "paper_id": "P16-1223",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1226table_4",
        "description": "Table 4 displays performance scores of HypeNET and the baselines. HypeNET Path-based is our path-based recurrent neural network model (Section 3.1). Comparing the path-based methods shows that generalizing paths improves recall while maintaining similar levels of precision, reassessing the behavior found in Nakashole et al. (2012). HypeNET Path-based outperforms both path-based baselines by a significant improvement in recall and with slightly lower precision. The recall boost is due to better path generalization, as demonstrated in Section 7.1.",
        "sentences": [
            "Table 4 displays performance scores of HypeNET and the baselines.",
            "HypeNET Path-based is our path-based recurrent neural network model (Section 3.1).",
            "Comparing the path-based methods shows that generalizing paths improves recall while maintaining similar levels of precision, reassessing the behavior found in Nakashole et al. (2012).",
            "HypeNET Path-based outperforms both path-based baselines by a significant improvement in recall and with slightly lower precision.",
            "The recall boost is due to better path generalization, as demonstrated in Section 7.1."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "HypeNET Path-based"
            ],
            [
                "HypeNET Path-based"
            ],
            [
                "HypeNET Path-based"
            ],
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_4",
        "paper_id": "P16-1226",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1228table_2",
        "description": "To further investigate the effectiveness of our framework in integrating structured rule knowledge, we compare with an extensive array of other possible integration approaches. Table 2 lists these methods and their performance on the SST2 task. We see that: 1) Although all methods lead to different degrees of improvement, our framework outperforms all other competitors with a large margin. 2) In particular, compared to the pipelined method in Row 6 which is in analogous to the structure compilation work (Liang et al., 2008), our iterative distillation (section 3.2) provides better performance. Another advantage of our method is that we only train one set of neural parameters, as opposed to two separate sets as in the pipelined approach. 3) The distilled student network \u201c-Rule-p\u201d achieves much superior accuracy compared to the base CNN, as well as \u201c-project\u201d and \u201c-opt-project\u201d which explicitly project CNN to the rule-constrained subspace. This validates that our distillation procedure transfers the structured knowledge into the neural parameters effectively. The inferior accuracy of \u201c-opt-project\u201d can be partially attributed to the poor performance of its neural network part which achieves only 85.1% accuracy and leads to inaccurate evaluation of the \u201c-but-clause\u201d rule in Eq.(5).",
        "sentences": [
            "To further investigate the effectiveness of our framework in integrating structured rule knowledge, we compare with an extensive array of other possible integration approaches.",
            "Table 2 lists these methods and their performance on the SST2 task.",
            "We see that: 1) Although all methods lead to different degrees of improvement, our framework outperforms all other competitors with a large margin.",
            "2) In particular, compared to the pipelined method in Row 6 which is in analogous to the structure compilation work (Liang et al., 2008), our iterative distillation (section 3.2) provides better performance.",
            "Another advantage of our method is that we only train one set of neural parameters, as opposed to two separate sets as in the pipelined approach.",
            "3) The distilled student network \u201c-Rule-p\u201d achieves much superior accuracy compared to the base CNN, as well as \u201c-project\u201d and \u201c-opt-project\u201d which explicitly project CNN to the rule-constrained subspace.",
            "This validates that our distillation procedure transfers the structured knowledge into the neural parameters effectively.",
            "The inferior accuracy of \u201c-opt-project\u201d can be partially attributed to the poor performance of its neural network part which achieves only 85.1% accuracy and leads to inaccurate evaluation of the \u201c-but-clause\u201d rule in Eq.(5)."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "-Rule-p",
                "-Rule-q"
            ],
            [
                "-Rule-p",
                "-Rule-q",
                "-pipeline"
            ],
            [
                "-Rule-p",
                "-Rule-q"
            ],
            [
                "-Rule-p",
                "-project",
                "-opt-project"
            ],
            [
                "-Rule-p"
            ],
            [
                "-opt-project",
                "-but-clause"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_2",
        "paper_id": "P16-1228",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1230table_2",
        "description": "Here we investigate further the accuracy of the model in predicting the subjective success rate. An evaluation of the on-line GP reward model between 1 and 850 training dialogues is presented in Table 2. Since three reward models were learnt each with 850 dialogues, there were a total of 2550 training dialogues. Of these, the models queried the user for feedback a total of 454 times, leaving 2096 dialogues for which learning relied on the reward model\u00e2\u20ac\u2122s prediction. The results shown in the table are thus the average over 2096 dialogues. As can be seen, there was a significant imbalance between success and fail labels since the policy was improving along with the training dialogues. This lowered the recall on failed dialogue prediction as the model was biased to data with positive labels. Nevertheless, its precision scores well. On the other hand, the successful dialogues were accurately predicted by the proposed model.",
        "sentences": [
            "Here we investigate further the accuracy of the model in predicting the subjective success rate.",
            "An evaluation of the on-line GP reward model between 1 and 850 training dialogues is presented in Table 2.",
            "Since three reward models were learnt each with 850 dialogues, there were a total of 2550 training dialogues.",
            "Of these, the models queried the user for feedback a total of 454 times, leaving 2096 dialogues for which learning relied on the reward model\u00e2\u20ac\u2122s prediction.",
            "The results shown in the table are thus the average over 2096 dialogues.",
            "As can be seen, there was a significant imbalance between success and fail labels since the policy was improving along with the training dialogues.",
            "This lowered the recall on failed dialogue prediction as the model was biased to data with positive labels.",
            "Nevertheless, its precision scores well.",
            "On the other hand, the successful dialogues were accurately predicted by the proposed model."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "Total",
                "Number"
            ],
            [
                "Total",
                "Number"
            ],
            [
                "Fail",
                "Suc.",
                "Number"
            ],
            [
                "Recall",
                "Fail",
                "Suc."
            ],
            [
                "Prec.",
                "Fail",
                "Suc."
            ],
            [
                "Suc.",
                "F-measure"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_2",
        "paper_id": "P16-1230",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1231table_1",
        "description": "In Table 1 we compare our model to a linear CRF and to the compositional character-to-word LSTM model of Ling et al. (2015). The CRF is a first-order linear model with exact inference and the same emission features as our model. It additionally also has transition features of the word, cluster and character n-gram up to length 3 on both endpoints of the transition. The results for Ling et al. (2015) were solicited from the authors. Our local model already compares favorably against these methods on average. Using beam search with a locally normalized model does not help, but with global normalization it leads to a 7% reduction in relative error, empirically demonstrating the effect of label bias. The set of character ngrams feature is very important, increasing average accuracy on the CoNLL 09 datasets by about 0.5% absolute. This shows that character-level modeling can also be done with a simple feed-forward network without recurrence.",
        "sentences": [
            "In Table 1 we compare our model to a linear CRF and to the compositional character-to-word LSTM model of Ling et al. (2015).",
            "The CRF is a first-order linear model with exact inference and the same emission features as our model.",
            "It additionally also has transition features of the word, cluster and character n-gram up to length 3 on both endpoints of the transition.",
            "The results for Ling et al. (2015) were solicited from the authors.",
            "Our local model already compares favorably against these methods on average.",
            "Using beam search with a locally normalized model does not help, but with global normalization it leads to a 7% reduction in relative error, empirically demonstrating the effect of label bias.",
            "The set of character ngrams feature is very important, increasing average accuracy on the CoNLL 09 datasets by about 0.5% absolute.",
            "This shows that character-level modeling can also be done with a simple feed-forward network without recurrence."
        ],
        "class_sentence": [
            1,
            2,
            2,
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "Our Local (B=1)",
                "Our Local (B=8)",
                "Our Global (B=8)",
                "Linear CRF",
                "Ling et al. (2015)"
            ],
            [
                "Linear CRF"
            ],
            [
                "Linear CRF"
            ],
            [
                "Ling et al. (2015)"
            ],
            [
                "Our Local (B=1)",
                "Our Local (B=8)",
                "Avg"
            ],
            [
                "Our Local (B=8)"
            ],
            [
                "CoNLL 09",
                "Our Global (B=8)"
            ],
            [
                "Our Global (B=8)"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_1",
        "paper_id": "P16-1231",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1231table_4",
        "description": "Table 4 shows our sentence compression results. Our globally normalized model again significantly outperforms the local model. Beam search with a locally normalized model suffers from severe label bias issues that we discuss on a concrete example in Section 5. We also compare to the sentence compression system from Filippova et al. (2015), a 3-layer stacked LSTM which uses dependency label information. The LSTM and our global model perform on par on both the automatic evaluation as well as the human ratings, but our model is roughly 100 times faster. All compressions kept approximately 42% of the tokens on average and all the models are significantly better than the automatic extractions (p < 0.05).",
        "sentences": [
            "Table 4 shows our sentence compression results.",
            "Our globally normalized model again significantly outperforms the local model.",
            "Beam search with a locally normalized model suffers from severe label bias issues that we discuss on a concrete example in Section 5.",
            "We also compare to the sentence compression system from Filippova et al. (2015), a 3-layer stacked LSTM which uses dependency label information.",
            "The LSTM and our global model perform on par on both the automatic evaluation as well as the human ratings, but our model is roughly 100 times faster.",
            "All compressions kept approximately 42% of the tokens on average and all the models are significantly better than the automatic extractions (p < 0.05)."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Our Global (B=8)",
                "Our Local (B=1)",
                "Our Local (B=8)"
            ],
            [
                "Our Local (B=1)",
                "Our Local (B=8)"
            ],
            [
                "Filippova et al. (2015)"
            ],
            [
                "Our Global (B=8)",
                "Generated corpus",
                "Human eval"
            ],
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_4",
        "paper_id": "P16-1231",
        "valid": 1
    },
    {
        "table_id_paper": "P16-2002table_1",
        "description": "3.6 Results of Intent Classification Task. Table 1 shows the performance of intent classification across domains. For the baseline, SVM without embedding (w/o Embed) achieved 91.99% accuracy, which is already very competitive. However, the models with word embedding trained on 6 billion tokens (6B-50d) and 840 billion tokens (840B-300d) (Pennington et al., 2014) achieved 92.89% and 93.00%, respectively. 50d and 300d denote size of embedding dimension. To use word embeddings as a sentence representation, we simply use averaged word vectors over a sentence, normalized and conjoined with the original representation as in (2). Surprisingly, when we use sentence representation (SENT) induced from the sketching method with our data set, we can boost the performance up to 93.49%, corresponding to a 18.78% decrease in error relative to a SVM without representation. Also, we see that the extended sentence representation (SENT+) can get additional gains.",
        "sentences": [
            "3.6 Results of Intent Classification Task.",
            "Table 1 shows the performance of intent classification across domains.",
            "For the baseline, SVM without embedding (w/o Embed) achieved 91.99% accuracy, which is already very competitive.",
            "However, the models with word embedding trained on 6 billion tokens (6B-50d) and 840 billion tokens (840B-300d) (Pennington et al., 2014) achieved 92.89% and 93.00%, respectively.",
            "50d and 300d denote size of embedding dimension.",
            "To use word embeddings as a sentence representation, we simply use averaged word vectors over a sentence, normalized and conjoined with the original representation as in (2).",
            "Surprisingly, when we use sentence representation (SENT) induced from the sketching method with our data set, we can boost the performance up to 93.49%, corresponding to a 18.78% decrease in error relative to a SVM without representation.",
            "Also, we see that the extended sentence representation (SENT+) can get additional gains."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "w/o Embed",
                "Average"
            ],
            [
                "6B-50d",
                "840B-300d",
                "Average"
            ],
            null,
            null,
            [
                "SENT",
                "w/o Embed",
                "Average"
            ],
            [
                "SENT+",
                "Average"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_1",
        "paper_id": "P16-2002",
        "valid": 1
    },
    {
        "table_id_paper": "P16-2002table_2",
        "description": "As in Table 2 , we also measured performance of our method (SENT+) as a function of the percentage of unlabeled data we used from total unlabeled sentences. The overall trend is clear: as the number of sentences are added to the data for inducing sentence representation, the test performance improves because of both better coverage and better quality of embedding. We believe that if we consume more data, we can boost up the performance even more.",
        "sentences": [
            "As in Table 2 , we also measured performance of our method (SENT+) as a function of the percentage of unlabeled data we used from total unlabeled sentences.",
            "The overall trend is clear: as the number of sentences are added to the data for inducing sentence representation, the test performance improves because of both better coverage and better quality of embedding.",
            "We believe that if we consume more data, we can boost up the performance even more."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "0",
                "10%",
                "20%",
                "30%",
                "40%",
                "50%",
                "60%",
                "70%",
                "80%",
                "90%",
                "100%"
            ],
            [
                "100%"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_2",
        "paper_id": "P16-2002",
        "valid": 1
    },
    {
        "table_id_paper": "P16-2006table_2",
        "description": "Table 2 shows results for English Penn Treebank using Stanford dependencies. Despite the minimally designed feature representation, relatively few training iterations, and lack of precomputed embeddings, the parser performed on par with state-of-the-art incremental dependency parsers, and slightly outperformed the state-of-the-art greedy parser.",
        "sentences": [
            "Table 2 shows results for English Penn Treebank using Stanford dependencies.",
            "Despite the minimally designed feature representation, relatively few training iterations, and lack of precomputed embeddings, the parser performed on par with state-of-the-art incremental dependency parsers, and slightly outperformed the state-of-the-art greedy parser."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Bi-LSTM",
                "2-Layer Bi-LSTM"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_2",
        "paper_id": "P16-2006",
        "valid": 1
    },
    {
        "table_id_paper": "P16-2011table_3",
        "description": "Table 3 shows the comparison results between our model and the state-of-the-art methods (Li et al., 2013; Chen et al., 2012). MaxEnt (Li et al., 2013) is a pipeline model, which employs human-designed lexical and syntactic features. Rich-C is developed by Chen et al. (2012), which also incorporates Chinese-specific features to improve Chinese event detection. We can see that our method outperforms methods based on human designed features for event trigger identification and achieves comparable F-score for event classification.",
        "sentences": [
            "Table 3 shows the comparison results between our model and the state-of-the-art methods (Li et al., 2013; Chen et al., 2012).",
            "MaxEnt (Li et al., 2013) is a pipeline model, which employs human-designed lexical and syntactic features.",
            "Rich-C is developed by Chen et al. (2012), which also incorporates Chinese-specific features to improve Chinese event detection.",
            "We can see that our method outperforms methods based on human designed features for event trigger identification and achieves comparable F-score for event classification."
        ],
        "class_sentence": [
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "MaxEnt"
            ],
            [
                "Rich-C"
            ],
            [
                "HNN",
                "Trigger Identification",
                "F"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "P16-2011",
        "valid": 1
    },
    {
        "table_id_paper": "P16-2018table_3",
        "description": "Table 3 shows the performance of the algorithms with the manually designed features against the automatically induced ones with LSTM-CRF. We show the performance of each individual product entity category. Compared to all models and settings, LSTM-CRF reaches the best performance of 90.92 F1 score. The most challenging entity types are product family and model, due to their \u201cwild\u201d and irregular nature.",
        "sentences": [
            "Table 3 shows the performance of the algorithms with the manually designed features against the automatically induced ones with LSTM-CRF.",
            "We show the performance of each individual product entity category.",
            "Compared to all models and settings, LSTM-CRF reaches the best performance of 90.92 F1 score.",
            "The most challenging entity types are product family and model, due to their \u201cwild\u201d and irregular nature."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "LSTM-CRF"
            ],
            null,
            [
                "LSTM-CRF",
                "F1"
            ],
            [
                "product family",
                "model"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "P16-2018",
        "valid": 1
    },
    {
        "table_id_paper": "P17-1003table_4",
        "description": "We compare the performance of the best programs found with and without curriculum learning in Table 4. We find that the best programs found with curriculum learning are substantially better than those found without curriculum learning by a large margin on every metric.",
        "sentences": [
            "We compare the performance of the best programs found with and without curriculum learning in Table 4.",
            "We find that the best programs found with curriculum learning are substantially better than those found without curriculum learning by a large margin on every metric."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "Curriculum",
                "No curriculum"
            ],
            [
                "Curriculum",
                "No curriculum"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_4",
        "paper_id": "P17-1003",
        "valid": 1
    },
    {
        "table_id_paper": "P17-1005table_4",
        "description": "Finally, Table 4 presents our results on GRAPHQUESTIONS. We report F1 for SCANNER,the neural baseline model, and three symbolic systems presented in Su et al. (2016). SCANNER achieves a new state of the art on this dataset with a gain of 4.23 F1 points over the best previously reported model.",
        "sentences": [
            "Finally, Table 4 presents our results on GRAPHQUESTIONS.",
            "We report F1 for SCANNER,the neural baseline model, and three symbolic systems presented in Su et al. (2016).",
            "SCANNER achieves a new state of the art on this dataset with a gain of 4.23 F1 points over the best previously reported model."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "F1",
                "SCANNER",
                "SEMPRE (Berant et al. 2013)",
                "PARASEMPRE (Berant and Liang 2014)",
                "JACANA (Yao and Van Durme 2014)"
            ],
            [
                "SCANNER",
                "PARASEMPRE (Berant and Liang 2014)",
                "F1"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_4",
        "paper_id": "P17-1005",
        "valid": 1
    },
    {
        "table_id_paper": "P17-1005table_6",
        "description": "Table 6 reports SCANNER's performance on SPADES. For all Freebase related datasets we use average F1 (Berant et al., 2013) as our evaluation metric. Previous work on this dataset has used a semantic parsing framework similar to ours where natural language is converted to an intermediate syntactic representation and then grounded to Freebase. Specifically, Bisk et al. (2016) evaluate the effectiveness of four different CCG parsers on the semantic parsing task when varying the amount of supervision required. As can be seen, SCANNER outperforms all CCG variants (from unsupervised to fully supervised) without having access to any manually annotated derivations or lexicons. For fair comparison, we also built a neural baseline that encodes an utterance with a recurrent neural network and then predicts a grounded meaning representation directly (Ture and Jojic, 2016; Yih et al., 2016). Again, we observe that SCANNER outperforms this baseline.",
        "sentences": [
            "Table 6 reports SCANNER's performance on SPADES.",
            "For all Freebase related datasets we use average F1 (Berant et al., 2013) as our evaluation metric.",
            "Previous work on this dataset has used a semantic parsing framework similar to ours where natural language is converted to an intermediate syntactic representation and then grounded to Freebase.",
            "Specifically, Bisk et al. (2016) evaluate the effectiveness of four different CCG parsers on the semantic parsing task when varying the amount of supervision required.",
            "As can be seen, SCANNER outperforms all CCG variants (from unsupervised to fully supervised) without having access to any manually annotated derivations or lexicons.",
            "For fair comparison, we also built a neural baseline that encodes an utterance with a recurrent neural network and then predicts a grounded meaning representation directly (Ture and Jojic, 2016; Yih et al., 2016).",
            "Again, we observe that SCANNER outperforms this baseline."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "F1"
            ],
            null,
            [
                "Unsupervised CCG (Bisk et al. 2016)",
                "Semi-supervised CCG (Bisk et al. 2016)",
                "Supervised CCG (Bisk et al. 2016)",
                "Rule-based system (Bisk et al. 2016)"
            ],
            [
                "SCANNER",
                "Unsupervised CCG (Bisk et al. 2016)",
                "Semi-supervised CCG (Bisk et al. 2016)",
                "Supervised CCG (Bisk et al. 2016)",
                "Rule-based system (Bisk et al. 2016)"
            ],
            [
                "Neural baseline"
            ],
            [
                "SCANNER",
                "Neural baseline"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_6",
        "paper_id": "P17-1005",
        "valid": 1
    },
    {
        "table_id_paper": "P17-1005table_8",
        "description": "As shown in Table 8, on SPADES and WEBQUESTIONS, the predicates learned by our model match the output of EASY CCG more closely than the heuristic baseline. But for GRAPHQUESTIONS which contains more compositional questions, the mismatch is higher. However,  since  the  key  idea  of  our  model  is  to  capture  salient  meaning  for  the  task  at  hand  rather than strictly obey syntax, we would not expect the predicates induced by our system to entirely agree with those produced by the syntactic parser. To further analyze how the learned predicates differ from syntax-based ones, we grouped utterances in SPADES into four types of linguistic constructions: coordination (conj), control and raising (control), prepositional phrase attachment (pp), and subordinate clauses (subord). Table 8 also shows the breakdown of matching scores per linguistic construction, with the number of utterances in each type.",
        "sentences": [
            "As shown in Table 8, on SPADES and WEBQUESTIONS, the predicates learned by our model match the output of EASY CCG more closely than the heuristic baseline.",
            "But for GRAPHQUESTIONS which contains more compositional questions, the mismatch is higher.",
            "However,  since  the  key  idea  of  our  model  is  to  capture  salient  meaning  for  the  task  at  hand  rather than strictly obey syntax, we would not expect the predicates induced by our system to entirely agree with those produced by the syntactic parser.",
            "To further analyze how the learned predicates differ from syntax-based ones, we grouped utterances in SPADES into four types of linguistic constructions: coordination (conj), control and raising (control), prepositional phrase attachment (pp), and subordinate clauses (subord).",
            "Table 8 also shows the breakdown of matching scores per linguistic construction, with the number of utterances in each type."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "SPADES",
                "WEBQUESTIONS",
                "SCANNER",
                "Baseline"
            ],
            [
                "GRAPHQUESTIONS",
                "SCANNER",
                "Baseline"
            ],
            [
                "SCANNER"
            ],
            [
                "linguistic constructions of spades",
                "conj (1422)",
                "control (132)",
                "pp (3489)",
                "subord (76)"
            ],
            [
                "linguistic constructions of spades"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_8",
        "paper_id": "P17-1005",
        "valid": 1
    },
    {
        "table_id_paper": "P17-1009table_2",
        "description": "Results are shown in Table 2 where performance on all three tasks (event coreference, trigger detection, and anaphoricity determination) is expressed in terms of F-score. Table 2 shows the results on the English evaluation set. Specifically, row 1 shows the performance of the best event coreference system participating in KBP 2016 (Lu and Ng,2016). This system adopts a pipeline architecture. It first uses an ensemble of one-nearest-neighbor classifiers for trigger detection. Using the extracted triggers, it then applies a pipeline of three sieves, each of which is a one-nearest-neighbor classifier, for event coreference. As we can see, this system achieves an AVG-F of 30.08 for event coreference and an F-score of 46.99 for trigger detection. Row 2 shows the performance of the independent models, each of which is trained independently of the other models. Specifically, each independent model is trained using only the unary factors associated with it. As we can see, the independent models outperform the top KBP 2016 system by 1.2 points in AVG-F for event coreference and 1.83 points for trigger detection. Results of our joint model are shown in row 3. The absolute performance differences between the joint model and the independent models are shown in row 4. As we can see, the joint model outperforms the independent models for all three tasks: by 1.80 points for event coreference, 0.48 points for trigger detection and 4.59 points for anaphoricity determination. Most encouragingly, the joint model outperforms the top KBP 2016 system for both event coreference and trigger detection. For event coreference, it outperforms the top KBP system w.r.t. all scoring metrics, yielding an improvement of 3 points in AVG-F. For trigger detection, it outperforms the top KBP system by 2.31 points.",
        "sentences": [
            "Results are shown in Table 2 where performance on all three tasks (event coreference, trigger detection, and anaphoricity determination) is expressed in terms of F-score.",
            "Table 2 shows the results on the English evaluation set.",
            "Specifically, row 1 shows the performance of the best event coreference system participating in KBP 2016 (Lu and Ng,2016).",
            "This system adopts a pipeline architecture.",
            "It first uses an ensemble of one-nearest-neighbor classifiers for trigger detection.",
            "Using the extracted triggers, it then applies a pipeline of three sieves, each of which is a one-nearest-neighbor classifier, for event coreference.",
            "As we can see, this system achieves an AVG-F of 30.08 for event coreference and an F-score of 46.99 for trigger detection.",
            "Row 2 shows the performance of the independent models, each of which is trained independently of the other models.",
            "Specifically, each independent model is trained using only the unary factors associated with it.",
            "As we can see, the independent models outperform the top KBP 2016 system by 1.2 points in AVG-F for event coreference and 1.83 points for trigger detection.",
            "Results of our joint model are shown in row 3.",
            "The absolute performance differences between the joint model and the independent models are shown in row 4.",
            "As we can see, the joint model outperforms the independent models for all three tasks: by 1.80 points for event coreference, 0.48 points for trigger detection and 4.59 points for anaphoricity determination.",
            "Most encouragingly, the joint model outperforms the top KBP 2016 system for both event coreference and trigger detection.",
            "For event coreference, it outperforms the top KBP system w.r.t. all scoring metrics, yielding an improvement of 3 points in AVG-F.",
            "For trigger detection, it outperforms the top KBP system by 2.31 points."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            2,
            2,
            1,
            1,
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "AVG-F",
                "Trigger",
                "Anaphoricity"
            ],
            [
                "English"
            ],
            [
                "KBP2016"
            ],
            [
                "KBP2016"
            ],
            [
                "KBP2016"
            ],
            [
                "KBP2016"
            ],
            [
                "KBP2016",
                "AVG-F",
                "Trigger"
            ],
            [
                "INDEP."
            ],
            [
                "INDEP."
            ],
            [
                "INDEP.",
                "KBP2016",
                "AVG-F",
                "Trigger"
            ],
            [
                "JOINT"
            ],
            [
                "delta over INDEP."
            ],
            [
                "delta over INDEP.",
                "Trigger",
                "Anaphoricity"
            ],
            [
                "JOINT",
                "KBP2016",
                "AVG-F",
                "Trigger"
            ],
            [
                "JOINT",
                "KBP2016",
                "AVG-F"
            ],
            [
                "JOINT",
                "KBP2016",
                "Trigger"
            ]
        ],
        "n_sentence": 16.0,
        "table_id": "table_2",
        "paper_id": "P17-1009",
        "valid": 1
    },
    {
        "table_id_paper": "P17-1009table_3",
        "description": "Table 3 shows the results on the English and Chinese datasets when we add each type of joint factors to the independent model and remove each type of joint factors from the full joint model. The results of each task are expressed in terms of changes to the corresponding independent model\u00e2\u20ac\u2122s F-score. Among the three types of factors, Coref-Trigger interactions contributes the most to coreference performance, regardless of whether it is applied in isolation or in combination with the other two types of factors to the independent coreference model. In addition, it is the most effective type of factor for improving trigger detection. When applied in combination, it also improves anaphoricity determination, although less effectively than the other two types of factors. When applied in isolation to the independent models, Coref-Anaphoricity interactions improves coreference resolution but has a mixed impact on anaphoricity determination. When applied in combination with other types of factors, it improves both tasks, particularly anaphoricity determination. Its impact on trigger detection, however, is generally negative. When applied in isolation to the independent models, Trigger-Anaphoricity interactions improves both trigger detection and anaphoricity determination. When applied in combination with other types of factors, it still improves anaphoricity determination (particularly on Chinese), but has a mixed effect on trigger detection. Among the three types of factors, it has the least impact on coreference resolution.",
        "sentences": [
            "Table 3 shows the results on the English and Chinese datasets when we add each type of joint factors to the independent model and remove each type of joint factors from the full joint model.",
            "The results of each task are expressed in terms of changes to the corresponding independent model\u00e2\u20ac\u2122s F-score.",
            "Among the three types of factors, Coref-Trigger interactions contributes the most to coreference performance, regardless of whether it is applied in isolation or in combination with the other two types of factors to the independent coreference model.",
            "In addition, it is the most effective type of factor for improving trigger detection.",
            "When applied in combination, it also improves anaphoricity determination, although less effectively than the other two types of factors.",
            "When applied in isolation to the independent models, Coref-Anaphoricity interactions improves coreference resolution but has a mixed impact on anaphoricity determination.",
            "When applied in combination with other types of factors, it improves both tasks, particularly anaphoricity determination.",
            "Its impact on trigger detection, however, is generally negative.",
            "When applied in isolation to the independent models, Trigger-Anaphoricity interactions improves both trigger detection and anaphoricity determination.",
            "When applied in combination with other types of factors, it still improves anaphoricity determination (particularly on Chinese), but has a mixed effect on trigger detection.",
            "Among the three types of factors, it has the least impact on coreference resolution."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "English",
                "Chinese"
            ],
            null,
            [
                "INDEP.+CorefTrigger",
                "JOINT"
            ],
            [
                "INDEP.+CorefTrigger",
                "Trigger"
            ],
            [
                "JOINT",
                "Anaph",
                "JOINT-CorefTrigger"
            ],
            [
                "INDEP.+CorefAnaph",
                "Coref",
                "Anaph"
            ],
            [
                "JOINT",
                "Coref",
                "Anaph"
            ],
            [
                "INDEP.+CorefAnaph",
                "Trigger"
            ],
            [
                "INDEP.+TriggerAnaph",
                "Trigger",
                "Anaph"
            ],
            [
                "JOINT",
                "Anaph",
                "Chinese",
                "Trigger"
            ],
            [
                "INDEP.+TriggerAnaph",
                "Coref"
            ]
        ],
        "n_sentence": 11.0,
        "table_id": "table_3",
        "paper_id": "P17-1009",
        "valid": 1
    },
    {
        "table_id_paper": "P17-1011table_6",
        "description": "Table 6 shows the evaluation results of AES on three datasets. We can see that the BLRR algorithm performs better than the SVR algorithm. No matter  which  algorithm  is  adopted,  adding  discourse mode features make positive contributions for AES compared  with using  basic feature sets. The trends are consistent over all three datasets.",
        "sentences": [
            "Table 6 shows the evaluation results of AES on three datasets.",
            "We can see that the BLRR algorithm performs better than the SVR algorithm.",
            "No matter  which  algorithm  is  adopted,  adding  discourse mode features make positive contributions for AES compared  with using  basic feature sets.",
            "The trends are consistent over all three datasets."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "BLRR-Basic",
                "SVR-Basic"
            ],
            [
                "SVR-Basic",
                "SVR-Basic+mode",
                "BLRR-Basic",
                "BLRR-Basic+mode"
            ],
            [
                "Prompt 1",
                "Prompt 2",
                "Prompt 3"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_6",
        "paper_id": "P17-1011",
        "valid": 1
    },
    {
        "table_id_paper": "P17-1012table_1",
        "description": "Table 1 shows that a single-layer convolutional model with position embeddings (Convolutional) can outperform both a uni-directional LSTM encoder (LSTM) as well as a bi-directional LSTM encoder (BiLSTM). Next, we increase the depth of the convolutional encoder. We choose a good setting by independently varying the number of layers in CNN-a and CNN-c between 1 and 10 and obtained best validation set perplexity with six layers forCNN-a and three layers for CNN-c. This configuration outperforms BiLSTM by 0.7 BLEU (Deep Convolutional 6/3). We investigate depth in the convolutional encoder more in \u00a75.3. Among recurrent encoders, the BiLSTM is 2.3 BLEU better than the uni-directional version. The simple pooling encoder which does not contain any parameters is only 1.3 BLEU lower than a unidirectional LSTM encoder and 3.6 BLEU lower than BiLSTM. The results without position embeddings (words) show that position information is crucial for convolutional encoders. In particular for shallow models (Pooling and Convolutional), whereas deeper models are less effected. Recurrent encoders do not benefit from explicit position information because this information can be naturally extracted through the sequential computation.",
        "sentences": [
            "Table 1 shows that a single-layer convolutional model with position embeddings (Convolutional) can outperform both a uni-directional LSTM encoder (LSTM) as well as a bi-directional LSTM encoder (BiLSTM).",
            "Next, we increase the depth of the convolutional encoder.",
            "We choose a good setting by independently varying the number of layers in CNN-a and CNN-c between 1 and 10 and obtained best validation set perplexity with six layers forCNN-a and three layers for CNN-c.",
            "This configuration outperforms BiLSTM by 0.7 BLEU (Deep Convolutional 6/3).",
            "We investigate depth in the convolutional encoder more in \u00a75.3.",
            "Among recurrent encoders, the BiLSTM is 2.3 BLEU better than the uni-directional version.",
            "The simple pooling encoder which does not contain any parameters is only 1.3 BLEU lower than a unidirectional LSTM encoder and 3.6 BLEU lower than BiLSTM.",
            "The results without position embeddings (words) show that position information is crucial for convolutional encoders.",
            "In particular for shallow models (Pooling and Convolutional), whereas deeper models are less effected.",
            "Recurrent encoders do not benefit from explicit position information because this information can be naturally extracted through the sequential computation."
        ],
        "class_sentence": [
            1,
            2,
            2,
            1,
            2,
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            [
                "wrd+pos",
                "Convolutional",
                "LSTM",
                "BiLSTM"
            ],
            null,
            null,
            [
                "wrd+pos",
                "Deep Convolutional 6/3",
                "BiLSTM",
                "BLEU"
            ],
            [
                "Convolutional"
            ],
            [
                "wrd+pos",
                "Convolutional",
                "BiLSTM",
                "BLEU"
            ],
            [
                "wrd+pos",
                "Pooling",
                "LSTM",
                "BiLSTM",
                "BLEU"
            ],
            [
                "wrd",
                "Convolutional",
                "wrd+pos"
            ],
            [
                "Pooling",
                "Convolutional"
            ],
            [
                "wrd+pos"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_1",
        "paper_id": "P17-1012",
        "valid": 1
    },
    {
        "table_id_paper": "P17-1012table_2",
        "description": "The results (Table 2) show that a deep convolutional encoder can perform competitively to the state of the art on this dataset (Sennrich et al., 2016a). Our bi-directional LSTM encoder baseline is 0.6 BLEU lower than the state of the art but uses only 512 hidden units compared to 1024. A singlelayer convolutional encoder with embedding size 256 performs at 27.1 BLEU. Increasing the number of convolutional layers to 8 in CNN-a and 4 in CNN-c achieves 27.8 BLEU which outperforms our baseline and is competitive to the state of the art.",
        "sentences": [
            "The results (Table 2) show that a deep convolutional encoder can perform competitively to the state of the art on this dataset (Sennrich et al., 2016a).",
            "Our bi-directional LSTM encoder baseline is 0.6 BLEU lower than the state of the art but uses only 512 hidden units compared to 1024.",
            "A singlelayer convolutional encoder with embedding size 256 performs at 27.1 BLEU.",
            "Increasing the number of convolutional layers to 8 in CNN-a and 4 in CNN-c achieves 27.8 BLEU which outperforms our baseline and is competitive to the state of the art."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "Deep Convolutional 8/4",
                "(Sennrich et al. 2016a)"
            ],
            [
                "Encoder",
                "BiLSTM",
                "(Sennrich et al. 2016a)",
                "BLEU"
            ],
            [
                "Single-layer decoder",
                "Encoder",
                "Convolutional",
                "BLEU"
            ],
            [
                "Deep Convolutional 8/4",
                "BLEU",
                "(Sennrich et al. 2016a)",
                "Convolutional",
                "BiLSTM"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "P17-1012",
        "valid": 1
    },
    {
        "table_id_paper": "P17-1014table_3",
        "description": "Table 3 summarizes our AMR generation results on the development and test set. We outperform all previous state-of-theart systems by the first round of self-training and further improve with the next rounds. Our final model trained on GIGA-20M outperforms TSP and TREETOSTR trained on LDC2015E86, by over 9 BLEU points. Overall, our model incorporates less data than previous approaches as all reported  methods  train  language  models  on  the whole  Gigaword  corpus. We leave scaling our models to all of Gigaword for future work.",
        "sentences": [
            "Table 3 summarizes our AMR generation results on the development and test set.",
            "We outperform all previous state-of-theart systems by the first round of self-training and further improve with the next rounds.",
            "Our final model trained on GIGA-20M outperforms TSP and TREETOSTR trained on LDC2015E86, by over 9 BLEU points.",
            "Overall, our model incorporates less data than previous approaches as all reported  methods  train  language  models  on  the whole  Gigaword  corpus.",
            "We leave scaling our models to all of Gigaword for future work."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "GIGA-20M",
                "GIGA-2M",
                "GIGA-200k"
            ],
            [
                "GIGA-20M",
                "TSP (Song et al. 2016)",
                "TREETOSTR (Flanigan et al. 2016)"
            ],
            [
                "GIGA-20M"
            ],
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_3",
        "paper_id": "P17-1014",
        "valid": 1
    },
    {
        "table_id_paper": "P17-1051table_5",
        "description": "Based on the results in Table 5, we make the following observations. Comparing MORSE-CV to MORSE reflects the fundamental difference between SD17 and MC datasets. Comparing MORSE-CV to Morfessor, we observe a significant jump in performance (an increase of 24%).",
        "sentences": [
            "Based on the results in Table 5, we make the following observations.",
            "Comparing MORSE-CV to MORSE reflects the fundamental difference between SD17 and MC datasets.",
            "Comparing MORSE-CV to Morfessor, we observe a significant jump in performance (an increase of 24%)."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "MORSE-CV",
                "MORSE"
            ],
            [
                "MORSE-CV",
                "Morfessor"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_5",
        "paper_id": "P17-1051",
        "valid": 1
    },
    {
        "table_id_paper": "P17-1052table_2",
        "description": "We first report the error rates of our full model (DPCNN with 15 weight layers plus unsupervised embeddings) on the larger five datasets (Table 2). To put it into perspective, we also show the previous results in the literature. The previous results are roughly sorted in the order of error rates from best to worst. On all the five datasets, DPCNN outperforms all of the previous results, which validates the effectiveness of our approach.",
        "sentences": [
            "We first report the error rates of our full model (DPCNN with 15 weight layers plus unsupervised embeddings) on the larger five datasets (Table 2).",
            "To put it into perspective, we also show the previous results in the literature.",
            "The previous results are roughly sorted in the order of error rates from best to worst.",
            "On all the five datasets, DPCNN outperforms all of the previous results, which validates the effectiveness of our approach."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            [
                "DPCNN + unsupervised embed."
            ],
            null,
            null,
            [
                "DPCNN + unsupervised embed."
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "P17-1052",
        "valid": 1
    },
    {
        "table_id_paper": "P17-1053table_2",
        "description": "Table 2 shows the results on two relation detection tasks. The AMPCNN result is from (Yin et al., 2016), which yielded state-of-the-art scores by outperforming several attention-based methods. We re-implemented the BiCNN model from (Yih et al., 2015), where both questions and relations are represented with the word hash trick on character tri-grams. The baseline BiLSTM with relation word sequence appears to be the best baseline on WebQSP and is close to the previous best result of AMPCNN on SimpleQuestions. Our proposed HR-BiLSTM outperformed the best baselines on both tasks by margins of 2-3% (p < 0.001 and 0.01 compared to the best baseline BiLSTM w/ words on SQ and WQ respectively). Note that using only relation names instead of words results in a weaker baseline BiLSTM model. The model yields a significant performance drop on SimpleQuestions (91.2% to 88.9%). However, the drop is much smaller on WebQSP, and it suggests that unseen relations have a much bigger impact on SimpleQuestions.",
        "sentences": [
            "Table 2 shows the results on two relation detection tasks.",
            "The AMPCNN result is from (Yin et al., 2016), which yielded state-of-the-art scores by outperforming several attention-based methods.",
            "We re-implemented the BiCNN model from (Yih et al., 2015), where both questions and relations are represented with the word hash trick on character tri-grams.",
            "The baseline BiLSTM with relation word sequence appears to be the best baseline on WebQSP and is close to the previous best result of AMPCNN on SimpleQuestions.",
            "Our proposed HR-BiLSTM outperformed the best baselines on both tasks by margins of 2-3% (p < 0.001 and 0.01 compared to the best baseline BiLSTM w/ words on SQ and WQ respectively).",
            "Note that using only relation names instead of words results in a weaker baseline BiLSTM model.",
            "The model yields a significant performance drop on SimpleQuestions (91.2% to 88.9%).",
            "However, the drop is much smaller on WebQSP, and it suggests that unseen relations have a much bigger impact on SimpleQuestions."
        ],
        "class_sentence": [
            1,
            2,
            2,
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "AMPCNN (Yin et al. 2016)"
            ],
            null,
            [
                "BiLSTM w/ words",
                "WebQSP"
            ],
            [
                "Hier-Res-BiLSTM (HR-BiLSTM)",
                "BiLSTM w/ words",
                "SimpleQuestions",
                "WebQSP"
            ],
            [
                "BiLSTM w/ relation names"
            ],
            [
                "BiLSTM w/ relation names",
                "SimpleQuestions"
            ],
            [
                "BiLSTM w/ relation names",
                "WebQSP",
                "SimpleQuestions"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_2",
        "paper_id": "P17-1053",
        "valid": 1
    },
    {
        "table_id_paper": "P17-1085table_1",
        "description": "Table 1 compares the performance of our system with respect to the baselines on ACE05 dataset. We find that our joint model significantly outperforms the joint structured perceptron model (Li and Ji, 2014) on both entities and relations, despite the unavailability of features such as dependency trees, POS tags, etc. However, if we compare our model to the SPTree models, then we find that their model has better recall on both entities and relations.",
        "sentences": [
            "Table 1 compares the performance of our system with respect to the baselines on ACE05 dataset.",
            "We find that our joint model significantly outperforms the joint structured perceptron model (Li and Ji, 2014) on both entities and relations, despite the unavailability of features such as dependency trees, POS tags, etc.",
            "However, if we compare our model to the SPTree models, then we find that their model has better recall on both entities and relations."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Our Model",
                "Li and Ji (2014)"
            ],
            [
                "Our Model",
                "SPTree",
                "SPTree1",
                "R",
                "Entity",
                "Relation",
                "Entity+Relation"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_1",
        "paper_id": "P17-1085",
        "valid": 1
    },
    {
        "table_id_paper": "P17-1171table_5",
        "description": "We conducted an ablation analysis on the feature vector of paragraph tokens. As shown in Table 5 all the features contribute to the performance of our final system. Without the aligned question embedding feature (only word embedding and a few manual features), our system is still able to achieve F1 over 77%. More interestingly, if we remove both faligned and fexact match, the performance drops dramatically.",
        "sentences": [
            "We conducted an ablation analysis on the feature vector of paragraph tokens.",
            "As shown in Table 5 all the features contribute to the performance of our final system.",
            "Without the aligned question embedding feature (only word embedding and a few manual features), our system is still able to achieve F1 over 77%.",
            "More interestingly, if we remove both faligned and fexact match, the performance drops dramatically."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Full"
            ],
            [
                "No faligned"
            ],
            [
                "No faligned and fexact match"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_5",
        "paper_id": "P17-1171",
        "valid": 1
    },
    {
        "table_id_paper": "P17-1171table_6",
        "description": "Table 6 presents the results. Despite the difficulty of the task compared to machine comprehension (where you are given the right paragraph) and unconstrained QA (using redundant resources), DrQA still provides reasonable performance across all four datasets. We are interested in a single, full system that can answer any question using Wikipedia. The single model trained only on SQuAD is outperformed on all four of the datasets by the multitask model that uses distant supervision. However performance when training on SQuAD alone is not far behind, indicating that task transfer is occurring. The majority of the improvement from SQuAD to Multitask (DS) however is likely not from task transfer as fine-tuning on each dataset alone using DS also gives improvements, showing that is is the introduction of extra data in the same domain that helps. Nevertheless, the best single model that we can find is our overall goal, and that is the Multitask (DS) system. We compare to an unconstrained QA system using redundant resources (not just Wikipedia), YodaQA (Baudis, 2015), giving results which were previously reported on CuratedTREC and WebQuestions. Despite the increased difficulty of our task, it is reassuring that our performance is not too far behind on CuratedTREC (31.3 vs. 25.4). The gap is slightly bigger on WebQuestions, likely because this dataset was created from the specific structure of Freebase which YodaQA uses directly.",
        "sentences": [
            "Table 6 presents the results.",
            "Despite the difficulty of the task compared to machine comprehension (where you are given the right paragraph) and unconstrained QA (using redundant resources), DrQA still provides reasonable performance across all four datasets.",
            "We are interested in a single, full system that can answer any question using Wikipedia.",
            "The single model trained only on SQuAD is outperformed on all four of the datasets by the multitask model that uses distant supervision.",
            "However performance when training on SQuAD alone is not far behind, indicating that task transfer is occurring.",
            "The majority of the improvement from SQuAD to Multitask (DS) however is likely not from task transfer as fine-tuning on each dataset alone using DS also gives improvements, showing that is is the introduction of extra data in the same domain that helps.",
            "Nevertheless, the best single model that we can find is our overall goal, and that is the Multitask (DS) system.",
            "We compare to an unconstrained QA system using redundant resources (not just Wikipedia), YodaQA (Baudis, 2015), giving results which were previously reported on CuratedTREC and WebQuestions.",
            "Despite the increased difficulty of our task, it is reassuring that our performance is not too far behind on CuratedTREC (31.3 vs. 25.4).",
            "The gap is slightly bigger on WebQuestions, likely because this dataset was created from the specific structure of Freebase which YodaQA uses directly."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            1,
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "DrQA",
                "Dataset"
            ],
            [
                "SQuAD"
            ],
            [
                "SQuAD"
            ],
            [
                "SQuAD"
            ],
            [
                "+Multitask (DS)"
            ],
            [
                "+Multitask (DS)"
            ],
            [
                "YodaQA",
                "CuratedTREC",
                "WebQuestions"
            ],
            [
                "YodaQA",
                "CuratedTREC",
                "+Multitask (DS)"
            ],
            [
                "WebQuestions",
                "YodaQA"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_6",
        "paper_id": "P17-1171",
        "valid": 1
    },
    {
        "table_id_paper": "P17-1176table_3",
        "description": "Table 3 gives BLEU scores on the Europarl corpus of our best performing sentence-level method (sent-beam) and word-level method (word-sampling) compared with pivot-based methods (Cheng et al., 2016a). We use the same data preprocessing as (Cheng et al., 2016a). We find that both the sent-beam and word-sampling methods outperform the pivot-based approaches in a zero-resource scenario across language pairs. Our word-sampling method improves over the best performing zero-resource soft method on Spanish-French translation by +3.29 BLEU points and German-French translation by +3.24 BLEU points. In addition, the word-sampling mothod surprisingly obtains improvement over the likelihood method, which leverages a source-target parallel corpus.",
        "sentences": [
            "Table 3 gives BLEU scores on the Europarl corpus of our best performing sentence-level method (sent-beam) and word-level method (word-sampling) compared with pivot-based methods (Cheng et al., 2016a).",
            "We use the same data preprocessing as (Cheng et al., 2016a).",
            "We find that both the sent-beam and word-sampling methods outperform the pivot-based approaches in a zero-resource scenario across language pairs.",
            "Our word-sampling method improves over the best performing zero-resource soft method on Spanish-French translation by +3.29 BLEU points and German-French translation by +3.24 BLEU points.",
            "In addition, the word-sampling mothod surprisingly obtains improvement over the likelihood method, which leverages a source-target parallel corpus."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "Ours sent-beam",
                "word-sampling",
                "Cheng et al. (2016a)"
            ],
            [
                "Cheng et al. (2016a)"
            ],
            [
                "Ours sent-beam",
                "word-sampling",
                "pivot"
            ],
            [
                "word-sampling",
                "soft",
                "Es\u2192 Fr"
            ],
            [
                "word-sampling",
                "likelihood"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_3",
        "paper_id": "P17-1176",
        "valid": 1
    },
    {
        "table_id_paper": "P17-1185table_2",
        "description": "However, the target performance measure of embedding models is the correlation between semantic similarity and human assessment (Section 4.2). Table 2 presents the comparison of the methods in terms of it. We see that our method outperforms the competitors on all datasets except for men dataset where it obtains slightly worse results. Moreover, it is important that the higher dimension entails higher performance gain of our method in comparison to the competitors.",
        "sentences": [
            "However, the target performance measure of embedding models is the correlation between semantic similarity and human assessment (Section 4.2).",
            "Table 2 presents the comparison of the methods in terms of it.",
            "We see that our method outperforms the competitors on all datasets except for men dataset where it obtains slightly worse results.",
            "Moreover, it is important that the higher dimension entails higher performance gain of our method in comparison to the competitors."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "RO-SGNS",
                "ws-sim",
                "ws-rel",
                "ws-full",
                "simlex",
                "men"
            ],
            [
                "RO-SGNS",
                "Dim. d"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "P17-1185",
        "valid": 1
    },
    {
        "table_id_paper": "P17-1187table_1",
        "description": "Table 1 shows the results of these models for word similarity computation. From the results we can observe that: (1) Our SAT model outperforms other models, including all baselines, on both two test sets. This indicates that, by utilizing sememe annotation properly, our model can better capture the semantic relations of words, and learn more accurate word embeddings. (2) The SSA model represents a word with the average of its sememe embeddings. In general, SSA model performs slightly better than baselines, which tentatively proves that sememe information is helpful. The reason is that words which share common sememe embeddings will benefit from each other. Especially, those words with lower frequency, which cannot be learned sufficiently using conventional WRL models, in contrast, can obtain better word embeddings from SSA simply because their sememe embeddings can be trained sufficiently through other words. (3) The SAT model performs much better than SSA and SAC. This indicates that SAT can obtain more precise sense distribution of a word. The reason has been mentioned above that, different from SAC using only one target word as attention for WSD, SAT adopts richer contextual information as attention for WSD. (4) SAT works better than MST, and we can conclude that a soft disambiguation over senses prevents inevitable errors when selecting only one most-probable sense. The result makes sense because, for many words, their various senses are not always entirely different from each other, but share some common elements. In some contexts, a single sense may not convey the exact meaning of this word.",
        "sentences": [
            "Table 1 shows the results of these models for word similarity computation.",
            "From the results we can observe that: (1) Our SAT model outperforms other models, including all baselines, on both two test sets.",
            "This indicates that, by utilizing sememe annotation properly, our model can better capture the semantic relations of words, and learn more accurate word embeddings.",
            "(2) The SSA model represents a word with the average of its sememe embeddings.",
            "In general, SSA model performs slightly better than baselines, which tentatively proves that sememe information is helpful.",
            "The reason is that words which share common sememe embeddings will benefit from each other.",
            "Especially, those words with lower frequency, which cannot be learned sufficiently using conventional WRL models, in contrast, can obtain better word embeddings from SSA simply because their sememe embeddings can be trained sufficiently through other words.",
            "(3) The SAT model performs much better than SSA and SAC.",
            "This indicates that SAT can obtain more precise sense distribution of a word.",
            "The reason has been mentioned above that, different from SAC using only one target word as attention for WSD, SAT adopts richer contextual information as attention for WSD.",
            "(4) SAT works better than MST, and we can conclude that a soft disambiguation over senses prevents inevitable errors when selecting only one most-probable sense.",
            "The result makes sense because, for many words, their various senses are not always entirely different from each other, but share some common elements.",
            "In some contexts, a single sense may not convey the exact meaning of this word."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            1,
            2,
            2,
            1,
            1,
            2,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "SAT",
                "Wordsim-240",
                "Wordsim-297"
            ],
            null,
            [
                "SSA"
            ],
            [
                "SSA"
            ],
            null,
            [
                "SSA"
            ],
            [
                "SAT",
                "SSA",
                "SAC"
            ],
            [
                "SAT"
            ],
            [
                "SAC"
            ],
            [
                "SAT",
                "MST"
            ],
            null,
            null
        ],
        "n_sentence": 13.0,
        "table_id": "table_1",
        "paper_id": "P17-1187",
        "valid": 1
    },
    {
        "table_id_paper": "P17-1187table_2",
        "description": "Table 2 shows the evaluation results of these models for word analogy inference. From the table, we can observe that: (1) The SAT model performs best among all models, and the superiority is more significant than that on word similarity computation. This indicates that SAT will enhance the modeling of implicit relations between word embeddings in the semantic space. The reason is that sememes annotated to word senses have encoded these word relations. For example, capital and Cuba are two sememes of the word \u201cHavana\u201d, which provide explicit semantic relations between the words \u201cCuba\u201d and \u201cHavana\u201d. (2) The SAT model does well on both classes of Capital and City, because some words in these classes have low frequencies, while their sememes occur so many times that sememe embeddings can be learned sufficiently. With these sememe embeddings, these low-frequent words can be learned more efficiently by SAT. (3) It seems that CBOW works better than SAT on Relationship class. Whereas for the mean rank, CBOW gets the worst results, which indicates the performance of CBOW is unstable. On the contrary, although the accuracy of SAT is a bit lower than that of CBOW, SAT seldom gives an outrageous prediction. In most wrong cases, SAT predicts the word \u201cgrandfather\u201d instead of \u201cgrandmother\u201d, which is not completely nonsense, because in HowNet the words \u201cgrandmother\u201d, \u201cgrandfather\u201d, \u201cgrandma\u201d and some other similar words share four common sememes while only one sememe of them are different. These similar sememes make the attention process less discriminative with each othe. But for the wrong cases of CBOW, we find that many mistakes are about words with low frequencies, such as \u201cstepdaughter\u201d which occurs merely for 358 times. Considering sememes may relieve this problem.",
        "sentences": [
            "Table 2 shows the evaluation results of these models for word analogy inference.",
            "From the table, we can observe that: (1) The SAT model performs best among all models, and the superiority is more significant than that on word similarity computation.",
            "This indicates that SAT will enhance the modeling of implicit relations between word embeddings in the semantic space.",
            "The reason is that sememes annotated to word senses have encoded these word relations.",
            "For example, capital and Cuba are two sememes of the word \u201cHavana\u201d, which provide explicit semantic relations between the words \u201cCuba\u201d and \u201cHavana\u201d.",
            "(2) The SAT model does well on both classes of Capital and City, because some words in these classes have low frequencies, while their sememes occur so many times that sememe embeddings can be learned sufficiently.",
            "With these sememe embeddings, these low-frequent words can be learned more efficiently by SAT.",
            "(3) It seems that CBOW works better than SAT on Relationship class.",
            "Whereas for the mean rank, CBOW gets the worst results, which indicates the performance of CBOW is unstable.",
            "On the contrary, although the accuracy of SAT is a bit lower than that of CBOW, SAT seldom gives an outrageous prediction.",
            "In most wrong cases, SAT predicts the word \u201cgrandfather\u201d instead of \u201cgrandmother\u201d, which is not completely nonsense, because in HowNet the words \u201cgrandmother\u201d, \u201cgrandfather\u201d, \u201cgrandma\u201d and some other similar words share four common sememes while only one sememe of them are different.",
            "These similar sememes make the attention process less discriminative with each othe.",
            "But for the wrong cases of CBOW, we find that many mistakes are about words with low frequencies, such as \u201cstepdaughter\u201d which occurs merely for 358 times.",
            "Considering sememes may relieve this problem."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            2,
            1,
            2,
            1,
            1,
            1,
            2,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "SAT"
            ],
            [
                "SAT"
            ],
            null,
            null,
            [
                "SAT",
                "Capital",
                "City"
            ],
            [
                "SAT"
            ],
            [
                "CBOW",
                "SAT",
                "Relationship"
            ],
            [
                "CBOW",
                "Mean Rank"
            ],
            [
                "SAT",
                "CBOW"
            ],
            [
                "SAT"
            ],
            null,
            [
                "CBOW"
            ],
            null
        ],
        "n_sentence": 14.0,
        "table_id": "table_2",
        "paper_id": "P17-1187",
        "valid": 1
    },
    {
        "table_id_paper": "P17-1189table_2",
        "description": "5.2 Results . Performance on Chinese SemBank. Table 2 gives the results of Experiment 1. We see that precision on CPB with automatic PoS tagging is about 0.9 percentage point higher than that on CSB, while recall is about 0.4 percentage point lower, and the gap between F1 scores on CPB and CSB is not significant, which is only about 0.3 percentage point, although the size of CSB is smaller.",
        "sentences": [
            "5.2 Results .",
            "Performance on Chinese SemBank.",
            "Table 2 gives the results of Experiment 1.",
            "We see that precision on CPB with automatic PoS tagging is about 0.9 percentage point higher than that on CSB, while recall is about 0.4 percentage point lower, and the gap between F1 scores on CPB and CSB is not significant, which is only about 0.3 percentage point, although the size of CSB is smaller."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "Pr. (%)",
                "CPB",
                "CSB",
                "Rec. (%)",
                "F1 (%)"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "P17-1189",
        "valid": 1
    },
    {
        "table_id_paper": "P17-1189table_3",
        "description": "Table 3 summarizes the SRL performance of previous benchmark methods and our experiments described above. Collobert and Weston only conducted their experiments on English corpus, but we notice that their approach has been implemented and tested on CPB by Wang et al. (2015), so we also put their result here for comparison. We can make several observations from these results. Our approach significantly outperforms Sha et al. (2016) by a large margin (Wilcoxon Signed Rank Test, p < 0.05), even without using GRA. This result can prove the ability of our model to capture underlying similarities between heterogeneous SRL resources. The results of methods using external language resources are also presented in Table 3. Not surprisingly, we see that the overall best F1 score, 79.67%, is achieved by the progressive nets with the GRAs. Without GRA, the F1 drops 0.37% percentage point to 79.30, confirming that gated recurrent adapter structure is more suitable for our task because it can remember what has been transferred in previous time steps.",
        "sentences": [
            "Table 3 summarizes the SRL performance of previous benchmark methods and our experiments described above.",
            "Collobert and Weston only conducted their experiments on English corpus, but we notice that their approach has been implemented and tested on CPB by Wang et al. (2015), so we also put their result here for comparison.",
            "We can make several observations from these results.",
            "Our approach significantly outperforms Sha et al. (2016) by a large margin (Wilcoxon Signed Rank Test, p < 0.05), even without using GRA.",
            "This result can prove the ability of our model to capture underlying similarities between heterogeneous SRL resources.",
            "The results of methods using external language resources are also presented in Table 3.",
            "Not surprisingly, we see that the overall best F1 score, 79.67%, is achieved by the progressive nets with the GRAs.",
            "Without GRA, the F1 drops 0.37% percentage point to 79.30, confirming that gated recurrent adapter structure is more suitable for our task because it can remember what has been transferred in previous time steps."
        ],
        "class_sentence": [
            1,
            2,
            2,
            1,
            2,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "Collobert and Weston (2008) MTL",
                "Wang et al. (2015) bi-LSTM"
            ],
            null,
            [
                "Two-column Progressive+GRA (ours)",
                "Sha et al. (2016) bi-LSTM+QOM"
            ],
            null,
            null,
            [
                "F1 (%)",
                "Two-column Progressive+GRA (ours)"
            ],
            [
                "Two-column progressive (ours)",
                "F1 (%)"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_3",
        "paper_id": "P17-1189",
        "valid": 1
    },
    {
        "table_id_paper": "P17-1190table_2",
        "description": "The results are shown in Table 2. They show that dropping entire word embeddings and scrambling input sequences is very effective in improving the result of the LSTM, while neither type of dropout improves AVG. Moreover, averaging the hidden states of the LSTM is the most effective modification to the LSTM in improving performance. All of these modifications can be combined to significantly improve the LSTM, finally allowing it to overtake AVG.",
        "sentences": [
            "The results are shown in Table 2.",
            "They show that dropping entire word embeddings and scrambling input sequences is very effective in improving the result of the LSTM, while neither type of dropout improves AVG.",
            "Moreover, averaging the hidden states of the LSTM is the most effective modification to the LSTM in improving performance.",
            "All of these modifications can be combined to significantly improve the LSTM, finally allowing it to overtake AVG."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Model",
                "dropout",
                "word dropout",
                "scrambling",
                "AVG",
                "none"
            ],
            [
                "LSTMAVG"
            ],
            [
                "LSTMAVG",
                "dropout + scrambling"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "P17-1190",
        "valid": 1
    },
    {
        "table_id_paper": "P17-1190table_3",
        "description": "In Table 3, we compare the various GRAN architectures. We find that the GRAN provides a small improvement over the best LSTM configuration, possibly because of its similarity to AVG. It also outperforms the other GRAN models, despite being the simplest.",
        "sentences": [
            "In Table 3, we compare the various GRAN architectures.",
            "We find that the GRAN provides a small improvement over the best LSTM configuration, possibly because of its similarity to AVG.",
            "It also outperforms the other GRAN models, despite being the simplest."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "GRAN"
            ],
            [
                "GRAN",
                "GRAN (no reg.)"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_3",
        "paper_id": "P17-1190",
        "valid": 1
    },
    {
        "table_id_paper": "P17-1190table_6",
        "description": "In Table 6 we compare the various GRAN architectures under the same settings as the previous experiment. We find that the GRAN still has the best overall performance.",
        "sentences": [
            "In Table 6 we compare the various GRAN architectures under the same settings as the previous experiment.",
            "We find that the GRAN still has the best overall performance."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "GRAN"
            ],
            [
                "GRAN"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_6",
        "paper_id": "P17-1190",
        "valid": 1
    },
    {
        "table_id_paper": "P17-1191table_1",
        "description": "Table 1 shows that our proposed token level embedding scheme OntoLSTM-PP outperforms the better variant of our baseline LSTM-PP (with GloVe-retro intialization) by an absolute accuracy difference of 4.9%, or a relative error reduction of 32%. OntoLSTM-PP also outperforms HPCD (full), the previous best result on this dataset.",
        "sentences": [
            "Table 1 shows that our proposed token level embedding scheme OntoLSTM-PP outperforms the better variant of our baseline LSTM-PP (with GloVe-retro intialization) by an absolute accuracy difference of 4.9%, or a relative error reduction of 32%.",
            "OntoLSTM-PP also outperforms HPCD (full), the previous best result on this dataset."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "OntoLSTM-PP",
                "Initialization",
                "GloVe-retro",
                "Acc."
            ],
            [
                "OntoLSTM-PP",
                "HPCD (full)"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_1",
        "paper_id": "P17-1191",
        "valid": 1
    },
    {
        "table_id_paper": "P17-1191table_2",
        "description": "Table 2 shows the effect of using the PP attachment predictions as features within a dependency parser. We note there is a relatively small difference in unlabeled attachment accuracy for all dependencies (not only PP attachments), even when gold PP attachments are used as additional features to the parser. However, when gold PP attachment are used, we note a large potential improvement of 10.46 points in PP attachment accuracies (between the PPA accuracy for RBG and RBG + Oracle PP), which confirms that adding PP predictions as features is an effective approach. Our proposed model RBG + OntoLSTM-PP recovers 15% of this potential improvement, while RBG + HPCD (full) recovers 10%, which illustrates that PP attachment remains a difficult problem with plenty of room for improvements even when using a dedicated model to predict PP attachments and using its predictions in a dependency parser. We also note that, although we use the same predictions of the HPCD (full) model in Belinkov et al. (2014) 5 , we report different results than Belinkov et al. (2014). For example, the unlabeled attachment score (UAS) of the baselines RBG and RBG + HPCD (full) are 94.17 and 94.19, respectively, in Table 2, compared to 93.96 and 94.05, respectively, in Belinkov et al. (2014). This is due to the use of different versions of the RBG parser.6.",
        "sentences": [
            "Table 2 shows the effect of using the PP attachment predictions as features within a dependency parser.",
            "We note there is a relatively small difference in unlabeled attachment accuracy for all dependencies (not only PP attachments), even when gold PP attachments are used as additional features to the parser.",
            "However, when gold PP attachment are used, we note a large potential improvement of 10.46 points in PP attachment accuracies (between the PPA accuracy for RBG and RBG + Oracle PP), which confirms that adding PP predictions as features is an effective approach.",
            "Our proposed model RBG + OntoLSTM-PP recovers 15% of this potential improvement, while RBG + HPCD (full) recovers 10%, which illustrates that PP attachment remains a difficult problem with plenty of room for improvements even when using a dedicated model to predict PP attachments and using its predictions in a dependency parser.",
            "We also note that, although we use the same predictions of the HPCD (full) model in Belinkov et al. (2014) 5 , we report different results than Belinkov et al. (2014).",
            "For example, the unlabeled attachment score (UAS) of the baselines RBG and RBG + HPCD (full) are 94.17 and 94.19, respectively, in Table 2, compared to 93.96 and 94.05, respectively, in Belinkov et al. (2014).",
            "This is due to the use of different versions of the RBG parser.6."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "PPA Acc.",
                "RBG",
                "RBG + Oracle PP"
            ],
            [
                "RBG + OntoLSTM-PP",
                "RBG + HPCD (full)"
            ],
            [
                "RBG + HPCD (full)"
            ],
            [
                "Full UAS",
                "RBG",
                "RBG + HPCD (full)"
            ],
            [
                "RBG"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_2",
        "paper_id": "P17-1191",
        "valid": 1
    },
    {
        "table_id_paper": "P17-1194table_2",
        "description": "Table 2 contains results for evaluating the different architectures on NER and chunking. On these tasks, the application of dropout provides a consistent improvement -applying some variance onto the input embeddings results in more robust models for NER and chunking. The addition of the language modeling objective consistently further improves performance on all benchmarks. While these results are comparable to the respective state-of-the-art results on most datasets, we did not fine-tune hyperparameters for any specific task, instead providing a controlled analysis of the language modeling objective in different settings. For JNLPBA, the system achieves 73.83% compared to 72.55% by Zhou and Su (2004) and 72.70% by Rei et al. (2016). On CoNLL-03, Lample et al. (2016) achieve a considerably higher result of 90.94%, possibly due to their use of specialised word embeddings and a custom version of LSTM. However, our system does outperform a similar architecture by Huang et al. (2015), achieving 86.26% compared to 84.26% F1 score on the CoNLL-03 dataset.",
        "sentences": [
            "Table 2 contains results for evaluating the different architectures on NER and chunking.",
            "On these tasks, the application of dropout provides a consistent improvement -applying some variance onto the input embeddings results in more robust models for NER and chunking.",
            "The addition of the language modeling objective consistently further improves performance on all benchmarks.",
            "While these results are comparable to the respective state-of-the-art results on most datasets, we did not fine-tune hyperparameters for any specific task, instead providing a controlled analysis of the language modeling objective in different settings.",
            "For JNLPBA, the system achieves 73.83% compared to 72.55% by Zhou and Su (2004) and 72.70% by Rei et al. (2016).",
            "On CoNLL-03, Lample et al. (2016) achieve a considerably higher result of 90.94%, possibly due to their use of specialised word embeddings and a custom version of LSTM.",
            "However, our system does outperform a similar architecture by Huang et al. (2015), achieving 86.26% compared to 84.26% F1 score on the CoNLL-03 dataset."
        ],
        "class_sentence": [
            1,
            2,
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            [
                "JNLPBA"
            ],
            [
                "CoNLL-03"
            ],
            [
                "+ LMcost",
                "CoNLL-03"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_2",
        "paper_id": "P17-1194",
        "valid": 1
    },
    {
        "table_id_paper": "P17-1195table_6",
        "description": "Table 6 presents the result of end-to-end problem solving on the UNIV data. It shows the failure in the semantic parsing is a major bottleneck in the current system. Since a problem in UNIV includes more than three sentences on average, parsing a whole problem is quite a high bar for a semantic parser. It is however necessary to solve it by the nature of the task. Once a problem-level logical form was produced, the system yielded a correct solution for 27.60% of such problems in DEV and 11.40% in TEST.",
        "sentences": [
            "Table 6 presents the result of end-to-end problem solving on the UNIV data.",
            "It shows the failure in the semantic parsing is a major bottleneck in the current system.",
            "Since a problem in UNIV includes more than three sentences on average, parsing a whole problem is quite a high bar for a semantic parser.",
            "It is however necessary to solve it by the nature of the task.",
            "Once a problem-level logical form was produced, the system yielded a correct solution for 27.60% of such problems in DEV and 11.40% in TEST."
        ],
        "class_sentence": [
            1,
            2,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            [
                "Correct",
                "DEV",
                "TEST"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_6",
        "paper_id": "P17-1195",
        "valid": 1
    },
    {
        "table_id_paper": "P17-2001table_2",
        "description": "Table 2 shows the detailed comparison to their work. Our system achieves higher performance on \u2018AFTER\u2019, \u2018VAGUE\u2019, while lower on \u2018BEFORE\u2019, \u2018INCLUDES\u2019 (5% of all data) and \u2018IS INCLUDED\u2019 (4% of all data). It is likely that their rich traditional features help the classifiers to capture more minority-class links. On the whole, our system reaches better \u2018Overall\u2019 on both E-E and E-D. As their E-T classifier does not include word embeddings, the E-T results are not listed.",
        "sentences": [
            "Table 2 shows the detailed comparison to their work.",
            "Our system achieves higher performance on \u2018AFTER\u2019, \u2018VAGUE\u2019, while lower on \u2018BEFORE\u2019, \u2018INCLUDES\u2019 (5% of all data) and \u2018IS INCLUDED\u2019 (4% of all data).",
            "It is likely that their rich traditional features help the classifiers to capture more minority-class links.",
            "On the whole, our system reaches better \u2018Overall\u2019 on both E-E and E-D.",
            "As their E-T classifier does not include word embeddings, the E-T results are not listed."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Our",
                "AFTER",
                "VAGUE",
                "BEFORE",
                "INCLUDES",
                "IS INCLUD."
            ],
            null,
            [
                "Our",
                "Overall",
                "E-E",
                "E-D"
            ],
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_2",
        "paper_id": "P17-2001",
        "valid": 1
    },
    {
        "table_id_paper": "P17-2007table_1",
        "description": "Table 1 compares the performance of the monolingual sequence-to-tree model (Dong and Lapata, 2016), SINGLE, and our multilingual model, MULTI, with separate and shared output parameters under the single-source setting as described in Section 3.1. On average, both variants of the multilingual model outperform the monolingual model by up to 1.34% average accuracy on GEO. Parameter sharing is shown to be helpful, in particular for GEO. We observe that the average performance increase on ATIS mainly comes from Chinese and Indonesian. We also learn that although including English is often helpful for the other languages, it may affect its individual performance.",
        "sentences": [
            "Table 1 compares the performance of the monolingual sequence-to-tree model (Dong and Lapata, 2016), SINGLE, and our multilingual model, MULTI, with separate and shared output parameters under the single-source setting as described in Section 3.1.",
            "On average, both variants of the multilingual model outperform the monolingual model by up to 1.34% average accuracy on GEO.",
            "Parameter sharing is shown to be helpful, in particular for GEO.",
            "We observe that the average performance increase on ATIS mainly comes from Chinese and Indonesian.",
            "We also learn that although including English is often helpful for the other languages, it may affect its individual performance."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "SINGLE",
                "MULTI"
            ],
            [
                "MULTI",
                "SINGLE",
                "GEO"
            ],
            [
                "GEO"
            ],
            [
                "ATIS",
                "zh",
                "id"
            ],
            [
                "en"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_1",
        "paper_id": "P17-2007",
        "valid": 1
    },
    {
        "table_id_paper": "P17-2007table_6",
        "description": "In Table 6, we report the accuracy of the 3 runs for each model and dataset. In both settings, we observe that the best accuracy on both datasets is often achieved by MULTI. This is the same conclusion that we reached when averaging the results over all runs.",
        "sentences": [
            "In Table 6, we report the accuracy of the 3 runs for each model and dataset.",
            "In both settings, we observe that the best accuracy on both datasets is often achieved by MULTI.",
            "This is the same conclusion that we reached when averaging the results over all runs."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "MULTI"
            ],
            null
        ],
        "n_sentence": 3.0,
        "table_id": "table_6",
        "paper_id": "P17-2007",
        "valid": 1
    },
    {
        "table_id_paper": "P17-2010table_1",
        "description": "Table 1 shows accuracy, precision, recall and F1-score for the entailment and non-entailment class on the RTE-3 dataset. As reflected in the results, reducing the opacity of compounds via the application of a compound splitter improves the subsequent RTE performance. This holds for all compound splitters that we used in our experiments. It is also noticeable that the different compound splitters yield different results in the downstream task, with FF2010 being the most beneficial and significantly outperforming the initial RTE setup without prior compound splitting (INIT) by up to four percentage points in accuracy and F1-score. As expected, manual splitting performs best overall. The performance difference with FF2010 is however not statistically significant. This is not surprising because FF2010 reaches an accuracy of around 90% in intrinsic evaluations (Ziering and van der Plas, 2016) and the small underperformance is leveled out by the small size of the test set. Moreover, manual inspections revealed that FF2010 has a higher recall than manual splitting in the non-entailment class due to its undersplitting which results in less lexical overlap between T and H, pointing to the non-entailment class.",
        "sentences": [
            "Table 1 shows accuracy, precision, recall and F1-score for the entailment and non-entailment class on the RTE-3 dataset.",
            "As reflected in the results, reducing the opacity of compounds via the application of a compound splitter improves the subsequent RTE performance.",
            "This holds for all compound splitters that we used in our experiments.",
            "It is also noticeable that the different compound splitters yield different results in the downstream task, with FF2010 being the most beneficial and significantly outperforming the initial RTE setup without prior compound splitting (INIT) by up to four percentage points in accuracy and F1-score.",
            "As expected, manual splitting performs best overall.",
            "The performance difference with FF2010 is however not statistically significant.",
            "This is not surprising because FF2010 reaches an accuracy of around 90% in intrinsic evaluations (Ziering and van der Plas, 2016) and the small underperformance is leveled out by the small size of the test set.",
            "Moreover, manual inspections revealed that FF2010 has a higher recall than manual splitting in the non-entailment class due to its undersplitting which results in less lexical overlap between T and H, pointing to the non-entailment class."
        ],
        "class_sentence": [
            1,
            2,
            2,
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            [
                "Acc",
                "P",
                "R",
                "F1",
                "Entailment",
                "Non-entailment"
            ],
            null,
            null,
            [
                "FF2010*",
                "INIT",
                "Acc",
                "F1"
            ],
            [
                "manual splitting*"
            ],
            [
                "FF2010*"
            ],
            null,
            [
                "FF2010*"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_1",
        "paper_id": "P17-2010",
        "valid": 1
    },
    {
        "table_id_paper": "P17-2021table_2",
        "description": "Table 2 shows the results in the low-resource setting, where the bpe2tree model is consistently better than the bpe2bpe baseline. We find this interesting as the syntax-aware system performs a much harder task (predicting trees on top of the translations, thus handling much longer output sequences) while having a nearly-identical amount of model parameters. In order to better understand where or how the syntactic information improves translation quality, we perform a closer analysis of the WMT16 experiment.",
        "sentences": [
            "Table 2 shows the results in the low-resource setting, where the bpe2tree model is consistently better than the bpe2bpe baseline.",
            "We find this interesting as the syntax-aware system performs a much harder task (predicting trees on top of the translations, thus handling much longer output sequences) while having a nearly-identical amount of model parameters.",
            "In order to better understand where or how the syntactic information improves translation quality, we perform a closer analysis of the WMT16 experiment."
        ],
        "class_sentence": [
            1,
            2,
            2
        ],
        "header_mention": [
            [
                "bpe2tree",
                "bpe2bpe"
            ],
            null,
            null
        ],
        "n_sentence": 3.0,
        "table_id": "table_2",
        "paper_id": "P17-2021",
        "valid": 1
    },
    {
        "table_id_paper": "P17-2022table_4",
        "description": "We present our experimental results and analyze the results in terms of the lexico-functional linguistic patterns we learn. Rows 1-3 of Table 4 show the results for the three baselines, in terms of F-score for each class and the macro F. Stanford outperforms both NRC and SVM, but misses many cases of positive sentiment. Row 4 of Table 4 shows the results for the AutoSlog classifier. Although AutoSlog itself does not perform highly, the patterns that it learns represent a different type of knowledge than what is contained in many sentiment analysis tools. We therefore hypothesized that a cascading classifier, which supplements one of the baseline sentiment classifiers with the lexicofunctional patterns that AutoSlog learns might yield higher performance. Row 5 of Table 4 shows the results for RETRAINED STANFORD. The F-scores for RETRAINED STANFORD are almost identical to the standard Stanford classifier. This may be because our data is a small percentage of the entire number of phrases used in training Stanford. Although RETRAINED STANFORD prioritizes our phrases, it would not make sense to remove the original training data.",
        "sentences": [
            "We present our experimental results and analyze the results in terms of the lexico-functional linguistic patterns we learn.",
            "Rows 1-3 of Table 4 show the results for the three baselines, in terms of F-score for each class and the macro F.",
            "Stanford outperforms both NRC and SVM, but misses many cases of positive sentiment.",
            "Row 4 of Table 4 shows the results for the AutoSlog classifier.",
            "Although AutoSlog itself does not perform highly, the patterns that it learns represent a different type of knowledge than what is contained in many sentiment analysis tools.",
            "We therefore hypothesized that a cascading classifier, which supplements one of the baseline sentiment classifiers with the lexicofunctional patterns that AutoSlog learns might yield higher performance.",
            "Row 5 of Table 4 shows the results for RETRAINED STANFORD.",
            "The F-scores for RETRAINED STANFORD are almost identical to the standard Stanford classifier.",
            "This may be because our data is a small percentage of the entire number of phrases used in training Stanford.",
            "Although RETRAINED STANFORD prioritizes our phrases, it would not make sense to remove the original training data."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            2,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "SVM",
                "NRC",
                "Stanford",
                "F1",
                "Macro F"
            ],
            [
                "SVM",
                "NRC",
                "Stanford",
                "Pos"
            ],
            [
                "AutoSlog (ASlog)"
            ],
            [
                "AutoSlog (ASlog)"
            ],
            [
                "AutoSlog (ASlog)"
            ],
            [
                "Retrained Stanford"
            ],
            [
                "Retrained Stanford",
                "Stanford"
            ],
            [
                "Stanford"
            ],
            [
                "Retrained Stanford"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_4",
        "paper_id": "P17-2022",
        "valid": 1
    },
    {
        "table_id_paper": "P17-2034table_3",
        "description": "We run each experiment ten times and report mean accuracy as well as standard deviation for randomly initialized models. Table 3 shows our results. NMN is the best performing model using images. For models using the structured representation, the MaxEnt model provides the best performance.",
        "sentences": [
            "We run each experiment ten times and report mean accuracy as well as standard deviation for randomly initialized models.",
            "Table 3 shows our results.",
            "NMN is the best performing model using images.",
            "For models using the structured representation, the MaxEnt model provides the best performance."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Raw image",
                "NMN"
            ],
            [
                "Structured representation",
                "MaxEnt"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "P17-2034",
        "valid": 1
    },
    {
        "table_id_paper": "P17-2043table_2",
        "description": "We conducted human evaluation to compare readability of extractive oracle summaries to that of compressive oracle summaries. We presented the oracle summaries to five human subjects and asked them to rate the summaries using an integer scale from 1 (very poor) to 5 (very good). Table 2 shows the results. Extractive oracle summaries achieved near perfect scores. Although the scores of compressive oracle summaries are inferior to those of extractive oracle summaries, they achieved good enough score, around 4. The results support that our trimming approach based on chunk is effective.",
        "sentences": [
            "We conducted human evaluation to compare readability of extractive oracle summaries to that of compressive oracle summaries.",
            "We presented the oracle summaries to five human subjects and asked them to rate the summaries using an integer scale from 1 (very poor) to 5 (very good).",
            "Table 2 shows the results.",
            "Extractive oracle summaries achieved near perfect scores.",
            "Although the scores of compressive oracle summaries are inferior to those of extractive oracle summaries, they achieved good enough score, around 4.",
            "The results support that our trimming approach based on chunk is effective."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "Score"
            ],
            [
                "Score"
            ],
            null,
            [
                "Ext."
            ],
            [
                "Comp."
            ],
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_2",
        "paper_id": "P17-2043",
        "valid": 1
    },
    {
        "table_id_paper": "P17-2045table_3",
        "description": "As Table 3 shows, our model trained on atomic data outperforms the baseline in all but one project with an average gain of 5 BLEU points. In particular, we observe bigger gains for java projects such as CoreNLP and guava. We hypothesize this is because program differences in Java tend to be longer than the rest. While this impacts on training time, at the same time it allows the model to work with a larger vocabulary space.",
        "sentences": [
            "As Table 3 shows, our model trained on atomic data outperforms the baseline in all but one project with an average gain of 5 BLEU points.",
            "In particular, we observe bigger gains for java projects such as CoreNLP and guava.",
            "We hypothesize this is because program differences in Java tend to be longer than the rest.",
            "While this impacts on training time, at the same time it allows the model to work with a larger vocabulary space."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            [
                "our model",
                "Moses",
                "atomic",
                "BLEU"
            ],
            [
                "CoreNLP",
                "guava"
            ],
            null,
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "P17-2045",
        "valid": 1
    },
    {
        "table_id_paper": "P17-2046table_3",
        "description": "Table 3 contains final results on the held-out evaluation data. The final translated language feature set (TRANS) comprises word, character and Cangjie features from Traditional Chinese, Simplified Chinese, Japanese and Korean. TRANS features provide a large F1 improvement of 17.4 over the baseline (BASE), similar to the benchmark lexical generalisation features (LEX). They differ in precision-recall tradeoff, with higher recall but lower precision from TRANS. LEX and TRANS are complementary, giving F1 of 55.0. This is 20.6 points higher than the baseline features alone, and improves both the precision of LEX and the recall of TRANS.",
        "sentences": [
            "Table 3 contains final results on the held-out evaluation data.",
            "The final translated language feature set (TRANS) comprises word, character and Cangjie features from Traditional Chinese, Simplified Chinese, Japanese and Korean.",
            "TRANS features provide a large F1 improvement of 17.4 over the baseline (BASE), similar to the benchmark lexical generalisation features (LEX).",
            "They differ in precision-recall tradeoff, with higher recall but lower precision from TRANS. LEX and TRANS are complementary, giving F1 of 55.0.",
            "This is 20.6 points higher than the baseline features alone, and improves both the precision of LEX and the recall of TRANS."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "BASE+TRANS",
                "BASE+LEX",
                "F"
            ],
            [
                "P",
                "R",
                "BASE+TRANS",
                "BASE+LEX+TRANS",
                "F"
            ],
            [
                "BASE",
                "BASE+LEX+TRANS",
                "F",
                "P",
                "BASE+LEX",
                "R",
                "BASE+TRANS"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_3",
        "paper_id": "P17-2046",
        "valid": 1
    },
    {
        "table_id_paper": "P17-2047table_5",
        "description": "Table 5 summarizes the results of EviNets and some baseline methods on the created Yahoo! Answers dataset. As we can see, knowledge base data is not enough to answer most of these questions, and a state-of-the-art KBQA system Aqqu gets only 0.116 precision. Adding textual data helps significantly, and Text2KB improves the precision to 0.17, which roughly matches the results of the AskMSR system, that ranks candidate entities by their popularity in the retrieved documents. Using text along with KB evidence gave higher performance metrics, boosting F1 from 0.271 to 0.291. EviNets significantly improves over the baseline approaches, beating AskMSR by 28% and KV MemN2N by almost 80% in F1 score.",
        "sentences": [
            "Table 5 summarizes the results of EviNets and some baseline methods on the created Yahoo! Answers dataset.",
            "As we can see, knowledge base data is not enough to answer most of these questions, and a state-of-the-art KBQA system Aqqu gets only 0.116 precision.",
            "Adding textual data helps significantly, and Text2KB improves the precision to 0.17, which roughly matches the results of the AskMSR system, that ranks candidate entities by their popularity in the retrieved documents.",
            "Using text along with KB evidence gave higher performance metrics, boosting F1 from 0.271 to 0.291.",
            "EviNets significantly improves over the baseline approaches, beating AskMSR by 28% and KV MemN2N by almost 80% in F1 score."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Aqqu",
                "P"
            ],
            [
                "Text2KB",
                "P"
            ],
            [
                "EviNets (text)",
                "EviNets (text+kb)",
                "F1"
            ],
            [
                "EviNets (text+kb)",
                "AskMSR (entities)",
                "MemN2N",
                "F1"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_5",
        "paper_id": "P17-2047",
        "valid": 1
    },
    {
        "table_id_paper": "P17-2052table_3",
        "description": "Table 3 summarizes our results. Starting with the baseline, we incrementally add the type pair, graph-based, and set size features discussed in 2.1. Adding type pair features results in an appreciable performance gain, while the graph features bring little benefit\u2014potentially because pairwise correlations suffice to summarize the set structure when the number of types is moderately low.",
        "sentences": [
            "Table 3 summarizes our results.",
            "Starting with the baseline, we incrementally add the type pair, graph-based, and set size features discussed in 2.1.",
            "Adding type pair features results in an appreciable performance gain, while the graph features bring little benefit\u2014potentially because pairwise correlations suffice to summarize the set structure when the number of types is moderately low."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Unstructured",
                "+ Pairs",
                "+ Graph"
            ],
            [
                "+ Pairs",
                "+ Graph"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_3",
        "paper_id": "P17-2052",
        "valid": 1
    },
    {
        "table_id_paper": "P17-2055table_2",
        "description": "Table 2 shows the performance of our CRF-based method in finding the correct relation cardinality, evaluated on manually annotated 20 (has part), 100 (admin. terr. entity) and 200 (child and spouse) randomly selected subjects that have at least one object. The random-number baseline achieves a precision of 5% (has part), 3.5% (admin. territ. entity), 0% (spouse) and 11.2% (child). Compared to that, especially using only-nummod, our method gives encouraging results for has part, admin. territ. entity and child, with 30-50% precision and around 30% F1-score. For spouse, the performance is significantly lower, reasons are discussed below. Furthermore, we can observe that using manual ground truth as training data for the child relation can boost performance considerably. Still, the performance is significantly below the stateof-the-art in fact extraction, where child triples can be extracted from Wikipedia text with 96% precision (Palomares et al., 2016). As shown by the last row of Table 2, higher quality of training data can considerably boost the performance of cardinality extraction.",
        "sentences": [
            "Table 2 shows the performance of our CRF-based method in finding the correct relation cardinality, evaluated on manually annotated 20 (has part), 100 (admin. terr. entity) and 200 (child and spouse) randomly selected subjects that have at least one object.",
            "The random-number baseline achieves a precision of 5% (has part), 3.5% (admin. territ. entity), 0% (spouse) and 11.2% (child).",
            "Compared to that, especially using only-nummod, our method gives encouraging results for has part, admin. territ. entity and child, with 30-50% precision and around 30% F1-score.",
            "For spouse, the performance is significantly lower, reasons are discussed below.",
            "Furthermore, we can observe that using manual ground truth as training data for the child relation can boost performance considerably.",
            "Still, the performance is significantly below the stateof-the-art in fact extraction, where child triples can be extracted from Wikipedia text with 96% precision (Palomares et al., 2016).",
            "As shown by the last row of Table 2, higher quality of training data can considerably boost the performance of cardinality extraction."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "baseline",
                "P",
                "has part (creative work series)",
                "contains admin. terr. entity",
                "spouse",
                "child"
            ],
            [
                "only-nummod",
                "P",
                "F1"
            ],
            [
                "spouse"
            ],
            [
                "child (manual ground truth)"
            ],
            [
                "child (manual ground truth)"
            ],
            [
                "child (manual ground truth)"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_2",
        "paper_id": "P17-2055",
        "valid": 1
    },
    {
        "table_id_paper": "P17-2059table_1",
        "description": "Table 1 presents the test-set accuracies obtained by different strategies. Results in Table 1 indicate that the AGT method achieved very competitive accuracy (with 50.5%), when compared to the state-of-the-art results obtained by the tree-LSTM (51.0%) (Tai et al., 2015, Zhu et al., 2015) and high-order CNN approaches (51.2%) (Lei et al., 2015).",
        "sentences": [
            "Table 1 presents the test-set accuracies obtained by different strategies.",
            "Results in Table 1 indicate that the AGT method achieved very competitive accuracy (with 50.5%), when compared to the state-of-the-art results obtained by the tree-LSTM (51.0%) (Tai et al., 2015, Zhu et al., 2015) and high-order CNN approaches (51.2%) (Lei et al., 2015)."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "AGT",
                "tree-LSTM",
                "high-order CNN"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_1",
        "paper_id": "P17-2059",
        "valid": 1
    },
    {
        "table_id_paper": "P17-2060table_1",
        "description": "We compare our neural combination system with the best individual engines, and the state-of-the-art traditional combination system Jane (Freitag et al., 2014). Table 1 shows the BLEU of different models on development data and test data. The BLEU score of the multi-source neural combination model is 2.53 higher than the best single model HPMT. The source language input gives a further improvement of +1.12 BLEU points. As shown in Table 1, Jane outperforms the best single MT system by 1.92 BLEU points. However, our neural combination system with source language gets an improvement of 1.67 BLEU points over Jane. Furthermore, when augmenting our neural combination system with ensemble decoding 2, it leads to another significant boost of +1.69 BLEU points.",
        "sentences": [
            "We compare our neural combination system with the best individual engines, and the state-of-the-art traditional combination system Jane (Freitag et al., 2014).",
            "Table 1 shows the BLEU of different models on development data and test data.",
            "The BLEU score of the multi-source neural combination model is 2.53 higher than the best single model HPMT.",
            "The source language input gives a further improvement of +1.12 BLEU points.",
            "As shown in Table 1, Jane outperforms the best single MT system by 1.92 BLEU points.",
            "However, our neural combination system with source language gets an improvement of 1.67 BLEU points over Jane.",
            "Furthermore, when augmenting our neural combination system with ensemble decoding 2, it leads to another significant boost of +1.69 BLEU points."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "System"
            ],
            null,
            [
                "Multi",
                "HPMT",
                "Average"
            ],
            [
                "Multi+Source"
            ],
            [
                "Jane (Freitag et al. 2014)"
            ],
            [
                "Jane (Freitag et al. 2014)",
                "Multi+Ensemble"
            ],
            [
                "Multi+Source+Ensemble"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_1",
        "paper_id": "P17-2060",
        "valid": 1
    },
    {
        "table_id_paper": "P17-2066table_2",
        "description": "Table 2 summarizes the experimental results. The results show that Ja-generator, that is, the approach in which Japanese captions were used as training data, outperformed En-generator \u2192 MT, which was trained without Japanese captions.",
        "sentences": [
            "Table 2 summarizes the experimental results.",
            "The results show that Ja-generator, that is, the approach in which Japanese captions were used as training data, outperformed En-generator \u2192 MT, which was trained without Japanese captions."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Ja-generator",
                "En-generator \u2192 MT"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_2",
        "paper_id": "P17-2066",
        "valid": 1
    },
    {
        "table_id_paper": "P17-2069table_3",
        "description": "Table 3 shows that both forms of distributional inference significantly outperform a baseline without DI. On average, offset inference outperforms the method of Kober et al. (2016) by a statistically significant margin on both datasets.",
        "sentences": [
            "Table 3 shows that both forms of distributional inference significantly outperform a baseline without DI.",
            "On average, offset inference outperforms the method of Kober et al. (2016) by a statistically significant margin on both datasets."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Offset Inference"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_3",
        "paper_id": "P17-2069",
        "valid": 1
    },
    {
        "table_id_paper": "P17-2070table_2",
        "description": "Table 2 provides the Spearman\u2019s correlation scores for different models against the human ranking. We see that with dimensions 100 and 300, two of our models obtain improvements over the baseline. The MSSG model of Neelakantan et al. (2014) performs only slightly better than our HLTE model by requiring considerably more parameters (600 vs. 100 embedding size).",
        "sentences": [
            "Table 2 provides the Spearman\u2019s correlation scores for different models against the human ranking.",
            "We see that with dimensions 100 and 300, two of our models obtain improvements over the baseline.",
            "The MSSG model of Neelakantan et al. (2014) performs only slightly better than our HLTE model by requiring considerably more parameters (600 vs. 100 embedding size)."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "100",
                "300",
                "HTLE",
                "HTLE add"
            ],
            [
                "MSSG (Neelakantan et al. 2014)",
                "HTLE",
                "600",
                "100"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_2",
        "paper_id": "P17-2070",
        "valid": 1
    },
    {
        "table_id_paper": "P17-2080table_1",
        "description": "As can be seen from Table 1, SPHRED outperforms both HRED and LM over all the three embedding-based metrics. This implies separating the single-line context RNN into two independent parts can actually lead to a better context representation. It is worth mentioning the size of context RNN hidden states in SPHRED is only half of that in HRED, but it still behaves better with fewer parameters. Hence it is reasonable to apply this context information to our framework. The last 4 rows in Table 1 display the results of our framework applied in two scenarios mentioned in Section 2.3 and 2.4. SCENE1-A and SCENE1-B correspond to Scenario 1 with the label fixed as 1 and 0. 90.9% of generated responses in SCENE1-A are generic and 86.9% in SCENE1-B are non-generic according to the manually-built rule, which verified the proper effect of the label. SCENE2-A and SCENE2-B correspond to rule 1 and 2 in Scenario 2. Both successfully predict the sentiment with very minor mismatches (0.2% and 0.8%). The high accuracy further demonstrated SPHRED\u2019s capability of maintaining individual context information. We also experimented by substituting the encoder with a normal HRED, the resulting model cannot predict the correct sentiment at all because the context information is highly mingled for both speakers. The embedding based scores of our framework are still comparable with SPHRED and even better than VHRED. Imposing an external label didn\u2019t bring any significant quality decline.",
        "sentences": [
            "As can be seen from Table 1, SPHRED outperforms both HRED and LM over all the three embedding-based metrics.",
            "This implies separating the single-line context RNN into two independent parts can actually lead to a better context representation.",
            "It is worth mentioning the size of context RNN hidden states in SPHRED is only half of that in HRED, but it still behaves better with fewer parameters.",
            "Hence it is reasonable to apply this context information to our framework.",
            "The last 4 rows in Table 1 display the results of our framework applied in two scenarios mentioned in Section 2.3 and 2.4.",
            "SCENE1-A and SCENE1-B correspond to Scenario 1 with the label fixed as 1 and 0. 90.9% of generated responses in SCENE1-A are generic and 86.9% in SCENE1-B are non-generic according to the manually-built rule, which verified the proper effect of the label.",
            "SCENE2-A and SCENE2-B correspond to rule 1 and 2 in Scenario 2.",
            "Both successfully predict the sentiment with very minor mismatches (0.2% and 0.8%).",
            "The high accuracy further demonstrated SPHRED\u2019s capability of maintaining individual context information.",
            "We also experimented by substituting the encoder with a normal HRED, the resulting model cannot predict the correct sentiment at all because the context information is highly mingled for both speakers.",
            "The embedding based scores of our framework are still comparable with SPHRED and even better than VHRED.",
            "Imposing an external label didn\u2019t bring any significant quality decline."
        ],
        "class_sentence": [
            1,
            2,
            2,
            2,
            1,
            2,
            2,
            1,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            [
                "SPHRED",
                "HRED",
                "LM"
            ],
            null,
            [
                "SPHRED",
                "HRED"
            ],
            null,
            null,
            [
                "SCENE1-A",
                "SCENE1-B"
            ],
            [
                "SCENE2-A",
                "SCENE2-B"
            ],
            [
                "SCENE1-A",
                "SCENE1-B"
            ],
            [
                "SPHRED"
            ],
            [
                "HRED"
            ],
            [
                "SCENE1-A",
                "SCENE1-B",
                "SCENE2-A",
                "SCENE2-B",
                "SPHRED",
                "HRED"
            ],
            null
        ],
        "n_sentence": 12.0,
        "table_id": "table_1",
        "paper_id": "P17-2080",
        "valid": 1
    },
    {
        "table_id_paper": "P17-2081table_4",
        "description": "Table 4 shows the transfer learning results of BiDAF-T on SICK dataset (Marelli et al., 2014), with various pretraining routines. Note that SNLI (Bowman et al., 2015) is a similar task to SICK and is significantly larger (150K/10K/10K train/dev/test examples). Here we highlight three observations. (a) BiDAF-T pretrained on SQuAD outperforms that without any pretraining by 6% and that pretrained on SQuAD-T by 2%, which demonstrates that the transfer learning from large span-based QA gives a clear improvement. (b) Pretraining on SQuAD+SNLI outperforms pretraining on SNLI only. Given that SNLI is larger than SQuAD, the difference in their performance is a strong indicator that we are benefiting from not only the scale of SQuAD, but also the fine-grained supervision that it provides. (c) We outperform the previous state of the art by 2% with the ensemble of SQuAD+SNLI pretraining routine. It is worth noting that Mou et al. (2016) also shows improvement on SICK by pretraining on SNLI.",
        "sentences": [
            "Table 4 shows the transfer learning results of BiDAF-T on SICK dataset (Marelli et al., 2014), with various pretraining routines.",
            "Note that SNLI (Bowman et al., 2015) is a similar task to SICK and is significantly larger (150K/10K/10K train/dev/test examples).",
            "Here we highlight three observations.",
            "(a) BiDAF-T pretrained on SQuAD outperforms that without any pretraining by 6% and that pretrained on SQuAD-T by 2%, which demonstrates that the transfer learning from large span-based QA gives a clear improvement.",
            "(b) Pretraining on SQuAD+SNLI outperforms pretraining on SNLI only.",
            "Given that SNLI is larger than SQuAD, the difference in their performance is a strong indicator that we are benefiting from not only the scale of SQuAD, but also the fine-grained supervision that it provides.",
            "(c) We outperform the previous state of the art by 2% with the ensemble of SQuAD+SNLI pretraining routine.",
            "It is worth noting that Mou et al. (2016) also shows improvement on SICK by pretraining on SNLI."
        ],
        "class_sentence": [
            1,
            2,
            2,
            1,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "SQuAD",
                "SQuAD-T"
            ],
            [
                "SQuAD + SNLI",
                "SNLI"
            ],
            [
                "SNLI",
                "SQuAD"
            ],
            [
                "SQuAD + SNLI*"
            ],
            null
        ],
        "n_sentence": 8.0,
        "table_id": "table_4",
        "paper_id": "P17-2081",
        "valid": 1
    },
    {
        "table_id_paper": "P17-2083table_1",
        "description": "Table 1 shows the classification accuracy of the nine variants of our model on the MapTask corpus. Table 1 shows that adding the attention mechanism is beneficial, as the traditional attention models always outperform their non-attention counterparts. The gated attention configurations, in turn, outperform those with the traditional attention mechanism by 0.49%-1.21%. As seen in Table 1, the performance gain from the HMM connection is larger than the gain from the attention mechanism. Without the attention mechanism, the HMM connection brings an increase of 3.63% with the gated bias HMM configuration and 2.58% with the fully gated HMM configuration. With the use of traditional attention, the improvement is 3.01% for the bias HMM configuration and 3.47% for the gated HMM configuration. Finally with the gated attention in place, the two HMM configurations improve the accuracy by 3.73%.",
        "sentences": [
            "Table 1 shows the classification accuracy of the nine variants of our model on the MapTask corpus.",
            "Table 1 shows that adding the attention mechanism is beneficial, as the traditional attention models always outperform their non-attention counterparts.",
            "The gated attention configurations, in turn, outperform those with the traditional attention mechanism by 0.49%-1.21%.",
            "As seen in Table 1, the performance gain from the HMM connection is larger than the gain from the attention mechanism.",
            "Without the attention mechanism, the HMM connection brings an increase of 3.63% with the gated bias HMM configuration and 2.58% with the fully gated HMM configuration.",
            "With the use of traditional attention, the improvement is 3.01% for the bias HMM configuration and 3.47% for the gated HMM configuration.",
            "Finally with the gated attention in place, the two HMM configurations improve the accuracy by 3.73%."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "gated attn.",
                "traditional",
                "no attn."
            ],
            [
                "gated attn.",
                "traditional"
            ],
            [
                "gate bias HMM",
                "gate all HMM",
                "traditional",
                "gated attn."
            ],
            [
                "no attn.",
                "gate bias HMM",
                "gate all HMM"
            ],
            [
                "traditional",
                "gate bias HMM",
                "gate all HMM"
            ],
            [
                "gated attn.",
                "gate bias HMM",
                "gate all HMM"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_1",
        "paper_id": "P17-2083",
        "valid": 1
    },
    {
        "table_id_paper": "P17-2085table_4",
        "description": "As Table 4 demonstrates, our method shows promising results (87.0 F1 score) on the balanced data set. Nevertheless, we notice the low linking precisions for entities in the Character and State lists, which are caused by different reasons. For the Character list, mentions do not suffice to select high-quality seeds, whereas for the State list, features of referential and non-referential mentions are usually similar.",
        "sentences": [
            "As Table 4 demonstrates, our method shows promising results (87.0 F1 score) on the balanced data set.",
            "Nevertheless, we notice the low linking precisions for entities in the Character and State lists, which are caused by different reasons.",
            "For the Character list, mentions do not suffice to select high-quality seeds, whereas for the State list, features of referential and non-referential mentions are usually similar."
        ],
        "class_sentence": [
            1,
            2,
            2
        ],
        "header_mention": [
            [
                "Balanced Subset",
                "Overall",
                "F"
            ],
            null,
            null
        ],
        "n_sentence": 3.0,
        "table_id": "table_4",
        "paper_id": "P17-2085",
        "valid": 1
    },
    {
        "table_id_paper": "P17-2095table_1",
        "description": "Table 1 presents MT results using various segmentation strategies. Compared to the UNSEG system, the MORPH system improved translation quality by 4.6 and 1.6 BLEU points in Arabic-to-English and English-to-Arabic systems, respectively. The results also improved by up to 3 BLEU points for cCNN and CHAR systems in the Arabic-to-English direction. However, the performance is lower by at least 0.6 BLEU points compared to the MORPH system. In the English-to-Arabic direction, where cCNN and CHAR are applied on the target side, the performance dropped significantly. In the case of CHAR, mapping one source word to many target characters makes it harder for NMT to learn a good model. This is in line with our finding on using a lower value of OP for BPE segmentation (see paragraph Analyzing the effect of OP). Surprisingly, the cCNN system results were inferior to the UNSEG system for English-to-Arabic. A possible explanation is that the decoder\u2019s predictions are still done at word level even when using the cCNN model (which encodes the target input during training but not the output). In practice, this can lead to generating unknown words. Indeed, in the Ar-toEn case cCNN significantly reduces the unknown words in the test sets, while in the English-to-Arabic case the number of unknown words remains roughly the same between UNSEG and cCNN. The BPE system outperformed all other systems in the Ar-to-En direction and is lower than MORPH by only 0.2 BLEU points in the opposite direction. This shows that machine translation involving the Arabic language can achieve competitive results with data-driven segmentation.",
        "sentences": [
            "Table 1 presents MT results using various segmentation strategies.",
            "Compared to the UNSEG system, the MORPH system improved translation quality by 4.6 and 1.6 BLEU points in Arabic-to-English and English-to-Arabic systems, respectively.",
            "The results also improved by up to 3 BLEU points for cCNN and CHAR systems in the Arabic-to-English direction.",
            "However, the performance is lower by at least 0.6 BLEU points compared to the MORPH system.",
            "In the English-to-Arabic direction, where cCNN and CHAR are applied on the target side, the performance dropped significantly.",
            "In the case of CHAR, mapping one source word to many target characters makes it harder for NMT to learn a good model.",
            "This is in line with our finding on using a lower value of OP for BPE segmentation (see paragraph Analyzing the effect of OP).",
            "Surprisingly, the cCNN system results were inferior to the UNSEG system for English-to-Arabic.",
            "A possible explanation is that the decoder\u2019s predictions are still done at word level even when using the cCNN model (which encodes the target input during training but not the output).",
            "In practice, this can lead to generating unknown words.",
            "Indeed, in the Ar-toEn case cCNN significantly reduces the unknown words in the test sets, while in the English-to-Arabic case the number of unknown words remains roughly the same between UNSEG and cCNN.",
            "The BPE system outperformed all other systems in the Ar-to-En direction and is lower than MORPH by only 0.2 BLEU points in the opposite direction.",
            "This shows that machine translation involving the Arabic language can achieve competitive results with data-driven segmentation."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            2,
            2,
            1,
            2,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "UNSEG",
                "MORPH",
                "Arabic-to-English"
            ],
            [
                "cCNN",
                "CHAR",
                "Arabic-to-English"
            ],
            [
                "MORPH",
                "Arabic-to-English"
            ],
            [
                "English-to-Arabic",
                "cCNN",
                "CHAR"
            ],
            [
                "CHAR"
            ],
            [
                "BPE"
            ],
            [
                "cCNN",
                "UNSEG",
                "English-to-Arabic"
            ],
            [
                "cCNN"
            ],
            [
                "cCNN"
            ],
            [
                "Arabic-to-English",
                "cCNN",
                "English-to-Arabic",
                "UNSEG"
            ],
            [
                "BPE",
                "Arabic-to-English",
                "MORPH"
            ],
            null
        ],
        "n_sentence": 13.0,
        "table_id": "table_1",
        "paper_id": "P17-2095",
        "valid": 1
    },
    {
        "table_id_paper": "P17-2096table_4",
        "description": "Table 4 compares our final results (greedy search is adopted by setting k=1) to prior neural models. Pre-training character embeddings on large scale unlabeled corpus (not limited to the training corpus) has been shown helpful for extra performance improvement. The results with or without pre trained character embeddings are listed separately for following the strict closed test setting of SIGHAN Bakeoff in which no linguistic resource other than training corpus is allowed. We also show the state of the art results in (Zhao and Kit, 2008b) of traditional methods. The comparison shows our neural word segmenter outperforms all state of the art neural systems with much less computational cost.",
        "sentences": [
            "Table 4 compares our final results (greedy search is adopted by setting k=1) to prior neural models.",
            "Pre-training character embeddings on large scale unlabeled corpus (not limited to the training corpus) has been shown helpful for extra performance improvement.",
            "The results with or without pre trained character embeddings are listed separately for following the strict closed test setting of SIGHAN Bakeoff in which no linguistic resource other than training corpus is allowed.",
            "We also show the state of the art results in (Zhao and Kit, 2008b) of traditional methods.",
            "The comparison shows our neural word segmenter outperforms all state of the art neural systems with much less computational cost."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "F1 + pre-train"
            ],
            [
                "F1 + pre-train"
            ],
            [
                "(Zhao and Kit 2008c)"
            ],
            [
                "Our results"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_4",
        "paper_id": "P17-2096",
        "valid": 1
    },
    {
        "table_id_paper": "P17-2097table_3",
        "description": "Table 3 shows final results. We report the best result from Mostafazadeh et al. (2016), the best result from the concurrently held LSDSem shared task (Schwartz et al., 2017b), and our final system configuration (with decisions tuned via cross validation as shown in Tables 1-2, then using the model with the best held-out fold accuracy). Our model achieves 74.7%, which is close to the state of the art result of 75.2%. We also report the results of stripping away the plots and running our system on just the endings (\u201cending only\u201d). We use the FLAT BiLSTM model on the ending followed by the feed forward scoring function, using the same loss as above for training. We again use 5 fold cross validation on the validation set and choose the model with the highest held out fold accuracy. We achieve 72.5%, matching the similar ending-only result of Schwartz et al. (2017b).",
        "sentences": [
            "Table 3 shows final results.",
            "We report the best result from Mostafazadeh et al. (2016), the best result from the concurrently held LSDSem shared task (Schwartz et al., 2017b), and our final system configuration (with decisions tuned via cross validation as shown in Tables 1-2, then using the model with the best held-out fold accuracy).",
            "Our model achieves 74.7%, which is close to the state of the art result of 75.2%.",
            "We also report the results of stripping away the plots and running our system on just the endings (\u201cending only\u201d).",
            "We use the FLAT BiLSTM model on the ending followed by the feed forward scoring function, using the same loss as above for training.",
            "We again use 5 fold cross validation on the validation set and choose the model with the highest held out fold accuracy.",
            "We achieve 72.5%, matching the similar ending-only result of Schwartz et al. (2017b)."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "UW (Schwartz et al. 2017b)",
                "Our model (HIER ENCPLOTEND ATT)",
                "Our model (ending only)"
            ],
            [
                "Our model (HIER ENCPLOTEND ATT)",
                "UW (Schwartz et al. 2017b)"
            ],
            [
                "Our model (ending only)"
            ],
            null,
            null,
            [
                "Our model (ending only)",
                "UW (Schwartz et al. 2017b)"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_3",
        "paper_id": "P17-2097",
        "valid": 1
    },
    {
        "table_id_paper": "P17-2100table_1",
        "description": "We compare our model with above baseline systems, including RNN and RNN context. We refer to our proposed Semantic Relevance Based neural model as SRB. Besides, SRB with a gated attention encoder is denoted as +Attention. Table 1 shows the results of our models and baseline systems. We can see SRB outperforms both RNN and RNN context in the F-score of ROUGE-1, ROUGE-2 and ROUGE-L. It concludes that SRB generates more key words and phrases. With a gated attention encoder, SRB achieves a better performance with 33.3 F-score of ROUGE-1, 20.0 ROUGE-2 and 30.1 ROUGE-L.",
        "sentences": [
            "We compare our model with above baseline systems, including RNN and RNN context.",
            "We refer to our proposed Semantic Relevance Based neural model as SRB.",
            "Besides, SRB with a gated attention encoder is denoted as +Attention.",
            "Table 1 shows the results of our models and baseline systems.",
            "We can see SRB outperforms both RNN and RNN context in the F-score of ROUGE-1, ROUGE-2 and ROUGE-L.",
            "It concludes that SRB generates more key words and phrases.",
            "With a gated attention encoder, SRB achieves a better performance with 33.3 F-score of ROUGE-1, 20.0 ROUGE-2 and 30.1 ROUGE-L."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            [
                "Model"
            ],
            [
                "RNN context + SRB (C)"
            ],
            [
                "+Attention (C)"
            ],
            null,
            [
                "RNN context + SRB (C)",
                "ROUGE-1",
                "ROUGE-2",
                "ROUGE-L"
            ],
            [
                "RNN context + SRB (C)"
            ],
            [
                "+Attention (C)",
                "ROUGE-1",
                "ROUGE-2",
                "ROUGE-L"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_1",
        "paper_id": "P17-2100",
        "valid": 1
    },
    {
        "table_id_paper": "P17-2100table_2",
        "description": "Table 2 summarizes the results of our model and state of the art systems. COPYNET has the highest socres, because it incorporates copying mechanism to deals with out of vocabulary word problem. In this paper, we do not implement this mechanism in our model. In the future work, we will try to incorporates copying mechanism to our model to solve the out of vocabulary problem.",
        "sentences": [
            "Table 2 summarizes the results of our model and state of the art systems.",
            "COPYNET has the highest socres, because it incorporates copying mechanism to deals with out of vocabulary word problem.",
            "In this paper, we do not implement this mechanism in our model.",
            "In the future work, we will try to incorporates copying mechanism to our model to solve the out of vocabulary problem."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "COPYNET (Gu et al. 2016)"
            ],
            [
                "this work"
            ],
            [
                "this work"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "P17-2100",
        "valid": 1
    },
    {
        "table_id_paper": "P17-2102table_2",
        "description": "Table 2 presents classification results for Task 1 (binary) suspicious vs. verified news posts and Task 2 (multi-class) four types of suspicious tweets e.g., propaganda, hoaxes, satire and clickbait. We report performance for different model and feature combinations. We find that our neural network models (both CNNs and RNNs) significantly outperform logistic regression baselines learned from all feature combinations. The accuracy improvement for the binary task is 0.2 and F1 macro boost for the multi-class task is 0.07. We also observe that all models learned from network and tweet text signals outperform models trained exclusively on tweets. We report 0.05 accuracy improvement for Task 1, and 0.02 F1 boost for Task 2. Adding linguistic cues to basic tweet representations significantly improves results across all models. Finally, by combining basic content with network and linguistic features via late fusion, our neural network models achieve best results in binary experiments. Interestingly, models perform best in the multiclass case when trained on tweet embeddings and fused network features alone. Syntax and grammar features have been predictive of deception in the product review domain (Feng et al., 2012, Perez Rosas and Mihalcea, 2015). However, unlike earlier work we find that fusing these features into our models significantly decreases performance by 0.02 accuracy for the binary task and 0.02 F1 for multi-class. This may be explained by the domain differences between reviews and tweets which are shorter, more noisy and difficult to parse.",
        "sentences": [
            "Table 2 presents classification results for Task 1 (binary) suspicious vs. verified news posts and Task 2 (multi-class) four types of suspicious tweets e.g., propaganda, hoaxes, satire and clickbait.",
            "We report performance for different model and feature combinations.",
            "We find that our neural network models (both CNNs and RNNs) significantly outperform logistic regression baselines learned from all feature combinations.",
            "The accuracy improvement for the binary task is 0.2 and F1 macro boost for the multi-class task is 0.07.",
            "We also observe that all models learned from network and tweet text signals outperform models trained exclusively on tweets.",
            "We report 0.05 accuracy improvement for Task 1, and 0.02 F1 boost for Task 2.",
            "Adding linguistic cues to basic tweet representations significantly improves results across all models.",
            "Finally, by combining basic content with network and linguistic features via late fusion, our neural network models achieve best results in binary experiments.",
            "Interestingly, models perform best in the multiclass case when trained on tweet embeddings and fused network features alone.",
            "Syntax and grammar features have been predictive of deception in the product review domain (Feng et al., 2012, Perez Rosas and Mihalcea, 2015).",
            "However, unlike earlier work we find that fusing these features into our models significantly decreases performance by 0.02 accuracy for the binary task and 0.02 F1 for multi-class.",
            "This may be explained by the domain differences between reviews and tweets which are shorter, more noisy and difficult to parse."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            1,
            1,
            1,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "RECURRENT NEURAL NETWORK",
                "CONVOLUTIONAL NEURAL NETWORK",
                "BASELINE 1: LOGISTIC REGRESSION (DOC2VEC)",
                "BASELINE 2: LOGISTIC REGRESSION (TFIDF)"
            ],
            [
                "RECURRENT NEURAL NETWORK",
                "CONVOLUTIONAL NEURAL NETWORK",
                "BINARY",
                "F1 macro",
                "MULTI-CLASS"
            ],
            null,
            [
                "Tweets"
            ],
            [
                " + cues"
            ],
            [
                "ALL",
                "BINARY"
            ],
            [
                " + network",
                "MULTI-CLASS"
            ],
            [
                " + syntax"
            ],
            [
                " + syntax",
                "BINARY",
                "F1",
                "MULTI-CLASS"
            ],
            [
                " + syntax"
            ]
        ],
        "n_sentence": 12.0,
        "table_id": "table_2",
        "paper_id": "P17-2102",
        "valid": 1
    },
    {
        "table_id_paper": "P17-2103table_3",
        "description": "Thus, in order to make the classifier robust to the imbalanced dataset, we designed a rule based model with counter-factual forms, which resulted in significantly higher F1 than statistical model. Moreover the rule based model captures positive samples of all possible forms which might not exist in the training set. A combined approach gives the best result. As Table 3 shows our whole pipeline (\u2018CF Parser\u2019 in Table 3) obtained the best overall performance with the combination of both approaches.",
        "sentences": [
            "Thus, in order to make the classifier robust to the imbalanced dataset, we designed a rule based model with counter-factual forms, which resulted in significantly higher F1 than statistical model.",
            "Moreover the rule based model captures positive samples of all possible forms which might not exist in the training set.",
            "A combined approach gives the best result.",
            "As Table 3 shows our whole pipeline (\u2018CF Parser\u2019 in Table 3) obtained the best overall performance with the combination of both approaches."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "Rules Only"
            ],
            [
                "Rules Only"
            ],
            [
                "CF Parser"
            ],
            [
                "CF Parser"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "P17-2103",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1001table_4",
        "description": "Table 4 shows the Spearman\u0081fs correlation results of our models. We outperform FASTTEXT on many word similarity benchmarks. Our results are also significantly better than the dictionary-based models, W2G and W2GM. We hypothesize that W2G and W2GM can perform better than the current reported results given proper pre-processing of words due to special characters such as accents. We investigate the nearest neighbors of polysemies in foreign languages and also observe clear sense separation. For example, piano in Italian can mean \u0081gfloor\u0081h or \u0081gslow\u0081h. These two meanings are  reflected in the nearest neighbors where one component is close to piano-piano, pianod which mean  \u0081gslowly\u0081h whereas the other component is close to piani(floors), istrutturazione (renovation) or infrastruttre (infrastructure).",
        "sentences": [
            "Table 4 shows the Spearman\u0081fs correlation results of our models.",
            "We outperform FASTTEXT on many word similarity benchmarks.",
            "Our results are also significantly better than the dictionary-based models, W2G and W2GM.",
            "We hypothesize that W2G and W2GM can perform better than the current reported results given proper pre-processing of words due to special characters such as accents.",
            "We investigate the nearest neighbors of polysemies in foreign languages and also observe clear sense separation.",
            "For example, piano in Italian can mean \u0081gfloor\u0081h or \u0081gslow\u0081h.",
            "These two meanings are  reflected in the nearest neighbors where one component is close to piano-piano, pianod which mean  \u0081gslowly\u0081h whereas the other component is close to piani(floors), istrutturazione (renovation) or infrastruttre (infrastructure)."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "FASTTEXT"
            ],
            [
                "w2g",
                "w2gm"
            ],
            [
                "w2g",
                "w2gm"
            ],
            null,
            null,
            null
        ],
        "n_sentence": 7.0,
        "table_id": "table_4",
        "paper_id": "P18-1001",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1002table_1",
        "description": "We use the same approach as in the nonce task, except that the chimera embedding is the result of summing over multiple sentences. From Table 1 we see that, while our method is consistently better than both the additive baseline and nonce2vec, removing stop-words from the additive baseline leads to stronger performance for more sentences. Since the `a la carte algorithm explicitly trains the transform to match the true word embedding rather than human similarity measures, it is perhaps not surprising that our approach is much more dominant on the definitional nonce task.",
        "sentences": [
            "We use the same approach as in the nonce task, except that the chimera embedding is the result of summing over multiple sentences.",
            "From Table 1 we see that, while our method is consistently better than both the additive baseline and nonce2vec, removing stop-words from the additive baseline leads to stronger performance for more sentences.",
            "Since the `a la carte algorithm explicitly trains the transform to match the true word embedding rather than human similarity measures, it is perhaps not surprising that our approach is much more dominant on the definitional nonce task."
        ],
        "class_sentence": [
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "nonce2vec"
            ],
            [
                "a la carte"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_1",
        "paper_id": "P18-1002",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1002table_4",
        "description": "In Table 4 we display the result of running cross-validated, (cid:96)2-regularized logistic regression on documents from MR movie reviews (Pang and Lee, 2005), CR customer reviews (Hu and Liu, 2004), SUBJ subjectivity dataset (Pang and Lee, 2004), MPQA opinion polarity subtask (Wiebe et al., 2005), TREC question classification (Li and Roth, 2002), SST sentiment classification (binary and fine-grained) (Socher et al., 2013), and IMDB movie reviews (Maas et al., 2011). The first four are evaluated using tenfold cross-validation, while the others have train-test splits. Despite the simplicity of our embeddings (a concatenation over sums of a la carte n-gram vectors), we find that our results are very competitive with many recent unsupervised methods, achieving the best word-level results on two of the tested datasets.",
        "sentences": [
            "In Table 4 we display the result of running cross-validated, (cid:96)2-regularized logistic regression on documents from MR movie reviews (Pang and Lee, 2005), CR customer reviews (Hu and Liu, 2004), SUBJ subjectivity dataset (Pang and Lee, 2004), MPQA opinion polarity subtask (Wiebe et al., 2005), TREC question classification (Li and Roth, 2002), SST sentiment classification (binary and fine-grained) (Socher et al., 2013), and IMDB movie reviews (Maas et al., 2011).",
            "The first four are evaluated using tenfold cross-validation, while the others have train-test splits.",
            "Despite the simplicity of our embeddings (a concatenation over sums of a la carte n-gram vectors), we find that our results are very competitive with many recent unsupervised methods, achieving the best word-level results on two of the tested datasets."
        ],
        "class_sentence": [
            1,
            2,
            1
        ],
        "header_mention": [
            [
                "MR",
                "CR",
                "SUBJ",
                "MPQA",
                "TREC",
                "SST",
                "IMDB"
            ],
            null,
            [
                "a la carte"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_4",
        "paper_id": "P18-1002",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1003table_1",
        "description": "The results are summarized in Table 1 in terms of accuracy and (macro-averaged) precision, recall and F1 score. As can be observed, our model outperforms the baselines, with the R2 ik variant outperforming the others.",
        "sentences": [
            "The results are summarized in Table 1 in terms of accuracy and (macro-averaged) precision, recall and F1 score.",
            "As can be observed, our model outperforms the baselines, with the R2 ik variant outperforming the others."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "R2ik"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_1",
        "paper_id": "P18-1003",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1004table_2",
        "description": "In Table 2 we show the specialization performance of the ER-CNT models (H = 5, \u0083\u00c9 = 0.3),using different types of constraints on SimLex999 (SL) and SimVerb-3500 (SV). We compare the standard model, which exploits both synonym and antonym pairs for creating training instances, with the models employing only synonym and only antonym constraints, respectively. Clearly, we obtain the best specialization when combining synonyms and antonyms. Note, however, that using\u0081@only synonyms or only antonyms also improves\r\nover the original distributional space.",
        "sentences": [
            "In Table 2 we show the specialization performance of the ER-CNT models (H = 5, \u0083\u00c9 = 0.3),using different types of constraints on SimLex999 (SL) and SimVerb-3500 (SV).",
            "We compare the standard model, which exploits both synonym and antonym pairs for creating training instances, with the models employing only synonym and only antonym constraints, respectively.",
            "Clearly, we obtain the best specialization when combining synonyms and antonyms.",
            "Note, however, that using\u0081@only synonyms or only antonyms also improves\r\nover the original distributional space."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "Constraints (ER-CNT model)",
                "SL",
                "SV"
            ],
            [
                "Synonyms only",
                "Antonyms only",
                "Synonyms + Antonyms"
            ],
            [
                "Synonyms + Antonyms"
            ],
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "P18-1004",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1005table_2",
        "description": "Table 2 shows the BLEU scores on English-German, English-French and English-to-Chinese test sets. As it can be seen, the proposed approach obtains significant improvements than the word-by-word baseline system, with at least +5.01 BLEU points in English-to-German translation and up to +13.37 BLEU points in English-to-French translation. This shows that the proposed model only trained with monolingual data effectively learns to use the context information and\r\nthe internal structure of each language. Compared to the work of (Lample et al., 2017), our model also achieves up to +1.92 BLEU points improvement on English-to-French translation task. We believe that the unsupervised NMT is very promising. However, there is still a large room for improvement compared to the supervised upper bound. The gap between the supervised and unsupervised model is as large as 12.3-25.5 BLEU points depending on the language pair and translation direction.",
        "sentences": [
            "Table 2 shows the BLEU scores on English-German, English-French and English-to-Chinese test sets.",
            "As it can be seen, the proposed approach obtains significant improvements than the word-by-word baseline system, with at least +5.01 BLEU points in English-to-German translation and up to +13.37 BLEU points in English-to-French translation.",
            "This shows that the proposed model only trained with monolingual data effectively learns to use the context information and\r\nthe internal structure of each language.",
            "Compared to the work of (Lample et al., 2017), our model also achieves up to +1.92 BLEU points improvement on English-to-French translation task.",
            "We believe that the unsupervised NMT is very promising.",
            "However, there is still a large room for improvement compared to the supervised upper bound.",
            "The gap between the supervised and unsupervised model is as large as 12.3-25.5 BLEU points depending on the language pair and translation direction."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "en-de",
                "en-fr",
                "zh-en"
            ],
            [
                "The proposed approach",
                "en-de",
                "en-fr"
            ],
            [
                "The proposed approach"
            ],
            [
                "The proposed approach",
                "Lample et al. (2017)",
                "en-fr"
            ],
            [
                "The proposed approach"
            ],
            [
                "The proposed approach",
                "Supervised"
            ],
            [
                "The proposed approach",
                "Supervised"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_2",
        "paper_id": "P18-1005",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1007table_5",
        "description": "Table 5 shows the comparison on different segmentation algorithms: word, character, mixed word/character (Wu et al., 2016), BPE (Sennrich et al., 2016) and our unigram model with or without subword regularization. The BLEU scores of word, character and mixed word/character models are cited from (Wu et al.,2016). As German is a morphologically rich language and needs a huge vocabulary for word models, subword-based algorithms perform a gain of more than 1 BLEU point than word model. Among subword-based algorithms, the unigram language model with subword regularization achieved the best BLEU score (25.04), which demonstrates the effectiveness of multiple subword segmentations.",
        "sentences": [
            "Table 5 shows the comparison on different segmentation algorithms: word, character, mixed word/character (Wu et al., 2016), BPE (Sennrich et al., 2016) and our unigram model with or without subword regularization.",
            "The BLEU scores of word, character and mixed word/character models are cited from (Wu et al.,2016).",
            "As German is a morphologically rich language and needs a huge vocabulary for word models, subword-based algorithms perform a gain of more than 1 BLEU point than word model.",
            "Among subword-based algorithms, the unigram language model with subword regularization achieved the best BLEU score (25.04), which demonstrates the effectiveness of multiple subword segmentations."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "Word",
                "Character (512 nodes)",
                "Mixed Word/Character",
                "BPE",
                "Unigram w/o SR (l = 1)",
                "Unigram w/ SR (l = 64 alpha = 0.1)"
            ],
            [
                "Word",
                "Character (512 nodes)",
                "Mixed Word/Character"
            ],
            [
                "Unigram w/o SR (l = 1)",
                "Unigram w/ SR (l = 64 alpha = 0.1)"
            ],
            [
                "Unigram w/ SR (l = 64 alpha = 0.1)"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_5",
        "paper_id": "P18-1007",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1008table_2",
        "description": "Table 2 shows our results on the WMT\u0081f14 En\u0081\u00a8De task. The Transformer Base model improves over GNMT and ConvS2S by more than 2 BLEU points while the Big model improves by over 3 BLEU points. RNMT+ further outperforms the Transformer Big model and establishes a new state of the art with an averaged value of 28.49. In this case, RNMT+ converged slightly faster than the Transformer Big model and maintained much more stable performance after convergence with a very small standard deviation, which is similar to what we observed on the En-Fr task.",
        "sentences": [
            "Table 2 shows our results on the WMT\u0081f14 En\u0081\u00a8De task.",
            "The Transformer Base model improves over GNMT and ConvS2S by more than 2 BLEU points while the Big model improves by over 3 BLEU points.",
            "RNMT+ further outperforms the Transformer Big model and establishes a new state of the art with an averaged value of 28.49.",
            "In this case, RNMT+ converged slightly faster than the Transformer Big model and maintained much more stable performance after convergence with a very small standard deviation, which is similar to what we observed on the En-Fr task."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Trans. Base",
                "GNMT",
                "ConvS2S",
                "Trans. Big"
            ],
            [
                "RNMT+",
                "Trans. Big"
            ],
            [
                "RNMT+",
                "Trans. Big"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "P18-1008",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1008table_4",
        "description": "From Table 4 we draw the following conclusions about the four techniques:. We observed that label smoothing improves both models, leading to an average increase of 0.7 BLEU for RNMT+ and 0.2 BLEU for Transformer Big models. ? Multi-head Attention Multi-head attention contributes significantly to the quality of both models, resulting in an average increase of 0.6 BLEU for RNMT+ and 0.9 BLEU for Transformer Big models. ? Layer Normalization Layer normalization is most critical to stabilize the training process of either model, especially when multi-head attention is used. Removing layer normalization results in unstable training runs for both models. Since by design, we remove one technique at a time in our ablation experiments, we were unable to quantify how much layer normalization helped in either case. To be able to successfully train a model without layer normalization, we would have to adjust other parts of the model and retune its hyper-parameters. ? Synchronous training Removing synchronous training has different effects on RNMT+ and Transformer. For RNMT+, it results in a significant quality drop, while for the Transformer Big model, it causes the model to become unstable. We also notice that synchronous training is only successful when coupled with a tailored learning rate schedule that has a warmup stage at the beginning (cf. Eq. 1 for RNMT+ and Eq. 2 for Transformer).  For RNMT+, removing this warmup stage during synchronous training causes the model to become unstable.",
        "sentences": [
            "From Table 4 we draw the following conclusions about the four techniques:.",
            "We observed that label smoothing improves both models, leading to an average increase of 0.7 BLEU for RNMT+ and 0.2 BLEU for Transformer Big models.",
            "? Multi-head Attention Multi-head attention contributes significantly to the quality of both models, resulting in an average increase of 0.6 BLEU for RNMT+ and 0.9 BLEU for Transformer Big models.",
            "? Layer Normalization Layer normalization is most critical to stabilize the training process of either model, especially when multi-head attention is used. Removing layer normalization results in unstable training runs for both models.",
            "Since by design, we remove one technique at a time in our ablation experiments, we were unable to quantify how much layer normalization helped in either case.",
            "To be able to successfully train a model without layer normalization, we would have to adjust other parts of the model and retune its hyper-parameters.",
            "? Synchronous training Removing synchronous training has different effects on RNMT+ and Transformer. For RNMT+, it results in a significant quality drop, while for the Transformer Big model, it causes the model to become unstable.",
            "We also notice that synchronous training is only successful when coupled with a tailored learning rate schedule that has a warmup stage at the beginning (cf. Eq. 1 for RNMT+ and Eq. 2 for Transformer).",
            " For RNMT+, removing this warmup stage during synchronous training causes the model to become unstable."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            1,
            2,
            1,
            2,
            2
        ],
        "header_mention": [
            [
                "-Label Smoothing",
                "-Multi-head Attention",
                "-Layer Norm.",
                "-Sync. Training"
            ],
            [
                "-Label Smoothing",
                "RNMT+",
                "Trans. Big"
            ],
            [
                "-Multi-head Attention",
                "RNMT+",
                "Trans. Big"
            ],
            [
                "-Layer Norm."
            ],
            [
                "-Layer Norm.",
                "RNMT+",
                "Trans. Big"
            ],
            [
                "-Layer Norm."
            ],
            [
                "-Sync. Training",
                "RNMT+",
                "Trans. Big"
            ],
            [
                "-Sync. Training"
            ],
            [
                "-Sync. Training",
                "RNMT+"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_4",
        "paper_id": "P18-1008",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1009table_3",
        "description": "Results Table 3 shows the performance of our model and our reimplementation of AttentiveNER. Our model, which uses a multitask objective to learn finer types without punishing more general types, shows recall gains at the cost of drop in precision. The MRR score shows that our model is slightly better than the baseline at ranking correct types above incorrect ones.",
        "sentences": [
            "Results Table 3 shows the performance of our model and our reimplementation of AttentiveNER.",
            "Our model, which uses a multitask objective to learn finer types without punishing more general types, shows recall gains at the cost of drop in precision.",
            "The MRR score shows that our model is slightly better than the baseline at ranking correct types above incorrect ones."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "Our Model",
                "AttentiveNER"
            ],
            [
                "Our Model"
            ],
            [
                "MRR",
                "Our Model",
                "AttentiveNER"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_3",
        "paper_id": "P18-1009",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1009table_4",
        "description": "Table 4 shows the performance breakdown for different type granularity and different supervision. Overall, as seen in previous work on finegrained NER literature (Gillick et al., 2014; Ren et al., 2016a), finer labels were more challenging to predict than coarse grained labels, and this issue is exacerbated when dealing with ultra-fine types. All sources of supervision appear to be useful, with crowdsourced examples making the biggest impact. Head word supervision is particularly helpful for predicting ultra-fine labels, while entity linking improves fine label prediction. The low general type performance is partially because of nominal/pronoun mentions (e.g. \u0081git\u0081h), and because of the large type inventory (sometimes \u0081glocation\u0081h and \u0081gplace\u0081h are annotated interchangeably).",
        "sentences": [
            "Table 4 shows the performance breakdown for different type granularity and different supervision.",
            "Overall, as seen in previous work on finegrained NER literature (Gillick et al., 2014; Ren et al., 2016a), finer labels were more challenging to predict than coarse grained labels, and this issue is exacerbated when dealing with ultra-fine types.",
            "All sources of supervision appear to be useful, with crowdsourced examples making the biggest impact.",
            "Head word supervision is particularly helpful for predicting ultra-fine labels, while entity linking improves fine label prediction.",
            "The low general type performance is partially because of nominal/pronoun mentions (e.g. \u0081git\u0081h), and because of the large type inventory (sometimes \u0081glocation\u0081h and \u0081gplace\u0081h are annotated interchangeably)."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "General",
                "Fine",
                "Ultra-Fine"
            ],
            [
                "All",
                "-Crowd"
            ],
            [
                "-Head",
                "Ultra-Fine",
                "-EL",
                "Fine"
            ],
            [
                "General"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_4",
        "paper_id": "P18-1009",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1009table_6",
        "description": "Results Table 6 shows the overall performance on the test set. Our combination of model and training data shows a clear improvement from prior work, setting a new state-of-the art result.",
        "sentences": [
            "Results Table 6 shows the overall performance on the test set.",
            "Our combination of model and training data shows a clear improvement from prior work, setting a new state-of-the art result."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Ours (ONTO+WIKI+HEAD)"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_6",
        "paper_id": "P18-1009",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1010table_6",
        "description": "Table 6 shows the results for entity level typing on our Wikipedia TypeNet dataset. We see that both the basic CNN and the CNN+Complex models perform similarly with the CNN+Complex model doing slightly better on the full data regime. We also see that both models get an improvement when adding an explicit hierarchy loss, even before adding in the transitive closure. The transitive closure itself gives an additional increase in performance to both models. In both of these cases, the basic CNN model improves by a greater amount than CNN+Complex.",
        "sentences": [
            "Table 6 shows the results for entity level typing on our Wikipedia TypeNet dataset.",
            "We see that both the basic CNN and the CNN+Complex models perform similarly with the CNN+Complex model doing slightly better on the full data regime.",
            "We also see that both models get an improvement when adding an explicit hierarchy loss, even before adding in the transitive closure.",
            "The transitive closure itself gives an additional increase in performance to both models.",
            "In both of these cases, the basic CNN model improves by a greater amount than CNN+Complex."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "CNN",
                "CNN+Complex",
                "Full Data"
            ],
            [
                "CNN + hierarchy",
                "CNN+Complex + hierarchy"
            ],
            [
                "CNN + transitive",
                "CNN+Complex + transitive"
            ],
            [
                "CNN",
                "CNN+Complex"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_6",
        "paper_id": "P18-1010",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1018table_3",
        "description": "Table 3 shows the interannotator agreement rates, averaged across all pairs of annotators. Average agreement is 74.4% on the scene role and 81.3% on the function (row 1). Agreement is higher on the function slot than on the scene role slot, which implies that the former is an easier task than the latter. This is expected considering the definition of construal: the function of an adposition is more lexical and less contextdependent, whereas the role depends on the context (the scene) and can be highly idiomatic (\u0081\u00983.3). The supersense hierarchy allows us to analyze agreement at different levels of granularity (rows2-4 in table 3; see also confusion matrix in supplement). Results show that most confusions are local with respect to the hierarchy.",
        "sentences": [
            "Table 3 shows the interannotator agreement rates, averaged across all pairs of annotators.",
            "Average agreement is 74.4% on the scene role and 81.3% on the function (row 1).",
            "Agreement is higher on the function slot than on the scene role slot, which implies that the former is an easier task than the latter.",
            "This is expected considering the definition of construal: the function of an adposition is more lexical and less contextdependent, whereas the role depends on the context (the scene) and can be highly idiomatic (\u0081\u00983.3).",
            "The supersense hierarchy allows us to analyze agreement at different levels of granularity (rows2-4 in table 3; see also confusion matrix in supplement).",
            "Results show that most confusions are local with respect to the hierarchy."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "Exact",
                "Role",
                "Function"
            ],
            [
                "Role",
                "Function"
            ],
            null,
            [
                "Depth-3",
                "Depth-2",
                "Depth-1"
            ],
            [
                "Depth-3",
                "Depth-2",
                "Depth-1"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_3",
        "paper_id": "P18-1018",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1022table_5",
        "description": "Table 5 shows the performance values for a generic classifier that predicts fake news across orientations, and orientation-specific classifiers that have been individually trained on articles from either orientation. Although all classifiers outperform the naive baselines of classifying everything into one of the classes in terms of precision, the slight increase comes at the cost of a large decrease in recall. While the orientation-specific classifiers are slightly better for most metrics, none of them outperform the naive baselines regarding the F Measure. We conclude that style-based fake news classification simply does not work in general.",
        "sentences": [
            "Table 5 shows the performance values for a generic classifier that predicts fake news across orientations, and orientation-specific classifiers that have been individually trained on articles from either orientation.",
            "Although all classifiers outperform the naive baselines of classifying everything into one of the classes in terms of precision, the slight increase comes at the cost of a large decrease in recall.",
            "While the orientation-specific classifiers are slightly better for most metrics, none of them outperform the naive baselines regarding the F Measure.",
            "We conclude that style-based fake news classification simply does not work in general."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "Generic classifier",
                "Orientation-specific classifier"
            ],
            [
                "Generic classifier",
                "Orientation-specific classifier"
            ],
            [
                "Orientation-specific classifier"
            ],
            [
                "Style",
                "fake"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_5",
        "paper_id": "P18-1022",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1026table_1",
        "description": "Table 1 shows the results on the test set. For the s2s models, we also report results without the scope marking procedure of Konstas et al.(2017). Our approach significantly outperforms the s2s baselines both with individual models and ensembles, while using a comparable number of parameters. In particular, we obtain these results without relying on scoping heuristics. Table 1 also show BLEU scores reported in previous work. These results are not strictly comparable because they used different training set versions and/or employ additional unlabelled corpora; nonetheless some insights can be made. In particular, our g2s ensemble performs better than many previous models that combine a smaller training set with a large unlabelled corpus. It is also most informative to compare our s2s model with Konstas et al.(2017), since this baseline is very similar to theirs. We expected our single model baseline to outperform theirs since we use a larger training set but we obtained similar performance. We speculate that better results could be obtained by more careful tuning, but nevertheless we believe such tuning would also benefit our proposed g2s architecture.",
        "sentences": [
            "Table 1 shows the results on the test set.",
            "For the s2s models, we also report results without the scope marking procedure of Konstas et al.(2017).",
            "Our approach significantly outperforms the s2s baselines both with individual models and ensembles, while using a comparable number of parameters.",
            "In particular, we obtain these results without relying on scoping heuristics.",
            "Table 1 also show BLEU scores reported in previous work.",
            "These results are not strictly comparable because they used different training set versions and/or employ additional unlabelled corpora; nonetheless some insights can be made.",
            "In particular, our g2s ensemble performs better than many previous models that combine a smaller training set with a large unlabelled corpus.",
            "It is also most informative to compare our s2s model with Konstas et al.(2017), since this baseline is very similar to theirs.",
            "We expected our single model baseline to outperform theirs since we use a larger training set but we obtained similar performance.",
            "We speculate that better results could be obtained by more careful tuning, but nevertheless we believe such tuning would also benefit our proposed g2s architecture."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            1,
            2,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "s2s"
            ],
            [
                "g2s",
                "s2s"
            ],
            [
                "g2s",
                "s2s"
            ],
            [
                "BLEU"
            ],
            [
                "BLEU"
            ],
            [
                "g2s"
            ],
            [
                "g2s",
                "s2s"
            ],
            [
                "g2s",
                "s2s"
            ],
            [
                "g2s"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_1",
        "paper_id": "P18-1026",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1028table_1",
        "description": "Table 1 shows our main experimental results. In two of the cases (SST and ROC), SoPa outperforms all models. On Amazon, SoPa performs within 0.3 points of CNN and BiLSTM, and outperforms the other two baselines. The table also shows the number of parameters used by each model for each task. Given enough data, models with more parameters should be expected to perform better. However, SoPa performs better or roughly the same as a BiLSTM, which has 3 to 6 times as many parameters.",
        "sentences": [
            "Table 1 shows our main experimental results.",
            "In two of the cases (SST and ROC), SoPa outperforms all models.",
            "On Amazon, SoPa performs within 0.3 points of CNN and BiLSTM, and outperforms the other two baselines.",
            "The table also shows the number of parameters used by each model for each task.",
            "Given enough data, models with more parameters should be expected to perform better.",
            "However, SoPa performs better or roughly the same as a BiLSTM, which has 3 to 6 times as many parameters."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "SoPa",
                "SST",
                "ROC"
            ],
            [
                "SoPa",
                "CNN",
                "BiLSTM",
                "Amazon"
            ],
            [
                "Hard",
                "DAN",
                "BiLSTM",
                "CNN",
                "SoPa"
            ],
            null,
            [
                "SoPa",
                "BiLSTM"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_1",
        "paper_id": "P18-1028",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1030table_2",
        "description": "Hyperparameters: Table 2 shows the development results of various S-LSTM settings, where Time refers to training time per epoch. Without the sentence-level node, the accuracy of S-LSTM drops to 81.76%, demonstrating the necessity of global information exchange. Adding one additional sentence-level node as described in Section 3.2 does not lead to accuracy improvements, although the number of parameters and decoding time increase accordingly. As a result, we use only 1 sentence-level node for the remaining experiments. The accuracies of S-LSTM increases as the hidden layer size for each node increases from 100 to 300, but does not further increase when the size increases beyond 300. We fix the hidden size to 300 accordingly. Without using hsi and h=si, the performance of S-LSTM drops from 82.64% to 82.36%, showing the effectiveness of having these additional nodes. Hyperparameters for BiLSTM models are also set according to the development data, which we omit here. State transition. In Table 2, the number of recurrent state transition steps of S-LSTM is decided according to the best development performance.",
        "sentences": [
            "Hyperparameters: Table 2 shows the development results of various S-LSTM settings, where Time refers to training time per epoch.",
            "Without the sentence-level node, the accuracy of S-LSTM drops to 81.76%, demonstrating the necessity of global information exchange.",
            "Adding one additional sentence-level node as described in Section 3.2 does not lead to accuracy improvements, although the number of parameters and decoding time increase accordingly.",
            "As a result, we use only 1 sentence-level node for the remaining experiments.",
            "The accuracies of S-LSTM increases as the hidden layer size for each node increases from 100 to 300, but does not further increase when the size increases beyond 300.",
            "We fix the hidden size to 300 accordingly.",
            "Without using hsi and h=si, the performance of S-LSTM drops from 82.64% to 82.36%, showing the effectiveness of having these additional nodes.",
            "Hyperparameters for BiLSTM models are also set according to the development data, which we omit here.",
            "State transition. In Table 2, the number of recurrent state transition steps of S-LSTM is decided according to the best development performance."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            1,
            2,
            1,
            0,
            2
        ],
        "header_mention": [
            [
                "Time (s)"
            ],
            [
                "+0 dummy node"
            ],
            [
                "+1 dummy node",
                "Time (s)",
                "# Param"
            ],
            [
                "+1 dummy node"
            ],
            [
                "Hidden size 100",
                "Hidden size 200",
                "Hidden size 300"
            ],
            [
                "Hidden size 300"
            ],
            [
                "Without s /s",
                "With s /s"
            ],
            null,
            null
        ],
        "n_sentence": 9.0,
        "table_id": "table_2",
        "paper_id": "P18-1030",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1030table_3",
        "description": "As shown in Table 3, BiLSTM gives significantly better accuracies compared to uni-directional LSTM, with the\r\ntraining time per epoch growing from 67 seconds to 106 seconds. Stacking 2 layers of BiLSTM gives further improvements to development results, with a larger time of 207 seconds. 3 layers of stacked BiLSTM does not further improve the results. In contrast, S-LSTM gives a development result of 82.64%, which is significantly better compared to 2-layer stacked BiLSTM, with a smaller number of model parameters and a shorter time of 65 seconds. We additionally make comparisons with stacked CNNs and hierarchical attention (Vaswani et al., 2017), shown in Table 3 (the CNN and Transformer rows), where N indicates the number of attention layers. CNN is the most efficient among all models compared, with the smallest model size. On the other hand, a 3-layer stacked CNN gives an accuracy of 81.46%, which is also the lowest compared with BiLSTM, hierarchical attention and S-LSTM. The best performance of hierarchical attention is between single-layer and two-layer BiLSTMs in terms of both accuracy and efficiency. S-LSTM gives significantly better accuracies compared with both CNN and hierarchical attention.",
        "sentences": [
            "As shown in Table 3, BiLSTM gives significantly better accuracies compared to uni-directional LSTM, with the\r\ntraining time per epoch growing from 67 seconds to 106 seconds.",
            "Stacking 2 layers of BiLSTM gives further improvements to development results, with a larger time of 207 seconds.",
            "3 layers of stacked BiLSTM does not further improve the results.",
            "In contrast, S-LSTM gives a development result of 82.64%, which is significantly better compared to 2-layer stacked BiLSTM, with a smaller number of model parameters and a shorter time of 65 seconds.",
            "We additionally make comparisons with stacked CNNs and hierarchical attention (Vaswani et al., 2017), shown in Table 3 (the CNN and Transformer rows), where N indicates the number of attention layers.",
            "CNN is the most efficient among all models compared, with the smallest model size.",
            "On the other hand, a 3-layer stacked CNN gives an accuracy of 81.46%, which is also the lowest compared with BiLSTM, hierarchical attention and S-LSTM.",
            "The best performance of hierarchical attention is between single-layer and two-layer BiLSTMs in terms of both accuracy and efficiency.",
            "S-LSTM gives significantly better accuracies compared with both CNN and hierarchical attention."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "BiLSTM",
                "LSTM",
                "Time (s)"
            ],
            [
                "2 stacked BiLSTM",
                "Time (s)"
            ],
            [
                "3 stacked BiLSTM",
                "Time (s)"
            ],
            [
                "S-LSTM",
                "2 stacked BiLSTM",
                "# Param",
                "Acc",
                "Time (s)"
            ],
            [
                "CNN",
                "2 stacked CNN",
                "3 stacked CNN",
                "4 stacked CNN",
                "Transformer (N=6)",
                "Transformer (N=8)",
                "Transformer (N=10)"
            ],
            [
                "CNN"
            ],
            [
                "3 stacked CNN",
                "Acc",
                "BiLSTM",
                "Transformer (N=6)",
                "Transformer (N=8)",
                "Transformer (N=10)",
                "S-LSTM"
            ],
            [
                "Transformer (N=6)",
                "Transformer (N=8)",
                "Transformer (N=10)",
                "Acc",
                "Time (s)"
            ],
            [
                "S-LSTM",
                "Acc",
                "CNN",
                "Transformer (N=6)",
                "Transformer (N=8)",
                "Transformer (N=10)"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_3",
        "paper_id": "P18-1030",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1034table_1",
        "description": "Our model is abbreviated as (STAMP), which is short for Syntaxand TableAware seMantic Parser. The STAMP model in Table 1 stands for the model we describe in \u0081\u00984.2 plus \u0081\u00984.3. STAMP+RL is the model that is fine-tuned with the reinforcement learning strategy as described in \u0081\u00984.4. We implement a simplified version of our approach (w/o cell), in which WHERE values come from the question. Thus, this setting differs from Aug.PntNet in the generation of WHERE column. We also study the influence of the relation-cell relation (w/o column-cell relation) through removing the enhanced column vector, which is calculated by weighted averaging cell vectors. From Table 1, we can see that STAMP performs better than existing systems on WikiSQL. Incorporating RL strategy does not significantly improve the performance. Our simplified model, STAMP (w/o cell), achieves better accuracy than Aug.PntNet, which further reveals the effects of the column channel. Results also demonstrate the effects of incorporating the column-cell relation, removing which leads to about 4% performance drop in terms of Accex.",
        "sentences": [
            "Our model is abbreviated as (STAMP), which is short for Syntaxand TableAware seMantic Parser.",
            "The STAMP model in Table 1 stands for the model we describe in \u0081\u00984.2 plus \u0081\u00984.3.",
            "STAMP+RL is the model that is fine-tuned with the reinforcement learning strategy as described in \u0081\u00984.4.",
            "We implement a simplified version of our approach (w/o cell), in which WHERE values come from the question.",
            "Thus, this setting differs from Aug.PntNet in the generation of WHERE column.",
            "We also study the influence of the relation-cell relation (w/o column-cell relation) through removing the enhanced column vector, which is calculated by weighted averaging cell vectors.",
            "From Table 1, we can see that STAMP performs better than existing systems on WikiSQL.",
            "Incorporating RL strategy does not significantly improve the performance.",
            "Our simplified model, STAMP (w/o cell), achieves better accuracy than Aug.PntNet, which further reveals the effects of the column channel.",
            "Results also demonstrate the effects of incorporating the column-cell relation, removing which leads to about 4% performance drop in terms of Accex."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            2,
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "STAMP"
            ],
            [
                "STAMP"
            ],
            [
                "STAMP+RL"
            ],
            [
                "STAMP (w/o cell)"
            ],
            [
                "Aug.PntNet (Zhong et al. 2017)"
            ],
            [
                "STAMP (w/o column-cell relation)"
            ],
            [
                "STAMP (w/o cell)",
                "STAMP (w/o column-cell relation)",
                "STAMP",
                "STAMP+RL"
            ],
            [
                "STAMP+RL"
            ],
            [
                "STAMP (w/o cell)",
                "Aug.PntNet (Zhong et al. 2017)"
            ],
            [
                "STAMP (w/o column-cell relation)",
                "Accex"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_1",
        "paper_id": "P18-1034",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1039table_6",
        "description": "Overall results are shown in Table 6. From the table, we can see that our final model (Seq2Seq LF+AttReg+Iter) outperforms the neural-based baseline models (Wang et al. (2017) 4 and Seq2Seq Equ). On Number word problem dataset, our model already outperforms the state-of-the-art feature-based model (Huang et al., 2017) by 40.8% and is comparable to the ruled-based model (Shi et al., 2015). Advantage of intermediate forms:. From the first two rows, we can see that the seq2seq model which is trained to generate intermediate forms (Seq2Seq LF) greatly outperforms the same model trained to generate equations(Seq2Seq Equ). The use of intermediate forms helps more on NumWord than on Dolphin18K. This result is expected as the Dolphin18K dataset is more challenging, containing many other types of difficulties discussed in Section 6.3.",
        "sentences": [
            "Overall results are shown in Table 6.",
            "From the table, we can see that our final model (Seq2Seq LF+AttReg+Iter) outperforms the neural-based baseline models (Wang et al. (2017) 4 and Seq2Seq Equ).",
            "On Number word problem dataset, our model already outperforms the state-of-the-art feature-based model (Huang et al., 2017) by 40.8% and is comparable to the ruled-based model (Shi et al., 2015).",
            "Advantage of intermediate forms:.",
            "From the first two rows, we can see that the seq2seq model which is trained to generate intermediate forms (Seq2Seq LF) greatly outperforms the same model trained to generate equations(Seq2Seq Equ).",
            "The use of intermediate forms helps more on NumWord than on Dolphin18K.",
            "This result is expected as the Dolphin18K dataset is more challenging, containing many other types of difficulties discussed in Section 6.3."
        ],
        "class_sentence": [
            1,
            1,
            1,
            0,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Seq2Seq LF+AttReg+Iter",
                "Wang et al. (2017)",
                "Seq2Seq Equ"
            ],
            [
                "NumWord",
                "Seq2Seq LF+AttReg+Iter",
                "Huang et al. (2017)",
                "Shi et al. (2015)"
            ],
            null,
            [
                "Seq2Seq LF",
                "Seq2Seq Equ"
            ],
            [
                "NumWord",
                "Dolphin18K"
            ],
            [
                "Dolphin18K"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_6",
        "paper_id": "P18-1039",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1044table_7",
        "description": "Table 7 shows the results of the data augmentation model and the GAN-based model. Our Gen+Adv model performs better than the data augmented model. Note that our data augmentation model does not use raw corpora directly.",
        "sentences": [
            "Table 7 shows the results of the data augmentation model and the GAN-based model.",
            "Our Gen+Adv model performs better than the data augmented model.",
            "Note that our data augmentation model does not use raw corpora directly."
        ],
        "class_sentence": [
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "Gen",
                "Gen+Aug",
                "Gen+Adv"
            ],
            [
                "Gen+Adv",
                "Gen+Aug"
            ],
            [
                "Gen+Aug"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_7",
        "paper_id": "P18-1044",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1047table_2",
        "description": "Table 2 shows the Precision, Recall and F1 value of NovelTagging model (Zheng et al., 2017) and our OneDecoder and MultiDecoder models. As we can see, in NYT dataset, our MultiDecoder model achieves the best F1 score, which is 0.587. There is 39.8% improvement compared with the NovelTagging model, which is 0.420. Besides, our OneDecoder model also outperforms the NovelTagging model. In the WebNLG dataset, MultiDecoder model achieves the highest F1 score (0.371). MultiDecoder and OneDecoder models outperform the NovelTagging model with 31.1% and 7.8% improvements, respectively. These observations verify the effectiveness of our models. We can also observe that, in both NYT and WebNLG dataset, the NovelTagging model achieves the highest precision value and lowest recall value. By contrast, our models are much more balanced.",
        "sentences": [
            "Table 2 shows the Precision, Recall and F1 value of NovelTagging model (Zheng et al., 2017) and our OneDecoder and MultiDecoder models.",
            "As we can see, in NYT dataset, our MultiDecoder model achieves the best F1 score, which is 0.587.",
            "There is 39.8% improvement compared with the NovelTagging model, which is 0.420.",
            "Besides, our OneDecoder model also outperforms the NovelTagging model.",
            "In the WebNLG dataset, MultiDecoder model achieves the highest F1 score (0.371).",
            "MultiDecoder and OneDecoder models outperform the NovelTagging model with 31.1% and 7.8% improvements, respectively.",
            "These observations verify the effectiveness of our models.",
            "We can also observe that, in both NYT and WebNLG dataset, the NovelTagging model achieves the highest precision value and lowest recall value.",
            "By contrast, our models are much more balanced."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "Precision",
                "Recall",
                "F1",
                "NovelTagging",
                "OneDecoder",
                "MultiDecoder"
            ],
            [
                "NYT",
                "MultiDecoder",
                "F1"
            ],
            [
                "NYT",
                "MultiDecoder",
                "NovelTagging",
                "F1"
            ],
            [
                "NYT",
                "OneDecoder",
                "NovelTagging"
            ],
            [
                "WebNLG",
                "MultiDecoder",
                "F1"
            ],
            [
                "MultiDecoder",
                "OneDecoder",
                "NovelTagging"
            ],
            [
                "MultiDecoder",
                "OneDecoder"
            ],
            [
                "NYT",
                "WebNLG",
                "NovelTagging",
                "Precision",
                "Recall"
            ],
            [
                "MultiDecoder",
                "OneDecoder",
                "Precision",
                "Recall",
                "F1"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_2",
        "paper_id": "P18-1047",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1048table_1",
        "description": "Table 1 shows the trigger identification performance. It can be observed that SELF outperforms other models, with a performance gain of no less than 1.1% F-score. Frankly, the performance mainly benefits from the higher recall (78.8%). But in fact the relatively comparable precision (75.3%) to the recall reinforces the advantages. By contrast, although most of the compared models achieve much higher precision over SELF, they suffer greatly from the substantial gaps between precision and recall. The advantage is offset by the greater loss of recall.",
        "sentences": [
            "Table 1 shows the trigger identification performance.",
            "It can be observed that SELF outperforms other models, with a performance gain of no less than 1.1% F-score.",
            "Frankly, the performance mainly benefits from the higher recall (78.8%).",
            "But in fact the relatively comparable precision (75.3%) to the recall reinforces the advantages.",
            "By contrast, although most of the compared models achieve much higher precision over SELF, they suffer greatly from the substantial gaps between precision and recall.",
            "The advantage is offset by the greater loss of recall."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "SELF: Bi-LSTM+GAN",
                "F (%)"
            ],
            [
                "SELF: Bi-LSTM+GAN",
                "R (%)"
            ],
            [
                "SELF: Bi-LSTM+GAN",
                "R (%)",
                "P (%)"
            ],
            [
                "Joint (Local+Global)",
                "MSEP-EMD",
                "DM-CNN",
                "Bi-RNN",
                "Hybrid: Bi-LSTM+CNN",
                "P (%)",
                "R (%)"
            ],
            [
                "Joint (Local+Global)",
                "MSEP-EMD",
                "DM-CNN",
                "Bi-RNN",
                "Hybrid: Bi-LSTM+CNN",
                "P (%)",
                "R (%)"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_1",
        "paper_id": "P18-1048",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1049table_1",
        "description": "The middle block of Table 1 shows the performance of the pairwise model after applying double-checking. Since all pairs are flipped, double-checking combines results from (ei , ej ) and (ej , ei), picking the label with the higher probability score, which typically boosts performance. The results without double-checking show similar trends. The bottom block of Table 1 presents the results, showing that all models from the present paper outperform existing models from the literature. One may argue the combined system adds more hidden layers over a pre-trained model, which contributes to the improvement in performance. We show a comparison to a baseline model which adds two dense layers on top of the pairwise model, without the GCL. The configuration of the two layers is the same as we used for the GCL models. The result shows that the performance is slightly higher than what we get from the pairwise model, but the difference is smaller than what we get from GCL models -suggesting that the performance improvement with GCL models is not just due to more parameters. We also tried adding an LSTM layer on top of the pre-trained model, and found the system cannot converge.",
        "sentences": [
            "The middle block of Table 1 shows the performance of the pairwise model after applying double-checking.",
            "Since all pairs are flipped, double-checking combines results from (ei , ej ) and (ej , ei), picking the label with the higher probability score, which typically boosts performance.",
            "The results without double-checking show similar trends.",
            "The bottom block of Table 1 presents the results, showing that all models from the present paper outperform existing models from the literature.",
            "One may argue the combined system adds more hidden layers over a pre-trained model, which contributes to the improvement in performance.",
            "We show a comparison to a baseline model which adds two dense layers on top of the pairwise model, without the GCL.",
            "The configuration of the two layers is the same as we used for the GCL models.",
            "The result shows that the performance is slightly higher than what we get from the pairwise model, but the difference is smaller than what we get from GCL models -suggesting that the performance improvement with GCL models is not just due to more parameters.",
            "We also tried adding an LSTM layer on top of the pre-trained model, and found the system cannot converge."
        ],
        "class_sentence": [
            1,
            2,
            2,
            1,
            2,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            [
                "pairwise"
            ],
            [
                "pairwise"
            ],
            [
                "pairwise"
            ],
            [
                "GCL w/ state-tracking controller",
                "GCL w/ pre-trained output layer",
                "GCL w/ stateless controller"
            ],
            [
                "Two more hidden layers",
                "GCL w/ pre-trained output layer"
            ],
            [
                "Model"
            ],
            [
                "GCL w/ state-tracking controller",
                "GCL w/ stateless controller",
                "GCL w/ pre-trained output layer"
            ],
            [
                "pairwise",
                "GCL w/ state-tracking controller",
                "GCL w/ stateless controller",
                "GCL w/ pre-trained output layer"
            ],
            [
                "GCL w/ pre-trained output layer"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_1",
        "paper_id": "P18-1049",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1050table_5",
        "description": "To facilitate direct comparisons, we used the same state-of-the-art temporal relation classification system as described in our previous work Choubey and Huang (2017) and considered all the 14 relations in classification. Choubey and Huang (2017) forms three sequences (i.e., word forms, POS tags, and dependency relations) of context words that align with the dependency path between two event mentions and uses three bidirectional LSTMs to get the embedding of each sequence. The final fully connected layer maps the concatenated embeddings of all sequences to 14 fine-grained temporal relations. We applied the same model here, but if an event pair appears in our learned list of event pairs, we concatenated the CP score of the event pair as additional evidence in the final layer. To be consistent with Choubey and Huang (2017), we used the same train/test splitting, the same parameters for the neural network and only considered intra-sentence event pairs. Table 5 shows that by incorporating our learned event knowledge, the overall prediction accuracy was improved by 1.1%. Not surprisingly, out of the 14 temporal relations, the performance on the relation before was improved the most by 4.9%.",
        "sentences": [
            "To facilitate direct comparisons, we used the same state-of-the-art temporal relation classification system as described in our previous work Choubey and Huang (2017) and considered all the 14 relations in classification.",
            "Choubey and Huang (2017) forms three sequences (i.e., word forms, POS tags, and dependency relations) of context words that align with the dependency path between two event mentions and uses three bidirectional LSTMs to get the embedding of each sequence.",
            "The final fully connected layer maps the concatenated embeddings of all sequences to 14 fine-grained temporal relations.",
            "We applied the same model here, but if an event pair appears in our learned list of event pairs, we concatenated the CP score of the event pair as additional evidence in the final layer.",
            "To be consistent with Choubey and Huang (2017), we used the same train/test splitting, the same parameters for the neural network and only considered intra-sentence event pairs.",
            "Table 5 shows that by incorporating our learned event knowledge, the overall prediction accuracy was improved by 1.1%.",
            "Not surprisingly, out of the 14 temporal relations, the performance on the relation before was improved the most by 4.9%."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            2,
            1,
            2
        ],
        "header_mention": [
            [
                "Choubey and Huang (2017)",
                "Choubey and Huang (2017) + CP score"
            ],
            [
                "Choubey and Huang (2017)"
            ],
            null,
            [
                "Choubey and Huang (2017) + CP score"
            ],
            [
                "Choubey and Huang (2017)",
                "Choubey and Huang (2017) + CP score"
            ],
            [
                "Choubey and Huang (2017)",
                "Choubey and Huang (2017) + CP score"
            ],
            null
        ],
        "n_sentence": 7.0,
        "table_id": "table_5",
        "paper_id": "P18-1050",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1053table_4",
        "description": "Table 4 shows the performance of our model with different random seeds on the test dataset. We report the minimum, the maximum, the median F-scores results and the standard deviation \u0192\u00d0of F-scores. We run the model with 38 different random seeds. The maximum F-score is 57.5% and the minimum on is 56.5%.",
        "sentences": [
            "Table 4 shows the performance of our model with different random seeds on the test dataset.",
            "We report the minimum, the maximum, the median F-scores results and the standard deviation \u0192\u00d0of F-scores.",
            "We run the model with 38 different random seeds.",
            "The maximum F-score is 57.5% and the minimum on is 56.5%."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            [
                "Our model"
            ],
            [
                "Min F",
                "Median F",
                "Max F",
                "\u03c3"
            ],
            null,
            [
                "Max F",
                "Min F"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_4",
        "paper_id": "P18-1053",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1061table_2",
        "description": "We use the official ROUGE script4 (version 1.5.5) to evaluate the summarization output. Table 2 summarizes the results on CNN/Daily Mail data set using full length ROUGE-F15 evaluation. It includes two unsupervised baselines, LEAD3 and TEXTRANK. The table also includes three stateof-the-art neural network based extractive models, i.e., CRSUM, NN-SE and SUMMARUNNER. In addition, we report the state-of-the-art abstractive PGN model. The result of SUMMARUNNER is on the anonymized dataset and not strictly comparable to our results on the non-anonymized version dataset. Therefore, we also include the result of LEAD3 on the anonymized dataset as a reference. NEUSUM achieves 19.01 ROUGE-2 F1 score on the CNN/Daily Mail dataset. Compared to the unsupervised baseline methods, NEUSUM performs better by a large margin. In terms of ROUGE2 F1, NEUSUM outperforms the strong baseline LEAD3 by 1.31 points. NEUSUM also outperforms the neural network based models. Compared to the state-of-the-art extractive model NNSE (Cheng and Lapata, 2016), NEUSUM performs significantly better in terms of ROUGE-1, ROUGE2 and ROUGE-L F1 scores.",
        "sentences": [
            "We use the official ROUGE script4 (version 1.5.5) to evaluate the summarization output.",
            "Table 2 summarizes the results on CNN/Daily Mail data set using full length ROUGE-F15 evaluation.",
            "It includes two unsupervised baselines, LEAD3 and TEXTRANK.",
            "The table also includes three stateof-the-art neural network based extractive models, i.e., CRSUM, NN-SE and SUMMARUNNER.",
            "In addition, we report the state-of-the-art abstractive PGN model.",
            "The result of SUMMARUNNER is on the anonymized dataset and not strictly comparable to our results on the non-anonymized version dataset.",
            "Therefore, we also include the result of LEAD3 on the anonymized dataset as a reference.",
            "NEUSUM achieves 19.01 ROUGE-2 F1 score on the CNN/Daily Mail dataset.",
            "Compared to the unsupervised baseline methods, NEUSUM performs better by a large margin.",
            "In terms of ROUGE2 F1, NEUSUM outperforms the strong baseline LEAD3 by 1.31 points.",
            "NEUSUM also outperforms the neural network based models.",
            "Compared to the state-of-the-art extractive model NNSE (Cheng and Lapata, 2016), NEUSUM performs significantly better in terms of ROUGE-1, ROUGE2 and ROUGE-L F1 scores."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "ROUGE-1",
                "ROUGE-2",
                "ROUGE-L"
            ],
            [
                "LEAD3",
                "TEXTRANK"
            ],
            [
                "CRSUM",
                "NN-SE",
                "SUMMARUNNER \u2021 *"
            ],
            [
                "PGN \u2021"
            ],
            [
                "SUMMARUNNER \u2021 *"
            ],
            [
                "LEAD3 \u2021 *"
            ],
            [
                "NEUSUM",
                "ROUGE-2"
            ],
            [
                "NEUSUM",
                "LEAD3",
                "TEXTRANK",
                "CRSUM",
                "NN-SE",
                "SUMMARUNNER \u2021 *",
                "PGN \u2021"
            ],
            [
                "NEUSUM",
                "ROUGE-2",
                "LEAD3"
            ],
            [
                "NEUSUM",
                "LEAD3",
                "TEXTRANK",
                "CRSUM",
                "NN-SE",
                "SUMMARUNNER \u2021 *",
                "PGN \u2021"
            ],
            [
                "NEUSUM",
                "NN-SE",
                "ROUGE-1",
                "ROUGE-2",
                "ROUGE-L"
            ]
        ],
        "n_sentence": 12.0,
        "table_id": "table_2",
        "paper_id": "P18-1061",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1063table_5",
        "description": "In Table 5, we show the substantial test-time speed-up of our model compared to See et al. (2017).18. We calculate the total decoding time for producing all summaries for the test set.19. Due to the fact that the main test-time speed bottleneck of RNN language generation model is that the model is constrained to generate one word at a time, the total decoding time is dependent on the number of total words generated; we hence also report the decoded words per second for a fair comparison. Our model without reranking is extremely fast. From Table 5 we can see that we achieve a speed up of 18x in time and 24x in word generation rate. Even after adding the (optional) reranker, we still maintain a 6-7x speed-up (and hence a user can choose to use the reranking component depending on their downstream application\u0081fs speed requirements).20.",
        "sentences": [
            "In Table 5, we show the substantial test-time speed-up of our model compared to See et al. (2017).18.",
            "We calculate the total decoding time for producing all summaries for the test set.19.",
            "Due to the fact that the main test-time speed bottleneck of RNN language generation model is that the model is constrained to generate one word at a time, the total decoding time is dependent on the number of total words generated; we hence also report the decoded words per second for a fair comparison.",
            "Our model without reranking is extremely fast.",
            "From Table 5 we can see that we achieve a speed up of 18x in time and 24x in word generation rate.",
            "Even after adding the (optional) reranker, we still maintain a 6-7x speed-up (and hence a user can choose to use the reranking component depending on their downstream application\u0081fs speed requirements).20."
        ],
        "class_sentence": [
            1,
            2,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "total time (hr)",
                "words / sec"
            ],
            null,
            null,
            [
                "rnn-ext + abs + RL"
            ],
            [
                "rnn-ext + abs + RL",
                "total time (hr)",
                "words / sec"
            ],
            [
                "rnn-ext + abs + RL + rerank",
                "total time (hr)",
                "words / sec"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_5",
        "paper_id": "P18-1063",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1064table_4",
        "description": "We also show human evaluation results on the Gigaword dataset in Table 4 (again based on pairwise comparisons for 100 samples), where we see that our MTL model is better than our state-of-theart baseline on both relevance and readability.7.",
        "sentences": [
            "We also show human evaluation results on the Gigaword dataset in Table 4 (again based on pairwise comparisons for 100 samples), where we see that our MTL model is better than our state-of-theart baseline on both relevance and readability.7."
        ],
        "class_sentence": [
            1
        ],
        "header_mention": [
            [
                "MTL wins",
                "Baseline wins"
            ]
        ],
        "n_sentence": 1.0,
        "table_id": "table_4",
        "paper_id": "P18-1064",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1064table_6",
        "description": " Table 6 compares our model\u00e2\u20ac\u2122s performance to Pasunuru and Bansal (2017). Our pointer mechanism gives a performance boost, since the entailment generation task involves copying from the given premise sentence, whereas the 2-layer model seems comparable to the 1-layer model. Also, the supplementary shows some output examples from our entailment generation model.",
        "sentences": [
            " Table 6 compares our model\u00e2\u20ac\u2122s performance to Pasunuru and Bansal (2017).",
            "Our pointer mechanism gives a performance boost, since the entailment generation task involves copying from the given premise sentence, whereas the 2-layer model seems comparable to the 1-layer model.",
            "Also, the supplementary shows some output examples from our entailment generation model."
        ],
        "class_sentence": [
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "Our 1-layer pointer EG",
                "Our 2-layer pointer EG",
                "Pasunuru&Bansal (2017)"
            ],
            [
                "Our 2-layer pointer EG",
                "Our 1-layer pointer EG"
            ],
            [
                "Our 1-layer pointer EG",
                "Our 2-layer pointer EG"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_6",
        "paper_id": "P18-1064",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1064table_9",
        "description": "We employ a state-of-the-art entailment classifier (Chen et al., 2017), and calculate the average of the entailment probability of each of the output summary\u0081fs sentences being entailed by the input source document.  We do this for output summaries of our baseline and 2-way-EG multi-task model (with entailment generation). As can be seen in Table 9, our multi-task model improves upon the baseline in the aspect of being entailed by the source document (with statistical significance p < 0.001).",
        "sentences": [
            "We employ a state-of-the-art entailment classifier (Chen et al., 2017), and calculate the average of the entailment probability of each of the output summary\u0081fs sentences being entailed by the input source document.",
            " We do this for output summaries of our baseline and 2-way-EG multi-task model (with entailment generation).",
            "As can be seen in Table 9, our multi-task model improves upon the baseline in the aspect of being entailed by the source document (with statistical significance p < 0.001)."
        ],
        "class_sentence": [
            2,
            2,
            1
        ],
        "header_mention": [
            [
                "Average Entailment Probability"
            ],
            [
                "Baseline",
                "Multi-Task (EG)"
            ],
            [
                "Multi-Task (EG)",
                "Baseline"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_9",
        "paper_id": "P18-1064",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1067table_6",
        "description": " Table 6 shows an overview of the average results of our supervised experiments for five of the PSL models. The first column lists the SVM or PSL model. The second column presents the results of a given model when using the MFD as the source of the unigrams for the initial model (M1). The final column shows the results when the AR unigrams are used as the initial source of supervision. The first two rows show the results of predicting the morals present in tweets using a bag-of-words (BoW) approach. Both the SVM and PSL models perform poorly due to the eleven predictive classes and noisy input features. The third row shows the results when taking a majority vote over the presence of MFD unigrams, similar to previous works. This approach is simpler and less noisy than M1, the PSL model closest to this approach.",
        "sentences": [
            " Table 6 shows an overview of the average results of our supervised experiments for five of the PSL models.",
            "The first column lists the SVM or PSL model.",
            "The second column presents the results of a given model when using the MFD as the source of the unigrams for the initial model (M1).",
            "The final column shows the results when the AR unigrams are used as the initial source of supervision.",
            "The first two rows show the results of predicting the morals present in tweets using a bag-of-words (BoW) approach.",
            "Both the SVM and PSL models perform poorly due to the eleven predictive classes and noisy input features.",
            "The third row shows the results when taking a majority vote over the presence of MFD unigrams, similar to previous works.",
            "This approach is simpler and less noisy than M1, the PSL model closest to this approach."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "SVM BOW",
                "PSL BOW",
                "M1 (UNIGRAMS)",
                "M3 (+ POLITICAL INFO)",
                "M5 (+ FRAMES)",
                "M9 (+ BIGRAMS)",
                "M13 (ALL FEATURES)"
            ],
            [
                "SVM BOW",
                "PSL BOW",
                "M1 (UNIGRAMS)",
                "M3 (+ POLITICAL INFO)",
                "M5 (+ FRAMES)",
                "M9 (+ BIGRAMS)",
                "M13 (ALL FEATURES)"
            ],
            [
                "MFD"
            ],
            [
                "AR"
            ],
            [
                "SVM BOW",
                "PSL BOW"
            ],
            [
                "SVM BOW",
                "PSL BOW"
            ],
            [
                "MAJORITY VOTE",
                "MFD"
            ],
            [
                "MAJORITY VOTE",
                "M1 (UNIGRAMS)"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_6",
        "paper_id": "P18-1067",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1067table_9",
        "description": "Table 9 shows the macro-weighted average F1 scores for three different models. The BASELINE model shows the results of predicting only the MORAL of the tweet using the non-joint model M13, which uses all features with frames initialized. The JOINT model is designed to predict both the moral foundation and frame of a tweet simultaneously (as shown in Table 5), with no frame initialization. Finally, the SKYLINE model is M13 with all features, where the frames are initialized with their known values. The joint model using AR unigrams outperforms the baseline, showing that there is some benefit to modeling moral foundations and frames together, as well as using domain-specific unigrams. However, it is unable to beat the MFDbased unigrams model. This is likely due to the large amount of noise introduced by incorrect frame predictions into the joint model. As expected, the joint model does not outperform the skyline model which is able to use the known values of the frames in order to accurately classify the moral foundations associated with the tweets.",
        "sentences": [
            "Table 9 shows the macro-weighted average F1 scores for three different models.",
            "The BASELINE model shows the results of predicting only the MORAL of the tweet using the non-joint model M13, which uses all features with frames initialized.",
            "The JOINT model is designed to predict both the moral foundation and frame of a tweet simultaneously (as shown in Table 5), with no frame initialization.",
            "Finally, the SKYLINE model is M13 with all features, where the frames are initialized with their known values.",
            "The joint model using AR unigrams outperforms the baseline, showing that there is some benefit to modeling moral foundations and frames together, as well as using domain-specific unigrams.",
            "However, it is unable to beat the MFDbased unigrams model.",
            "This is likely due to the large amount of noise introduced by incorrect frame predictions into the joint model.",
            "As expected, the joint model does not outperform the skyline model which is able to use the known values of the frames in order to accurately classify the moral foundations associated with the tweets."
        ],
        "class_sentence": [
            1,
            2,
            2,
            2,
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            [
                "BASELINE",
                "JOINT",
                "SKYLINE"
            ],
            [
                "BASELINE"
            ],
            [
                "JOINT"
            ],
            [
                "SKYLINE"
            ],
            [
                "BASELINE",
                "AR",
                "MFD"
            ],
            [
                "MFD"
            ],
            [
                "JOINT"
            ],
            [
                "JOINT",
                "SKYLINE"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_9",
        "paper_id": "P18-1067",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1068table_3",
        "description": "Table 3 reports results on DJANGO where we observe similar tendencies. COARSE2FINE outperforms ONESTAGE by a wide margin. It is also superior to the best reported result in the literature (SNM+COPY; see the second block in the table). Again we observe that the sketch encoder is beneficial and that there is an 8.9 point difference in accuracy between COARSE2FINE and the oracle.",
        "sentences": [
            "Table 3 reports results on DJANGO where we observe similar tendencies.",
            "COARSE2FINE outperforms ONESTAGE by a wide margin.",
            "It is also superior to the best reported result in the literature (SNM+COPY; see the second block in the table).",
            "Again we observe that the sketch encoder is beneficial and that there is an 8.9 point difference in accuracy between COARSE2FINE and the oracle."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "COARSE2FINE",
                "ONESTAGE"
            ],
            [
                "COARSE2FINE",
                "SNM+COPY (Yin and Neubig 2017)"
            ],
            [
                "COARSE2FINE - sketch encoder",
                "COARSE2FINE + oracle sketch",
                "COARSE2FINE"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "P18-1068",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1069table_5",
        "description": "Table 5 shows the relative importance of individual metrics in the regression model. As importance score we use the average gain (i.e., loss reduction) brought by the confidence metric once added as feature to the branch of the decision tree (Chen and Guestrin, 2016). The results indicate that model uncertainty (Noise/Dropout/Posterior/Perplexity) plays the most important role. On IFTTT, the number of unknown tokens (#UNK) and the variance of top candidates (var(K-best)) are also very helpful because this dataset is relatively noisy and contains many ambiguous inputs. Dout is short for dropout, PR for posterior probability, PPL for perplexity, LM for probability based on a language model,\r\n#UNK for number of unknown tokens, Var for variance of top candidates, and Ent for Entropy.",
        "sentences": [
            "Table 5 shows the relative importance of individual metrics in the regression model.",
            "As importance score we use the average gain (i.e., loss reduction) brought by the confidence metric once added as feature to the branch of the decision tree (Chen and Guestrin, 2016).",
            "The results indicate that model uncertainty (Noise/Dropout/Posterior/Perplexity) plays the most important role.",
            "On IFTTT, the number of unknown tokens (#UNK) and the variance of top candidates (var(K-best)) are also very helpful because this dataset is relatively noisy and contains many ambiguous inputs.",
            "Dout is short for dropout, PR for posterior probability, PPL for perplexity, LM for probability based on a language model,\r\n#UNK for number of unknown tokens, Var for variance of top candidates, and Ent for Entropy."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "Metric"
            ],
            null,
            [
                "Noise",
                "Dout",
                "PR",
                "PPL"
            ],
            [
                "IFTTT",
                "#UNK",
                "Var"
            ],
            [
                "Dout",
                "PR",
                "PPL",
                "LM",
                "#UNK",
                "Var",
                "Ent"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_5",
        "paper_id": "P18-1069",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1073table_3",
        "description": "Table 3 shows the results of the proposed method in comparison to previous systems, including those with different degrees of supervision. We focus on the widely used English-Italian dataset of Dinu et al(2015) and its extensions. Despite being fully unsupervised, our method achieves the best results in all language pairs but one, even surpassing previous supervised approaches. The only exception is English-Finnish, where Artetxe et al.(2018a) gets marginally better results with a difference of 0.3 points, yet ours is the only unsupervised system that works for this pair. At the same time, it is remarkable that the proposed system gets substantially better results than Artetxe et al.(2017), the only other system based on selflearning, with the additional advantage of being fully unsupervised.",
        "sentences": [
            "Table 3 shows the results of the proposed method in comparison to previous systems, including those with different degrees of supervision.",
            "We focus on the widely used English-Italian dataset of Dinu et al(2015) and its extensions.",
            "Despite being fully unsupervised, our method achieves the best results in all language pairs but one, even surpassing previous supervised approaches.",
            "The only exception is English-Finnish, where Artetxe et al.(2018a) gets marginally better results with a difference of 0.3 points, yet ours is the only unsupervised system that works for this pair.",
            "At the same time, it is remarkable that the proposed system gets substantially better results than Artetxe et al.(2017), the only other system based on selflearning, with the additional advantage of being fully unsupervised."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "Proposed method"
            ],
            [
                "Dinu et al. (2015)",
                "EN-IT"
            ],
            [
                "Proposed method"
            ],
            [
                "Proposed method",
                "EN-FI",
                "Artetxe et al. (2018a)"
            ],
            [
                "Proposed method",
                "Artetxe et al. (2017)"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_3",
        "paper_id": "P18-1073",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1074table_6",
        "description": "As Table 6 shows, adding each component usually enhances the performance (F-score, %), while the impact also depends on the size of the target task data. For example, the language-specific layer slightly impairs the performance with only 10 training sentences. However, this is unsurprising as it introduces additional parameters that are only trained by the target task data.",
        "sentences": [
            "As Table 6 shows, adding each component usually enhances the performance (F-score, %), while the impact also depends on the size of the target task data.",
            "For example, the language-specific layer slightly impairs the performance with only 10 training sentences.",
            "However, this is unsurprising as it introduces additional parameters that are only trained by the target task data."
        ],
        "class_sentence": [
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "Basic + C",
                "Basic + CL",
                "Basic + CLS",
                "Basic + CLSH",
                "Basic + CLSHD",
                "0",
                "10",
                "100",
                "200",
                "All"
            ],
            [
                "Basic + CLS",
                "Basic + CLSH",
                "Basic + CLSHD",
                "10"
            ],
            null
        ],
        "n_sentence": 3.0,
        "table_id": "table_6",
        "paper_id": "P18-1074",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1075table_2",
        "description": "Table 2 compares its performance with our adapted BWEs, with both cosine similarity and classification based systems. \"top\"F1 scores are based on the most probable word as prediction only; \"all\"F1 scores use all words as prediction whose probability is above the threshold. It can be seen that the cosine similarity based system using adapted BWEs clearly outperforms the nonadapted BWEs which were trained in a resource poor setup.4. Moreover, the best performance was reached using the general seed lexicon for the mapping which is due to the fact that general domain words have better quality embeddings in the MWE models, which in turn gives a better quality mapping. The classification based system performs significantly better comparing to cosine similarity by exploiting the seed lexicon better. Using adapted BWEs as input word embeddings for the system further improvements were achieved which shows the better quality of our BWEs. Simulating an even poorer setup by using a general lexicon, the performance gain of the classifier is lower. This shows the significance of the medical seed lexicon for this system. On the other hand, adapted BWEs have better performance compared to non-adapted ones using the best translation while they have just slightly lower F1 using multiple translations. This result shows that while with adapted BWEs the system predicts better \u0081gtop\u0081h translations, it has a harder time when predicting \u0081gall\u0081h due to the increased vocabulary size.",
        "sentences": [
            "Table 2 compares its performance with our adapted BWEs, with both cosine similarity and classification based systems.",
            "\"top\"F1 scores are based on the most probable word as prediction only; \"all\"F1 scores use all words as prediction whose probability is above the threshold.",
            "It can be seen that the cosine similarity based system using adapted BWEs clearly outperforms the nonadapted BWEs which were trained in a resource poor setup.4.",
            "Moreover, the best performance was reached using the general seed lexicon for the mapping which is due to the fact that general domain words have better quality embeddings in the MWE models, which in turn gives a better quality mapping.",
            "The classification based system performs significantly better comparing to cosine similarity by exploiting the seed lexicon better.",
            "Using adapted BWEs as input word embeddings for the system further improvements were achieved which shows the better quality of our BWEs.",
            "Simulating an even poorer setup by using a general lexicon, the performance gain of the classifier is lower.",
            "This shows the significance of the medical seed lexicon for this system.",
            "On the other hand, adapted BWEs have better performance compared to non-adapted ones using the best translation while they have just slightly lower F1 using multiple translations.",
            "This result shows that while with adapted BWEs the system predicts better \u0081gtop\u0081h translations, it has a harder time when predicting \u0081gall\u0081h due to the increased vocabulary size."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            1,
            1,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "Adapted medical lexicon",
                "Adapted BNC lexicon",
                "cosine similarity",
                "classifier"
            ],
            [
                "F1 (top)",
                "F1 (all)"
            ],
            [
                "cosine similarity",
                "F1 (top)",
                "F1 (all)",
                "Baseline",
                "Adapted BNC lexicon"
            ],
            null,
            [
                "classifier",
                "cosine similarity"
            ],
            [
                "classifier",
                "Adapted BNC lexicon"
            ],
            null,
            [
                "Adapted medical lexicon"
            ],
            [
                "Adapted BNC lexicon",
                "Baseline BNC lexicon",
                "classifier",
                "F1 (all)"
            ],
            [
                "Adapted BNC lexicon",
                "F1 (top)",
                "F1 (all)"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_2",
        "paper_id": "P18-1075",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1075table_5",
        "description": "Results in Table 5 show that adding semisup to the classifier further increases performance for BLI as well. For the baseline system, when using only in-domain text for creating BWEs, only the medical unlabeled set was effective, general domain word pairs could not be exploited due to the lack of general semantic knowledge in the BWE model. On the other hand, by using our domain adapted BWEs, which contain both general domain and in-domain semantical knowledge, we can exploit word pairs from both domains. Results for adapted BWEs increased in 3 out of 4 cases, where the only exception is when using multiple translations for a given source word (which may have been caused by the bigger vocabulary size).",
        "sentences": [
            "Results in Table 5 show that adding semisup to the classifier further increases performance for BLI as well.",
            "For the baseline system, when using only in-domain text for creating BWEs, only the medical unlabeled set was effective, general domain word pairs could not be exploited due to the lack of general semantic knowledge in the BWE model.",
            "On the other hand, by using our domain adapted BWEs, which contain both general domain and in-domain semantical knowledge, we can exploit word pairs from both domains.",
            "Results for adapted BWEs increased in 3 out of 4 cases, where the only exception is when using multiple translations for a given source word (which may have been caused by the bigger vocabulary size)."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Baseline+BNC",
                "Baseline+medical"
            ],
            [
                "Adapted+BNC",
                "Adapted+medical"
            ],
            [
                "F1 (top)",
                "F1 (all)",
                "Adapted+BNC",
                "Adapted+medical"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_5",
        "paper_id": "P18-1075",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1076table_4",
        "description": "Table 4 shows that the combination of different interactions between ctx and ctx+kn representations leads to clear improvement over the w/o knowledge setup, in particular for the Common Nouns dataset. We also performed ablations for a model with 100 facts (see Supplement).",
        "sentences": [
            "Table 4 shows that the combination of different interactions between ctx and ctx+kn representations leads to clear improvement over the w/o knowledge setup, in particular for the Common Nouns dataset.",
            "We also performed ablations for a model with 100 facts (see Supplement)."
        ],
        "class_sentence": [
            1,
            0
        ],
        "header_mention": [
            [
                "CN",
                "Dctx+kn Qctx+kn",
                "Dctx Qctx+kn",
                "Dctx+kn Qctx"
            ],
            null
        ],
        "n_sentence": 2.0,
        "table_id": "table_4",
        "paper_id": "P18-1076",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1076table_6",
        "description": "Table 6 compares our model (Knowledgeable Reader) to previous work on the CBT datasets. We show the results of our model with the settings that performed best on the Dev sets of the two datasets NE and CN: for NE, (Dctx+kn, Qctx) with 100 facts; for CN the Full model with 50 facts, both with CN5Sel. Note that our work focuses on the impact of external knowledge and employs a single interaction (single-hop) between the document context and the question so we primarily compare to and aim at improving over similar models. KnReader clearly outperforms prior single-hop models on both datasets. While we do not improve over the state of the art, our model stands well among other models that perform multiple hops.",
        "sentences": [
            "Table 6 compares our model (Knowledgeable Reader) to previous work on the CBT datasets.",
            "We show the results of our model with the settings that performed best on the Dev sets of the two datasets NE and CN: for NE, (Dctx+kn, Qctx) with 100 facts; for CN the Full model with 50 facts, both with CN5Sel.",
            "Note that our work focuses on the impact of external knowledge and employs a single interaction (single-hop) between the document context and the question so we primarily compare to and aim at improving over similar models.",
            "KnReader clearly outperforms prior single-hop models on both datasets.",
            "While we do not improve over the state of the art, our model stands well among other models that perform multiple hops."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "KnReader (ours)"
            ],
            [
                "KnReader (ours)",
                "NE",
                "CN"
            ],
            [
                "Single interaction"
            ],
            [
                "KnReader (ours)",
                "Single interaction",
                "NE",
                "CN"
            ],
            [
                "KnReader (ours)",
                "Multiple interactions"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_6",
        "paper_id": "P18-1076",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1084table_2",
        "description": "We report two standard evaluation metrics: 1) Recall at 1 (R@1) scores, and 2) the sentence-level BLEU+1 metric (Lin and Och, 2004), a variant of BLEU which smooths terms for higher-order n-grams, making it more suitable for evaluating short sentences. The scores for the retrieval task with all models are summarized in Table 2. The results clearly demonstrate the superiority of DPCCA (with a slight advantage to the more complex Variant B) and of the concatenation of their representation with that of the DCCA NOI (strongest) baseline.",
        "sentences": [
            "We report two standard evaluation metrics: 1) Recall at 1 (R@1) scores, and 2) the sentence-level BLEU+1 metric (Lin and Och, 2004), a variant of BLEU which smooths terms for higher-order n-grams, making it more suitable for evaluating short sentences.",
            "The scores for the retrieval task with all models are summarized in Table 2.",
            "The results clearly demonstrate the superiority of DPCCA (with a slight advantage to the more complex Variant B) and of the concatenation of their representation with that of the DCCA NOI (strongest) baseline."
        ],
        "class_sentence": [
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "R@1",
                "BLEU+1"
            ],
            null,
            [
                "DPCCA(B)+DCCA NOI (concat)"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_2",
        "paper_id": "P18-1084",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1084table_3",
        "description": "The results on the POS classes represented in SimLex-999 (nouns, verbs, adjectives, Table 3) form our main finding: conditioning the multilingual representations on a shared image leads to improvements in verb and adjective representations. While for nouns one of the DPCCA variants is the best performing model for both languages, the gaps from the best performing baselines are much smaller. This is interesting since, e.g., verbs are more abstract than nouns (Hartmann and S?gaard,2017; Hill et al., 2014).",
        "sentences": [
            "The results on the POS classes represented in SimLex-999 (nouns, verbs, adjectives, Table 3) form our main finding: conditioning the multilingual representations on a shared image leads to improvements in verb and adjective representations.",
            "While for nouns one of the DPCCA variants is the best performing model for both languages, the gaps from the best performing baselines are much smaller.",
            "This is interesting since, e.g., verbs are more abstract than nouns (Hartmann and S?gaard,2017; Hill et al., 2014)."
        ],
        "class_sentence": [
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "EN-Adj",
                "EN-Verbs",
                "DE-Adj",
                "DE-Verbs"
            ],
            [
                "DPCCA (Variant A)",
                "DPCCA (Variant B)",
                "EN-Nouns",
                "DE-Nouns"
            ],
            null
        ],
        "n_sentence": 3.0,
        "table_id": "table_3",
        "paper_id": "P18-1084",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1084table_4",
        "description": "Further, Table 4 presents results on all SimLex word pairs. The POS class result patterns for EN-IT and EN-RU are very similar to the patterns in Table 3 and are provided in the supplementary material. First, the results over the initial monolingual embeddings before training (INIT EMB) clearly indicate that multilingual information is beneficial for the word similarity task. We observe improvements with all models (the only exception being extremely lowscoring PPCCA and NCCA, not shown). Moreover, by additionally grounding concepts from two languages in the visual modality it is possible to further boost word similarity scores.",
        "sentences": [
            "Further, Table 4 presents results on all SimLex word pairs.",
            "The POS class result patterns for EN-IT and EN-RU are very similar to the patterns in Table 3 and are provided in the supplementary material.",
            "First, the results over the initial monolingual embeddings before training (INIT EMB) clearly indicate that multilingual information is beneficial for the word similarity task.",
            "We observe improvements with all models (the only exception being extremely lowscoring PPCCA and NCCA, not shown).",
            "Moreover, by additionally grounding concepts from two languages in the visual modality it is possible to further boost word similarity scores."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "EN",
                "IT",
                "RU"
            ],
            [
                "INIT EMB"
            ],
            [
                "DPCCA (A)",
                "DPCCA (B)",
                "PCCA",
                "DCCA NOI",
                "GCCA"
            ],
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_4",
        "paper_id": "P18-1084",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1085table_3",
        "description": "Table 3 displays our results, from which several observations can be made. First, we observe that combining Glove and Picturebook leads to improved similarity across most categories. For adjectives and the most abstract category, Glove performs significantly better, while for the most concrete category Picturebook is significantly better. This result confirms that Glove and Picturebook capture very different properties of words. Next we observe that the performance of Picturebook gets progressively better across each concreteness quartile rating, with a 20 point improvement over Glove for the most concrete category. For the hardest subset of words, Picturebook performs slightly better than Glove while Glove performs better across all pairs. We also compare to a convolutional network trained with visual similarity. We observe a performance difference between our visual and semantic embeddings: on all categories except verbs, the semantic embeddings outperform visual ones, even on the most concrete categories. This indicates the importance of the type of similarity used for training the model. Finally we note that adding more images nearly consistently improves similarity scores across categories. Kiela et al (2016) showed that after 10-20 images, performance tends to saturate. All subsequent experiments use 10 images with semantic Picturebook.",
        "sentences": [
            "Table 3 displays our results, from which several observations can be made.",
            "First, we observe that combining Glove and Picturebook leads to improved similarity across most categories.",
            "For adjectives and the most abstract category, Glove performs significantly better, while for the most concrete category Picturebook is significantly better.",
            "This result confirms that Glove and Picturebook capture very different properties of words.",
            "Next we observe that the performance of Picturebook gets progressively better across each concreteness quartile rating, with a 20 point improvement over Glove for the most concrete category.",
            "For the hardest subset of words, Picturebook performs slightly better than Glove while Glove performs better across all pairs.",
            "We also compare to a convolutional network trained with visual similarity.",
            "We observe a performance difference between our visual and semantic embeddings: on all categories except verbs, the semantic embeddings outperform visual ones, even on the most concrete categories.",
            "This indicates the importance of the type of similarity used for training the model.",
            "Finally we note that adding more images nearly consistently improves similarity scores across categories.",
            "Kiela et al (2016) showed that after 10-20 images, performance tends to saturate. All subsequent experiments use 10 images with semantic Picturebook."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Glove + Picturebook"
            ],
            [
                "adjs",
                "Glove",
                "conc-q4",
                "Picturebook"
            ],
            [
                "Glove",
                "Picturebook"
            ],
            [
                "Picturebook",
                "conc-q1",
                "conc-q2",
                "conc-q3",
                "conc-q4"
            ],
            [
                "hard",
                "Picturebook",
                "all",
                "Glove"
            ],
            [
                "Picturebook (Visual)",
                "Picturebook (Semantic)"
            ],
            [
                "Picturebook (Visual)",
                "Picturebook (Semantic)",
                "all",
                "adjs",
                "nouns",
                "conc-q1",
                "conc-q2",
                "conc-q3",
                "conc-q4",
                "hard"
            ],
            [
                "Picturebook (Visual)",
                "Picturebook (Semantic)"
            ],
            [
                "Picturebook (1)",
                "Picturebook (2)",
                "Picturebook (3)",
                "Picturebook (5)",
                "Picturebook (10)"
            ],
            [
                "Picturebook (10)"
            ]
        ],
        "n_sentence": 11.0,
        "table_id": "table_3",
        "paper_id": "P18-1085",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1085table_4",
        "description": "Table 4 displays our results. For BoW models, adding Picturebook embeddings to Glove results in significant gains across all three tasks. For BiLSTM-Max, our contextual gating sets a new state-of-the-art on SNLI sentence encoding methods (methods without interaction layers), outperforming the recently proposed methods of Im and Cho (2017); Shen et al.(2018). It is worth noting the effect that different encoders have when using our embeddings. While non-contextual gating is sufficient to improve bag-of-words methods, with BiLSTM-Max it slightly hurts performance over the Glove baseline. Adding contextual gating was necessary to improve over the Glove baseline on SNLI. Finally we note the strength of our own Glove baseline over the reported results of Conneau et al.(2017a), from which we improve on their accuracy from 85.0 to 86.8 on the development set.",
        "sentences": [
            "Table 4 displays our results.",
            "For BoW models, adding Picturebook embeddings to Glove results in significant gains across all three tasks.",
            "For BiLSTM-Max, our contextual gating sets a new state-of-the-art on SNLI sentence encoding methods (methods without interaction layers), outperforming the recently proposed methods of Im and Cho (2017); Shen et al.(2018).",
            "It is worth noting the effect that different encoders have when using our embeddings.",
            "While non-contextual gating is sufficient to improve bag-of-words methods, with BiLSTM-Max it slightly hurts performance over the Glove baseline.",
            "Adding contextual gating was necessary to improve over the Glove baseline on SNLI.",
            "Finally we note the strength of our own Glove baseline over the reported results of Conneau et al.(2017a), from which we improve on their accuracy from 85.0 to 86.8 on the development set."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Glove (bow)",
                "Picturebook (bow)",
                "Glove + Picturebook (bow)"
            ],
            [
                "BiLSTM-Max (Conneau et al. 2017a)",
                "SNLI"
            ],
            null,
            [
                "Glove (bow)",
                "Picturebook (bow)",
                "Glove + Picturebook (bow)",
                "BiLSTM-Max (Conneau et al. 2017a)",
                "Glove"
            ],
            [
                "Glove + Picturebook + Contextual Gating",
                "SNLI"
            ],
            [
                "BiLSTM-Max (Conneau et al. 2017a)",
                "Glove"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_4",
        "paper_id": "P18-1085",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1085table_6",
        "description": "Table 6 displays our  results on this task. Our Glove baseline was able to match or outperform the reported results in Faghri et al.(2017) with the exception of Recall@10 for image annotation, where it performs slightly worse. Glove+Picturebook improves over the Glove baseline for image search but falls short on image annotation. However, using contextual gating results in improvements over the baseline on all metrics except R@1 for image annotation.",
        "sentences": [
            "Table 6 displays our  results on this task.",
            "Our Glove baseline was able to match or outperform the reported results in Faghri et al.(2017) with the exception of Recall@10 for image annotation, where it performs slightly worse.",
            "Glove+Picturebook improves over the Glove baseline for image search but falls short on image annotation.",
            "However, using contextual gating results in improvements over the baseline on all metrics except R@1 for image annotation."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Glove",
                "VSE++ (Faghri et al. 2017)"
            ],
            [
                "Glove + Picturebook",
                "Image Search",
                "Glove",
                "R@10"
            ],
            [
                "Glove + Picturebook + Contextual Gating",
                "Image Annotation",
                "R@5",
                "R@10",
                "Image Search",
                "R@1"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_6",
        "paper_id": "P18-1085",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1085table_7",
        "description": "On the English \u0081\u00a8 German tasks, we find our Picturebook model to perform on average 0.8 BLEU or 0.7 METEOR over our baseline. On the German task, compared to the previously best published results (Caglayan et al.,2017) we do better in BLEU but slightly worse in METEOR. We suspect this is due to the fact that we did not use BPE.",
        "sentences": [
            "On the English \u0081\u00a8 German tasks, we find our Picturebook model to perform on average 0.8 BLEU or 0.7 METEOR over our baseline.",
            "On the German task, compared to the previously best published results (Caglayan et al.,2017) we do better in BLEU but slightly worse in METEOR.",
            "We suspect this is due to the fact that we did not use BPE."
        ],
        "class_sentence": [
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "Picturebook",
                "BLEU",
                "METEOR"
            ],
            [
                "BPE (Caglayan et al. 2017)",
                "Picturebook",
                "Picturebook + Inverse Picturebook",
                "Picturebook + Inverse Picturebook + Gating",
                "BLEU",
                "METEOR"
            ],
            null
        ],
        "n_sentence": 3.0,
        "table_id": "table_7",
        "paper_id": "P18-1085",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1087table_3",
        "description": "As shown in Table 3, both TNet-LF and TNet-AS consistently achieve the best performance on all datasets, which verifies the efficacy of our whole TNet model. Moreover, TNet can perform well for different kinds of user generated content, such as product reviews with relatively formal sentences in LAPTOP and REST, and tweets with more ungrammatical sentences in TWITTER. The reason is the CNN-based feature extractor arms TNet with more power to extract accurate features from ungrammatical sentences.  Indeed, we can also observe that another CNN-based baseline, i.e., CNNASP implemented by us, also obtains good results on TWITTER. On the other hand, the performance of those comparison methods is mostly unstable. For the tweet in TWITTER, the competitive BILSTMATT-G and RAM cannot perform as effective as they do for the reviews in LAPTOP and REST, due to the fact that they are heavily rooted in LSTMs and the ungrammatical sentences hinder their capability in capturing the context features.  Another difficulty caused by the ungrammatical sentences is that the dependency parsing might be errorprone, which will affect those methods such as AdaRNN using dependency information. To investigate the impact of each component such as deep transformation, context-preserving mechanism, and positional relevance, we perform comparison between the full TNet models and its ablations (the third group in Table 3). After removing the deep transformation (i.e., the techniques introduced in Section 2.2), both TNet-LF and TNetAS are reduced to TNet w/o transformation (where position relevance is kept), and their results in both accuracy and F1 measure are incomparable with those of TNet.  It shows that the integration of target information into the word-level representations is crucial for good performanc. Comparing the results of TNet and TNet w/o context (where TST and position relevance are kept), we observe that the performance of TNet w/o context drops significantly on LAPTOP and REST7, while on TWITTER, TNet w/o context performs very competitive (p-values with TNetLF and TNet-AS are 0.066 and 0.053 respectively for Accuracy). Again, we could attribute this phenomenon to the ungrammatical user generated content of twitter, because the contextpreserving component becomes less important for such data. TNet w/o context performs consistently better than TNet w/o transformation, which verifies the efficacy of the target specific transformation (TST), before applying context-preserving. As for the position information, we conduct statistical t-test between TNet-LF/AS and TNetLF/AS w/o position together with performance comparison.",
        "sentences": [
            "As shown in Table 3, both TNet-LF and TNet-AS consistently achieve the best performance on all datasets, which verifies the efficacy of our whole TNet model.",
            "Moreover, TNet can perform well for different kinds of user generated content, such as product reviews with relatively formal sentences in LAPTOP and REST, and tweets with more ungrammatical sentences in TWITTER.",
            "The reason is the CNN-based feature extractor arms TNet with more power to extract accurate features from ungrammatical sentences.",
            " Indeed, we can also observe that another CNN-based baseline, i.e., CNNASP implemented by us, also obtains good results on TWITTER.",
            "On the other hand, the performance of those comparison methods is mostly unstable.",
            "For the tweet in TWITTER, the competitive BILSTMATT-G and RAM cannot perform as effective as they do for the reviews in LAPTOP and REST, due to the fact that they are heavily rooted in LSTMs and the ungrammatical sentences hinder their capability in capturing the context features.",
            " Another difficulty caused by the ungrammatical sentences is that the dependency parsing might be errorprone, which will affect those methods such as AdaRNN using dependency information.",
            "To investigate the impact of each component such as deep transformation, context-preserving mechanism, and positional relevance, we perform comparison between the full TNet models and its ablations (the third group in Table 3).",
            "After removing the deep transformation (i.e., the techniques introduced in Section 2.2), both TNet-LF and TNetAS are reduced to TNet w/o transformation (where position relevance is kept), and their results in both accuracy and F1 measure are incomparable with those of TNet.",
            " It shows that the integration of target information into the word-level representations is crucial for good performanc.",
            "Comparing the results of TNet and TNet w/o context (where TST and position relevance are kept), we observe that the performance of TNet w/o context drops significantly on LAPTOP and REST7, while on TWITTER, TNet w/o context performs very competitive (p-values with TNetLF and TNet-AS are 0.066 and 0.053 respectively for Accuracy).",
            "Again, we could attribute this phenomenon to the ungrammatical user generated content of twitter, because the contextpreserving component becomes less important for such data.",
            "TNet w/o context performs consistently better than TNet w/o transformation, which verifies the efficacy of the target specific transformation (TST), before applying context-preserving.",
            "As for the position information, we conduct statistical t-test between TNet-LF/AS and TNetLF/AS w/o position together with performance comparison."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            2,
            1,
            2,
            1,
            1,
            2,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            [
                "TNet-LF",
                "TNet-AS"
            ],
            [
                "TNet-LF",
                "TNet-AS",
                "LAPTOP",
                "REST",
                "TWITTER"
            ],
            [
                "TNet variants",
                "CNN-ASP"
            ],
            [
                "CNN-ASP",
                "TWITTER",
                "TNet-LF",
                "TNet-AS"
            ],
            null,
            [
                "BILSTM-ATT-G",
                "RAM",
                "LAPTOP",
                "REST"
            ],
            [
                "AdaRNN"
            ],
            [
                "TNet w/o transformation",
                "TNet w/o context",
                "TNet-LF w/o position",
                "TNet-AS w/o position"
            ],
            [
                "TNet-LF",
                "TNet-AS",
                "TNet w/o transformation",
                "ACC",
                "Macro-F1"
            ],
            null,
            [
                "TNet-LF",
                "TNet-AS",
                "TNet w/o context",
                "LAPTOP",
                "REST",
                "TWITTER"
            ],
            [
                "TWITTER"
            ],
            [
                "TNet w/o context",
                "TNet w/o transformation"
            ],
            [
                "TNet-LF w/o position",
                "TNet-AS w/o position"
            ]
        ],
        "n_sentence": 14.0,
        "table_id": "table_3",
        "paper_id": "P18-1087",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1090table_2",
        "description": "Table 2 shows the human evaluation results. It can be clearly seen that the proposed method obviously improves semantic preservation. The semantic score is increased from 3.87 to 5.08 on the Yelp dataset, and from 3.22 to 4.67 on the Amazon dataset. In general, our proposed model achieves the best overall performance. Furthermore, it also needs to be noticed that with the large improvement in content preservation, the sentiment accuracy of the proposed method is lower than that of CAAE on the two datasets. It shows that simultaneously promoting sentiment transformation and content preservation remains to be studied further.",
        "sentences": [
            "Table 2 shows the human evaluation results.",
            "It can be clearly seen that the proposed method obviously improves semantic preservation.",
            "The semantic score is increased from 3.87 to 5.08 on the Yelp dataset, and from 3.22 to 4.67 on the Amazon dataset.",
            "In general, our proposed model achieves the best overall performance.",
            "Furthermore, it also needs to be noticed that with the large improvement in content preservation, the sentiment accuracy of the proposed method is lower than that of CAAE on the two datasets.",
            "It shows that simultaneously promoting sentiment transformation and content preservation remains to be studied further."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Proposed Method",
                "CAAE (Shen et al. 2017)",
                "MDAL (Fu et al. 2018)",
                "Semantic"
            ],
            [
                "Proposed Method",
                "CAAE (Shen et al. 2017)",
                "MDAL (Fu et al. 2018)",
                "Semantic",
                "Yelp",
                "Amazon"
            ],
            [
                "Proposed Method",
                "Semantic",
                "G-score"
            ],
            [
                "Sentiment",
                "CAAE (Shen et al. 2017)",
                "Proposed Method"
            ],
            [
                "Sentiment"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_2",
        "paper_id": "P18-1090",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1093table_3",
        "description": "Table 3 reports a performance comparison of all benchmarked models on the Reddit datasets. Our proposed SIARN and MIARN models achieve very competitive performance on the Reddit datasets, with an average of ? 2% margin improvement over the best baselines. Notably, the baselines we compare against are extremely competitive state-of-the-art neural network models. This further reinforces the effectiveness of our proposed approach.",
        "sentences": [
            "Table 3 reports a performance comparison of all benchmarked models on the Reddit datasets.",
            "Our proposed SIARN and MIARN models achieve very competitive performance on the Reddit datasets, with an average of ? 2% margin improvement over the best baselines.",
            "Notably, the baselines we compare against are extremely competitive state-of-the-art neural network models.",
            "This further reinforces the effectiveness of our proposed approach."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "SIARN (this paper)",
                "MIARN (this paper)",
                "Reddit (/r/movies)",
                "Reddit (/r/technology)"
            ],
            [
                "SIARN (this paper)",
                "MIARN (this paper)",
                "CNN-LSTM-DNN (Ghosh and Veale)",
                "GRNN (Zhang et al.)",
                "Vanilla CNN",
                "NBOW"
            ],
            [
                "SIARN (this paper)",
                "MIARN (this paper)"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "P18-1093",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1093table_4",
        "description": "Table 4 reports a performance comparison of all benchmarked models on the Debates datasets. The performance improvement on Debates (long text) is significantly larger than short text (i.e., Twitter and Reddit). For example, MIARN outperforms GRNN and CNN-LSTM-DNN by 8% to 10% on both IAC-V1 and IAC-V2.",
        "sentences": [
            "Table 4 reports a performance comparison of all benchmarked models on the Debates datasets.",
            "The performance improvement on Debates (long text) is significantly larger than short text (i.e., Twitter and Reddit).",
            "For example, MIARN outperforms GRNN and CNN-LSTM-DNN by 8% to 10% on both IAC-V1 and IAC-V2."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Debates (IAC-V1)",
                "Debates (IAC-V2)"
            ],
            [
                "Debates (IAC-V1)",
                "Debates (IAC-V2)",
                "MIARN (this paper)",
                "GRNN (Zhang et al.)",
                "CNN-LSTM-DNN (Ghosh and Veale)"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_4",
        "paper_id": "P18-1093",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1097table_2",
        "description": "The effectiveness of various inference approaches can be observed by comparing the results in Table 2 by column. Compared to the normal seq2seq inference and seq2seq (+LM) baselines, fluency boost inference brings about on average 0.14 and 0.18 gain on F0.5 respectively, which is a significant6 improvement, demonstrating multi-round edits by fluency boost inference is effective. Take our best system (the last row in Table 2) as an example, among 1,312 sentences in the CoNLL-2014 dataset, seq2seq inference with shallow fusion LM edits 566 sentences. In contrast, fluency boost inference additionally edits 23 sentences during the second round inference, improving F0.5 from 52.59 to 52.72.",
        "sentences": [
            "The effectiveness of various inference approaches can be observed by comparing the results in Table 2 by column.",
            "Compared to the normal seq2seq inference and seq2seq (+LM) baselines, fluency boost inference brings about on average 0.14 and 0.18 gain on F0.5 respectively, which is a significant6 improvement, demonstrating multi-round edits by fluency boost inference is effective.",
            "Take our best system (the last row in Table 2) as an example, among 1,312 sentences in the CoNLL-2014 dataset, seq2seq inference with shallow fusion LM edits 566 sentences.",
            "In contrast, fluency boost inference additionally edits 23 sentences during the second round inference, improving F0.5 from 52.59 to 52.72."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "seq2seq",
                "seq2seq (+LM)",
                "fluency boost",
                "fluency boost (+LM)",
                "F0.5"
            ],
            [
                "self-boost (+native)\u2605",
                "dual-boost (+native)\u2605"
            ],
            [
                "fluency boost (+LM)",
                "F0.5",
                "dual-boost (+native)\u2605"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "P18-1097",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1103table_1",
        "description": "Table 1 shows the evaluation results of DAM as well as all comparison models. As demonstrated, DAM significantly outperforms other competitors on both Ubuntu Corpus and Douban Conversation Corpus, including SMNdynamic, which is the state-of-the-art baseline, demonstrating the superior power of attention mechanism in matching response with multi-turn context. Besides, both the performances of DAMf irst and DAMself decrease a lot compared with DAM, which shows the effectiveness of self-attention and cross-attention. Both DAMf irst and DAMlast underperform DAM, which demonstrates the benefits of using multigrained representations. Also the absence of self-attention-match brings down the precision, as shown in DAMcross, exhibiting the necessity of jointly considering textual relevance and dependency information in response selection. One notable point is that, while DAMf irst is able to achieve close performance to SMNdynamic, it is about 2.3 times faster than SMNdynamic in our implementation as it is very simple in computation. We believe that DAMf irst is more suitable to the scenario that has limitations in computation time or memories but requires high precise, such as industry application or working as an component in other neural networks like GANs.",
        "sentences": [
            "Table 1 shows the evaluation results of DAM as well as all comparison models.",
            "As demonstrated, DAM significantly outperforms other competitors on both Ubuntu Corpus and Douban Conversation Corpus, including SMNdynamic, which is the state-of-the-art baseline, demonstrating the superior power of attention mechanism in matching response with multi-turn context.",
            "Besides, both the performances of DAMf irst and DAMself decrease a lot compared with DAM, which shows the effectiveness of self-attention and cross-attention.",
            "Both DAMf irst and DAMlast underperform DAM, which demonstrates the benefits of using multigrained representations.",
            "Also the absence of self-attention-match brings down the precision, as shown in DAMcross, exhibiting the necessity of jointly considering textual relevance and dependency information in response selection.",
            "One notable point is that, while DAMf irst is able to achieve close performance to SMNdynamic, it is about 2.3 times faster than SMNdynamic in our implementation as it is very simple in computation.",
            "We believe that DAMf irst is more suitable to the scenario that has limitations in computation time or memories but requires high precise, such as industry application or working as an component in other neural networks like GANs."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "DAM"
            ],
            [
                "DAM",
                "Ubuntu Corpus",
                "Douban Conversation Corpus",
                "SMNdynamic"
            ],
            [
                "DAMfirst",
                "DAMself",
                "DAM"
            ],
            [
                "DAMfirst",
                "DAMlast",
                "DAM"
            ],
            [
                "DAMcross"
            ],
            [
                "DAMfirst",
                "SMNdynamic"
            ],
            [
                "DAMfirst"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_1",
        "paper_id": "P18-1103",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1108table_2",
        "description": "Table 2 reports our results compared to other benchmarks. To the best of our knowledge, we set a new stateof-the-art for single-model parsing achieving 86.5 F1 on the test set.",
        "sentences": [
            "Table 2 reports our results compared to other benchmarks.",
            "To the best of our knowledge, we set a new stateof-the-art for single-model parsing achieving 86.5 F1 on the test set."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Our Model",
                "F1"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_2",
        "paper_id": "P18-1108",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1110table_4",
        "description": "Surprisingly, with only 50 annotated questions (see Table 4), performance on QBANKDEV jumps 5 points, from 89.9% to 94.9%. This is only 1.5% below training with all of WSJTRAIN and QBANKTRAIN.",
        "sentences": [
            "Surprisingly, with only 50 annotated questions (see Table 4), performance on QBANKDEV jumps 5 points, from 89.9% to 94.9%.",
            "This is only 1.5% below training with all of WSJTRAIN and QBANKTRAIN."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "F1",
                "WSJ",
                "QBANK"
            ],
            [
                "F1",
                "WSJ",
                "QBANK"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_4",
        "paper_id": "P18-1110",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1110table_5",
        "description": "On the more difficult GENIA corpus of biomedical abstracts (Tateisi et al., 2005), we see a similar, if somewhat less dramatic, trend. See Table 5. With 50 annotated sentences, performance on GENIADEV jumps from 79.5% to 86.2%, outperforming all but one parser from David McClosky\u0081fs thesis (McClosky, 2010) ? the one that trains on all 14k sentences from GENIATRAIN and self-trains using 270k sentences from PubMed. That parser achieves 87.6%, which we outperform with just 500 sentences from GENIATRAIN.",
        "sentences": [
            "On the more difficult GENIA corpus of biomedical abstracts (Tateisi et al., 2005), we see a similar, if somewhat less dramatic, trend.",
            "See Table 5.",
            "With 50 annotated sentences, performance on GENIADEV jumps from 79.5% to 86.2%, outperforming all but one parser from David McClosky\u0081fs thesis (McClosky, 2010) ? the one that trains on all 14k sentences from GENIATRAIN and self-trains using 270k sentences from PubMed.",
            "That parser achieves 87.6%, which we outperform with just 500 sentences from GENIATRAIN."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "GENIA"
            ],
            null,
            [
                "GENIA",
                "F1"
            ],
            [
                "GENIA",
                "F1"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_5",
        "paper_id": "P18-1110",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1111table_2",
        "description": "Table 2 displays the performance of the proposed method and the baselines in the two evaluation settings. Our method outperforms all the methods in the isomorphic setting. In the nonisomorphic setting, it outperforms the other two systems that score reasonably on the isomorphic setting (SFS and IIITH) but cannot compete with the systems that focus on achieving high precision. The main advantage of our proposed model is in its ability to generalize, and that is also demonstrated in comparison to our baseline performance.",
        "sentences": [
            "Table 2 displays the performance of the proposed method and the baselines in the two evaluation settings.",
            "Our method outperforms all the methods in the isomorphic setting.",
            "In the nonisomorphic setting, it outperforms the other two systems that score reasonably on the isomorphic setting (SFS and IIITH) but cannot compete with the systems that focus on achieving high precision.",
            "The main advantage of our proposed model is in its ability to generalize, and that is also demonstrated in comparison to our baseline performance."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "Our method",
                "Baselines"
            ],
            [
                "Our method",
                "Baselines",
                "isomorphic"
            ],
            [
                "Our method",
                "SFS (Versley 2013)",
                "IIITH (Surtani et al. 2013)",
                "non-isomorphic"
            ],
            [
                "Our method",
                "Baseline"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "P18-1111",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1111table_4",
        "description": "Table 4 displays the methods' performance on the two versions of the Tratz (2011) dataset and the two dataset splits. The paraphrase model on its own is inferior to the distributional model, however, the integrated version improves upon the distributional model in 3 out of 4 settings, demonstrating the complementary nature of the distributional and paraphrase-based methods. The contribution of the paraphrase component is especially noticeable in the lexical splits. As expected, the integrated method in Shwartz and Waterson (2018), in which the paraphrase representation was trained with the objective of classification, performs better than our integrated model. The superiority of both integrated models in the lexical splits confirms that paraphrases are beneficial for classification.",
        "sentences": [
            "Table 4 displays the methods' performance on the two versions of the Tratz (2011) dataset and the two dataset splits.",
            "The paraphrase model on its own is inferior to the distributional model, however, the integrated version improves upon the distributional model in 3 out of 4 settings, demonstrating the complementary nature of the distributional and paraphrase-based methods.",
            "The contribution of the paraphrase component is especially noticeable in the lexical splits.",
            "As expected, the integrated method in Shwartz and Waterson (2018), in which the paraphrase representation was trained with the objective of classification, performs better than our integrated model.",
            "The superiority of both integrated models in the lexical splits confirms that paraphrases are beneficial for classification."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "Tratz fine Random",
                "Tratz fine Lexical",
                "Tratz coarse Random",
                "Tratz coarse Lexical"
            ],
            [
                "paraphrase",
                "distributional",
                "integrated"
            ],
            [
                "Tratz coarse Lexical",
                "Tratz fine Lexical"
            ],
            [
                "Shwartz and Waterson (2018)",
                "Tratz coarse Lexical",
                "integrated",
                "Tratz fine Lexical"
            ],
            [
                "Tratz coarse Lexical",
                "integrated",
                "Tratz fine Lexical"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_4",
        "paper_id": "P18-1111",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1112table_3",
        "description": "To control for the \u201camount\u201d of sentiment in the Subjective and Objective corpora, we use sentiment lexicon compiled by Hu and Liu (2004). For each corpus, we create two subcorpora: With Sentiment contains only the sentences with at least one word from the sentiment lexicon, while Without Sentiment is the complement. We match the corpora on the number of sentences, downsampling the larger corpus, train word embeddings on each subcorpus, and proceed with the classification experiments. Table 3 shows the results, including that of random word embeddings for reference. Sentiment lexicon has a significant impact on the performance of sentiment and subjectivity classifications, and a smaller impact on topic classification. Without sentiment, the Subjective embeddings prove more robust, still outperforming the Objective on sentiment classification, while the Objective performs close to random word embeddings on Amazon.",
        "sentences": [
            "To control for the \u201camount\u201d of sentiment in the Subjective and Objective corpora, we use sentiment lexicon compiled by Hu and Liu (2004).",
            "For each corpus, we create two subcorpora: With Sentiment contains only the sentences with at least one word from the sentiment lexicon, while Without Sentiment is the complement.",
            "We match the corpora on the number of sentences, downsampling the larger corpus, train word embeddings on each subcorpus, and proceed with the classification experiments.",
            "Table 3 shows the results, including that of random word embeddings for reference.",
            "Sentiment lexicon has a significant impact on the performance of sentiment and subjectivity classifications, and a smaller impact on topic classification.",
            "Without sentiment, the Subjective embeddings prove more robust, still outperforming the Objective on sentiment classification, while the Objective performs close to random word embeddings on Amazon."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "Subjective",
                "Objective"
            ],
            [
                "Subcorpus Sentiment?",
                "With",
                "Without"
            ],
            null,
            [
                "Random Embeddings"
            ],
            [
                "Sentiment",
                "Subjectivity",
                "Topic"
            ],
            [
                "Without",
                "Subjective",
                "Objective",
                "Sentiment",
                "Random Embeddings",
                "Amazon"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_3",
        "paper_id": "P18-1112",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1112table_4",
        "description": "Table 4 shows the unfolded results for the 24 classification datasets of Amazon, as well as for Rotten Tomatoes. For each classification dataset (row), and for the Objective and Subjective embedding corpora respectively, the best word embedding methods are shown in bold. An asterisk indicates statistically significant8 results at 5% in comparison to Word2Vec. Both SentiVec variants outperform Word2Vec in the vast majority of the cases. The degree of outperformance is higher for the Objective than the Subjective word embeddings. This is a reasonable trend given our previous findings in Section 3. As the Objective Corpus encodes less information than the Subjective Corpus for sentiment classification, the former is more likely to benefit from the infusion of sentiment information from additional lexical resources. Note that the sentiment infusion into the word embeddings comes from separate lexical resources, and does not involve any sentiment classification label. SentiVec also outperforms the two baselines that benefit from the same lexical resources. Retrofitting does not improve upon Word2Vec, with the two embeddings essentially indistinguishable (the difference is only noticeable at the second decimal point). Refining makes the word embeddings perform worse on the sentiment classification task.",
        "sentences": [
            "Table 4 shows the unfolded results for the 24 classification datasets of Amazon, as well as for Rotten Tomatoes.",
            "For each classification dataset (row), and for the Objective and Subjective embedding corpora respectively, the best word embedding methods are shown in bold.",
            "An asterisk indicates statistically significant8 results at 5% in comparison to Word2Vec.",
            "Both SentiVec variants outperform Word2Vec in the vast majority of the cases.",
            "The degree of outperformance is higher for the Objective than the Subjective word embeddings.",
            "This is a reasonable trend given our previous findings in Section 3.",
            "As the Objective Corpus encodes less information than the Subjective Corpus for sentiment classification, the former is more likely to benefit from the infusion of sentiment information from additional lexical resources.",
            "Note that the sentiment infusion into the word embeddings comes from separate lexical resources, and does not involve any sentiment classification label.",
            "SentiVec also outperforms the two baselines that benefit from the same lexical resources.",
            "Retrofitting does not improve upon Word2Vec, with the two embeddings essentially indistinguishable (the difference is only noticeable at the second decimal point).",
            "Refining makes the word embeddings perform worse on the sentiment classification task."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            2,
            2,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "Amazon",
                "Rotten Tomatoes"
            ],
            [
                "Objective Embeddings",
                "Subjective Embeddings"
            ],
            [
                "Word2Vec"
            ],
            [
                "Word2Vec",
                "SentiVec",
                "Objective Embeddings",
                "Subjective Embeddings"
            ],
            [
                "SentiVec",
                "Objective Embeddings",
                "Subjective Embeddings"
            ],
            null,
            [
                "Corpus/Category"
            ],
            null,
            [
                "SentiVec",
                "Retrofitting",
                "Refining"
            ],
            [
                "Retrofitting",
                "Word2Vec",
                "Objective Embeddings",
                "Subjective Embeddings"
            ],
            [
                "Refining"
            ]
        ],
        "n_sentence": 11.0,
        "table_id": "table_4",
        "paper_id": "P18-1112",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1112table_5",
        "description": "Table 5 shows that the unfolded results for topic classification on the six datasets, and the result for subjectivity classification are similar across methods. Neither the SentiVec variants, nor Retrofitting and Refining, change the subjectivity and topic classification capabilities much, which means that the used sentiment lexicon is targeted only at the sentiment subspace of embeddings.",
        "sentences": [
            "Table 5 shows that the unfolded results for topic classification on the six datasets, and the result for subjectivity classification are similar across methods.",
            "Neither the SentiVec variants, nor Retrofitting and Refining, change the subjectivity and topic classification capabilities much, which means that the used sentiment lexicon is targeted only at the sentiment subspace of embeddings."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "Computers",
                "Misc",
                "Politics",
                "Recreation",
                "Religion"
            ],
            [
                "SentiVec",
                "Retrofitting",
                "Refining",
                "Topic"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_5",
        "paper_id": "P18-1112",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1113table_1",
        "description": "Table 1 shows the performance of our model and the baselines on the task of metaphor identification. All the results for our models are based on a threshold of 0.6, which is empirically determined based on the developing set. For sentence level metaphor identification, it can be observed that all our models outperform the baseline (Melamud et al., 2016), with SIM-CBOWI+O giving the highest F1 score of 75% which is a 6% gain over the baseline. We also see that models based on both input and output vectors (i.e.,SIM-CBOWI+O and SIM-SGI+O) yield better performance than the models based on input vectors only (i.e., SIM-CBOWI and SIM-SGI ). Such an observation supports our assumption that using input and output vectors can better model similarity between words that have different types of POS, than simply using input vectors. When comparing CBOW and Skip-gram based models, we see that CBOW based models generally achieve better performance in precision whereas Skip-gram based models perform better in recall. In terms of phrase level metaphor identification, we compare our best performing models (i.e.,\r\nSIM-CBOWI+O and SIM-SGI+O) against the approaches of Shutova et al. (2016) and Rei et al.(2017). In contrast to the sentence level evaluation in which SIM-CBOWI+O gives the best\r\nperformance, SIM-SGI+O performs best for the phrase level evaluation. This is likely due to the fact that Skip-gram is trained by using a centre word to maximise the probability of each context word, whereas CBOW uses the average of context word input vectors to maximise the probability of the centre word. Thus, Skip-gram performs better in modelling one-word context, while CBOW has better performance in modelling multi-context words. When comparing to the baselines, our model SIM-SGI+O significantly outperforms the word embedding based approach by Shutova et al. (2016), and gives the same performance as the deep supervised method (Rei et al., 2017) which requires a large amount of labelled data for training and cost in training time. SIM-CBOWI+O and SIM-SGI+O are also evaluated with different thresholds for both phrase and sentence level metaphor identification.",
        "sentences": [
            "Table 1 shows the performance of our model and the baselines on the task of metaphor identification.",
            "All the results for our models are based on a threshold of 0.6, which is empirically determined based on the developing set.",
            "For sentence level metaphor identification, it can be observed that all our models outperform the baseline (Melamud et al., 2016), with SIM-CBOWI+O giving the highest F1 score of 75% which is a 6% gain over the baseline.",
            "We also see that models based on both input and output vectors (i.e.,SIM-CBOWI+O and SIM-SGI+O) yield better performance than the models based on input vectors only (i.e., SIM-CBOWI and SIM-SGI ).",
            "Such an observation supports our assumption that using input and output vectors can better model similarity between words that have different types of POS, than simply using input vectors.",
            "When comparing CBOW and Skip-gram based models, we see that CBOW based models generally achieve better performance in precision whereas Skip-gram based models perform better in recall.",
            "In terms of phrase level metaphor identification, we compare our best performing models (i.e.,\r\nSIM-CBOWI+O and SIM-SGI+O) against the approaches of Shutova et al. (2016) and Rei et al.(2017).",
            "In contrast to the sentence level evaluation in which SIM-CBOWI+O gives the best\r\nperformance, SIM-SGI+O performs best for the phrase level evaluation.",
            "This is likely due to the fact that Skip-gram is trained by using a centre word to maximise the probability of each context word, whereas CBOW uses the average of context word input vectors to maximise the probability of the centre word.",
            "Thus, Skip-gram performs better in modelling one-word context, while CBOW has better performance in modelling multi-context words.",
            "When comparing to the baselines, our model SIM-SGI+O significantly outperforms the word embedding based approach by Shutova et al. (2016), and gives the same performance as the deep supervised method (Rei et al., 2017) which requires a large amount of labelled data for training and cost in training time.",
            "SIM-CBOWI+O and SIM-SGI+O are also evaluated with different thresholds for both phrase and sentence level metaphor identification."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            2,
            1,
            1,
            1,
            2,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "SIM-CBOWI+O",
                "SIM-SGI+O",
                "SIM-SGI",
                "SIM-CBOWI"
            ],
            [
                "Melamud et al. (2016)",
                "Sent.",
                "SIM-SGI",
                "SIM-SGI+O",
                "SIM-CBOWI",
                "F1"
            ],
            [
                "SIM-CBOWI+O",
                "SIM-SGI+O",
                "SIM-CBOWI",
                "SIM-SGI"
            ],
            null,
            [
                "P",
                "R",
                "SIM-CBOWI+O",
                "SIM-SGI+O",
                "SIM-SGI",
                "SIM-CBOWI"
            ],
            [
                "SIM-CBOWI+O",
                "SIM-SGI+O",
                "Shutova et al. (2016)",
                "Rei et al. (2017)"
            ],
            [
                "SIM-CBOWI+O",
                "SIM-SGI+O",
                "Phrase",
                "Sent."
            ],
            [
                "SIM-CBOWI+O",
                "SIM-SGI+O",
                "Phrase",
                "Sent."
            ],
            [
                "SIM-CBOWI+O",
                "SIM-SGI+O",
                "Phrase",
                "Sent."
            ],
            [
                "SIM-SGI+O",
                "Shutova et al. (2016)",
                "Rei et al. (2017)"
            ],
            [
                "SIM-CBOWI+O",
                "SIM-SGI+O",
                "Phrase",
                "Sent."
            ]
        ],
        "n_sentence": 12.0,
        "table_id": "table_1",
        "paper_id": "P18-1113",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1114table_2",
        "description": "Word similarity is conducted to test the semantic information which is encoded in word embeddings, and the results are listed in Table 2 (first 6 rows). We observe that our models surpass the comparative baselines on five datasets. Compared with the base model CBOW, it is remarkable that our models approximately achieve improvements of more than 5% and 7%, respectively, in the performance on the golden standard Wordsim-353 and RG-65. On WS-353-REL, the difference between CBOW and LMM-S even reaches 8%. The advantage demonstrates the effectiveness of our methods. Based on our strategy, more semantic information will be captured in corpus when adding more latent meanings in the context window. B. By incorporating mophemes, EMM also performs better than other baselines but fails to get the performance as well as ours. Actually, EMM mainly tunes the distributions of words in vector space to let the morpheme-similar words gather closer, which means it just encodes more morphological properties into word embeddings but lacks the ability to capture more semantic information. Specially, because of the medium size corpus and the experimental settings, GloVe\ndoesn\u0081ft perform as well as that described in\n(Pennington et al., 2014).\n. 5.2 The Results on Syntactic Analogy.  In (Mikolov et al., 2013c), the dataset is divided into adjectives, nouns and verbs. For brevity, we only report performance on the whole dataset. As the middle row of Table 2 shows, all of our models outperform the comparative baselines to a great extent. Compared with CBOW, the advantage of LMM-A even reaches to 7%. Besides, we observe that the suffix of \u0081gb\u0081h usually is the same as the suffix of \u0081gd\u0081h when answering question \u0081ga is to b as c is to d\u0081h. Based on our strategy, morphemesimilar words will not only gather closer but have a trend to group near the latent meanings of their morphemes, which makes our embeddings have the advantage to deal with the syntactic analogy problem. EMM also performs well on this task but is still weaker than our models. Actually, syntactic analogy is also a semantics-related task because \u0081gc\u0081h and \u0081gd\u0081h are with similar meanings. Since our models are better to capture semantic information, they lead to higher performance than the explicitly morphology-based models. The results are displayed in the bottom row of Table 2. Since we simply use the average embedding of words as the feature vector for 10-categorization classification, the overall classification accuracies of all models are merely aroud 80%. However, the classification accuracies of our LMMs still surpass all the baselines, especailly CBOW and GloVe.",
        "sentences": [
            "Word similarity is conducted to test the semantic information which is encoded in word embeddings, and the results are listed in Table 2 (first 6 rows).",
            "We observe that our models surpass the comparative baselines on five datasets.",
            "Compared with the base model CBOW, it is remarkable that our models approximately achieve improvements of more than 5% and 7%, respectively, in the performance on the golden standard Wordsim-353 and RG-65.",
            "On WS-353-REL, the difference between CBOW and LMM-S even reaches 8%.",
            "The advantage demonstrates the effectiveness of our methods.",
            "Based on our strategy, more semantic information will be captured in corpus when adding more latent meanings in the context window. B.",
            "By incorporating mophemes, EMM also performs better than other baselines but fails to get the performance as well as ours.",
            "Actually, EMM mainly tunes the distributions of words in vector space to let the morpheme-similar words gather closer, which means it just encodes more morphological properties into word embeddings but lacks the ability to capture more semantic information.",
            "Specially, because of the medium size corpus and the experimental settings, GloVe\ndoesn\u0081ft perform as well as that described in\n(Pennington et al., 2014).\n.",
            "5.2 The Results on Syntactic Analogy.",
            " In (Mikolov et al., 2013c), the dataset is divided into adjectives, nouns and verbs.",
            "For brevity, we only report performance on the whole dataset.",
            "As the middle row of Table 2 shows, all of our models outperform the comparative baselines to a great extent.",
            "Compared with CBOW, the advantage of LMM-A even reaches to 7%.",
            "Besides, we observe that the suffix of \u0081gb\u0081h usually is the same as the suffix of \u0081gd\u0081h when answering question \u0081ga is to b as c is to d\u0081h.",
            "Based on our strategy, morphemesimilar words will not only gather closer but have a trend to group near the latent meanings of their morphemes, which makes our embeddings have the advantage to deal with the syntactic analogy problem.",
            "EMM also performs well on this task but is still weaker than our models.",
            "Actually, syntactic analogy is also a semantics-related task because \u0081gc\u0081h and \u0081gd\u0081h are with similar meanings.",
            "Since our models are better to capture semantic information, they lead to higher performance than the explicitly morphology-based models.",
            "The results are displayed in the bottom row of Table 2.",
            "Since we simply use the average embedding of words as the feature vector for 10-categorization classification, the overall classification accuracies of all models are merely aroud 80%.",
            "However, the classification accuracies of our LMMs still surpass all the baselines, especailly CBOW and GloVe."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            2,
            1,
            2,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            2,
            1,
            0,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "LMM-A",
                "LMM-S",
                "LMM-M"
            ],
            [
                "CBOW",
                "Wordsim-353",
                "RG-65"
            ],
            [
                "WS-353-REL",
                "CBOW",
                "LMM-S"
            ],
            [
                "LMM-A",
                "LMM-S",
                "LMM-M"
            ],
            null,
            [
                "EMM",
                "LMM-A",
                "LMM-S",
                "LMM-M"
            ],
            [
                "EMM"
            ],
            [
                "GloVe"
            ],
            null,
            null,
            null,
            [
                "Syntactic Analogy",
                "EMM",
                "LMM-A",
                "LMM-S",
                "LMM-M"
            ],
            [
                "Syntactic Analogy",
                "CBOW",
                "LMM-A"
            ],
            null,
            null,
            [
                "Syntactic Analogy",
                "EMM",
                "LMM-A",
                "LMM-S",
                "LMM-M"
            ],
            null,
            [
                "EMM",
                "LMM-A",
                "LMM-S",
                "LMM-M"
            ],
            [
                "Text Classification"
            ],
            [
                "Text Classification"
            ],
            [
                "Text Classification",
                "LMM-A",
                "LMM-S",
                "LMM-M",
                "CBOW",
                "Skip-gram",
                "GloVe"
            ]
        ],
        "n_sentence": 22.0,
        "table_id": "table_2",
        "paper_id": "P18-1114",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1118table_4",
        "description": "Table 4 shows comparison of our Memory-to-Context model variants source context-NMT models (Jean et al., 2017; Wang et al., 2017). For German\u2192English, our S-NMT+src mem model is comparable to Jean et al. (2017) but outperforms Wang et al. (2017) for one test set according to BLEU, and for both test sets according to METEOR. For Estonian\u2192English, our model outperforms Jean et al. (2017). Our global source context model has only surface-level sentence information, and is oblivious to the individual words in the context since we do an offline training to get the sentence representations (as previously mentioned). Our global source context model has only surface-level sentence information, and is oblivious to the individual words in the context since we do an offline training to get the sentence representations (as previously mentioned).",
        "sentences": [
            "Table 4 shows comparison of our Memory-to-Context model variants source context-NMT models (Jean et al., 2017; Wang et al., 2017).",
            "For German\u2192English, our S-NMT+src mem model is comparable to Jean et al. (2017) but outperforms Wang et al. (2017) for one test set according to BLEU, and for both test sets according to METEOR.",
            "For Estonian\u2192English, our model outperforms Jean et al. (2017). Our global source context model has only surface-level sentence information, and is oblivious to the individual words in the context since we do an offline training to get the sentence representations (as previously mentioned).",
            "Our global source context model has only surface-level sentence information, and is oblivious to the individual words in the context since we do an offline training to get the sentence representations (as previously mentioned)."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "S-NMT",
                "S-NMT + src mem",
                "S-NMT + both mems",
                "Jean et al. (2017)",
                "Wang et al. (2017)"
            ],
            [
                "De\u2192En",
                "S-NMT + src mem",
                "Jean et al. (2017)",
                "Wang et al. (2017)",
                "BLEU",
                "METEOR"
            ],
            [
                "Et\u2192En",
                "S-NMT + src mem",
                "S-NMT + both mems",
                "Jean et al. (2017)"
            ],
            [
                "S-NMT + src mem",
                "S-NMT + both mems"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_4",
        "paper_id": "P18-1118",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1119table_1",
        "description": "Each system geoparses its particular majority of the dataset to obtain a representative data sample, shown in Table 1 as strongly correlated scores for subsets of different sizes, with which to assess model performance. Table 1 also shows scores in brackets for the overlapping partition of all systems in order to compare performance on identical instances: GeoVirus 601 (26%), LGL 787 (17%) and WikToR 2,202 (9%). The geocoding difficulty based on the ambiguity of each dataset is: LGL (moderate to hard), WIK (very hard), GEO (easy to moderate). A population baseline also features in the evaluation. The baseline is conceptually simple: choose the candidate with the highest population, akin to the most frequent word sense in WSD. Table 1 shows the effectiveness of this heuristic, which is competitive with many geocoders, even outperforming some. However, the baseline is not effective on WikToR as the dataset was deliberately constructed as a tough ambiguity test. Table 1 shows how several geocoders mirror the behaviour of the population baseline. This simple but effective heuristic is rarely used in system comparisons, and where evaluated (Santos et al., 2015; Leidner, 2008), it is inconsistent with expected figures (due to unpublished resources, we are unable to investigate). We note that no single computational paradigm dominates Table 1. The rule-based (Edinburgh, GeoTxt, CLAVIN), (Topocluster), machine learning (CamCoder, Santos) and other (Yahoo!, Population) geocoders occupy different ranks across the three datasets. Due to space constraints, Table 1 does not show figures for another type of scenario we tested, a shorter lexical context, using 200 words instead of the standard 400. CamCoder proved to be robust to reduced context, with only a small performance decline. Using the same format as Table 1, AUC errors for LGL increased from 22 (18) to 23 (19), WIK from 33 (37) to 37 (40) and GEO remained the same at 31 (32). This means that reducing model input size to save computational resources would still deliver accurate results.",
        "sentences": [
            "Each system geoparses its particular majority of the dataset to obtain a representative data sample, shown in Table 1 as strongly correlated scores for subsets of different sizes, with which to assess model performance.",
            "Table 1 also shows scores in brackets for the overlapping partition of all systems in order to compare performance on identical instances: GeoVirus 601 (26%), LGL 787 (17%) and WikToR 2,202 (9%).",
            "The geocoding difficulty based on the ambiguity of each dataset is: LGL (moderate to hard), WIK (very hard), GEO (easy to moderate).",
            "A population baseline also features in the evaluation.",
            "The baseline is conceptually simple: choose the candidate with the highest population, akin to the most frequent word sense in WSD.",
            "Table 1 shows the effectiveness of this heuristic, which is competitive with many geocoders, even outperforming some.",
            "However, the baseline is not effective on WikToR as the dataset was deliberately constructed as a tough ambiguity test.",
            "Table 1 shows how several geocoders mirror the behaviour of the population baseline.",
            "This simple but effective heuristic is rarely used in system comparisons, and where evaluated (Santos et al., 2015; Leidner, 2008), it is inconsistent with expected figures (due to unpublished resources, we are unable to investigate).",
            "We note that no single computational paradigm dominates Table 1.",
            "The rule-based (Edinburgh, GeoTxt, CLAVIN), (Topocluster), machine learning (CamCoder, Santos) and other (Yahoo!, Population) geocoders occupy different ranks across the three datasets.",
            "Due to space constraints, Table 1 does not show figures for another type of scenario we tested, a shorter lexical context, using 200 words instead of the standard 400.",
            "CamCoder proved to be robust to reduced context, with only a small performance decline.",
            "Using the same format as Table 1, AUC errors for LGL increased from 22 (18) to 23 (19), WIK from 33 (37) to 37 (40) and GEO remained the same at 31 (32).",
            "This means that reducing model input size to save computational resources would still deliver accurate results."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            2,
            1,
            1,
            1,
            2,
            2,
            1,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "LGL",
                "WIK",
                "GEO"
            ],
            [
                "Geocoder",
                "LGL",
                "WIK",
                "GEO"
            ],
            null,
            null,
            [
                "Geocoder"
            ],
            [
                "WIK"
            ],
            null,
            null,
            null,
            [
                "Edinburgh",
                "GeoTxt",
                "CLAVIN",
                "Topocluster",
                "CamCoder",
                "Santos et al.",
                "Yahoo!",
                "Population"
            ],
            null,
            [
                "CamCoder"
            ],
            [
                "CamCoder",
                "Area Under Curve",
                "LGL",
                "WIK",
                "GEO"
            ],
            [
                "CamCoder",
                "Area Under Curve",
                "LGL",
                "WIK",
                "GEO"
            ]
        ],
        "n_sentence": 15.0,
        "table_id": "table_1",
        "paper_id": "P18-1119",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1129table_2",
        "description": "Table 2 shows our PTB experimental results. From this result, we can see that the ensemble model outperforms the baseline model by 1.90 in LAS. For our distillation from reference, when setting alpha = 1.0, best performance on development set is achieved and the test LAS is 91.99. We also compare our parser with the other parsers in Table 2. The second group shows the greedy transition-based parsers in previous literatures. Andor et al. (2016) presented an alternative state representation and explored both greedy and beam search decoding. (Ballesteros et al., 2016) explores training the greedy parser with dynamic oracle. Our distillation parser outperforms all these greedy counterparts. The third group shows parsers trained on different techniques including decoding with beam search (Buckman et al.,2016; Andor et al., 2016), training transitionbased parser with beam search (Andor et al.,2016), graph-based parsing (Dozat and Manning,2016), distilling a graph-based parser from the output of 20 parsers (Kuncoro et al., 2016), and converting constituent parsing results to dependencies (Kuncoro et al., 2017). Our distillation parser still outperforms its transition-based counterparts but lags the others.",
        "sentences": [
            "Table 2 shows our PTB experimental results.",
            "From this result, we can see that the ensemble model outperforms the baseline model by 1.90 in LAS.",
            "For our distillation from reference, when setting alpha = 1.0, best performance on development set is achieved and the test LAS is 91.99.",
            "We also compare our parser with the other parsers in Table 2.",
            "The second group shows the greedy transition-based parsers in previous literatures.",
            "Andor et al. (2016) presented an alternative state representation and explored both greedy and beam search decoding.",
            "(Ballesteros et al., 2016) explores training the greedy parser with dynamic oracle.",
            "Our distillation parser outperforms all these greedy counterparts.",
            "The third group shows parsers trained on different techniques including decoding with beam search (Buckman et al.,2016; Andor et al., 2016), training transitionbased parser with beam search (Andor et al.,2016), graph-based parsing (Dozat and Manning,2016), distilling a graph-based parser from the output of 20 parsers (Kuncoro et al., 2016), and converting constituent parsing results to dependencies (Kuncoro et al., 2017).",
            "Our distillation parser still outperforms its transition-based counterparts but lags the others."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Ensemble",
                "Baseline",
                "LAS"
            ],
            [
                "Distill (reference alpha=1.0)",
                "LAS"
            ],
            [
                "Distill (reference alpha=1.0)",
                "Distill (exploration T=1.0)",
                "Distill (both)",
                "Ballesteros et al. (2016) (dyn. oracle)",
                "Andor et al. (2016) (local B=1)"
            ],
            [
                "Ballesteros et al. (2016) (dyn. oracle)",
                "Andor et al. (2016) (local B=1)"
            ],
            [
                "Andor et al. (2016) (local B=1)"
            ],
            [
                "Ballesteros et al. (2016) (dyn. oracle)"
            ],
            [
                "Distill (reference alpha=1.0)",
                "Distill (exploration T=1.0)",
                "Distill (both)",
                "Ballesteros et al. (2016) (dyn. oracle)",
                "Andor et al. (2016) (local B=1)"
            ],
            [
                "Buckman et al. (2016) (local B=8)",
                "Andor et al. (2016) (local B=32)",
                "Andor et al. (2016) (global B=32)",
                "Dozat and Manning (2016)",
                "Kuncoro et al. (2016)",
                "Kuncoro et al. (2017)"
            ],
            [
                "Distill (reference alpha=1.0)",
                "Distill (exploration T=1.0)",
                "Distill (both)",
                "Buckman et al. (2016) (local B=8)",
                "Andor et al. (2016) (local B=32)",
                "Andor et al. (2016) (global B=32)",
                "Dozat and Manning (2016)",
                "Kuncoro et al. (2016)",
                "Kuncoro et al. (2017)"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_2",
        "paper_id": "P18-1129",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1129table_3",
        "description": "Table 3 shows the experimental results on IWSLT 2014 dataset. Similar to the PTB parsing results, the ensemble 10 translators outperforms the baseline translator by 3.47 in BLEU score. Distilling from the ensemble by following the reference leads to a single translator of 24.76 BLEU score. Like in the parsing experiments, sharpen the distribution when exploring the search space is more helpful to the model\u00e2\u0080\u0099s performance but the differences when T \u00e2\u0089\u00a4 0.2 is not significant as shown in Figure 3. We set T = 0.1 in our distillation from exploration experiments since it achieves the best development score. Table 3 shows the exploration result of a BLEU score of 24.64 and it slightly lags the best reference model. Distilling from both the reference and exploration improves the single model\u00e2\u0080\u0099s performance by a large margin and achieves a BLEU score of 25.44. We also compare our model with other translation models including the one trained with reinforcement learning (Ranzato et al., 2015) and that using beam search in training (Wiseman and Rush, 2016). Our distillation translator outperforms these models.",
        "sentences": [
            "Table 3 shows the experimental results on IWSLT 2014 dataset.",
            "Similar to the PTB parsing results, the ensemble 10 translators outperforms the baseline translator by 3.47 in BLEU score.",
            "Distilling from the ensemble by following the reference leads to a single translator of 24.76 BLEU score.",
            "Like in the parsing experiments, sharpen the distribution when exploring the search space is more helpful to the model\u00e2\u0080\u0099s performance but the differences when T \u00e2\u0089\u00a4 0.2 is not significant as shown in Figure 3.",
            "We set T = 0.1 in our distillation from exploration experiments since it achieves the best development score.",
            "Table 3 shows the exploration result of a BLEU score of 24.64 and it slightly lags the best reference model.",
            "Distilling from both the reference and exploration improves the single model\u00e2\u0080\u0099s performance by a large margin and achieves a BLEU score of 25.44.",
            "We also compare our model with other translation models including the one trained with reinforcement learning (Ranzato et al., 2015) and that using beam search in training (Wiseman and Rush, 2016).",
            "Our distillation translator outperforms these models."
        ],
        "class_sentence": [
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Ensemble",
                "Baseline",
                "BLEU"
            ],
            [
                "Distill (reference alpha=0.8)",
                "BLEU"
            ],
            null,
            null,
            [
                "Distill (exploration T=0.1)",
                "BLEU"
            ],
            [
                "Distill (both)",
                "BLEU"
            ],
            [
                "MIXER",
                "BSO (local B=1)",
                "BSO (global B=1)"
            ],
            [
                "Distill (reference alpha=0.8)",
                "Distill (exploration T=0.1)",
                "Distill (both)"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_3",
        "paper_id": "P18-1129",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1129table_4",
        "description": "The comparison in Table 4 shows that the ensemble model significantly outperforms the baseline on ambiguous and non-optimal states. This observation indicates the ensemble output distribution is more informative thus generalizes well on problematic states and achieves better performance. We also observe that the distillation model perform better than both the baseline and ensemble. We attribute this to the fact that the distillation model is learned from exploration.",
        "sentences": [
            "The comparison in Table 4 shows that the ensemble model significantly outperforms the baseline on ambiguous and non-optimal states.",
            "This observation indicates the ensemble output distribution is more informative thus generalizes well on problematic states and achieves better performance.",
            "We also observe that the distillation model perform better than both the baseline and ensemble.",
            "We attribute this to the fact that the distillation model is learned from exploration."
        ],
        "class_sentence": [
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            [
                "Ensemble",
                "Baseline",
                "optimal-yet-ambiguous",
                "non-optimal"
            ],
            [
                "Ensemble"
            ],
            [
                "Distill (both)",
                "Ensemble",
                "Baseline",
                "optimal-yet-ambiguous",
                "non-optimal"
            ],
            [
                "Distill (both)"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_4",
        "paper_id": "P18-1129",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1130table_1",
        "description": "Table 1 illustrates the UAS and LAS of the four versions of our model (with decoding beam size 10) on the three treebanks, together with previous top-performing systems for comparison. Note that the results of STACKPTR and our reimplementation of BIAF are the average of 5 repetitions instead of a single run. Our Full model significantly outperforms all the transition-based parsers on all three languages, and achieves better results than most graph-based parsers. Our re-implementation of BIAF obtains better performance than the original one in Dozat and Manning (2017), demonstrating the effectiveness of the\ncharacter-level information. Our model achieves state-of-the-art performance on both UAS and LAS on Chinese, and best UAS on English. On German, the performance is competitive with BIAF, and significantly better than other models.",
        "sentences": [
            "Table 1 illustrates the UAS and LAS of the four versions of our model (with decoding beam size 10) on the three treebanks, together with previous top-performing systems for comparison.",
            "Note that the results of STACKPTR and our reimplementation of BIAF are the average of 5 repetitions instead of a single run.",
            "Our Full model significantly outperforms all the transition-based parsers on all three languages, and achieves better results than most graph-based parsers.",
            "Our re-implementation of BIAF obtains better performance than the original one in Dozat and Manning (2017), demonstrating the effectiveness of the\ncharacter-level information.",
            "Our model achieves state-of-the-art performance on both UAS and LAS on Chinese, and best UAS on English.",
            "On German, the performance is competitive with BIAF, and significantly better than other models."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "STACKPTR: Org",
                "STACKPTR: +gpar",
                "STACKPTR: +sib",
                "STACKPTR: Full",
                "UAS",
                "LAS"
            ],
            [
                "BIAF: re-impl",
                "STACKPTR: Org",
                "STACKPTR: +gpar",
                "STACKPTR: +sib",
                "STACKPTR: Full"
            ],
            [
                "STACKPTR: Full",
                "English",
                "Chinese",
                "German"
            ],
            [
                "BIAF: re-impl",
                "BIAF: Dozat and Manning (2017)"
            ],
            [
                "UAS",
                "LAS",
                "Chinese",
                "English",
                "STACKPTR: Full"
            ],
            [
                "STACKPTR: Org",
                "STACKPTR: +gpar",
                "STACKPTR: +sib",
                "STACKPTR: Full",
                "UAS",
                "LAS",
                "German",
                "BIAF: re-impl"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_1",
        "paper_id": "P18-1130",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1130table_2",
        "description": "Table 2 gives results of the parsers with different versions of POS tags on the test data of PTB. The parser with gold-standard POS tags significantly outperforms the other two parsers, showing that dependency parsers can still benefit from accurate POS information. The parser with predicted (imperfect) POS tags, however, performs even slightly worse than the parser without using POS tags. It illustrates that an end-to-end parser that doesn\u0081ft rely on POS information can obtain competitive (or even better) performance than parsers using imperfect predicted POS tags, even if the POS tagger is relative high accuracy (accuracy > 97% in this experiment on PTB).",
        "sentences": [
            "Table 2 gives results of the parsers with different versions of POS tags on the test data of PTB.",
            "The parser with gold-standard POS tags significantly outperforms the other two parsers, showing that dependency parsers can still benefit from accurate POS information.",
            "The parser with predicted (imperfect) POS tags, however, performs even slightly worse than the parser without using POS tags.",
            "It illustrates that an end-to-end parser that doesn\u0081ft rely on POS information can obtain competitive (or even better) performance than parsers using imperfect predicted POS tags, even if the POS tagger is relative high accuracy (accuracy > 97% in this experiment on PTB)."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "POS",
                "UAS",
                "LAS",
                "UCM",
                "LCM"
            ],
            [
                "Gold",
                "Pred",
                "None"
            ],
            [
                "Pred",
                "None"
            ],
            [
                "Pred",
                "None"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "P18-1130",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1130table_4",
        "description": "Table 4 summarizes the results of the STACKPTR parser, along with BIAF for comparison, on both the development and test datasets for each language. First, both BIAF and STACKPTR parsers achieve relatively high parsing accuracies on all the 12 languages ? all with UAS are higher than 90%. On nine languages ? Catalan, Czech, Dutch, English, French, German, Norwegian, Russian and Spanish ? STACKPTR outperforms BIAF for both UAS and LAS. On Bulgarian, STACKPTR achieves slightly better UAS while LAS is slightly worse than BIAF. On Italian and Romanian, BIAF obtains marginally better parsing performance than STACKPTR.",
        "sentences": [
            "Table 4 summarizes the results of the STACKPTR parser, along with BIAF for comparison, on both the development and test datasets for each language.",
            "First, both BIAF and STACKPTR parsers achieve relatively high parsing accuracies on all the 12 languages ? all with UAS are higher than 90%.",
            "On nine languages ? Catalan, Czech, Dutch, English, French, German, Norwegian, Russian and Spanish ? STACKPTR outperforms BIAF for both UAS and LAS.",
            "On Bulgarian, STACKPTR achieves slightly better UAS while LAS is slightly worse than BIAF.",
            "On Italian and Romanian, BIAF obtains marginally better parsing performance than STACKPTR."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "BIAF",
                "STACKPTR"
            ],
            [
                "BIAF",
                "STACKPTR",
                "UAS"
            ],
            [
                "ca",
                "cs",
                "nl",
                "de",
                "en",
                "fr",
                "no",
                "ru",
                "es",
                "STACKPTR",
                "UAS",
                "LAS"
            ],
            [
                "bg",
                "STACKPTR",
                "UAS",
                "BIAF",
                "LAS"
            ],
            [
                "it",
                "ro",
                "BIAF"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_4",
        "paper_id": "P18-1130",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1132table_2",
        "description": "Table 2 indicates that, given enough capacity, LSTM language models without explicit syntactic supervision are able to perform well in number agreement. For cases with multiple attractors, we observe that the LSTM language model with 50 hidden units trails behind its larger counterparts by a substantial margin despite comparable performance for zero attractor cases, suggesting that network capacity plays an especially important role in propagating relevant structural information across a large number of steps.5 . As demonstrated on the last row of Table 2, we find that the character LSTM language model performs much worse at number agreement with multiple attractors compared to its word-based counterparts.",
        "sentences": [
            "Table 2 indicates that, given enough capacity, LSTM language models without explicit syntactic supervision are able to perform well in number agreement.",
            "For cases with multiple attractors, we observe that the LSTM language model with 50 hidden units trails behind its larger counterparts by a substantial margin despite comparable performance for zero attractor cases, suggesting that network capacity plays an especially important role in propagating relevant structural information across a large number of steps.5 .",
            "As demonstrated on the last row of Table 2, we find that the character LSTM language model performs much worse at number agreement with multiple attractors compared to its word-based counterparts."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "Our LSTM H=350"
            ],
            [
                "LSTM H=50",
                "Our LSTM H=50",
                "Our LSTM H=150",
                "Our LSTM H=250",
                "Our LSTM H=350"
            ],
            [
                "Char LSTM",
                "1B Word LSTM (repl)"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_2",
        "paper_id": "P18-1132",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1136table_5",
        "description": "In Table 5, our model can achieve highest 12.6 BLEU score. In addition, Mem2Seq has shown promising results in terms of Entity F1 scores (33.4%), which are, in general, much higher than those of other baselines. Note that the numbers reported from Eric et al. (2017) are not directly comparable to ours as we mention below. The other baselines such as Seq2Seq or PtrUnk especially have worse performances in this dataset since it is very inefficient for RNN methods to encode longer KB information, which is the advantage of Mem2Seq. Furthermore, we observe an interesting phenomenon that humans can easily achieve a high entity F1 score with a low BLEU score. This implies that stronger reasoning ability over entities (hops) is crucial, but the results may not be similar to the golden answer. We believe humans can produce good answers even with a low BLEU score, since there could be different ways to express the same concepts. Note that the results of KV Retrieval Net baseline reported in Table 5 come from the original paper (Eric et al., 2017) of In-Car Assistant, where they simplified the task by mapping the expression of entities to a canonical form using named entity recognition (NER) and linking.",
        "sentences": [
            "In Table 5, our model can achieve highest 12.6 BLEU score.",
            "In addition, Mem2Seq has shown promising results in terms of Entity F1 scores (33.4%), which are, in general, much higher than those of other baselines.",
            "Note that the numbers reported from Eric et al. (2017) are not directly comparable to ours as we mention below.",
            "The other baselines such as Seq2Seq or PtrUnk especially have worse performances in this dataset since it is very inefficient for RNN methods to encode longer KB information, which is the advantage of Mem2Seq.",
            "Furthermore, we observe an interesting phenomenon that humans can easily achieve a high entity F1 score with a low BLEU score.",
            "This implies that stronger reasoning ability over entities (hops) is crucial, but the results may not be similar to the golden answer.",
            "We believe humans can produce good answers even with a low BLEU score, since there could be different ways to express the same concepts.",
            "Note that the results of KV Retrieval Net baseline reported in Table 5 come from the original paper (Eric et al., 2017) of In-Car Assistant, where they simplified the task by mapping the expression of entities to a canonical form using named entity recognition (NER) and linking."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            [
                "Mem2Seq H3",
                "BLEU"
            ],
            [
                "Mem2Seq H3",
                "Ent. F1"
            ],
            [
                "Human*",
                "Rule-Based*",
                "KV Retrieval Net*"
            ],
            [
                "Seq2Seq",
                "Ptr-Unk",
                "Mem2Seq H1",
                "Mem2Seq H3",
                "Mem2Seq H6"
            ],
            [
                "Human*",
                "Ent. F1",
                "Sch. F1",
                "Wea. F1",
                "Nav. F1"
            ],
            [
                "Human*",
                "Ent. F1",
                "Sch. F1",
                "Wea. F1",
                "Nav. F1"
            ],
            [
                "Human*",
                "BLEU"
            ],
            [
                "KV Retrieval Net*"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_5",
        "paper_id": "P18-1136",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1138table_3",
        "description": "Table 3 displays the accuracy and recall of entities on factoid question answering dialogues. The performance of NKD is slightly better than the specific QA solution GenDS, while LSTM and HRED which are designed for chi-chat almost fail in this task. All the variants of NKD models are capable of generating entities with an accuracy of 60% to 70%, and NKD-gated achieves the best performance with an accuracy of 77.6% and a recall of 77.3%.",
        "sentences": [
            "Table 3 displays the accuracy and recall of entities on factoid question answering dialogues.",
            "The performance of NKD is slightly better than the specific QA solution GenDS, while LSTM and HRED which are designed for chi-chat almost fail in this task.",
            "All the variants of NKD models are capable of generating entities with an accuracy of 60% to 70%, and NKD-gated achieves the best performance with an accuracy of 77.6% and a recall of 77.3%."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "accuracy (%)",
                "recall (%)"
            ],
            [
                "NKD-gated",
                "GenDS",
                "LSTM",
                "HRED"
            ],
            [
                "NKD-ori",
                "NKD-gated",
                "NKD-atte",
                "accuracy (%)",
                "recall (%)"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_3",
        "paper_id": "P18-1138",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1138table_4",
        "description": "Table 4 lists the accuracy and recall of entities on the entire dataset including both the factoid QA and knowledge grounded chit-chats. Not surprisingly, both NKD-ori and NKD-gated outperform GenDS on the entire dataset, and the relative improvement over GenDS is even higher than the improvement in QA dialogues. It confirms that although NKD and GenDS are comparable in answering factoid questions, NKD is better at introducing the knowledge entities for knowledge grounded chit-chats. All the NKD variants in Table 4 generate more entities than GenDS. LSTM and HRED also produce a certain amount of entities, but are of low accuracies and recalls. We also noticed that NKDgated achieves the highest accuracy and recall, but generates fewer entities compared with NKDori and NKD-gated, whereas NKD-atte generates more entities but also with relatively low accuracies and recalls. This demonstrates that NKDgated not only learns to generate more entities but also maintains the quality ( with a relatively high accuracy and recall ).",
        "sentences": [
            "Table 4 lists the accuracy and recall of entities on the entire dataset including both the factoid QA and knowledge grounded chit-chats.",
            "Not surprisingly, both NKD-ori and NKD-gated outperform GenDS on the entire dataset, and the relative improvement over GenDS is even higher than the improvement in QA dialogues.",
            "It confirms that although NKD and GenDS are comparable in answering factoid questions, NKD is better at introducing the knowledge entities for knowledge grounded chit-chats.",
            "All the NKD variants in Table 4 generate more entities than GenDS.",
            "LSTM and HRED also produce a certain amount of entities, but are of low accuracies and recalls.",
            "We also noticed that NKDgated achieves the highest accuracy and recall, but generates fewer entities compared with NKDori and NKD-gated, whereas NKD-atte generates more entities but also with relatively low accuracies and recalls.",
            "This demonstrates that NKDgated not only learns to generate more entities but also maintains the quality ( with a relatively high accuracy and recall )."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "accuracy (%)",
                "recall (%)",
                "entity number"
            ],
            [
                "NKD-ori",
                "NKD-gated",
                "GenDS"
            ],
            [
                "NKD-ori",
                "NKD-gated",
                "GenDS"
            ],
            [
                "NKD-ori",
                "NKD-gated",
                "NKD-atte",
                "entity number"
            ],
            [
                "LSTM",
                "HRED",
                "entity number",
                "accuracy (%)",
                "recall (%)"
            ],
            [
                "NKD-gated",
                "NKD-ori",
                "NKD-atte",
                "accuracy (%)",
                "recall (%)",
                "entity number"
            ],
            [
                "NKD-gated",
                "accuracy (%)",
                "recall (%)",
                "entity number"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_4",
        "paper_id": "P18-1138",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1138table_5",
        "description": "The results of human evaluation in Table 5 also validate the superiority of the proposed model, especially on appropriateness. Responses generated by LSTM and HRED are of high fluency, but are simply repetitions, or even dull responses as \u0081gI don\u0081ft know.\u0081h, \u0081gGood.\u0081h. NKD-gated is more adept at incorporating the knowledge base with respect to appropriateness and correctness, while NKDatte generates more fluent responses. NKD-ori is a compromise, and obtains the best correctness in completing an entire dialogue. Four evaluators rated the scores independently. The pairwise Cohen\u0081fs Kappa agreement scores are 0.67 on fluency, 0.54 on appropriateness, and 0.60 on entire correctness, which indicate a strong annotator agreement.",
        "sentences": [
            "The results of human evaluation in Table 5 also validate the superiority of the proposed model, especially on appropriateness.",
            "Responses generated by LSTM and HRED are of high fluency, but are simply repetitions, or even dull responses as \u0081gI don\u0081ft know.\u0081h, \u0081gGood.\u0081h.",
            "NKD-gated is more adept at incorporating the knowledge base with respect to appropriateness and correctness, while NKDatte generates more fluent responses.",
            "NKD-ori is a compromise, and obtains the best correctness in completing an entire dialogue.",
            "Four evaluators rated the scores independently.",
            "The pairwise Cohen\u0081fs Kappa agreement scores are 0.67 on fluency, 0.54 on appropriateness, and 0.60 on entire correctness, which indicate a strong annotator agreement."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "LSTM",
                "HRED",
                "Fluency"
            ],
            [
                "NKD-gated",
                "Appropriateness of knowledge",
                "Entire Correctness",
                "NKD-atte",
                "Fluency"
            ],
            [
                "NKD-ori",
                "Appropriateness of knowledge",
                "Entire Correctness"
            ],
            null,
            [
                "Fluency",
                "Appropriateness of knowledge",
                "Entire Correctness"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_5",
        "paper_id": "P18-1138",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1141table_3",
        "description": "Table 3 presents evaluation results for roundtrip translation and sentiment analysis. Validity of roundtrip (RT) evaluation results. RTSIMPLE (line 1) is not competitive; e.g., its accuracy is lower by almost half compared to N(t). We also see that RT is an excellent differentiator of poor multilingual embeddings (e.g., BOW) vs. higher-quality ones like S-ID and N(t). This indicates that RT translation can serve as an effective evaluation measure. The concept-based multilingual embedding learning algorithms CLIQUE and N(t)(lines 5-6) consistently (except S1 WORD) outperform BOW and S-ID (lines 2-3) that are not based on concepts. BOW performs poorly in our low-resource setting; this is not surprising since BOW methods rely on large datasets and are therefore expected to fail in the face of severe sparseness. S-ID performs reasonably well for WORD, but even in that case it is outperformed by N(t), in some cases by a large margin, e.g., \u00b5 of 63 for S-ID vs. 80 for N(t) for S4. For CHAR, S-ID results are poor. On sentiment classification, N(t) also consistently outperforms S-ID. While S-ID provides a clearer signal to the embedding learner than BOW, it is still relatively crude to represent a word as - essentially - its binary vector of verse occurrence. Concept-based methods perform better because they can exploit the more informative dictionary graph. Comparison of graph-theoretic definitions of concepts:N(t)-CLIQUE, N(t)-CC. N(t) (line6) has the most consistent good performance across tasks and evaluation measures. Postfiltering target neighborhoods down to cliques (line 7) and CCs (line 8) does not work. The reason is that the resulting number of concepts is too small; see, e.g., low coverages of N = 18 (N(t)-CLIQUE) and N = 5 (N(t)-CC) for WORD and N = 21 (N(t)-CC) for CHAR. N(t)-CLIQUE results are highly increased for CHAR, but still poorer by a large margin than the best methods. We can interpret this result as an instance of a precision-recall tradeoff: presumably the quality of the concepts found by N(t)-CLIQUE and N(t)-CC is better (higher precision), but there are too few of them (low recall) to get good evaluation numbers. Comparison of graph-theoretic definitions of concepts: CLIQUE. CLIQUE has strong performance for a subset of measures, e.g., ranks consistently second for RT (except S1 WORD) and sentiment analysis in WORD. Although CLIQUE is perhaps the most intuitive way of inducing a concept from a dictionary graph, it may suffer in relatively high-noise settings like ours. Comparison of graph-theoretic definitions of concepts: N(t) vs. N(t)-EDGE. Recall that N(t)-EDGE postfilters target neighborhoods by only considering pairs of pivot words that are\r\nlinked by a dictionary edge. This \u0081gquality\u0081h filter does seem to work in some cases, e.g., best performance S16 Md for CHAR. But results for WORD are much poorer. SAMPLE performs best for CHAR: best results in five out of eight cases. However, its coverage is low: N = 58. This is also the reason that it does not perform well on sentiment analysis for CHAR (F1 = 77 for pos). Target neighborhoods N(t). The overall best method is N(t). It is the best method more often than any other method and in the other cases, it ranks second.",
        "sentences": [
            "Table 3 presents evaluation results for roundtrip translation and sentiment analysis.",
            "Validity of roundtrip (RT) evaluation results.",
            "RTSIMPLE (line 1) is not competitive; e.g., its accuracy is lower by almost half compared to N(t).",
            "We also see that RT is an excellent differentiator of poor multilingual embeddings (e.g., BOW) vs. higher-quality ones like S-ID and N(t).",
            "This indicates that RT translation can serve as an effective evaluation measure.",
            "The concept-based multilingual embedding learning algorithms CLIQUE and N(t)(lines 5-6) consistently (except S1 WORD) outperform BOW and S-ID (lines 2-3) that are not based on concepts.",
            "BOW performs poorly in our low-resource setting; this is not surprising since BOW methods rely on large datasets and are therefore expected to fail in the face of severe sparseness.",
            "S-ID performs reasonably well for WORD, but even in that case it is outperformed by N(t), in some cases by a large margin, e.g., \u00b5 of 63 for S-ID vs. 80 for N(t) for S4.",
            "For CHAR, S-ID results are poor.",
            "On sentiment classification, N(t) also consistently outperforms S-ID.",
            "While S-ID provides a clearer signal to the embedding learner than BOW, it is still relatively crude to represent a word as - essentially - its binary vector of verse occurrence.",
            "Concept-based methods perform better because they can exploit the more informative dictionary graph.",
            "Comparison of graph-theoretic definitions of concepts:N(t)-CLIQUE, N(t)-CC.",
            "N(t) (line6) has the most consistent good performance across tasks and evaluation measures.",
            "Postfiltering target neighborhoods down to cliques (line 7) and CCs (line 8) does not work.",
            "The reason is that the resulting number of concepts is too small; see, e.g., low coverages of N = 18 (N(t)-CLIQUE) and N = 5 (N(t)-CC) for WORD and N = 21 (N(t)-CC) for CHAR. N(t)-CLIQUE results are highly increased for CHAR, but still poorer by a large margin than the best methods.",
            "We can interpret this result as an instance of a precision-recall tradeoff: presumably the quality of the concepts found by N(t)-CLIQUE and N(t)-CC is better (higher precision), but there are too few of them (low recall) to get good evaluation numbers.",
            "Comparison of graph-theoretic definitions of concepts: CLIQUE.",
            "CLIQUE has strong performance for a subset of measures, e.g., ranks consistently second for RT (except S1 WORD) and sentiment analysis in WORD.",
            "Although CLIQUE is perhaps the most intuitive way of inducing a concept from a dictionary graph, it may suffer in relatively high-noise settings like ours.",
            "Comparison of graph-theoretic definitions of concepts: N(t) vs. N(t)-EDGE.",
            "Recall that N(t)-EDGE postfilters target neighborhoods by only considering pairs of pivot words that are\r\nlinked by a dictionary edge.",
            "This \u0081gquality\u0081h filter does seem to work in some cases, e.g., best performance S16 Md for CHAR.",
            "But results for WORD are much poorer.",
            "SAMPLE performs best for CHAR: best results in five out of eight cases.",
            "However, its coverage is low: N = 58.",
            "This is also the reason that it does not perform well on sentiment analysis for CHAR (F1 = 77 for pos).",
            "Target neighborhoods N(t).",
            "The overall best method is N(t).",
            "It is the best method more often than any other method and in the other cases, it ranks second."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            2,
            1,
            1,
            1,
            1,
            2,
            1,
            1,
            2,
            1,
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "roundtrip translation",
                "sentiment analysis"
            ],
            [
                "roundtrip translation"
            ],
            [
                "RTSIMPLE",
                "N(t)"
            ],
            [
                "RTSIMPLE",
                "S-ID",
                "N(t)",
                "BOW"
            ],
            null,
            [
                "CLIQUE",
                "N(t)"
            ],
            [
                "BOW"
            ],
            [
                "S-ID",
                "WORD",
                "N(t)",
                "S4",
                "\u00b5"
            ],
            [
                "CHAR",
                "S-ID"
            ],
            [
                "sentiment analysis",
                "N(t)",
                "S-ID"
            ],
            [
                "S-ID",
                "BOW"
            ],
            [
                "CLIQUE"
            ],
            [
                "N(t)-CLIQUE",
                "N(t)-CC"
            ],
            [
                "N(t)",
                "N(t)-CLIQUE",
                "N(t)-CC"
            ],
            [
                "N(t)-CLIQUE",
                "N(t)-CC"
            ],
            [
                "N(t)-CLIQUE",
                "N(t)-CC",
                "N",
                "WORD",
                "CHAR",
                "roundtrip translation"
            ],
            [
                "N(t)-CLIQUE",
                "N(t)-CC",
                "roundtrip translation"
            ],
            [
                "CLIQUE"
            ],
            [
                "CLIQUE",
                "roundtrip translation",
                "sentiment analysis",
                "WORD"
            ],
            [
                "CLIQUE"
            ],
            [
                "N(t)",
                "N(t)-EDGE"
            ],
            [
                "N(t)-EDGE"
            ],
            [
                "N(t)-EDGE",
                "CHAR",
                "S16",
                "Md"
            ],
            [
                "N(t)-EDGE",
                "WORD"
            ],
            [
                "SAMPLE",
                "roundtrip translation",
                "CHAR"
            ],
            [
                "SAMPLE",
                "roundtrip translation",
                "CHAR",
                "N"
            ],
            [
                "SAMPLE",
                "sentiment analysis",
                "pos"
            ],
            [
                "N(t)"
            ],
            [
                "N(t)"
            ],
            [
                "N(t)"
            ]
        ],
        "n_sentence": 30.0,
        "table_id": "table_3",
        "paper_id": "P18-1141",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1144table_4",
        "description": "As shown in Table 4, without using word segmentation, a characterbased LSTM-CRF model gives a development F1- score of 62.47%. Adding character-bigram and softword representations as described in Section 3.1 increases the F1-score to 67.63% and 65.71%, respectively, demonstrating the usefulness of both sources of information. In addition, a combination of both gives a 69.64% F1-score, which is the best among various character representations. We thus choose this model in the remaining experiments. Word-based NER. Table 4 shows a variety of different settings for word-based Chinese NER. With automatic segmentation, a word-based LSTM CRF baseline gives a 64.12% F1-score, which is higher compared to the character-based baseline. This demonstrates that both word information and character information are useful for Chinese NER. The two methods of using character LSTM to enrich word representations in Section 3.2, namely word+char LSTM and word+char LSTM(cid:48), lead to similar improvements. A CNN representation of character sequences gives a slightly higher F1-score compared to LSTM character representations. On the other hand, further using character bigram information leads to increased F1-score over word+char LSTM, but decreased F1-score over word+char CNN. A possible reason is that CNN inherently captures character n-gram information. As a result, we use word+char+bichar LSTM for wordbased NER in the remaining experiments, which gives the best development results, and is structurally consistent with the state-of-the-art English NER models in the literature. As shown in Table 4, the lattice LSTM-CRF model gives a development F1-score of 71.62%, which is significantly higher compared with both the word-based and character-based methods, despite that it does not use character bigrams or word segmentation information. The fact that it significantly outperforms char+softword shows the advantage of lattice word information as compared with segmentor word information.",
        "sentences": [
            "As shown in Table 4, without using word segmentation, a characterbased LSTM-CRF model gives a development F1- score of 62.47%.",
            "Adding character-bigram and softword representations as described in Section 3.1 increases the F1-score to 67.63% and 65.71%, respectively, demonstrating the usefulness of both sources of information.",
            "In addition, a combination of both gives a 69.64% F1-score, which is the best among various character representations.",
            "We thus choose this model in the remaining experiments.",
            "Word-based NER.",
            "Table 4 shows a variety of different settings for word-based Chinese NER.",
            "With automatic segmentation, a word-based LSTM CRF baseline gives a 64.12% F1-score, which is higher compared to the character-based baseline.",
            "This demonstrates that both word information and character information are useful for Chinese NER.",
            "The two methods of using character LSTM to enrich word representations in Section 3.2, namely word+char LSTM and word+char LSTM(cid:48), lead to similar improvements.",
            "A CNN representation of character sequences gives a slightly higher F1-score compared to LSTM character representations.",
            "On the other hand, further using character bigram information leads to increased F1-score over word+char LSTM, but decreased F1-score over word+char CNN.",
            "A possible reason is that CNN inherently captures character n-gram information.",
            "As a result, we use word+char+bichar LSTM for wordbased NER in the remaining experiments, which gives the best development results, and is structurally consistent with the state-of-the-art English NER models in the literature.",
            "As shown in Table 4, the lattice LSTM-CRF model gives a development F1-score of 71.62%, which is significantly higher compared with both the word-based and character-based methods, despite that it does not use character bigrams or word segmentation information.",
            "The fact that it significantly outperforms char+softword shows the advantage of lattice word information as compared with segmentor word information."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            0,
            1,
            1,
            2,
            1,
            1,
            1,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "F1",
                "Char baseline"
            ],
            [
                "F1",
                "Char+softword",
                "Char+bichar"
            ],
            [
                "F1",
                "Char+bichar+softword"
            ],
            [
                "Char+bichar+softword"
            ],
            null,
            [
                "Word+char LSTM",
                "Word+char LSTM'",
                "Word+char+bichar LSTM",
                "Word+char CNN",
                "Word+char+bichar CNN"
            ],
            [
                "Auto seg",
                "F1",
                "Word baseline",
                "Char baseline"
            ],
            [
                "Word baseline",
                "Char baseline"
            ],
            [
                "Word+char LSTM"
            ],
            [
                "Word+char LSTM",
                "Word+char CNN"
            ],
            [
                "F1",
                "Word+char+bichar CNN",
                "Word+char LSTM",
                "Word+char CNN"
            ],
            [
                "Word+char CNN",
                "Word+char+bichar CNN"
            ],
            [
                "Word+char+bichar LSTM"
            ],
            [
                "Lattice",
                "F1"
            ],
            [
                "Lattice",
                "F1",
                "Char+softword"
            ]
        ],
        "n_sentence": 15.0,
        "table_id": "table_4",
        "paper_id": "P18-1144",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1145table_3",
        "description": "Table 3 shows the results on KBP2017Eval. We can see that NPN(Task-specific) outperforms other methods significantly. We believe this is because: 1) FBRNN(Char) only regards tokens in the candidate table as potential trigger nuggets, which limits the choice of possible trigger nuggets and\nresults in a very low recall rate.\n. 2) To accurately identify a trigger, NPN(IOB) and conventional character-based methods require all characters in a trigger being classified correctly, which is very challenging (Zeng et al., 2016): many characters appear in a trigger nugget will not serve as a part of a trigger nugget in the majority of contexts, thus they will be easily classified into \u0081gNIL\u0081h.",
        "sentences": [
            "Table 3 shows the results on KBP2017Eval.",
            "We can see that NPN(Task-specific) outperforms other methods significantly.",
            "We believe this is because: 1) FBRNN(Char) only regards tokens in the candidate table as potential trigger nuggets, which limits the choice of possible trigger nuggets and\nresults in a very low recall rate.\n.",
            "2) To accurately identify a trigger, NPN(IOB) and conventional character-based methods require all characters in a trigger being classified correctly, which is very challenging (Zeng et al., 2016): many characters appear in a trigger nugget will not serve as a part of a trigger nugget in the majority of contexts, thus they will be easily classified into \u0081gNIL\u0081h."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "NPN(Task-specific)"
            ],
            [
                "FBRNN(Char)"
            ],
            [
                "NPN(IOB)"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "P18-1145",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1145table_6",
        "description": "Table 6 shows the experiment results. We can see that neither character-level or wordlevel representation can achieve competitive results with the NPNs. This verified the necessity of hybrid representation.",
        "sentences": [
            "Table 6 shows the experiment results.",
            "We can see that neither character-level or wordlevel representation can achieve competitive results with the NPNs.",
            "This verified the necessity of hybrid representation."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "NPN(Task-specific)",
                "DMCNN(Word)",
                "NPN(Char)"
            ],
            [
                "NPN(Task-specific)"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_6",
        "paper_id": "P18-1145",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1150table_2",
        "description": "Table 2 compares our final results with existing work. MSeq2seq+Anon (Konstas et al., 2017) is an attentional multi-layer sequence-to-sequence model trained with the anonymized data. PBMT (Pourdamghani et al., 2016) adopts a phrase-based model for machine translation (Koehn et al., 2003) on the input of linearized AMR graph, SNRG (Song et al., 2017) uses synchronous node replacement grammar for parsing the AMR graph while generating the text, and Tree2Str (Flanigan et al., 2016b) converts AMR graphs into trees by splitting the re-entrances before using a tree transducer to generate the results. Graph2seq+charLSTM+copy achieves a BLEU score of 23.3, which is 1.3 points better than MSeq2seq+Anon trained on the same AMR corpus. In addition, our model without character LSTM is still 0.7 BLEU points higher than MSeq2seq+Anon. Note that MSeq2seq+Anon relies on anonymization, which requires additional manual work for defining mapping rules, thus limiting its usability on other languages and domains. The neural models tend to underperform statistical models when trained on limited (16K) gold data, but performs better with scaled silver data (Konstas et al., 2017). Following Konstas et al. (2017), we also evaluate our model using both the AMR corpus and sampled sentences from Gigaword. Using additional 200K or 2M gigaword sentences, Graph2seq+charLSTM+copy achieves BLEU scores of 28.2 and 33.0, respectively, which are 0.8 and 0.7 BLEU points better than MSeq2seq+Anon using the same amount of data, respectively. The BLEU scores are 5.3 and 10.1 points better than the result when it is only trained with the AMR corpus, respectively. This shows that our model can benefit from scaled data with automatically generated AMR graphs, and it is more effective than MSeq2seq+Anon using the same amount of data.",
        "sentences": [
            "Table 2 compares our final results with existing work.",
            "MSeq2seq+Anon (Konstas et al., 2017) is an attentional multi-layer sequence-to-sequence model trained with the anonymized data.",
            "PBMT (Pourdamghani et al., 2016) adopts a phrase-based model for machine translation (Koehn et al., 2003) on the input of linearized AMR graph, SNRG (Song et al., 2017) uses synchronous node replacement grammar for parsing the AMR graph while generating the text, and Tree2Str (Flanigan et al., 2016b) converts AMR graphs into trees by splitting the re-entrances before using a tree transducer to generate the results.",
            "Graph2seq+charLSTM+copy achieves a BLEU score of 23.3, which is 1.3 points better than MSeq2seq+Anon trained on the same AMR corpus.",
            "In addition, our model without character LSTM is still 0.7 BLEU points higher than MSeq2seq+Anon.",
            "Note that MSeq2seq+Anon relies on anonymization, which requires additional manual work for defining mapping rules, thus limiting its usability on other languages and domains.",
            "The neural models tend to underperform statistical models when trained on limited (16K) gold data, but performs better with scaled silver data (Konstas et al., 2017).",
            "Following Konstas et al. (2017), we also evaluate our model using both the AMR corpus and sampled sentences from Gigaword.",
            "Using additional 200K or 2M gigaword sentences, Graph2seq+charLSTM+copy achieves BLEU scores of 28.2 and 33.0, respectively, which are 0.8 and 0.7 BLEU points better than MSeq2seq+Anon using the same amount of data, respectively.",
            "The BLEU scores are 5.3 and 10.1 points better than the result when it is only trained with the AMR corpus, respectively.",
            "This shows that our model can benefit from scaled data with automatically generated AMR graphs, and it is more effective than MSeq2seq+Anon using the same amount of data."
        ],
        "class_sentence": [
            1,
            2,
            2,
            1,
            1,
            2,
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "MSeq2seq+Anon"
            ],
            [
                "PBMT",
                "SNRG",
                "Tree2Str"
            ],
            [
                "BLEU",
                "Graph2seq+charLSTM+copy",
                "MSeq2seq+Anon"
            ],
            [
                "BLEU",
                "MSeq2seq+Anon",
                "Graph2seq+copy"
            ],
            [
                "MSeq2seq+Anon"
            ],
            null,
            null,
            [
                "BLEU",
                "Graph2seq+charLSTM+copy (200K)",
                "Graph2seq+charLSTM+copy (2M)",
                "MSeq2seq+Anon (200K)",
                "MSeq2seq+Anon (2M)"
            ],
            [
                "BLEU",
                "Graph2seq+charLSTM+copy (200K)",
                "Graph2seq+charLSTM+copy (2M)"
            ],
            [
                "Graph2seq+charLSTM+copy (200K)",
                "Graph2seq+charLSTM+copy (2M)",
                "MSeq2seq+Anon"
            ]
        ],
        "n_sentence": 11.0,
        "table_id": "table_2",
        "paper_id": "P18-1150",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1151table_4",
        "description": "Table 4 shows the results of the human evaluations. The results confirm the automatic evaluation in which our proposed model achieves the best scores.",
        "sentences": [
            "Table 4 shows the results of the human evaluations.",
            "The results confirm the automatic evaluation in which our proposed model achieves the best scores."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "GTR-LSTM"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_4",
        "paper_id": "P18-1151",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1152table_4",
        "description": "To investigate the effect of individual discriminators on the overall performance, we report the results of ablations of our model in Table 4. For each ablation we include only one of the communication modules, and train a single mixture coefficient for combining that module and the language model. The diagonal of Table 4 contains only positive numbers, indicating that each discriminator does help with the purpose it was designed for. Interestingly, most discriminators help with most aspects of writing, but all except repetition fail to actually improve the overall quality over ADAPTIVELM.",
        "sentences": [
            "To investigate the effect of individual discriminators on the overall performance, we report the results of ablations of our model in Table 4.",
            "For each ablation we include only one of the communication modules, and train a single mixture coefficient for combining that module and the language model.",
            "The diagonal of Table 4 contains only positive numbers, indicating that each discriminator does help with the purpose it was designed for.",
            "Interestingly, most discriminators help with most aspects of writing, but all except repetition fail to actually improve the overall quality over ADAPTIVELM."
        ],
        "class_sentence": [
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "Ablation vs. LM",
                "Repetition",
                "Contradiction",
                "Relevance",
                "Clarity"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_4",
        "paper_id": "P18-1152",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1154table_3",
        "description": "Table 3 shows the BLEU and BLEU-2 scores for the proposed model under different subsets of features. Overall BLEU scores are low, likely due to the inherent variance in the language generation task (Novikova et al., 2017) , although a precursory examination of the outputs for data points selected randomly from test set indicated that they were reasonable.",
        "sentences": [
            "Table 3 shows the BLEU and BLEU-2 scores for the proposed model under different subsets of features.",
            "Overall BLEU scores are low, likely due to the inherent variance in the language generation task (Novikova et al., 2017) , although a precursory examination of the outputs for data points selected randomly from test set indicated that they were reasonable."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "BLEU",
                "BLEU-2"
            ],
            [
                "BLEU",
                "BLEU-2"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_3",
        "paper_id": "P18-1154",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1156table_1",
        "description": "In Table 1, we compare various RC datasets with two embodiments of our dataset i.e. the SelfRC and ParaphraseRC. We use NER and noun phrase/verb phrase extraction over the entire dataset to identify key entities in the question, plot and answer which is in turn used to compute the metrics mentioned in the table. The metrics Avg word distance and Avg sentence distance indicate the average distance (in terms of words/sentences) between the occurrence of the question entities and closest occurrence of the answer entities in the passage. Number of sentences for inferencing is indicative of the minimum number of sentences required to cover all the question and answer entities. It is evident that tackling ParaphraseRC is much harder than the others on account of (i) larger distance between the query and answer, (ii) low word-overlap between query & passage, and (iii) higher number of sentences required to infer an answer.",
        "sentences": [
            "In Table 1, we compare various RC datasets with two embodiments of our dataset i.e. the SelfRC and ParaphraseRC.",
            "We use NER and noun phrase/verb phrase extraction over the entire dataset to identify key entities in the question, plot and answer which is in turn used to compute the metrics mentioned in the table.",
            "The metrics Avg word distance and Avg sentence distance indicate the average distance (in terms of words/sentences) between the occurrence of the question entities and closest occurrence of the answer entities in the passage.",
            "Number of sentences for inferencing is indicative of the minimum number of sentences required to cover all the question and answer entities.",
            "It is evident that tackling ParaphraseRC is much harder than the others on account of (i) larger distance between the query and answer, (ii) low word-overlap between query & passage, and (iii) higher number of sentences required to infer an answer."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            [
                "SelfRC",
                "ParaphraseRC"
            ],
            [
                "Movie QA",
                "NarrativeQA over plot-summaries"
            ],
            [
                "Avg. word distance",
                "Avg. sentence distance"
            ],
            [
                "Number of sentences for inferencing"
            ],
            [
                "ParaphraseRC"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_1",
        "paper_id": "P18-1156",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1156table_3",
        "description": "SpanModel v/s GenModel:. Comparing the first two rows (SelfRC) and the last two rows (ParaphraseRC) of Table 3 we see that the SpanModel clearly outperforms the GenModel. This is not very surprising for two reasons. First, around 70% (and 50%) of the answers in SelfRC (and ParaphraseRC)\nrespectively, match an exact span in the document\nso the SpanModel still has scope to do well on\nthese answers. On the other hand, even if the first stage of the GenModel predicts the span correctly, the second stage could make an error in generating the correct answer from it because generation is a harder problem. For the second stage, it is expected that the GenModel should learn to copy the predicted span to produce the answer output (as is required in most cases) and only occasionally where necessary, generate an answer. However, surprisingly the GenModel fails to even do this. Manual inspection of the generated answers shows that in many cases the generator ends up generating either more or fewer words compared the true answer. This demonstrates the clear scope for the GenModel to perform better. SelfRC v/s ParaphraseRC:. Comparing the SelfRC and ParaphraseRC numbers in Table 3, we observe that the performance of the models clearly drops for the latter task, thus validating our hypothesis that ParaphraseRC is a indeed a much harder task. Finally, comparing the SpanModel with and without 1691 Paraphrasing in Table 3 for ParaphraseRC, we observe that the pre-processing step indeed improves the performance of the Span Detection Model.",
        "sentences": [
            "SpanModel v/s GenModel:.",
            "Comparing the first two rows (SelfRC) and the last two rows (ParaphraseRC) of Table 3 we see that the SpanModel clearly outperforms the GenModel.",
            "This is not very surprising for two reasons.",
            "First, around 70% (and 50%) of the answers in SelfRC (and ParaphraseRC)\nrespectively, match an exact span in the document\nso the SpanModel still has scope to do well on\nthese answers.",
            "On the other hand, even if the first stage of the GenModel predicts the span correctly, the second stage could make an error in generating the correct answer from it because generation is a harder problem.",
            "For the second stage, it is expected that the GenModel should learn to copy the predicted span to produce the answer output (as is required in most cases) and only occasionally where necessary, generate an answer.",
            "However, surprisingly the GenModel fails to even do this.",
            "Manual inspection of the generated answers shows that in many cases the generator ends up generating either more or fewer words compared the true answer.",
            "This demonstrates the clear scope for the GenModel to perform better.",
            "SelfRC v/s ParaphraseRC:.",
            "Comparing the SelfRC and ParaphraseRC numbers in Table 3, we observe that the performance of the models clearly drops for the latter task, thus validating our hypothesis that ParaphraseRC is a indeed a much harder task.",
            "Finally, comparing the SpanModel with and without 1691 Paraphrasing in Table 3 for ParaphraseRC, we observe that the pre-processing step indeed improves the performance of the Span Detection Model."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "SpanModel",
                "GenModel (with augmented training data)",
                "SpanModel with Preprocessed Data"
            ],
            null,
            [
                "SpanModel",
                "SpanModel with Preprocessed Data"
            ],
            [
                "GenModel (with augmented training data)"
            ],
            [
                "GenModel (with augmented training data)"
            ],
            [
                "GenModel (with augmented training data)"
            ],
            [
                "GenModel (with augmented training data)"
            ],
            [
                "GenModel (with augmented training data)"
            ],
            [
                "SelfRC",
                "ParaphraseRC"
            ],
            [
                "SelfRC",
                "ParaphraseRC"
            ],
            [
                "ParaphraseRC",
                "SpanModel",
                "SpanModel with Preprocessed Data"
            ]
        ],
        "n_sentence": 12.0,
        "table_id": "table_3",
        "paper_id": "P18-1156",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1157table_1",
        "description": "The main results in terms of EM and F1 are shown in Table 1. We observe that SAN achieves 76.235 EM and 84.056 F1, outperforming all other models. Standard 1-step model only achieves 75.139 EM and dynamic steps (via ReasoNet) achieves only 75.355 EM. SAN also outperforms a 5-step memory net with averaging, which implies averaging predictions is not the only thing that led to SAN\u0081fs superior results; indeed, stochastic prediction dropout is an effective technique.",
        "sentences": [
            "The main results in terms of EM and F1 are shown in Table 1.",
            "We observe that SAN achieves 76.235 EM and 84.056 F1, outperforming all other models.",
            "Standard 1-step model only achieves 75.139 EM and dynamic steps (via ReasoNet) achieves only 75.355 EM.",
            "SAN also outperforms a 5-step memory net with averaging, which implies averaging predictions is not the only thing that led to SAN\u0081fs superior results; indeed, stochastic prediction dropout is an effective technique."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "EM",
                "F1"
            ],
            [
                "Stochastic Answer Network (SAN) Fixed 5-step",
                "EM",
                "F1"
            ],
            [
                "Standard 1-step",
                "EM",
                "F1",
                "Dynamic steps (max 5) with ReasoNet"
            ],
            [
                "Stochastic Answer Network (SAN) Fixed 5-step"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_1",
        "paper_id": "P18-1157",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1157table_4",
        "description": "Table 4 shows the development set scores for T = 1 to T = 10. We observe that there is a gradual improvement as we increase T = 1 to T = 5, but after 5 steps the improvements have saturated. In fact, the EM/F1 scores drop slightly, but considering that the random initialization results in Table 3 show a standard deviation of 0.142 and a spread of 0.426 (for EM), we believe that the T = 10 result does not statistically differ from the T = 5 result. In summary, we think it is useful to perform some approximate hyper-parameter tuning for the number of steps, but it is not necessary to find the exact optimal value.",
        "sentences": [
            "Table 4 shows the development set scores for T = 1 to T = 10.",
            "We observe that there is a gradual improvement as we increase T = 1 to T = 5, but after 5 steps the improvements have saturated.",
            "In fact, the EM/F1 scores drop slightly, but considering that the random initialization results in Table 3 show a standard deviation of 0.142 and a spread of 0.426 (for EM), we believe that the T = 10 result does not statistically differ from the T = 5 result.",
            "In summary, we think it is useful to perform some approximate hyper-parameter tuning for the number of steps, but it is not necessary to find the exact optimal value."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "1 step",
                "2 step",
                "3 step",
                "4 step",
                "5 step",
                "6 step",
                "7 step",
                "8 step",
                "9 step",
                "10 step"
            ],
            [
                "1 step",
                "2 step",
                "3 step",
                "4 step",
                "5 step"
            ],
            [
                "EM",
                "F1",
                "10 step",
                "5 step"
            ],
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_4",
        "paper_id": "P18-1157",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1157table_5",
        "description": "The results in Table 5 show that SAN achieves the new state-of-the-art performance and SAN\u0081fs superior result is mainly attributed to the multi-step answer module, which leads to significant improvement in F1 score over the Standard 1-step answer module, i.e., +1.2 on AddSent and +0.7 on AddOneSent.",
        "sentences": [
            "The results in Table 5 show that SAN achieves the new state-of-the-art performance and SAN\u0081fs superior result is mainly attributed to the multi-step answer module, which leads to significant improvement in F1 score over the Standard 1-step answer module, i.e., +1.2 on AddSent and +0.7 on AddOneSent."
        ],
        "class_sentence": [
            1
        ],
        "header_mention": [
            [
                "AddSent",
                "AddOneSent",
                "SAN",
                "Standard 1-step in Table 1"
            ]
        ],
        "n_sentence": 1.0,
        "table_id": "table_5",
        "paper_id": "P18-1157",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1157table_7",
        "description": "The results in Table 7 show that SAN outperforms V-Net (Wang et al., 2018) and becomes the new state of the art6.",
        "sentences": [
            "The results in Table 7 show that SAN outperforms V-Net (Wang et al., 2018) and becomes the new state of the art."
        ],
        "class_sentence": [
            1
        ],
        "header_mention": [
            [
                "V-Net(Wang et al. 2018)",
                "SAN"
            ]
        ],
        "n_sentence": 1.0,
        "table_id": "table_7",
        "paper_id": "P18-1157",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1160table_5",
        "description": "Table 5 shows results in the task of QA on SQuAD and NewsQA. MINIMAL is more efficient in training and inference than FULL. On SQuAD, S-Reader achieves 6.7 training and 3.6 inference speedup on SQuAD, and 15.0 training and 6.9 inference speedup on NewsQA. In addition to the speedup, MINIMAL achieves comparable result to FULL (using S-Reader, 79.9 vs 79.8 F1 on SQuAD and 63.8 vs 63.2 F1 on NewsQA).",
        "sentences": [
            "Table 5 shows results in the task of QA on SQuAD and NewsQA.",
            "MINIMAL is more efficient in training and inference than FULL.",
            "On SQuAD, S-Reader achieves 6.7 training and 3.6 inference speedup on SQuAD, and 15.0 training and 6.9 inference speedup on NewsQA.",
            "In addition to the speedup, MINIMAL achieves comparable result to FULL (using S-Reader, 79.9 vs 79.8 F1 on SQuAD and 63.8 vs 63.2 F1 on NewsQA)."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "SQuAD (with S-Reader)",
                "SQuAD (with DCN+)",
                "NewsQA (with S-Reader)"
            ],
            [
                "MINIMAL(Top k)",
                "MINIMAL(Dyn)",
                "FULL"
            ],
            [
                "SQuAD (with S-Reader)",
                "NewsQA (with S-Reader)",
                "MINIMAL(Top k)",
                "MINIMAL(Dyn)",
                "Train Sp",
                "Infer Sp"
            ],
            [
                "SQuAD (with S-Reader)",
                "FULL",
                "MINIMAL(Dyn)",
                "NewsQA (with S-Reader)",
                "F1"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_5",
        "paper_id": "P18-1160",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1160table_8",
        "description": "Table 8 shows results on TriviaQA (Wikipedia) and SQuAD-Open. First, MINIMAL obtains higher F1 and EM over FULL, with the inference speedup of up to 13.8. Second, the model with our sentence selector with Dyn achieves higher F1 and EM over the model with TF-IDF selector. For example, on the development-full set, with 5 sentences per question on average, the model with Dyn achieves 59.5 F1 while the model with TF-IDF method achieves 51.9 F1. Third, we outperforms the published state-of-the-art on both dataset.",
        "sentences": [
            "Table 8 shows results on TriviaQA (Wikipedia) and SQuAD-Open.",
            "First, MINIMAL obtains higher F1 and EM over FULL, with the inference speedup of up to 13.8.",
            "Second, the model with our sentence selector with Dyn achieves higher F1 and EM over the model with TF-IDF selector.",
            "For example, on the development-full set, with 5 sentences per question on average, the model with Dyn achieves 59.5 F1 while the model with TF-IDF method achieves 51.9 F1.",
            "Third, we outperforms the published state-of-the-art on both dataset."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "TriviaQA (Wikipedia)",
                "SQuAD-Open"
            ],
            [
                "TriviaQA (Wikipedia)",
                "SQuAD-Open",
                "FULL",
                "Our Selector",
                "Sp",
                "F1",
                "EM"
            ],
            [
                "Our Selector",
                "TF-IDF",
                "F1",
                "EM"
            ],
            [
                "TriviaQA (Wikipedia)",
                "F1",
                "Our Selector",
                "TF-IDF"
            ],
            [
                "Our Selector",
                "Rank 1",
                "Rank 2",
                "Rank 3"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_8",
        "paper_id": "P18-1160",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1160table_9",
        "description": "Table 9 shows that MINIMAL outperforms FULL, achieving the new state-of-the-art by large margin (+11.1 and +11.5 F1 on AddSent and AddOneSent, respectively).",
        "sentences": [
            "Table 9 shows that MINIMAL outperforms FULL, achieving the new state-of-the-art by large margin (+11.1 and +11.5 F1 on AddSent and AddOneSent, respectively)."
        ],
        "class_sentence": [
            1
        ],
        "header_mention": [
            [
                "MINIMAL",
                "FULL",
                "AddSent",
                "AddOneSent",
                "Mnemonic Reader"
            ]
        ],
        "n_sentence": 1.0,
        "table_id": "table_9",
        "paper_id": "P18-1160",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1165table_4",
        "description": "Table 4 lists the results for this simulation experiment in rows 2-5 (S). If unlimited clean feedback was given (RL with direct simulated rewards), improvements of over 5 BLEU can be achieved. When limiting the amount of feedback to a log of 800 translations, the improvements over the baseline are only marginal (OPL). When replacing the direct reward by the simulated reward estimators from \u0081\u00985, i.e. having unlimited amounts of approximately clean rewards, however, improvements of 1.2 BLEU for MSE estimators (RL+MSE) and 0.8 BLEU for pairwise estimators (RL+PW) are found. This suggests that the reward estimation model helps to tackle the challenge of generalization over a small set of ratings.\n. Table 4 shows the results for training with human rewards in rows 6-8: . The improvements for OPL are very similar to OPL with simulated rewards, both suffering from overfitting. For RL we observe that the MSEbased reward estimator (RL+MSE) leads to significantly higher improvements as a the pairwise reward estimator (RL+PW) ? the same trend as for simulated ratings. Finally, the improvement of 1.1 BLEU over the baseline showcases that we are able to improve NMT with only a small number of human rewards.",
        "sentences": [
            "Table 4 lists the results for this simulation experiment in rows 2-5 (S).",
            "If unlimited clean feedback was given (RL with direct simulated rewards), improvements of over 5 BLEU can be achieved.",
            "When limiting the amount of feedback to a log of 800 translations, the improvements over the baseline are only marginal (OPL).",
            "When replacing the direct reward by the simulated reward estimators from \u0081\u00985, i.e. having unlimited amounts of approximately clean rewards, however, improvements of 1.2 BLEU for MSE estimators (RL+MSE) and 0.8 BLEU for pairwise estimators (RL+PW) are found.",
            "This suggests that the reward estimation model helps to tackle the challenge of generalization over a small set of ratings.\n.",
            "Table 4 shows the results for training with human rewards in rows 6-8: .",
            "The improvements for OPL are very similar to OPL with simulated rewards, both suffering from overfitting.",
            "For RL we observe that the MSEbased reward estimator (RL+MSE) leads to significantly higher improvements as a the pairwise reward estimator (RL+PW) ? the same trend as for simulated ratings.",
            "Finally, the improvement of 1.1 BLEU over the baseline showcases that we are able to improve NMT with only a small number of human rewards."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "Baseline",
                "RL",
                "OPL",
                "RL+MSE",
                "RL+PW"
            ],
            [
                "BLEU",
                "RL"
            ],
            [
                "BLEU",
                "OPL"
            ],
            [
                "BLEU",
                "RL+MSE",
                "RL+PW"
            ],
            null,
            [
                "OPL",
                "RL+MSE",
                "RL+PW"
            ],
            [
                "OPL"
            ],
            [
                "RL+MSE",
                "RL+PW"
            ],
            [
                "Baseline",
                "RL+MSE"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_4",
        "paper_id": "P18-1165",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1166table_4",
        "description": "Table 4 shows the overall results on 12 translation directions. We also provide the results from WMT17 winning systems4. Notice that unlike the Transformer and our model, these winner systems typically use model ensemble, system combination and large-scale monolingual corpus. Although different languages have different linguistic and syntactic structures, our model consistently yields rather competitive results against the Transformer on all language pairs in both directions. Particularly, on the De\u0081\u00a8En translation task, our model achieves a slight improvement of 0.10/0.07 case-sensitive/case-insensitive BLEU points over the Transformer. The largest performance gap between our model and the Transformer occurs on the En\u2192Tr translation task, where our model is lower than the Transformer by 0.52/0.53 case-sensitive/case-insensitive BLEU points. We conjecture that this difference may be due to the small training corpus of the En-Tr task. In all, these results suggest that our AAN is able to perform comparably to Transformer on different language pairs with different scales of training data.",
        "sentences": [
            "Table 4 shows the overall results on 12 translation directions.",
            "We also provide the results from WMT17 winning systems4.",
            "Notice that unlike the Transformer and our model, these winner systems typically use model ensemble, system combination and large-scale monolingual corpus.",
            "Although different languages have different linguistic and syntactic structures, our model consistently yields rather competitive results against the Transformer on all language pairs in both directions.",
            "Particularly, on the De\u0081\u00a8En translation task, our model achieves a slight improvement of 0.10/0.07 case-sensitive/case-insensitive BLEU points over the Transformer.",
            "The largest performance gap between our model and the Transformer occurs on the En\u2192Tr translation task, where our model is lower than the Transformer by 0.52/0.53 case-sensitive/case-insensitive BLEU points.",
            "We conjecture that this difference may be due to the small training corpus of the En-Tr task.",
            "In all, these results suggest that our AAN is able to perform comparably to Transformer on different language pairs with different scales of training data."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "winner"
            ],
            [
                "Transformer",
                "Our Model",
                "winner"
            ],
            [
                "Our Model",
                "Transformer"
            ],
            [
                "Our Model",
                "De\u2192En",
                "\u0394d",
                "Transformer"
            ],
            [
                "Our Model",
                "Transformer",
                "En\u2192Tr"
            ],
            [
                "En\u2192Tr"
            ],
            [
                "Our Model",
                "Transformer"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_4",
        "paper_id": "P18-1166",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1171table_1",
        "description": "Table 1 shows the phase-wise accuracy of our sequence-to-sequence model. Peng et al. (2018) use a separate feedforward network to predict each phase independently. We use the same alignment from the SemEval dataset as in Peng et al. (2018) to avoid differences resulting from the aligner. Soft+feats shows the performance of our sequence-to-sequence model with soft attention and transition state features, while Hard+feats is using hard attention. We can see that the hard attention model outperforms the soft attention model in all phases, which shows that the single-pointer attention finds more relevant information than the soft attention on the relatively small dataset. The sequence-to-sequence models perform better than the feedforward model of Peng et al. (2018) on ShiftOrPop and ArcBinary, which shows that the whole-sentence context information is important for the prediction of these two phases. On the other hand, the sequence-tosequence models perform worse than the feedforward models on PushIndex and ArcLabel.",
        "sentences": [
            "Table 1 shows the phase-wise accuracy of our sequence-to-sequence model.",
            "Peng et al. (2018) use a separate feedforward network to predict each phase independently.",
            "We use the same alignment from the SemEval dataset as in Peng et al. (2018) to avoid differences resulting from the aligner.",
            "Soft+feats shows the performance of our sequence-to-sequence model with soft attention and transition state features, while Hard+feats is using hard attention.",
            "We can see that the hard attention model outperforms the soft attention model in all phases, which shows that the single-pointer attention finds more relevant information than the soft attention on the relatively small dataset.",
            "The sequence-to-sequence models perform better than the feedforward model of Peng et al. (2018) on ShiftOrPop and ArcBinary, which shows that the whole-sentence context information is important for the prediction of these two phases.",
            "On the other hand, the sequence-tosequence models perform worse than the feedforward models on PushIndex and ArcLabel."
        ],
        "class_sentence": [
            1,
            2,
            2,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Peng et al. (2018)"
            ],
            [
                "Peng et al. (2018)"
            ],
            [
                "Soft+feats",
                "Hard+feats"
            ],
            [
                "Hard+feats",
                "Soft+feats",
                "ShiftOrPop",
                "PushIndex",
                "ArcBinary",
                "ArcLabel"
            ],
            [
                "Soft+feats",
                "Hard+feats",
                "Peng et al. (2018)",
                "ShiftOrPop",
                "ArcBinary"
            ],
            [
                "Soft+feats",
                "Hard+feats",
                "Peng et al. (2018)",
                "PushIndex",
                "ArcLabel"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_1",
        "paper_id": "P18-1171",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1171table_4",
        "description": "Table 4 shows the comparison with other AMR parsers. The first three systems are some competitive neural models. We can see that our parser significantly outperforms the sequence-to-action-sequence model of Buys and Blunsom (2017). Konstas et al.(2017) use a linearization approach that linearizes the AMR graph to a sequence structure and use selftraining on 20M unlabeled Gigaword sentences. Our model achieves better results without using additional unlabeled data, which shows that relevant information from the transition system is very useful for the prediction. Our model also outperforms the stack-LSTM model by Ballesteros and Al-Onaizan (2017), while their model is evaluated on the previous release of LDC2014T12. We also show the performance of some of the best-performing models. While our hard attention achieves slightly lower performance in comparison with Wang et al. (2015a) and Wang and Xue (2017), it is worth noting that their approaches of using WordNet, semantic role labels and word cluster features are complimentary to ours. The alignment from the aligner and the concept identification identifier also play an important role for improving the performance. Wang and Xue (2017) propose to improve AMR parsing by improving the alignment and concept identification, which can also be combined with our system to improve the performance of a sequence-to-sequence model.",
        "sentences": [
            "Table 4 shows the comparison with other AMR parsers.",
            "The first three systems are some competitive neural models.",
            "We can see that our parser significantly outperforms the sequence-to-action-sequence model of Buys and Blunsom (2017).",
            "Konstas et al.(2017) use a linearization approach that linearizes the AMR graph to a sequence structure and use selftraining on 20M unlabeled Gigaword sentences.",
            "Our model achieves better results without using additional unlabeled data, which shows that relevant information from the transition system is very useful for the prediction.",
            "Our model also outperforms the stack-LSTM model by Ballesteros and Al-Onaizan (2017), while their model is evaluated on the previous release of LDC2014T12.",
            "We also show the performance of some of the best-performing models.",
            "While our hard attention achieves slightly lower performance in comparison with Wang et al. (2015a) and Wang and Xue (2017), it is worth noting that their approaches of using WordNet, semantic role labels and word cluster features are complimentary to ours.",
            "The alignment from the aligner and the concept identification identifier also play an important role for improving the performance.",
            "Wang and Xue (2017) propose to improve AMR parsing by improving the alignment and concept identification, which can also be combined with our system to improve the performance of a sequence-to-sequence model."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            2,
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "Buys and Blunsom (2017)",
                "Konstas et al. (2017)",
                "Ballesteros and Al-Onaizan (2017)*"
            ],
            [
                "Ours soft attention",
                "Ours hard attention",
                "Buys and Blunsom (2017)",
                "F"
            ],
            [
                "Konstas et al. (2017)"
            ],
            [
                "Ours soft attention",
                "Ours hard attention"
            ],
            [
                "Ours soft attention",
                "Ours hard attention",
                "Ballesteros and Al-Onaizan (2017)*",
                "F"
            ],
            null,
            [
                "Ours soft attention",
                "Ours hard attention",
                "Wang et al. (2015a)",
                "Wang and Xue (2017)",
                "P",
                "R",
                "F"
            ],
            null,
            [
                "Wang and Xue (2017)"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_4",
        "paper_id": "P18-1171",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1173table_2",
        "description": "Table 2 compares our SPIGOT method to three baselines. Pipelined semantic dependency predictions brings 0.9% absolute improvement in classification accuracy, and SPIGOT outperforms all baselines. In this task STE achieves slightly worse performance than a fixed pre-trained PIPELINE.",
        "sentences": [
            "Table 2 compares our SPIGOT method to three baselines.",
            "Pipelined semantic dependency predictions brings 0.9% absolute improvement in classification accuracy, and SPIGOT outperforms all baselines.",
            "In this task STE achieves slightly worse performance than a fixed pre-trained PIPELINE."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "SPIGOT",
                "BILSTM",
                "PIPELINE",
                "STE"
            ],
            [
                "PIPELINE",
                "BILSTM",
                "SPIGOT",
                "Accuracy (%)"
            ],
            [
                "STE",
                "PIPELINE"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_2",
        "paper_id": "P18-1173",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1173table_3",
        "description": "Table 3 compares a pipelined system to one jointly trained using SPIGOT. We consider the development set instances where both syntactic and semantic annotations are available, and partition them based on whether the two systems\u0081fsyntactic predictions agree (SAME), or not (DIFF). The second group includes sentences with much lower syntactic parsing accuracy (91.3 vs. 97.4 UAS), and SPIGOT further reduces this to 89.6. Even though these changes hurt syntactic parsing accuracy, they lead to a 1.1% absolute gain in labeled F1 for semantic parsing.",
        "sentences": [
            "Table 3 compares a pipelined system to one jointly trained using SPIGOT.",
            "We consider the development set instances where both syntactic and semantic annotations are available, and partition them based on whether the two systems\u0081fsyntactic predictions agree (SAME), or not (DIFF).",
            "The second group includes sentences with much lower syntactic parsing accuracy (91.3 vs. 97.4 UAS), and SPIGOT further reduces this to 89.6.",
            "Even though these changes hurt syntactic parsing accuracy, they lead to a 1.1% absolute gain in labeled F1 for semantic parsing."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "PIPELINE",
                "SPIGOT"
            ],
            [
                "SAME",
                "DIFF"
            ],
            [
                "PIPELINE",
                "SPIGOT",
                "UAS"
            ],
            [
                "PIPELINE",
                "SPIGOT",
                "DM"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "P18-1173",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1177table_2",
        "description": "Table 2 shows the BLEU-{3, 4} and METEOR scores of different models. Our CorefNQG outperforms the seq2seq baseline of Du et al. (2017) by a large margin. This shows that the copy mechanism, answer features and coreference resolution all aid question generation. In addition, CorefNQG outperforms both Seq2seq+Copy models significantly, whether or not they have access to the full context. This demonstrates that the coreference knowledge encoded with the gating network explicitly helps with the training and generation: it is more difficult for the neural sequence model to learn the coreference knowledge in a latent way. (See input 1 in Figure 3 for an example.). We also show in Table 2 the results of the QG models trained on the training set augmented with noisy examples with predicted answer spans. There is a consistent but acceptable drop for each model on this new training set, given the inaccuracy of predicted answer spans. We see that CorefNQG still outperforms the baseline models across all metrics.",
        "sentences": [
            "Table 2 shows the BLEU-{3, 4} and METEOR scores of different models.",
            "Our CorefNQG outperforms the seq2seq baseline of Du et al. (2017) by a large margin.",
            "This shows that the copy mechanism, answer features and coreference resolution all aid question generation.",
            "In addition, CorefNQG outperforms both Seq2seq+Copy models significantly, whether or not they have access to the full context.",
            "This demonstrates that the coreference knowledge encoded with the gating network explicitly helps with the training and generation: it is more difficult for the neural sequence model to learn the coreference knowledge in a latent way. (See input 1 in Figure 3 for an example.).",
            "We also show in Table 2 the results of the QG models trained on the training set augmented with noisy examples with predicted answer spans.",
            "There is a consistent but acceptable drop for each model on this new training set, given the inaccuracy of predicted answer spans.",
            "We see that CorefNQG still outperforms the baseline models across all metrics."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "BLEU-3",
                "BLEU-4",
                "METEOR"
            ],
            [
                "CorefNQG",
                "Baseline (Du et al. 2017) (w/o answer)",
                "Training set"
            ],
            [
                "CorefNQG"
            ],
            [
                "CorefNQG",
                "Seq2seq + copy (w/ answer)",
                "ContextNQG: Seq2seq + copy (w/ full context + answer)",
                "Training set"
            ],
            [
                "CorefNQG"
            ],
            [
                "Training set w/ noisy examples"
            ],
            [
                "Training set",
                "Training set w/ noisy examples"
            ],
            [
                "CorefNQG",
                "Baseline (Du et al. 2017) (w/o answer)"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_2",
        "paper_id": "P18-1177",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1177table_6",
        "description": "Table 6 shows the performance of a topperforming system for the SQuAD dataset (Document Reader (Chen et al., 2017)) when applied to the development and test set portions of our generated dataset. The system was trained on the training set portion of our dataset. We use the SQuAD evaluation scripts, which calculate exact match (EM) and F-1 scores.2 . Performance of the neural machine reading model is reasonable.",
        "sentences": [
            "Table 6 shows the performance of a topperforming system for the SQuAD dataset (Document Reader (Chen et al., 2017)) when applied to the development and test set portions of our generated dataset.",
            "The system was trained on the training set portion of our dataset.",
            "We use the SQuAD evaluation scripts, which calculate exact match (EM) and F-1 scores.2 .",
            "Performance of the neural machine reading model is reasonable."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "DocReader (Chen et al. 2017)"
            ],
            null,
            [
                "Exact Match",
                "F-1"
            ],
            [
                "DocReader (Chen et al. 2017)"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_6",
        "paper_id": "P18-1177",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1178table_3",
        "description": "Table 3 shows the results of our system and other state-of-the-art models on the MS-MARCO test set. We adopt the official evaluation metrics, including ROUGE-L (Lin, 2004) and BLEU-1 (Papineni et al., 2002). As we can see, for both metrics, our single model outperforms all the other competing models with an evident margin, which is extremely hard considering the near-human peformance. If we ensemble the models trained with different random seeds and hyper-parameters, the results can be further improved and outperform the ensemble model in Tan et al. (2017), especially in terms of the BLEU-1.",
        "sentences": [
            "Table 3 shows the results of our system and other state-of-the-art models on the MS-MARCO test set.",
            "We adopt the official evaluation metrics, including ROUGE-L (Lin, 2004) and BLEU-1 (Papineni et al., 2002).",
            "As we can see, for both metrics, our single model outperforms all the other competing models with an evident margin, which is extremely hard considering the near-human peformance.",
            "If we ensemble the models trained with different random seeds and hyper-parameters, the results can be further improved and outperform the ensemble model in Tan et al. (2017), especially in terms of the BLEU-1."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "Our Model",
                "FastQA Ext (Weissenborn et al. 2017)",
                "Prediction (Wang and Jiang 2016)",
                "ReasoNet (Shen et al. 2017)",
                "R-Net (Wang et al. 2017c)",
                "S-Net (Tan et al. 2017)"
            ],
            [
                "ROUGE-L",
                "BLEU-1"
            ],
            [
                "Our Model",
                "ROUGE-L",
                "BLEU-1"
            ],
            [
                "Our Model (Ensemble)",
                "ROUGE-L",
                "BLEU-1",
                "S-Net (Ensemble)"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "P18-1178",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1181table_2",
        "description": "Perplexity on the test partition is detailed in Table 2. Encouragingly, we see that the incorporation of character encodings and preceding context improves performance substantially, reducing perplexity by almost 10 points from LM to LM??. The inferior performance of LM??-C compared to LM?? demonstrates that our approach of processing context with recurrent networks with selective encoding is more effective than convolutional networks. The full model LM??+PM+RM, which learns stress and rhyme patterns simultaneously, also appears\r\nto improve the language model slightly. We present stress accuracy in Table 2. LM??+PM+RM performs competitively, and informal inspection reveals that a number of mistakes are due to dictionary errors. Table 2 details the rhyming results. The rhyme model performs very strongly at F1 > 0.90, well above both baselines. Rhyme-EM performs poorly because it operates at the word level (i.e.it ignores character/orthographic information) and hence does not generalise well to unseen words and word pairs.26.",
        "sentences": [
            "Perplexity on the test partition is detailed in Table 2.",
            "Encouragingly, we see that the incorporation of character encodings and preceding context improves performance substantially, reducing perplexity by almost 10 points from LM to LM??.",
            "The inferior performance of LM??-C compared to LM?? demonstrates that our approach of processing context with recurrent networks with selective encoding is more effective than convolutional networks.",
            "The full model LM??+PM+RM, which learns stress and rhyme patterns simultaneously, also appears\r\nto improve the language model slightly.",
            "We present stress accuracy in Table 2.",
            "LM??+PM+RM performs competitively, and informal inspection reveals that a number of mistakes are due to dictionary errors.",
            "Table 2 details the rhyming results.",
            "The rhyme model performs very strongly at F1 > 0.90, well above both baselines.",
            "Rhyme-EM performs poorly because it operates at the word level (i.e.it ignores character/orthographic information) and hence does not generalise well to unseen words and word pairs.26."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "Ppl"
            ],
            [
                "LM",
                "LM**",
                "Ppl"
            ],
            [
                "LM**-C",
                "LM**",
                "Ppl"
            ],
            [
                "LM**+PM+RM",
                "Ppl"
            ],
            [
                "Stress Acc"
            ],
            [
                "LM**+PM+RM"
            ],
            [
                "Rhyme F1"
            ],
            [
                "LM**+PM+RM",
                "Rhyme-BL",
                "Rhyme-EM"
            ],
            [
                "Rhyme-EM"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_2",
        "paper_id": "P18-1181",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1182table_1",
        "description": "Table 1 summarizes the results for all models on all metrics on the test set and Table 2 depicts a text example lexicalized by each model. The first thing to note in the results of the first table is that the baselines in the top two rows performed quite strong on this task, generating more than half of the referring expressions exactly as in the goldstandard. The method based on Castro Ferreira et al.(2016) performed statistically better than OnlyNames on all metrics due to its capability, albeit to a limited extent, to predict pronominal references (which OnlyNames obviously cannot). We reported results on the test set for NeuralREG+Seq2Seq and NeuralREG+CAtt using dropout probability 0.3 and beam size 5, and NeuralREG+HierAtt with dropout probability of 0.3 and beam size of 1 selected based on the highest accuracy on the development set. Importantly, the three NeuralREG variant models statistically outperformed the two baseline systems. They achieved BLEU scores, text and referential accuracies as well as string edit distances in the range of 79.01-79.39, 28%-30%, 73%-74% and 2.25- 2.36, respectively. This means that NeuralREG predicted 3 out of 4 references completely correct, whereas the incorrect ones needed an average of 2 post-edition operations in character level to be equal to the gold-standard. When considering the texts lexicalized with the referring expressions produced by NeuralREG, at least 28% of them are similar to the original texts. Especially noteworthy was the score on pronoun accuracy, indicating that the model was well capable of predicting when to generate a pronominal reference in our dataset. The results for the different decoding methods for NeuralREG were similar, with the NeuralREG+CAtt performing slightly better in terms of the BLEU score, text accuracy and String Edit Distance. The more complex NeuralREG+HierAtt yielded the lowest results, eventhough the differences with the other two models\r\nwere small and not even statistically significant in many of the cases.",
        "sentences": [
            "Table 1 summarizes the results for all models on all metrics on the test set and Table 2 depicts a text example lexicalized by each model.",
            "The first thing to note in the results of the first table is that the baselines in the top two rows performed quite strong on this task, generating more than half of the referring expressions exactly as in the goldstandard.",
            "The method based on Castro Ferreira et al.(2016) performed statistically better than OnlyNames on all metrics due to its capability, albeit to a limited extent, to predict pronominal references (which OnlyNames obviously cannot).",
            "We reported results on the test set for NeuralREG+Seq2Seq and NeuralREG+CAtt using dropout probability 0.3 and beam size 5, and NeuralREG+HierAtt with dropout probability of 0.3 and beam size of 1 selected based on the highest accuracy on the development set.",
            "Importantly, the three NeuralREG variant models statistically outperformed the two baseline systems.",
            "They achieved BLEU scores, text and referential accuracies as well as string edit distances in the range of 79.01-79.39, 28%-30%, 73%-74% and 2.25- 2.36, respectively.",
            "This means that NeuralREG predicted 3 out of 4 references completely correct, whereas the incorrect ones needed an average of 2 post-edition operations in character level to be equal to the gold-standard.",
            "When considering the texts lexicalized with the referring expressions produced by NeuralREG, at least 28% of them are similar to the original texts.",
            "Especially noteworthy was the score on pronoun accuracy, indicating that the model was well capable of predicting when to generate a pronominal reference in our dataset.",
            "The results for the different decoding methods for NeuralREG were similar, with the NeuralREG+CAtt performing slightly better in terms of the BLEU score, text accuracy and String Edit Distance.",
            "The more complex NeuralREG+HierAtt yielded the lowest results, eventhough the differences with the other two models\r\nwere small and not even statistically significant in many of the cases."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            1,
            1,
            2,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "OnlyNames",
                "Ferreira"
            ],
            [
                "OnlyNames",
                "Ferreira"
            ],
            [
                "NeuralREG+Seq2Seq",
                "NeuralREG+CAtt",
                "NeuralREG+HierAtt"
            ],
            [
                "NeuralREG+Seq2Seq",
                "NeuralREG+CAtt",
                "NeuralREG+HierAtt"
            ],
            [
                "NeuralREG+Seq2Seq",
                "NeuralREG+CAtt",
                "NeuralREG+HierAtt",
                "Text",
                "BLEU",
                "All References",
                "Acc.",
                "SED"
            ],
            [
                "NeuralREG+Seq2Seq",
                "NeuralREG+CAtt",
                "NeuralREG+HierAtt"
            ],
            [
                "NeuralREG+Seq2Seq",
                "NeuralREG+CAtt",
                "NeuralREG+HierAtt"
            ],
            [
                "NeuralREG+Seq2Seq",
                "NeuralREG+CAtt",
                "NeuralREG+HierAtt"
            ],
            [
                "NeuralREG+CAtt"
            ],
            [
                "NeuralREG+HierAtt"
            ]
        ],
        "n_sentence": 11.0,
        "table_id": "table_1",
        "paper_id": "P18-1182",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1182table_3",
        "description": "Table 3 summarizes the results. Inspection of the Table reveals a clear pattern: all three neural models scored higher than the baselines on all metrics, with especially NeuralREG+CAtt approaching the ratings for the original sentences, although? again ?differences between the neural models were small. Concerning the size of the triple sets, we did not find any clear pattern. To test the statistical significance of the pairwise comparisons, we used the Wilcoxon signedrank test corrected for multiple comparisons using the Bonferroni method. Different from the automatic evaluation, the results of both baselines were not statistically significant for the three metrics. In comparison with the neural models, NeuralREG+CAtt significantly outperformed the baselines in terms of fluency, whereas the other comparisons between baselines and neural models were not statistically significant. The results for the 3 different decoding methods of NeuralREG\nalso did not reveal a significant difference. Finally, the original texts were rated significantly higher than both baselines in terms of the three metrics, also than NeuralREG+Seq2Seq and NeuralREG+HierAtt in terms of fluency, and than NeuralREG+Seq2Seq in terms of clarity.",
        "sentences": [
            "Table 3 summarizes the results.",
            "Inspection of the Table reveals a clear pattern: all three neural models scored higher than the baselines on all metrics, with especially NeuralREG+CAtt approaching the ratings for the original sentences, although? again ?differences between the neural models were small.",
            "Concerning the size of the triple sets, we did not find any clear pattern.",
            "To test the statistical significance of the pairwise comparisons, we used the Wilcoxon signedrank test corrected for multiple comparisons using the Bonferroni method.",
            "Different from the automatic evaluation, the results of both baselines were not statistically significant for the three metrics.",
            "In comparison with the neural models, NeuralREG+CAtt significantly outperformed the baselines in terms of fluency, whereas the other comparisons between baselines and neural models were not statistically significant.",
            "The results for the 3 different decoding methods of NeuralREG\nalso did not reveal a significant difference.",
            "Finally, the original texts were rated significantly higher than both baselines in terms of the three metrics, also than NeuralREG+Seq2Seq and NeuralREG+HierAtt in terms of fluency, and than NeuralREG+Seq2Seq in terms of clarity."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "NeuralREG+Seq2Seq",
                "NeuralREG+CAtt",
                "NeuralREG+HierAtt",
                "Fluency",
                "Grammar",
                "Clarity"
            ],
            [
                "NeuralREG+Seq2Seq",
                "NeuralREG+CAtt",
                "NeuralREG+HierAtt"
            ],
            null,
            null,
            [
                "NeuralREG+CAtt",
                "OnlyNames",
                "Ferreira"
            ],
            [
                "NeuralREG+Seq2Seq",
                "NeuralREG+CAtt",
                "NeuralREG+HierAtt"
            ],
            [
                "Original",
                "NeuralREG+Seq2Seq",
                "NeuralREG+CAtt",
                "NeuralREG+HierAtt",
                "Fluency",
                "Clarity"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_3",
        "paper_id": "P18-1182",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1186table_1",
        "description": "Table 1 shows the Top-1, 3, 5, 10, and 50 candidates retrieval accuracy results on the Snap Captions dataset. We see that the proposed approach significantly outperforms the baselines which use fixed candidates generation method. Note that m\u2192e hash list-based methods, which retrieve as candidates the KB entities that appear in the training set of captions only, has upper performance limit at 66.9%, showing the limitance of fixed candidates generation method for unseen entities in social media posts. k-NN methods which retrieve lexical neighbors of mention (in an attempt to perform soft normalization on mentions) also do not perform well. Our proposed zeroshot approaches, however, do not fixate candidate generation, and instead compares combined contextual and lexical similarities among all 1M KB entities, achieving higher upper performance limit (Top-50 retrieval accuracy reaches 88.1%). This result indicates that the proposed zeroshot model is capable of predicting for unseen entities as well. The lexical sub-model can also be interpreted as functioning as soft neural mapping of mention to potential candidates, rather than heuristic matching to fixed candidates. In addition, when visual context is available (W+C+V), the performance generally improves over the textual models (W+C), showing that visual information can provide additional contexts for disambiguation. The modality attention module also adds performance gain by re-weighting the modalities based on their informativeness.",
        "sentences": [
            "Table 1 shows the Top-1, 3, 5, 10, and 50 candidates retrieval accuracy results on the Snap Captions dataset.",
            "We see that the proposed approach significantly outperforms the baselines which use fixed candidates generation method.",
            "Note that m\u2192e hash list-based methods, which retrieve as candidates the KB entities that appear in the training set of captions only, has upper performance limit at 66.9%, showing the limitance of fixed candidates generation method for unseen entities in social media posts.",
            "k-NN methods which retrieve lexical neighbors of mention (in an attempt to perform soft normalization on mentions) also do not perform well.",
            "Our proposed zeroshot approaches, however, do not fixate candidate generation, and instead compares combined contextual and lexical similarities among all 1M KB entities, achieving higher upper performance limit (Top-50 retrieval accuracy reaches 88.1%).",
            "This result indicates that the proposed zeroshot model is capable of predicting for unseen entities as well.",
            "The lexical sub-model can also be interpreted as functioning as soft neural mapping of mention to potential candidates, rather than heuristic matching to fixed candidates.",
            "In addition, when visual context is available (W+C+V), the performance generally improves over the textual models (W+C), showing that visual information can provide additional contexts for disambiguation.",
            "The modality attention module also adds performance gain by re-weighting the modalities based on their informativeness."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            2,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "Accuracy (%)",
                "Top-1",
                "Top-3",
                "Top-5",
                "Top-10",
                "Top-50"
            ],
            [
                "DZMNED"
            ],
            [
                "m\u2192e list"
            ],
            [
                "5-NN (lexical)",
                "10-NN (lexical)"
            ],
            [
                "Zeroshot"
            ],
            [
                "Zeroshot"
            ],
            null,
            [
                "W + C + V",
                "W + C"
            ],
            [
                "DZMNED + Modality Attention"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_1",
        "paper_id": "P18-1186",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1186table_2",
        "description": "To characterize this aspect, we provide Table 2 which shows MNED performance with varying quality of embeddings as follows: KB embeddings learned from 1M knowledge graph entities (same as in the main experiments), from 10K subset of entities (less triplets to train with in Eq.3, hence lower quality), and random embeddings (poorest) while all the other parameters are kept the same. It can be seen that the performance notably drops with lower quality of KB embeddings. When KB embeddings are replaced by random embeddings, the network effectively prevents the contextual zeroshot matching to KB entities and relies only on lexical similarities, achieving the poorest performance.",
        "sentences": [
            "To characterize this aspect, we provide Table 2 which shows MNED performance with varying quality of embeddings as follows: KB embeddings learned from 1M knowledge graph entities (same as in the main experiments), from 10K subset of entities (less triplets to train with in Eq.3, hence lower quality), and random embeddings (poorest) while all the other parameters are kept the same.",
            "It can be seen that the performance notably drops with lower quality of KB embeddings.",
            "When KB embeddings are replaced by random embeddings, the network effectively prevents the contextual zeroshot matching to KB entities and relies only on lexical similarities, achieving the poorest performance."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "Trained with 1M entities",
                "Trained with 10K entities",
                "Random embeddings"
            ],
            [
                "Trained with 1M entities",
                "Trained with 10K entities",
                "Random embeddings"
            ],
            [
                "Random embeddings"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_2",
        "paper_id": "P18-1186",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1188table_1",
        "description": "We report the performance of several variants of XNET on the validation set in Table 1. We also compare them against the LEAD baseline and POINTERNET. These two systems do not use any additional information. Interestingly, all the variants of XNET significantly outperform LEAD and POINTERNET. When the title (TITLE), image captions (CAPTION) and the first sentence (FS) are used separately as additional information, XNET performs best with TITLE as its external information. Our result demonstrates the importance of the title of the document in extractive summarization (Edmundson, 1969; Kupiec et al., 1995; Mani, 2001). The performance with TITLE and CAPTION is better than that with FS. We also tried possible combinations of TITLE, CAPTION and FS. All XNET models are superior to the ones without any external information. XNET performs best when TITLE and CAPTION are jointly used as external information (55.4%, 21.8%, 11.8%, 7.5%, and 49.2% for R1, R2, R3, R4, and RL respectively). I. It is better than the the LEAD baseline by 3.7 points on average and than POINTERNET by 1.8 points on average, indicating that external information is useful to identify the gist of the document.",
        "sentences": [
            "We report the performance of several variants of XNET on the validation set in Table 1.",
            "We also compare them against the LEAD baseline and POINTERNET.",
            "These two systems do not use any additional information.",
            "Interestingly, all the variants of XNET significantly outperform LEAD and POINTERNET.",
            "When the title (TITLE), image captions (CAPTION) and the first sentence (FS) are used separately as additional information, XNET performs best with TITLE as its external information.",
            "Our result demonstrates the importance of the title of the document in extractive summarization (Edmundson, 1969; Kupiec et al., 1995; Mani, 2001).",
            "The performance with TITLE and CAPTION is better than that with FS.",
            "We also tried possible combinations of TITLE, CAPTION and FS.",
            "All XNET models are superior to the ones without any external information.",
            "XNET performs best when TITLE and CAPTION are jointly used as external information (55.4%, 21.8%, 11.8%, 7.5%, and 49.2% for R1, R2, R3, R4, and RL respectively). I.",
            "It is better than the the LEAD baseline by 3.7 points on average and than POINTERNET by 1.8 points on average, indicating that external information is useful to identify the gist of the document."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            1,
            2,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "XNET+TITLE",
                "XNET+CAPTION",
                "XNET+FS",
                "TITLE+CAPTION",
                "TITLE+FS",
                "CAPTION+FS",
                "TITLE+CAPTION+FS"
            ],
            [
                "LEAD",
                "POINTERNET"
            ],
            null,
            [
                "XNET+TITLE",
                "XNET+CAPTION",
                "XNET+FS"
            ],
            [
                "XNET+TITLE",
                "XNET+CAPTION",
                "XNET+FS"
            ],
            [
                "XNET+TITLE"
            ],
            [
                "XNET+TITLE",
                "XNET+CAPTION",
                "XNET+FS"
            ],
            [
                "TITLE+CAPTION",
                "TITLE+FS",
                "CAPTION+FS",
                "TITLE+CAPTION+FS"
            ],
            [
                "TITLE+CAPTION",
                "TITLE+FS",
                "CAPTION+FS",
                "TITLE+CAPTION+FS"
            ],
            [
                "TITLE+CAPTION",
                "R1",
                "R2",
                "R3",
                "R4",
                "RL"
            ],
            [
                "TITLE+CAPTION",
                "TITLE+FS",
                "CAPTION+FS",
                "TITLE+CAPTION+FS",
                "LEAD",
                "POINTERNET"
            ]
        ],
        "n_sentence": 11.0,
        "table_id": "table_1",
        "paper_id": "P18-1188",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1188table_4",
        "description": "Table 4 gives the results for the test sets of NewsQA and WikiQA, and the original validation sets of SQuAD and MSMarco. Our first observation is that XNET outperforms PAIRCNN, supporting our claim that it is beneficial to read the whole document in order to make decisions, instead of only observing each candidate in isolation. Secondly, we can observe that ISF is indeed a strong baseline that outperforms XNET. This means that just reading the document using a vanilla version of XNET is not sufficient, and help is required through a coarse filtering. Indeed, we observe that XNET+ outperforms all baselines except for COMPAGGR. Our ensemble model LRXNET can ultimately surpass COMPAGGR on majority of the datasets. This consistent behavior validates the machine reading capabilities and the improved document representation with external features of our model for answer selection. Specifically, the combination of document reading and word overlap features is required to be done in a soft manner, using a classification technique. Using it as a hard constraint, with XNETTOPK, does not achieve the best result. We believe that often the ISF score is a better indicator of answer presence in the vicinity of certain candidate instead of in the candidate itself. As such, XNET+ is capable of using this feature in datasets with richer context. It is worth noting that the improvement gained by LRXNET over the state-of-the-art follows a pattern. For the SQuAD dataset, the results are comparable (less than 1%). However, the improvement for WikiQA reaches ?3% and then the gap shrinks again for NewsQA, with an improvement of ?1%. This could be explained by the fact that each sample of the SQuAD is a paragraph, compared to an article summary for WikiQA, and to an entire article for NewsQA. Hence, we further strengthen our hypothesis that a richer context is needed to achieve better results, in this case expressed as document length, but as the length of the context increases the limitation of sequential models to learn from long rich sequences arises. Interestingly, our model lags behind COMPAGGR on the MSMarco dataset. It turns out this is due to contextual independence between candidates in the MSMarco dataset, i.e., each candidate is a stand-alone paragraph in this dataset, in contrast to contextually dependent candidate sentences from a document in the NewsQA, SQuAD and WikiQA datasets. As a result, our models (XNET+ and LRXNET) with document reading abilities perform poorly. This can be observed by the fact that XNET and PAIRCNN obtain comparable results. COMPAGGR performs better because comparing each candidate independently is a better strategy.",
        "sentences": [
            "Table 4 gives the results for the test sets of NewsQA and WikiQA, and the original validation sets of SQuAD and MSMarco.",
            "Our first observation is that XNET outperforms PAIRCNN, supporting our claim that it is beneficial to read the whole document in order to make decisions, instead of only observing each candidate in isolation.",
            "Secondly, we can observe that ISF is indeed a strong baseline that outperforms XNET.",
            "This means that just reading the document using a vanilla version of XNET is not sufficient, and help is required through a coarse filtering.",
            "Indeed, we observe that XNET+ outperforms all baselines except for COMPAGGR.",
            "Our ensemble model LRXNET can ultimately surpass COMPAGGR on majority of the datasets.",
            "This consistent behavior validates the machine reading capabilities and the improved document representation with external features of our model for answer selection.",
            "Specifically, the combination of document reading and word overlap features is required to be done in a soft manner, using a classification technique.",
            "Using it as a hard constraint, with XNETTOPK, does not achieve the best result.",
            "We believe that often the ISF score is a better indicator of answer presence in the vicinity of certain candidate instead of in the candidate itself.",
            "As such, XNET+ is capable of using this feature in datasets with richer context.",
            "It is worth noting that the improvement gained by LRXNET over the state-of-the-art follows a pattern.",
            "For the SQuAD dataset, the results are comparable (less than 1%).",
            "However, the improvement for WikiQA reaches ?3% and then the gap shrinks again for NewsQA, with an improvement of ?1%.",
            "This could be explained by the fact that each sample of the SQuAD is a paragraph, compared to an article summary for WikiQA, and to an entire article for NewsQA.",
            "Hence, we further strengthen our hypothesis that a richer context is needed to achieve better results, in this case expressed as document length, but as the length of the context increases the limitation of sequential models to learn from long rich sequences arises.",
            "Interestingly, our model lags behind COMPAGGR on the MSMarco dataset.",
            "It turns out this is due to contextual independence between candidates in the MSMarco dataset, i.e., each candidate is a stand-alone paragraph in this dataset, in contrast to contextually dependent candidate sentences from a document in the NewsQA, SQuAD and WikiQA datasets.",
            "As a result, our models (XNET+ and LRXNET) with document reading abilities perform poorly.",
            "This can be observed by the fact that XNET and PAIRCNN obtain comparable results.",
            "COMPAGGR performs better because comparing each candidate independently is a better strategy."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            1,
            1,
            0,
            0,
            1,
            2,
            2,
            1,
            1,
            1,
            1,
            2,
            1,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "SQuAD",
                "WikiQA",
                "NewsQA",
                "MSMarco"
            ],
            [
                "XNET",
                "PAIRCNN"
            ],
            [
                "ISF",
                "XNET"
            ],
            [
                "XNET"
            ],
            [
                "XNET+"
            ],
            [
                "LRXNET",
                "COMPAGGR"
            ],
            null,
            null,
            [
                "XNETTOPK"
            ],
            [
                "ISF"
            ],
            [
                "XNET+"
            ],
            [
                "LRXNET"
            ],
            [
                "SQuAD",
                "LRXNET",
                "COMPAGGR"
            ],
            [
                "LRXNET",
                "COMPAGGR",
                "WikiQA",
                "NewsQA"
            ],
            [
                "WikiQA",
                "NewsQA",
                "SQuAD"
            ],
            null,
            [
                "MSMarco",
                "LRXNET",
                "COMPAGGR"
            ],
            [
                "MSMarco",
                "WikiQA",
                "NewsQA",
                "SQuAD"
            ],
            [
                "XNET+",
                "LRXNET"
            ],
            [
                "XNET+",
                "PAIRCNN"
            ],
            [
                "COMPAGGR"
            ]
        ],
        "n_sentence": 21.0,
        "table_id": "table_4",
        "paper_id": "P18-1188",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1189table_1",
        "description": "We therefore use the same experimental setup as Srivastava and Sutton (2017) (learning rate, momentum, batch size, and number of epochs) and find the same general patterns as they reported (see Table 1 and supplementary material): our model returns more coherent topics than LDA, but at the cost of worse perplexity. SAGE, by contrast, attains very high levels of sparsity, but at the cost of worse perplexity and coherence than LDA. As expected, the NVDM produces relatively low perplexity, but very poor coherence, due to its lack of constraints on \u0192\u00c6. Further experimentation revealed that the VAE framework involves a tradeoff among the scores; running for more epochs tends to result in better perplexity on held-out data, but at the cost of worse coherence. Adding regularization to encourage sparse topics has a similar effect as in SAGE, leading to worse perplexity and coherence, but it does create sparse topics. Interestingly, initializing the encoder with pretrained word2vec embeddings, and not updating them returned a model with the best internal coherence of any model we considered for IMDB and Yahoo answers, and the second-best for 20 newsgroups.",
        "sentences": [
            "We therefore use the same experimental setup as Srivastava and Sutton (2017) (learning rate, momentum, batch size, and number of epochs) and find the same general patterns as they reported (see Table 1 and supplementary material): our model returns more coherent topics than LDA, but at the cost of worse perplexity.",
            "SAGE, by contrast, attains very high levels of sparsity, but at the cost of worse perplexity and coherence than LDA.",
            "As expected, the NVDM produces relatively low perplexity, but very poor coherence, due to its lack of constraints on \u0192\u00c6.",
            "Further experimentation revealed that the VAE framework involves a tradeoff among the scores; running for more epochs tends to result in better perplexity on held-out data, but at the cost of worse coherence.",
            "Adding regularization to encourage sparse topics has a similar effect as in SAGE, leading to worse perplexity and coherence, but it does create sparse topics.",
            "Interestingly, initializing the encoder with pretrained word2vec embeddings, and not updating them returned a model with the best internal coherence of any model we considered for IMDB and Yahoo answers, and the second-best for 20 newsgroups."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "LDA"
            ],
            [
                "SAGE",
                "Sparsity"
            ],
            [
                "NVDM",
                "NPMI (int.)",
                "NPMI (ext.)"
            ],
            null,
            [
                "SCHOLAR + REG."
            ],
            [
                "SCHOLAR + W.V."
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_1",
        "paper_id": "P18-1189",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1191table_4",
        "description": "Table 4 shows span detection results on the development set. We report results for the span-based models at two threshold values tau : tau = 0.5, and tau = tau* maximizing F1. The span-based model significantly improves over the BIO model in both precision and recall, although the difference is less pronounced under IOU matching.",
        "sentences": [
            "Table 4 shows span detection results on the development set.",
            "We report results for the span-based models at two threshold values tau : tau = 0.5, and tau = tau* maximizing F1.",
            "The span-based model significantly improves over the BIO model in both precision and recall, although the difference is less pronounced under IOU matching."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Span (tau = 0.5)",
                "Span (tau = tau*)"
            ],
            [
                "Span (tau = 0.5)",
                "Span (tau = tau*)",
                "P",
                "R"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_4",
        "paper_id": "P18-1191",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1191table_5",
        "description": "Table 5 shows the results for question generation on the development set. The sequential model exact match accuracy is significantly higher, while word-level accuracy is roughly comparable, reflecting the fact that the local model learns the slot-level posteriors.",
        "sentences": [
            "Table 5 shows the results for question generation on the development set.",
            "The sequential model exact match accuracy is significantly higher, while word-level accuracy is roughly comparable, reflecting the fact that the local model learns the slot-level posteriors."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Seq.",
                "EM",
                "PM",
                "SA"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_5",
        "paper_id": "P18-1191",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1192table_4",
        "description": "Table 4 presents the results on Chinese test set. Even though we use the same parameters as for English, our model also outperforms the best reported results by 0.3% (syntax-aware) and 0.6% (syntax-agnostic) in F1 scores.",
        "sentences": [
            "Table 4 presents the results on Chinese test set.",
            "Even though we use the same parameters as for English, our model also outperforms the best reported results by 0.3% (syntax-aware) and 0.6% (syntax-agnostic) in F1 scores."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Ours",
                "Marcheggiani and Titov (2017)",
                "System (syntax-aware)",
                "F1",
                "System (syntax-agnostic)"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_4",
        "paper_id": "P18-1192",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1192table_5",
        "description": "Table 5 shows the results from our syntax-aware model with lower order argument pruning. Compared to the best previous model, our system still yields an increment in recall by more than 1%, leading to improvements in F1 score. It demonstrates that refining syntactic parser tree based candidate pruning does help in argument recognition.",
        "sentences": [
            "Table 5 shows the results from our syntax-aware model with lower order argument pruning.",
            "Compared to the best previous model, our system still yields an increment in recall by more than 1%, leading to improvements in F1 score.",
            "It demonstrates that refining syntactic parser tree based candidate pruning does help in argument recognition."
        ],
        "class_sentence": [
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "1st-order",
                "2nd-order",
                "3rd-order"
            ],
            [
                "Marcheggiani and Titov (2017)",
                "1st-order",
                "2nd-order",
                "3rd-order",
                "R",
                "F1"
            ],
            [
                "1st-order",
                "2nd-order",
                "3rd-order"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_5",
        "paper_id": "P18-1192",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1192table_9",
        "description": "Table 9 reports the performance of existing models7 in term of Sem-F1/LAS ratio on CoNLL2009 English test set. Interestingly, even though our system has significantly lower scores than others by 3.8% LAS in syntactic components, we obtain the highest results both on Sem-F1 and\nthe Sem-F1/LAS ratio, respectively. These results show that our SRL component is relatively much stronger. Moreover, the ratio comparison in Table 9 also shows that since the CoNLL-2009 shared task, most SRL works actually benefit from the enhanced syntactic component rather than the improved SRL component itself. All post-CoNLL SRL systems, either traditional or neural types, did not exceed the top systems of CoNLL-2009 shared task, (Zhao et al., 2009c) (SRL-only track using the provided predicated syntax) and (Zhao et al., 2009a) (Joint track using self-developed parser). We believe that this work for the first time reports both higher Sem-F1 and higher Sem-F1/LAS ratio since CoNLL-2009 shared task.",
        "sentences": [
            "Table 9 reports the performance of existing models7 in term of Sem-F1/LAS ratio on CoNLL2009 English test set.",
            "Interestingly, even though our system has significantly lower scores than others by 3.8% LAS in syntactic components, we obtain the highest results both on Sem-F1 and\nthe Sem-F1/LAS ratio, respectively.",
            "These results show that our SRL component is relatively much stronger.",
            "Moreover, the ratio comparison in Table 9 also shows that since the CoNLL-2009 shared task, most SRL works actually benefit from the enhanced syntactic component rather than the improved SRL component itself.",
            "All post-CoNLL SRL systems, either traditional or neural types, did not exceed the top systems of CoNLL-2009 shared task, (Zhao et al., 2009c) (SRL-only track using the provided predicated syntax) and (Zhao et al., 2009a) (Joint track using self-developed parser).",
            "We believe that this work for the first time reports both higher Sem-F1 and higher Sem-F1/LAS ratio since CoNLL-2009 shared task."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            2,
            1
        ],
        "header_mention": [
            [
                "Sem-F1 (%)",
                "Sem-F1/LAS (%)"
            ],
            [
                "Ours + CoNLL-2009 predicted",
                "Ours + Auto syntax",
                "LAS (%)",
                "Sem-F1 (%)",
                "Sem-F1/LAS (%)"
            ],
            null,
            null,
            null,
            [
                "Sem-F1 (%)",
                "Sem-F1/LAS (%)"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_9",
        "paper_id": "P18-1192",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1195table_1",
        "description": "For reference, we include in Table 1 baseline results obtained using MLE, and our implementation of MLE with entropy regularization (MLE + lambda H) (Pereyra et al., 2017), as well as the RAML approach of Norouzi et al. (2016) which corresponds to sequence-level smoothing based on the Hamming reward and sampling replacements from the full vocabulary (Seq, Hamming, V). We observe that entropy smoothing is not able to improve performance much over MLE for the model without attention, and even deteriorates for the attention model. We improve upon RAML by choosing an adequate subset of vocabulary for substitutions. We also report the performances of token-level smoothing, where the promotion of rare tokens boosted the scores in both attentive and nonattentive models. For sequence-level smoothing, choosing a taskrelevant reward with importance sampling yielded better results than plain Hamming distance. Moreover, we used the two smoothing schemes (Tok-Seq) and achieved the best results with CIDER as a reward for sequence-level smoothing combined with a token-level smoothing that promotes rare tokens improving CIDER from 93.59 (MLE) to 99.92 for the model without attention, and improving from 101.63 to 103.81 with attention.",
        "sentences": [
            "For reference, we include in Table 1 baseline results obtained using MLE, and our implementation of MLE with entropy regularization (MLE + lambda H) (Pereyra et al., 2017), as well as the RAML approach of Norouzi et al. (2016) which corresponds to sequence-level smoothing based on the Hamming reward and sampling replacements from the full vocabulary (Seq, Hamming, V).",
            "We observe that entropy smoothing is not able to improve performance much over MLE for the model without attention, and even deteriorates for the attention model.",
            "We improve upon RAML by choosing an adequate subset of vocabulary for substitutions.",
            "We also report the performances of token-level smoothing, where the promotion of rare tokens boosted the scores in both attentive and nonattentive models.",
            "For sequence-level smoothing, choosing a taskrelevant reward with importance sampling yielded better results than plain Hamming distance.",
            "Moreover, we used the two smoothing schemes (Tok-Seq) and achieved the best results with CIDER as a reward for sequence-level smoothing combined with a token-level smoothing that promotes rare tokens improving CIDER from 93.59 (MLE) to 99.92 for the model without attention, and improving from 101.63 to 103.81 with attention."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "MLE",
                "MLE + lambda H",
                "Seq",
                "Hamming",
                "V"
            ],
            [
                "MLE",
                "MLE + lambda H",
                "Without attention",
                "With attention"
            ],
            null,
            [
                "Tok",
                "Without attention",
                "With attention"
            ],
            [
                "Seq",
                "CIDER",
                "Hamming"
            ],
            [
                "Tok-Seq",
                "CIDER",
                "Without attention",
                "With attention"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_1",
        "paper_id": "P18-1195",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1196table_3",
        "description": "Table 3 shows evaluation results, where we also include two naive baselines of constant predictions: with the mean and median of the training data. For both datasets, RMSE and MAE were too sensitive to extreme errors to allow drawing safe conclusions, particularly for the scientific dataset, where both metrics were in the order of 109. MdAE can be of some use, as 50% of the errors are absolutely smaller than that. Along percentage metrics, MoG achieved the best MAPE in both datasets (18% and 54% better that the second best) and was the only model to perform better than the median baseline for the clinical data. However, it had the worst MdAPE, which means that MoG mainly reduced larger percentage errors. The d-RNN model came third and second in the clinical and scientific datasets, respectively. In the latter it achieved the best MdAPE, i.e.it was effective at reducing errors for 50% of the numbers. The combination model did not perform better than its constituents. This is possibly because MoG is the only strategy that takes into account the numerical magnitudes of the numerals.",
        "sentences": [
            "Table 3 shows evaluation results, where we also include two naive baselines of constant predictions: with the mean and median of the training data.",
            "For both datasets, RMSE and MAE were too sensitive to extreme errors to allow drawing safe conclusions, particularly for the scientific dataset, where both metrics were in the order of 109.",
            "MdAE can be of some use, as 50% of the errors are absolutely smaller than that.",
            "Along percentage metrics, MoG achieved the best MAPE in both datasets (18% and 54% better that the second best) and was the only model to perform better than the median baseline for the clinical data.",
            "However, it had the worst MdAPE, which means that MoG mainly reduced larger percentage errors.",
            "The d-RNN model came third and second in the clinical and scientific datasets, respectively.",
            "In the latter it achieved the best MdAPE, i.e.it was effective at reducing errors for 50% of the numbers.",
            "The combination model did not perform better than its constituents.",
            "This is possibly because MoG is the only strategy that takes into account the numerical magnitudes of the numerals."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "mean",
                "median"
            ],
            [
                "Clinical",
                "Scientific",
                "RMSE",
                "MAE"
            ],
            [
                "MdAE"
            ],
            [
                "MoG",
                "Clinical",
                "Scientific",
                "MAPE%"
            ],
            [
                "MoG",
                "Scientific",
                "MdAPE%"
            ],
            [
                "d-RNN",
                "Clinical",
                "Scientific"
            ],
            [
                "d-RNN",
                "Scientific",
                "MdAPE%"
            ],
            [
                "combination"
            ],
            [
                "MoG"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_3",
        "paper_id": "P18-1196",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1197table_1",
        "description": "Table 1 summarizes our results. According to (Marelli et al., 2014), we compute three evaluation metrics: Pearson r, Spearman rho and Mean Squared Error (MSE). We compare our attention models against the original Tree-LSTM (Tai et al., 2015), instantiated on both constituency trees and dependency trees. We also compare earlier baselines with our models, and the best results are in bold. Since Tree-LSTM is a generalization of Linear LSTM, we also implemented our attention models on Linear Bidirectional LSTM (BiLSTM). All results are average of 5 runs. It is witnessed that the Progressive-Attn mechanism combined with Constituency Tree-LSTM is overall the strongest contender, but PA failed to yield any performance gain on Dependency Tree-LSTM in either dataset.",
        "sentences": [
            "Table 1 summarizes our results.",
            "According to (Marelli et al., 2014), we compute three evaluation metrics: Pearson r, Spearman rho and Mean Squared Error (MSE).",
            "We compare our attention models against the original Tree-LSTM (Tai et al., 2015), instantiated on both constituency trees and dependency trees.",
            "We also compare earlier baselines with our models, and the best results are in bold.",
            "Since Tree-LSTM is a generalization of Linear LSTM, we also implemented our attention models on Linear Bidirectional LSTM (BiLSTM).",
            "All results are average of 5 runs.",
            "It is witnessed that the Progressive-Attn mechanism combined with Constituency Tree-LSTM is overall the strongest contender, but PA failed to yield any performance gain on Dependency Tree-LSTM in either dataset."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "Pearson r",
                "Spearman rho",
                "MSE"
            ],
            [
                "Dependency Tree-LSTM (2015)",
                "Constituency Tree-LSTM (2015)",
                "Decomp-Attn (Dependency)",
                "Progressive-Attn (Dependency)",
                "Decomp-Attn (Constituency)",
                "Progressive-Attn (Constituency)",
                "Decomp-Attn (Linear)",
                "Progressive-Attn (Linear)"
            ],
            [
                "Illinois-LH (2014)",
                "UNAL-NLP (2014)",
                "Meaning factory (2014)",
                "ECNU (2014)",
                "ParagramPhrase (2015)",
                "Projection (2015)",
                "GloVe (2015)",
                "PSL (2015)",
                "ParagramPhrase-XXL (2015)"
            ],
            [
                "Linear Bi-LSTM"
            ],
            null,
            [
                "Progressive-Attn (Constituency)",
                "Dependency Tree-LSTM",
                "Progressive-Attn (Dependency)",
                "SICK",
                "MSRpar"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_1",
        "paper_id": "P18-1197",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1197table_2",
        "description": "Table 2 summarizes our results where best results are highlighted in bold within each category. It should be noted that Quora is a new dataset and we have done our analysis on only 50,000 samples. Therefore, to the best of our knowledge, there is no published baseline result yet. For this task, we considered four standard evaluation metrics: Accuracy, F1-score, Precision and Recall. The Progressive-Attn + Constituency Tree-LSTM model still exhibits the best performance by a small margin, but the Progressive-Attn mechanism works surprisingly well on the linear bi-LSTM.",
        "sentences": [
            "Table 2 summarizes our results where best results are highlighted in bold within each category.",
            "It should be noted that Quora is a new dataset and we have done our analysis on only 50,000 samples.",
            "Therefore, to the best of our knowledge, there is no published baseline result yet.",
            "For this task, we considered four standard evaluation metrics: Accuracy, F1-score, Precision and Recall.",
            "The Progressive-Attn + Constituency Tree-LSTM model still exhibits the best performance by a small margin, but the Progressive-Attn mechanism works surprisingly well on the linear bi-LSTM."
        ],
        "class_sentence": [
            1,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "Accuracy",
                "F-1 score (class=1)",
                "Precision (class=1)",
                "Recall (class=1)"
            ],
            [
                "Progressive-Attn (Constituency)",
                "Progressive-Attn (Linear)"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_2",
        "paper_id": "P18-1197",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1201table_6",
        "description": "We further evaluated the performance of our transfer approach on similar and distinct unseen types. The 33 subtypes defined in ACE fall within 8 coarse-grained main types, such as Life and Justice. Each subtype belongs to one main type. Subtypes that belong to the same main type tend to have similar structures. For example, TrialHearing and Charge-Indict have the same set of argument roles. For training our transfer model, we selected 4 subtypes of Justice: Arrest-Jail, Convict, Charge-Indict, Execute. For testing, we selected 3 other subtypes of Justice: Sentence, Appeal, Release-Parole. Additionally, we selected one subtype from each of the other seven main types for comparison. Table 6 shows that, when testing on a new unseen type, the more similar it is to the seen types, the better performance is achieved.",
        "sentences": [
            "We further evaluated the performance of our transfer approach on similar and distinct unseen types.",
            "The 33 subtypes defined in ACE fall within 8 coarse-grained main types, such as Life and Justice.",
            "Each subtype belongs to one main type.",
            "Subtypes that belong to the same main type tend to have similar structures.",
            "For example, TrialHearing and Charge-Indict have the same set of argument roles.",
            "For training our transfer model, we selected 4 subtypes of Justice: Arrest-Jail, Convict, Charge-Indict, Execute.",
            "For testing, we selected 3 other subtypes of Justice: Sentence, Appeal, Release-Parole.",
            "Additionally, we selected one subtype from each of the other seven main types for comparison.",
            "Table 6 shows that, when testing on a new unseen type, the more similar it is to the seen types, the better performance is achieved."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "Justice",
                "Conflict",
                "Transaction",
                "Business",
                "Movement",
                "Personnel",
                "Contact",
                "Life"
            ],
            [
                "Subtype"
            ],
            [
                "Subtype",
                "Type"
            ],
            [
                "Subtype",
                "Type"
            ],
            null,
            [
                "Justice",
                "Sentence",
                "Appeal",
                "Release-Parole"
            ],
            [
                "Subtype",
                "Type"
            ],
            [
                "Subtype",
                "Type"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_6",
        "paper_id": "P18-1201",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1201table_7",
        "description": "We first identified the candidate triggers and arguments, then mapped each of these to the target event ontology. We evaluated our model on their extracting of event mentions which were classified into 23 testing ACE types. Table 7 shows the performance. To further demonstrate the effectiveness of zero-shot learning in our framework and its impact in saving human annotation effort, we used the supervised LSTM approach for comparison. The training data of LSTM contained 3,464 sentences with 905 annotated event mentions for the 23 unseen event types. We divided these event annotations into 10 subsets and successively added one subset at a time (10% of annotations) into the training data of LSTM. Figure 4 shows the LSTM learning curve. By contrast, without any annotated mentions on the 23 unseen test event types in its training set, our transfer learning approach achieved performance comparable to that of the LSTM, which was trained on 3,000 sentences5 with 500 annotated event mentions.",
        "sentences": [
            "We first identified the candidate triggers and arguments, then mapped each of these to the target event ontology.",
            "We evaluated our model on their extracting of event mentions which were classified into 23 testing ACE types.",
            "Table 7 shows the performance.",
            "To further demonstrate the effectiveness of zero-shot learning in our framework and its impact in saving human annotation effort, we used the supervised LSTM approach for comparison.",
            "The training data of LSTM contained 3,464 sentences with 905 annotated event mentions for the 23 unseen event types.",
            "We divided these event annotations into 10 subsets and successively added one subset at a time (10% of annotations) into the training data of LSTM.",
            "Figure 4 shows the LSTM learning curve.",
            "By contrast, without any annotated mentions on the 23 unseen test event types in its training set, our transfer learning approach achieved performance comparable to that of the LSTM, which was trained on 3,000 sentences5 with 500 annotated event mentions."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            2,
            2,
            0,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "Supervised LSTM"
            ],
            [
                "Supervised LSTM"
            ],
            [
                "Supervised LSTM"
            ],
            null,
            [
                "Transfer",
                "Supervised LSTM"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_7",
        "paper_id": "P18-1201",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1202table_2",
        "description": "The overall comparison results with the baselines are shown in Table 2 with average F1 scores and standard deviations over three random splits. Clearly, the results for aspect terms (AS) transfer are much lower than opinion terms (OP) transfer, which indicate that the aspect terms are usually quite different across domains, whereas the opinion terms could be more common and similar. Hence the ability to adapt the aspect extraction from the source domain to the target domain becomes more crucial. On this behalf, our proposed model shows clear advantage over other baselines for this more difficult transfer problem. Specifically, we achieve 6.77%, 5.88%, 10.55% improvement over the bestperforming baselines for aspect extraction in R\u0081\u00a8L,L\u0081\u00a8D and D\u0081\u00a8L,  respectively. By comparing with RNCRF and RNGRU, we show that the structural correspondence network is indeed effective when integrated into RNN.",
        "sentences": [
            "The overall comparison results with the baselines are shown in Table 2 with average F1 scores and standard deviations over three random splits.",
            "Clearly, the results for aspect terms (AS) transfer are much lower than opinion terms (OP) transfer, which indicate that the aspect terms are usually quite different across domains, whereas the opinion terms could be more common and similar.",
            "Hence the ability to adapt the aspect extraction from the source domain to the target domain becomes more crucial.",
            "On this behalf, our proposed model shows clear advantage over other baselines for this more difficult transfer problem.",
            "Specifically, we achieve 6.77%, 5.88%, 10.55% improvement over the bestperforming baselines for aspect extraction in R\u0081\u00a8L,L\u0081\u00a8D and D\u0081\u00a8L,  respectively.",
            "By comparing with RNCRF and RNGRU, we show that the structural correspondence network is indeed effective when integrated into RNN."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "CrossCRF",
                "RAP",
                "Hier-Joint",
                "RNCRF",
                "RNGRU",
                "RNSCN-CRF",
                "RNSCN-GRU",
                "RNSCN+-GRU"
            ],
            [
                "AS",
                "OP"
            ],
            null,
            [
                "RNSCN-CRF",
                "RNSCN-GRU",
                "RNSCN+-GRU"
            ],
            [
                "RNSCN-CRF",
                "RNSCN-GRU",
                "RNSCN+-GRU",
                "R\u0081\u00a8L",
                "L\u0081\u00a8D",
                "D\u0081\u00a8L",
                "AS"
            ],
            [
                "RNCRF",
                "RNGRU"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_2",
        "paper_id": "P18-1202",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1205table_4",
        "description": "The results are reported in Table 4 for the best performing generative and ranking models, in both the No Persona and Self Persona categories, 100 dialogues each. We also evaluate the scores of human performance by replacing the chatbot with a human (another Turker). This effectively gives us upper bound scores which we can aim for with our models. Finally, and importantly, we compare our models trained on PERSONA-CHAT with chit-chat models trained with the Twitter and OpenSubtitles datasets (2009 and 2018 versions) instead, following Vinyals and Le (2015). Example chats from a few of the models are shown in the Appendix in Tables 7, 8, 9, 10, 11 and 12. Firstly, we see a difference in fluency, engagingness and consistency between all PERSONACHAT models and the models trained on OpenSubtitles and Twitter. PERSONA-CHAT is a resource that is particularly strong at providing training data for the beginning of conversations, when the two speakers do not know each other, focusing on asking and answering questions, in contrast to other resources.",
        "sentences": [
            "The results are reported in Table 4 for the best performing generative and ranking models, in both the No Persona and Self Persona categories, 100 dialogues each.",
            "We also evaluate the scores of human performance by replacing the chatbot with a human (another Turker).",
            "This effectively gives us upper bound scores which we can aim for with our models.",
            "Finally, and importantly, we compare our models trained on PERSONA-CHAT with chit-chat models trained with the Twitter and OpenSubtitles datasets (2009 and 2018 versions) instead, following Vinyals and Le (2015).",
            "Example chats from a few of the models are shown in the Appendix in Tables 7, 8, 9, 10, 11 and 12.",
            "Firstly, we see a difference in fluency, engagingness and consistency between all PERSONACHAT models and the models trained on OpenSubtitles and Twitter.",
            "PERSONA-CHAT is a resource that is particularly strong at providing training data for the beginning of conversations, when the two speakers do not know each other, focusing on asking and answering questions, in contrast to other resources."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            0,
            1,
            2
        ],
        "header_mention": [
            [
                "Generative PersonaChat Models",
                "Ranking PersonaChat Models",
                "Profile"
            ],
            [
                "Human"
            ],
            null,
            [
                "KV Memory",
                "KV Profile Memory",
                "Twitter LM",
                "OpenSubtitles 2018 LM",
                "OpenSubtitles 2009 LM",
                "OpenSubtitles 2009 KV Memory"
            ],
            null,
            [
                "Fluency",
                "Engagingness",
                "Consistency",
                "Seq2Seq",
                "Profile Memory",
                "KV Memory",
                "KV Profile Memory",
                "Twitter LM",
                "OpenSubtitles 2018 LM",
                "OpenSubtitles 2009 LM",
                "OpenSubtitles 2009 KV Memory"
            ],
            [
                "Generative PersonaChat Models",
                "Ranking PersonaChat Models"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_4",
        "paper_id": "P18-1205",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1209table_3",
        "description": "Table 3 illustrates the impact of Low-rank Multimodal Fusion on the training and testing speeds compared with TFN model. Here we set rank to be 4 since it can generally achieve fairly competent performance. Based on these results, performing a low-rank multimodal fusion with modality-specific low-rank factors significantly reduces the amount of time needed for training and testing the model. On an NVIDIA Quadro K4200 GPU, LMF trains with an average frequency of 1134.82 IPS (data point inferences per second) while the TFN model trains at an average of 340.74 IPS.",
        "sentences": [
            "Table 3 illustrates the impact of Low-rank Multimodal Fusion on the training and testing speeds compared with TFN model.",
            "Here we set rank to be 4 since it can generally achieve fairly competent performance.",
            "Based on these results, performing a low-rank multimodal fusion with modality-specific low-rank factors significantly reduces the amount of time needed for training and testing the model.",
            "On an NVIDIA Quadro K4200 GPU, LMF trains with an average frequency of 1134.82 IPS (data point inferences per second) while the TFN model trains at an average of 340.74 IPS."
        ],
        "class_sentence": [
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            [
                "LMF",
                "TFN",
                "Training Speed (IPS)",
                "Testing Speed (IPS)"
            ],
            null,
            null,
            [
                "LMF",
                "Training Speed (IPS)",
                "TFN"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "P18-1209",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1211table_1",
        "description": "Table 1 shows the performance of variants of our approach for the task. Our baselines include previous approaches for the same task: DSSM is a deep-learning based approach, which maps the context and ending to the same space, and is the best-performing method in Mostafazadeh et al.(2016). GenSim and N-gram return the ending that is more similar to the context based on word2vec embeddings (Mikolov et al., 2013) and n-grams, respectively. Narrative-Chains computes the probability of each alternative based on eventchains, following the approach of Chambers and Jurafsky (2008). We note that our method improves on the previous best unsupervised methods for the task. This is quite surprising, since our Sequential-CG model in this case is trained on bag-of-lemma representations, and only needs sentence segmentation, tokenization and lemmatization for preprocessing. On the other hand, approaches such as Narrative-Chains require parsing and eventrecognition, while approaches such as GenSim require learning word embeddings on large text corpora for training. Further, we note that predicting the ending without normalizing for the probability of the words in the ending results in significantly weaker performance, as expected. We train another variant of Sequential-CG with the sentence-level sentiment annotation (from Stanford CoreNLP) also added as a feature. This does not improve performance, consistent with findings in Mostafazadeh et al. (2016). We also experiment with a variant where we perform Brown clustering (Brown et al., 1992) of words in the unlabeled stories (K = 500 clusters), and include cluster-annotations as features for training the method. Doing this explicitly incorporates lexical similarity into the model, leading to a small improvement in performance. Finally, a mixture model consisting of the Sequential-CG and a unigram language model leads to a further improvement in performance.",
        "sentences": [
            "Table 1 shows the performance of variants of our approach for the task.",
            "Our baselines include previous approaches for the same task: DSSM is a deep-learning based approach, which maps the context and ending to the same space, and is the best-performing method in Mostafazadeh et al.(2016).",
            "GenSim and N-gram return the ending that is more similar to the context based on word2vec embeddings (Mikolov et al., 2013) and n-grams, respectively.",
            "Narrative-Chains computes the probability of each alternative based on eventchains, following the approach of Chambers and Jurafsky (2008).",
            "We note that our method improves on the previous best unsupervised methods for the task.",
            "This is quite surprising, since our Sequential-CG model in this case is trained on bag-of-lemma representations, and only needs sentence segmentation, tokenization and lemmatization for preprocessing.",
            "On the other hand, approaches such as Narrative-Chains require parsing and eventrecognition, while approaches such as GenSim require learning word embeddings on large text corpora for training.",
            "Further, we note that predicting the ending without normalizing for the probability of the words in the ending results in significantly weaker performance, as expected.",
            "We train another variant of Sequential-CG with the sentence-level sentiment annotation (from Stanford CoreNLP) also added as a feature.",
            "This does not improve performance, consistent with findings in Mostafazadeh et al. (2016).",
            "We also experiment with a variant where we perform Brown clustering (Brown et al., 1992) of words in the unlabeled stories (K = 500 clusters), and include cluster-annotations as features for training the method.",
            "Doing this explicitly incorporates lexical similarity into the model, leading to a small improvement in performance.",
            "Finally, a mixture model consisting of the Sequential-CG and a unigram language model leads to a further improvement in performance."
        ],
        "class_sentence": [
            1,
            2,
            2,
            2,
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "DSSM"
            ],
            [
                "GenSim",
                "N-grams"
            ],
            [
                "Narrative-Chain(Stories)"
            ],
            [
                "Our Method variants"
            ],
            [
                "Sequential CG + Unigram Mixture",
                "Sequential CG + Brown clustering",
                "Sequential CG + Sentiment",
                "Sequential CG"
            ],
            [
                "Narrative-Chain(Stories)",
                "GenSim"
            ],
            [
                "Sequential CG (unnormalized)"
            ],
            [
                "Sequential CG + Sentiment"
            ],
            [
                "Sequential CG + Sentiment"
            ],
            [
                "Sequential CG + Brown clustering"
            ],
            [
                "Sequential CG + Brown clustering"
            ],
            [
                "Sequential CG + Unigram Mixture"
            ]
        ],
        "n_sentence": 13.0,
        "table_id": "table_1",
        "paper_id": "P18-1211",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1220table_4",
        "description": "The results from Table 4 reveal that average attention combination performs best among all the decoding strategies on RDD newspapers and TCP books datasets. It reduces the CER of single input decoding by 41.5% for OCR\u0081fd lines in RDD newspapers and 9.76% for TCP books. The comparison between two hierarchical attention combination strategies shows that averaging evidence from each input works better than a weighted summation mechanism. Flat attention combination, which merges all the inputs into a long sequence when computing the strength of each encoder hidden state, obtains the worst performance in terms of both CER and WER.",
        "sentences": [
            "The results from Table 4 reveal that average attention combination performs best among all the decoding strategies on RDD newspapers and TCP books datasets.",
            "It reduces the CER of single input decoding by 41.5% for OCR\u0081fd lines in RDD newspapers and 9.76% for TCP books.",
            "The comparison between two hierarchical attention combination strategies shows that averaging evidence from each input works better than a weighted summation mechanism.",
            "Flat attention combination, which merges all the inputs into a long sequence when computing the strength of each encoder hidden state, obtains the worst performance in terms of both CER and WER."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "Average",
                "RDD Newspapers",
                "TCP Books"
            ],
            [
                "CER",
                "Single",
                "RDD Newspapers",
                "TCP Books"
            ],
            [
                "Average",
                "Weighted"
            ],
            [
                "Flat",
                "CER",
                "WER"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_4",
        "paper_id": "P18-1220",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1220table_5",
        "description": "Table 5 presents the results for our model trained in different training settings as well as the baseline language model reranking (LMR) and majority vote methods. Multiple input decoding performs better than single input decoding for every training setting, and the model trained in supervised mode with multi-input decoding achieves the best performance. The majority vote baseline, which works only on more than two inputs, performs worst on both the TCP books and RDD newspapers. Our proposed unsupervised framework Seq2Seq-Noisy and Seq2SeqBoots achieves performance comparable with the supervised model via multi-input decoding on the RDD newspaper dataset. The performance of Seq2Seq-Noisy is worse on the TCP Books than the RDD newspapers, since those old books contain the character long s 6, which is formerly used where s occurred in the middle or at the beginning of a word. These characters are recognized as f in all the witnesses because of similar shape. Thus, the model trained on noisy data are unable to correct them into s. Nonetheless, by removing the factor of long s, i.e., replacing the long s in the ground truth with f, Seq2Seq-Noisy could achieve a CER of 0.062 for single-input decoding and 0.058 for multi-input decoding on the TCP books. Both Seq2Seq-Syn and Seq2Seq-Boots work better on the RDD newspapers than the TCP books dataset.",
        "sentences": [
            "Table 5 presents the results for our model trained in different training settings as well as the baseline language model reranking (LMR) and majority vote methods.",
            "Multiple input decoding performs better than single input decoding for every training setting, and the model trained in supervised mode with multi-input decoding achieves the best performance.",
            "The majority vote baseline, which works only on more than two inputs, performs worst on both the TCP books and RDD newspapers.",
            "Our proposed unsupervised framework Seq2Seq-Noisy and Seq2SeqBoots achieves performance comparable with the supervised model via multi-input decoding on the RDD newspaper dataset.",
            "The performance of Seq2Seq-Noisy is worse on the TCP Books than the RDD newspapers, since those old books contain the character long s 6, which is formerly used where s occurred in the middle or at the beginning of a word.",
            "These characters are recognized as f in all the witnesses because of similar shape.",
            "Thus, the model trained on noisy data are unable to correct them into s.",
            "Nonetheless, by removing the factor of long s, i.e., replacing the long s in the ground truth with f, Seq2Seq-Noisy could achieve a CER of 0.062 for single-input decoding and 0.058 for multi-input decoding on the TCP books.",
            "Both Seq2Seq-Syn and Seq2Seq-Boots work better on the RDD newspapers than the TCP books dataset."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "Seq2Seq-Super",
                "Seq2Seq-Noisy",
                "Seq2Seq-Syn",
                "Seq2Seq-Boots",
                "LMR",
                "Majority Vote"
            ],
            [
                "Multi",
                "Single",
                "Seq2Seq-Super"
            ],
            [
                "Majority Vote",
                "RDD Newspapers",
                "TCP Books"
            ],
            [
                "Seq2Seq-Noisy",
                "Seq2Seq-Boots",
                "RDD Newspapers",
                "Seq2Seq-Super"
            ],
            [
                "Seq2Seq-Noisy",
                "RDD Newspapers",
                "TCP Books"
            ],
            null,
            [
                "Seq2Seq-Noisy"
            ],
            [
                "Seq2Seq-Noisy",
                "CER",
                "Single",
                "Multi",
                "TCP Books"
            ],
            [
                "Seq2Seq-Syn",
                "Seq2Seq-Boots",
                "RDD Newspapers",
                "TCP Books"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_5",
        "paper_id": "P18-1220",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1221table_1",
        "description": "We compare our model with the baselines using perplexity metric?lower perplexity means the better prediction. Table 1 summarizes the result. The 3rd row shows that adding type as a simple feature does not guarantee a significant performance improvement while our proposed method significantly outperforms both baselines and achieves 52.2% improvement with respect to baseline in terms of perplexity.",
        "sentences": [
            "We compare our model with the baselines using perplexity metric?lower perplexity means the better prediction.",
            "Table 1 summarizes the result.",
            "The 3rd row shows that adding type as a simple feature does not guarantee a significant performance improvement while our proposed method significantly outperforms both baselines and achieves 52.2% improvement with respect to baseline in terms of perplexity."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "Perplexity"
            ],
            null,
            [
                "Perplexity",
                "AWD LSTM with type feature",
                "our model",
                "AWD LSTM",
                "AWD LSTM type model"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_1",
        "paper_id": "P18-1221",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1221table_2",
        "description": "Table 2 shows that adding type as simple features does not guarantee a significant performance improvement while our proposed method significantly outperforms both forward and backward LSTM baselines. Our approach with backward LSTM has 40.3% better perplexity than original backward LSTM and forward has 63.14% lower (i.e., better) perplexity than original forward LSTM. With respect to SLP-Core performance, our model is 22.06% better in perplexity.",
        "sentences": [
            "Table 2 shows that adding type as simple features does not guarantee a significant performance improvement while our proposed method significantly outperforms both forward and backward LSTM baselines.",
            "Our approach with backward LSTM has 40.3% better perplexity than original backward LSTM and forward has 63.14% lower (i.e., better) perplexity than original forward LSTM.",
            "With respect to SLP-Core performance, our model is 22.06% better in perplexity."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "fLSTM",
                "bLSTM",
                "our model (fLSTM)",
                "our model (bLSTM)"
            ],
            [
                "our model (bLSTM)",
                "Perplexity",
                "bLSTM",
                "our model (fLSTM)",
                "fLSTM"
            ],
            [
                "SLP-Core",
                "Perplexity",
                "our model (bLSTM)"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_2",
        "paper_id": "P18-1221",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1222table_7",
        "description": "Table 7 analyzes the impact of newcomer friendliness. Opposite from what is done in Section 5.2.2,we only evaluate on testing examples where at least a ground-truth paper is a newcomer.  Please note that newcomer unfriendly approaches do not necessarily get zero scores. The table shows that newcomer friendly approaches are superior to unfriendly ones. Note that, like Table 5, this table is also based on controlled experiments and not intended for comparing approaches.",
        "sentences": [
            "Table 7 analyzes the impact of newcomer friendliness. Opposite from what is done in Section 5.2.2,we only evaluate on testing examples where at least a ground-truth paper is a newcomer.",
            " Please note that newcomer unfriendly approaches do not necessarily get zero scores.",
            "The table shows that newcomer friendly approaches are superior to unfriendly ones.",
            "Note that, like Table 5, this table is also based on controlled experiments and not intended for comparing approaches."
        ],
        "class_sentence": [
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            [
                "Newcomer Friendly"
            ],
            null,
            [
                "d2v-nc",
                "d2v-cac",
                "h-d2v",
                "w2v (I4O)",
                "NPM"
            ],
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_7",
        "paper_id": "P18-1222",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1228table_2",
        "description": "Table 2 summarizes the performance. Surprisingly, SEMAXIS - the simplest approach - outperforms others on both Standard English and Twitter datasets across all measures.",
        "sentences": [
            "Table 2 summarizes the performance.",
            "Surprisingly, SEMAXIS - the simplest approach - outperforms others on both Standard English and Twitter datasets across all measures."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "SEMAXIS",
                "Standard English",
                "Twitter",
                "AUC",
                "Ternary F1",
                "Tau"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_2",
        "paper_id": "P18-1228",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1229table_1",
        "description": "Table 1 shows the results of the first experiment. HypeNET (Shwartz et al., 2016) uses additional surface features described in Section 2.2. HypeNET+MST extends HypeNET by first constructing a hypernym graph using HypeNET output as weights of edges and then finding the MST (Chu, 1965) of this graph. TaxoRL (RE) denotes our RL approach which assumes a common Root Embedding, and TaxoRL (NR) denotes its variant that allows a New Root to be added. We can see that TAXI has the lowest F1 a while HypeNET performs the worst in F1 e. Both TAXI and HypeNET F1 a and F1 e are lower than 30. HypeNET+MST outperforms HypeNET in both F1 a and F1 e, because it considers the global taxonomy structure, although the two phases are performed independently. TaxoRL (RE) uses exactly the same input as HypeNET+MST and yet achieves significantly better performance, which demonstrates the superiority of combining the phases of hypernymy detection and hypernymy organization. Also, we found that presuming a shared root embedding for all taxonomies can be inappropriate if they are from different domains, which explains why TaxoRL (NR) performs better than TaxoRL (RE). Finally, after we add the frequency and generality features (TaxoRL (NR) + FG), our approach outperforms Bansal et al. (2014), even if a much smaller corpus is used.",
        "sentences": [
            "Table 1 shows the results of the first experiment.",
            "HypeNET (Shwartz et al., 2016) uses additional surface features described in Section 2.2.",
            "HypeNET+MST extends HypeNET by first constructing a hypernym graph using HypeNET output as weights of edges and then finding the MST (Chu, 1965) of this graph.",
            "TaxoRL (RE) denotes our RL approach which assumes a common Root Embedding, and TaxoRL (NR) denotes its variant that allows a New Root to be added.",
            "We can see that TAXI has the lowest F1 a while HypeNET performs the worst in F1 e.",
            "Both TAXI and HypeNET F1 a and F1 e are lower than 30.",
            "HypeNET+MST outperforms HypeNET in both F1 a and F1 e, because it considers the global taxonomy structure, although the two phases are performed independently.",
            "TaxoRL (RE) uses exactly the same input as HypeNET+MST and yet achieves significantly better performance, which demonstrates the superiority of combining the phases of hypernymy detection and hypernymy organization.",
            "Also, we found that presuming a shared root embedding for all taxonomies can be inappropriate if they are from different domains, which explains why TaxoRL (NR) performs better than TaxoRL (RE).",
            "Finally, after we add the frequency and generality features (TaxoRL (NR) + FG), our approach outperforms Bansal et al. (2014), even if a much smaller corpus is used."
        ],
        "class_sentence": [
            1,
            2,
            2,
            2,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "HypeNET"
            ],
            [
                "HypeNET+MST"
            ],
            [
                "TaxoRL (RE)",
                "TaxoRL (NR)"
            ],
            [
                "HypeNET",
                "F1 e",
                "TAXI",
                "F1 a"
            ],
            [
                "HypeNET",
                "F1 e",
                "TAXI",
                "F1 a"
            ],
            [
                "HypeNET+MST",
                "HypeNET",
                "F1 a",
                "F1 e"
            ],
            [
                "TaxoRL (RE)",
                "HypeNET+MST"
            ],
            [
                "TaxoRL (RE)",
                "TaxoRL (NR)"
            ],
            [
                "TaxoRL (NR) + FG"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_1",
        "paper_id": "P18-1229",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1232table_2",
        "description": "Table 2 shows the experimental results of lexicon term sentiment classification. Our DSE method can achieve competitive performance among all the methods. Compared with SSWE, our DSE is still competitive because both of them consider the sentiment information in the embeddings. Our DSE model outperforms other methods which do not consider sentiments such as Yang, EmbeddingCat and EmbeddingAll. Note that the advantage of domain-sensitive embeddings would be insufficient for this task because the sentiment lexicons are not domain-specific.",
        "sentences": [
            "Table 2 shows the experimental results of lexicon term sentiment classification.",
            "Our DSE method can achieve competitive performance among all the methods.",
            "Compared with SSWE, our DSE is still competitive because both of them consider the sentiment information in the embeddings.",
            "Our DSE model outperforms other methods which do not consider sentiments such as Yang, EmbeddingCat and EmbeddingAll.",
            "Note that the advantage of domain-sensitive embeddings would be insufficient for this task because the sentiment lexicons are not domain-specific."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "DSE"
            ],
            [
                "SSWE",
                "DSE"
            ],
            [
                "DSE",
                "Yang",
                "EmbeddingCat",
                "EmbeddingAll"
            ],
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_2",
        "paper_id": "P18-1232",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1239table_2",
        "description": "Table 2 shows the results reported by Kiela et al. (2015) on the BERGSMA500 dataset, along with results using our image crawl method (Section 3.2) on BERGSMA500\u0081fs vocabulary. On all five languages, our dataset performs better than that of Kiela et al. (2015). We attribute this to improvements in image search since they collected images. We additionally note that in the BERGSMA500 vocabularies, approximately 11% of the translation pairs are string-identical, like film ? film. In all subsequent experiments, we remove trivial translation pairs like this. We also evaluate the identical model on our full data set, which contains 8,500 words, covering all parts of speech and the full range of concreteness ratings. The top-1 accuracy of the model is 23% on our more realistic and challenging data set, versus\r\n68% on the easier concrete nouns set.",
        "sentences": [
            "Table 2 shows the results reported by Kiela et al. (2015) on the BERGSMA500 dataset, along with results using our image crawl method (Section 3.2) on BERGSMA500\u0081fs vocabulary.",
            "On all five languages, our dataset performs better than that of Kiela et al. (2015).",
            "We attribute this to improvements in image search since they collected images.",
            "We additionally note that in the BERGSMA500 vocabularies, approximately 11% of the translation pairs are string-identical, like film ? film.",
            "In all subsequent experiments, we remove trivial translation pairs like this.",
            "We also evaluate the identical model on our full data set, which contains 8,500 words, covering all parts of speech and the full range of concreteness ratings.",
            "The top-1 accuracy of the model is 23% on our more realistic and challenging data set, versus\r\n68% on the easier concrete nouns set."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "BERGSMA500 Kiela et al. (2015)",
                "BERGSMA500 (ours)"
            ],
            [
                "BERGSMA500 (ours)"
            ],
            null,
            [
                "BERGSMA500 (ours)"
            ],
            [
                "BERGSMA500 (ours)"
            ],
            [
                "all (ours)"
            ],
            [
                "all (ours)",
                "Top 1",
                "BERGSMA500 (ours)"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_2",
        "paper_id": "P18-1239",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1246table_2",
        "description": "Table 2 contains the results of this task for the large treebanks. Because Dozat et al. (2017) won the challenge for the majority of the languages, we first compare our results with the performance of their system. Our model outperforms Dozat et al. (2017) in 32 of the 54 treebanks with 13 ties. These ties correspond mostly to languages where XPOS tagging anyhow obtains accuracies above 99%. Our model tends to produce better results, especially for morphologically rich languages (e.g. Slaviclanguages), whereas Dozat et al (2017) showed higher performance in 10 languages in particular English, Greek, Brazilian Portuguese and Estonian.",
        "sentences": [
            "Table 2 contains the results of this task for the large treebanks.",
            "Because Dozat et al. (2017) won the challenge for the majority of the languages, we first compare our results with the performance of their system.",
            "Our model outperforms Dozat et al. (2017) in 32 of the 54 treebanks with 13 ties.",
            "These ties correspond mostly to languages where XPOS tagging anyhow obtains accuracies above 99%.",
            "Our model tends to produce better results, especially for morphologically rich languages (e.g. Slaviclanguages), whereas Dozat et al (2017) showed higher performance in 10 languages in particular English, Greek, Brazilian Portuguese and Estonian."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "DQM"
            ],
            [
                "DQM",
                "ours"
            ],
            null,
            [
                "ours",
                "sv",
                "DQM",
                "en",
                "gl_treegal",
                "pt_br",
                "et"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_2",
        "paper_id": "P18-1246",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1246table_3",
        "description": "Table 3 shows the results of our model in comparison to the results reported in state-ofthe-art literature. Our model significantly outperforms these systems, with an absolute difference of 0.32% in accuracy, which corresponds to a RRIE of 12%.",
        "sentences": [
            "Table 3 shows the results of our model in comparison to the results reported in state-ofthe-art literature.",
            "Our model significantly outperforms these systems, with an absolute difference of 0.32% in accuracy, which corresponds to a RRIE of 12%."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "ours",
                "Accuracy",
                "Sogaard (2011)",
                "Huang et al. (2015)",
                "Choi (2016)",
                "Andor et al. (2016)",
                "Dozat et al. (2017)"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_3",
        "paper_id": "P18-1246",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1246table_5",
        "description": "Table 5 shows that separately optimized models are significantly more accurate on average than jointly optimized models.",
        "sentences": [
            "Table 5 shows that separately optimized models are significantly more accurate on average than jointly optimized models."
        ],
        "class_sentence": [
            1
        ],
        "header_mention": [
            [
                "separate",
                "jointly"
            ]
        ],
        "n_sentence": 1.0,
        "table_id": "table_5",
        "paper_id": "P18-1246",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1246table_8",
        "description": "Table 8 reports, for a few morphological rich languages, the part-of-speech tagging performance of different strategies to gather the characters when creating initial word encodings. The strategies were defined in 3.1. The Table also contains a column with results for our reimplementation of Dozat et al.(2017). We removed, for all systems, the word model in order to assess each strategy in isolation. The performance is quite different per language. E.g., for Latin, the outputs of the forward and backward LSTMs of the last character scored highest.",
        "sentences": [
            "Table 8 reports, for a few morphological rich languages, the part-of-speech tagging performance of different strategies to gather the characters when creating initial word encodings.",
            "The strategies were defined in 3.1.",
            "The Table also contains a column with results for our reimplementation of Dozat et al.(2017).",
            "We removed, for all systems, the word model in order to assess each strategy in isolation.",
            "The performance is quite different per language.",
            "E.g., for Latin, the outputs of the forward and backward LSTMs of the last character scored highest."
        ],
        "class_sentence": [
            1,
            2,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "el",
                "grc",
                "la_ittb",
                "ru",
                "tr"
            ],
            null,
            [
                "DQM"
            ],
            null,
            [
                "el",
                "grc",
                "la_ittb",
                "ru",
                "tr"
            ],
            [
                "la_ittb",
                "Flast Blast"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_8",
        "paper_id": "P18-1246",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1248table_4",
        "description": "Table 4 shows the developmentset performance of our models as compared with baseline systems. MST considers non-projective structures, and thus enjoys a theoretical advantage over projective MH 3, especially for the most non-projective languages. However, it has a vastly larger output space, making the selection of correct structures difficult. Further, the scoring is edge-factored, and does not take any structural contexts into consideration. This tradeoff leads to the similar performance of MST comparing to MH 3. In comparison, both 1EC and MH 4 are mildly non-projective parsing algorithms, limiting the size of the output space. 1EC includes higherorder features that look at tree-structural contexts; MH 4 derives its features from parsing configurations of a transition system, hence leveraging contexts within transition sequences. These considerations explain their significant improvements over MST. We also observe that MH 4 recovers more short dependencies than 1EC, while 1EC is better at longer-distance ones. In comparison to MH 4-two, the richer feature representation of MH 4-hybrid helps in all our languages. Interestingly, MH 4 and MH 3 react differently to switching from global to greedy models. MH 4 covers more structures than MH 3, and is naturally more capable in the global case, even when the feature functions are the same (MH 4-two). However, its greedy version is outperformed by MH 3. We conjecture that this is because MH 4 explores only the same number of configurations as MH 3, despite the fact that introducing non-projectivity expands the search space dramatically.",
        "sentences": [
            "Table 4 shows the developmentset performance of our models as compared with baseline systems.",
            "MST considers non-projective structures, and thus enjoys a theoretical advantage over projective MH 3, especially for the most non-projective languages.",
            "However, it has a vastly larger output space, making the selection of correct structures difficult.",
            "Further, the scoring is edge-factored, and does not take any structural contexts into consideration.",
            "This tradeoff leads to the similar performance of MST comparing to MH 3.",
            "In comparison, both 1EC and MH 4 are mildly non-projective parsing algorithms, limiting the size of the output space.",
            "1EC includes higherorder features that look at tree-structural contexts; MH 4 derives its features from parsing configurations of a transition system, hence leveraging contexts within transition sequences.",
            "These considerations explain their significant improvements over MST.",
            "We also observe that MH 4 recovers more short dependencies than 1EC, while 1EC is better at longer-distance ones.",
            "In comparison to MH 4-two, the richer feature representation of MH 4-hybrid helps in all our languages.",
            "Interestingly, MH 4 and MH 3 react differently to switching from global to greedy models.",
            "MH 4 covers more structures than MH 3, and is naturally more capable in the global case, even when the feature functions are the same (MH 4-two).",
            "However, its greedy version is outperformed by MH 3.",
            "We conjecture that this is because MH 4 explores only the same number of configurations as MH 3, despite the fact that introducing non-projectivity expands the search space dramatically."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            2,
            2,
            2,
            2,
            1,
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "MH 4-two",
                "MH 4-hybrid",
                "MH 3",
                "MST",
                "1EC"
            ],
            [
                "MH 3",
                "MST"
            ],
            [
                "MST"
            ],
            [
                "MST"
            ],
            [
                "MH 3",
                "MST"
            ],
            [
                "MH 4-two",
                "MH 4-hybrid",
                "1EC"
            ],
            [
                "MH 4-two",
                "MH 4-hybrid",
                "1EC"
            ],
            [
                "MST"
            ],
            [
                "MH 4-two",
                "MH 4-hybrid",
                "1EC"
            ],
            [
                "MH 4-two",
                "MH 4-hybrid"
            ],
            [
                "Greedy Models",
                "MH 3",
                "MH 4"
            ],
            [
                "Global Models",
                "MH 3",
                "MH 4-two",
                "MH 4-hybrid"
            ],
            [
                "Greedy Models",
                "MH 3",
                "MH 4"
            ],
            [
                "Greedy Models",
                "MH 3",
                "MH 4"
            ]
        ],
        "n_sentence": 14.0,
        "table_id": "table_4",
        "paper_id": "P18-1248",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1250table_3",
        "description": "Table 3 shows overall performances of the two sequential models on development data. From the results, we can clearly see that the introduction of neural structure pushes up the scores exceptionally. The reason is that our LSTM-CRF model not only benefits from the linear weighted combination of local characteristics like ordinary CRF models, but also has the ability to integrate more contextual information, especially long-distance information. It confirms LSTM-based models\u0081fgreat superiority in sequence labeling problems. Further more, we find that the difference among the four kinds of representations is not so obvious. The most performing one with LSTM-CRF model is Interspace, but the advantage is narrow. Pre3 uses a larger window length to incorporate richer contextual tokens, but at the same time, the searching space for decoding grows larger. It explains that the performance drops slightly with increasing window length. In general, experiments with POS tags show higher scores as more syntactic clues are incorporated.",
        "sentences": [
            "Table 3 shows overall performances of the two sequential models on development data.",
            "From the results, we can clearly see that the introduction of neural structure pushes up the scores exceptionally.",
            "The reason is that our LSTM-CRF model not only benefits from the linear weighted combination of local characteristics like ordinary CRF models, but also has the ability to integrate more contextual information, especially long-distance information.",
            "It confirms LSTM-based models\u0081fgreat superiority in sequence labeling problems.",
            "Further more, we find that the difference among the four kinds of representations is not so obvious.",
            "The most performing one with LSTM-CRF model is Interspace, but the advantage is narrow.",
            "Pre3 uses a larger window length to incorporate richer contextual tokens, but at the same time, the searching space for decoding grows larger.",
            "It explains that the performance drops slightly with increasing window length.",
            "In general, experiments with POS tags show higher scores as more syntactic clues are incorporated."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            [
                "Linear CRF",
                "LSTM-CRF"
            ],
            [
                "LSTM-CRF"
            ],
            [
                "LSTM-CRF"
            ],
            [
                "LSTM-CRF"
            ],
            [
                "Interspace",
                "Pre2",
                "Pre3",
                "Prepost"
            ],
            [
                "LSTM-CRF",
                "Interspace"
            ],
            [
                "Pre3"
            ],
            null,
            [
                "With POS",
                "Without POS"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_3",
        "paper_id": "P18-1250",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1250table_6",
        "description": "Table 6 presents detailed results of the in-parsing models on test data. Compared with the stateof-the-art, the first-order model performs a little worse while the second-order model achieves a remarkable score. The first-order parsing model only constrains the dependencies of both the covert and overt tokens to make up a tree. Due to the loose scoring constraint of the first-order model, the prediction of empty nodes is affected little from the prediction of dependencies of overt words. The four bold numbers in the table intuitively elicits the conclusion that integrating an empty edge and its sibling overt edges is necessary to boost the performance. It makes sense because empty categories are highly related to syntactic analysis. When we conduct ECD and dependency parsing simultaneously, we can leverage more hierarchical contextual information. Comparing results regarding EC types, we can find that OP and T benefit most from the parsing information, the F1 score increasing by about ten points, more markedly than other types.",
        "sentences": [
            "Table 6 presents detailed results of the in-parsing models on test data.",
            "Compared with the stateof-the-art, the first-order model performs a little worse while the second-order model achieves a remarkable score.",
            "The first-order parsing model only constrains the dependencies of both the covert and overt tokens to make up a tree.",
            "Due to the loose scoring constraint of the first-order model, the prediction of empty nodes is affected little from the prediction of dependencies of overt words.",
            "The four bold numbers in the table intuitively elicits the conclusion that integrating an empty edge and its sibling overt edges is necessary to boost the performance.",
            "It makes sense because empty categories are highly related to syntactic analysis.",
            "When we conduct ECD and dependency parsing simultaneously, we can leverage more hierarchical contextual information.",
            "Comparing results regarding EC types, we can find that OP and T benefit most from the parsing information, the F1 score increasing by about ten points, more markedly than other types."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "First-order",
                "Second-order"
            ],
            [
                "First-order"
            ],
            [
                "First-order"
            ],
            [
                "Overall"
            ],
            [
                "pro",
                "OP",
                "T",
                "RNR",
                "*"
            ],
            null,
            [
                "OP",
                "T",
                "F1"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_6",
        "paper_id": "P18-1250",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1252table_5",
        "description": "Table 5 shows the empirical results. Please kindly note that the parsing accuracy looks very low, because the test data is partially annotated and only about 30% most uncertain (difficult) words are manually labeled with their heads according to our guideline, as discussed in Section 2.1. The first-row, \u0081gsingle\u0081h is the baseline targetside parser trained on the train data. The second-row \u0081gsingle (hetero)\u0081h refers to the source-side heterogeneous parser trained on train-HIT and evaluated on the target-side test data. Since the similarity between the two guidelines is high, as discussed in Section 2.2, the source-side parser achieves even higher UAS by 0.21 (76.20 ? 75.99) than the baseline target-side parser trained on the small-scale train data. The LAS is obtained by mapping the HIT-CDT labels to ours (Section 2.2). In the third row, \u0081gmulti-task\u0081h is the targetside parser trained on train & train-HIT with the multi-task learning approach. It significantly outperforms the baseline parser by 4.30 (74.51 ? 70.21) in LAS. This shows that the multi-task learning approach can effectively utilize the large-scale train-HIT to help the target-side parsing. In the fourth row, \u0081gsingle (large)\u0081h is the basic parser trained on the large-scale converted train-HIT (homogeneous). We employ the treeLSTM approach to convert all sentences in train-HIT into our guideline.7. We can see that the single parser trained on the converted data significantly outperforms the parser in the multi-task learning approach by 1.32 (75.83 ?74.51) in LAS. In summary, we can conclude that treebank conversion is superior to multi-task learning in multi-treebank exploitation for its simplicity and better performance.",
        "sentences": [
            "Table 5 shows the empirical results.",
            "Please kindly note that the parsing accuracy looks very low, because the test data is partially annotated and only about 30% most uncertain (difficult) words are manually labeled with their heads according to our guideline, as discussed in Section 2.1.",
            "The first-row, \u0081gsingle\u0081h is the baseline targetside parser trained on the train data.",
            "The second-row \u0081gsingle (hetero)\u0081h refers to the source-side heterogeneous parser trained on train-HIT and evaluated on the target-side test data.",
            "Since the similarity between the two guidelines is high, as discussed in Section 2.2, the source-side parser achieves even higher UAS by 0.21 (76.20 ? 75.99) than the baseline target-side parser trained on the small-scale train data.",
            "The LAS is obtained by mapping the HIT-CDT labels to ours (Section 2.2).",
            "In the third row, \u0081gmulti-task\u0081h is the targetside parser trained on train & train-HIT with the multi-task learning approach.",
            "It significantly outperforms the baseline parser by 4.30 (74.51 ? 70.21) in LAS.",
            "This shows that the multi-task learning approach can effectively utilize the large-scale train-HIT to help the target-side parsing.",
            "In the fourth row, \u0081gsingle (large)\u0081h is the basic parser trained on the large-scale converted train-HIT (homogeneous).",
            "We employ the treeLSTM approach to convert all sentences in train-HIT into our guideline.7.",
            "We can see that the single parser trained on the converted data significantly outperforms the parser in the multi-task learning approach by 1.32 (75.83 ?74.51) in LAS.",
            "In summary, we can conclude that treebank conversion is superior to multi-task learning in multi-treebank exploitation for its simplicity and better performance."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            1,
            2,
            1,
            1,
            2,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "Single",
                "train"
            ],
            [
                "Single (hetero)",
                "train-HIT"
            ],
            [
                "UAS",
                "Single",
                "Single (hetero)"
            ],
            [
                "LAS"
            ],
            [
                "Multi-task",
                "train & train-HIT"
            ],
            [
                "Multi-task",
                "LAS",
                "Single"
            ],
            [
                "Multi-task",
                "train & train-HIT"
            ],
            [
                "Single (large)",
                "converted train-HIT"
            ],
            [
                "Single (large)",
                "converted train-HIT"
            ],
            [
                "Single (large)",
                "Multi-task",
                "LAS"
            ],
            null
        ],
        "n_sentence": 13.0,
        "table_id": "table_5",
        "paper_id": "P18-1252",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1255table_2",
        "description": "We first describe the results of the different models when evaluated against the expert annotations we collect on 500 samples (4). Since the annotators had a low agreement on a single best, we evaluate against the union of the best annotations (B1 \u222a B2 in Table 2) and against the intersection of the valid annotations (V1 \u2229 V2 in Table 2). Among non-neural baselines, we find that the bag-of-ngrams baseline performs slightly better than random but worse than all the other models. The Community QA baseline, on the other hand, performs better than the neural baseline (Neural (p, q)), both of which are trained without using the answers. The neural baselines with answers (Neural(p, q, a) and Neural(p, a)) outperform the neural baseline without answers (Neural(p, q)), showing that answer helps in selecting the right question. More importantly, EVPI outperforms the Neural (p, q, a) baseline across most metrics. Both models use the same information regarding the true question and answer and are trained using the same number of model parameters.17. However, the EVPI model, unlike the neural baseline, additionally makes use of alternate question and answer candidates to compute its loss function. This shows that when the candidate set consists of questions similar to the original question, summing over their utilities gives us a boost. 5.2.2 Evaluating against the original question. The last column in Table 2 shows the results when evaluated against the original question paired with the post. The bag-of-ngrams baseline performs similar to random, unlike when evaluated against human judgments. The Community QA baseline again outperforms Neural(p, q) model and comes very close to the Neural (p, a) model. As before, the neural baselines that make use of the answer outperform the one that does not use the answer and the EVPI model performs significantly better than Neural(p, q, a).",
        "sentences": [
            "We first describe the results of the different models when evaluated against the expert annotations we collect on 500 samples (4).",
            "Since the annotators had a low agreement on a single best, we evaluate against the union of the best annotations (B1 \u222a B2 in Table 2) and against the intersection of the valid annotations (V1 \u2229 V2 in Table 2).",
            "Among non-neural baselines, we find that the bag-of-ngrams baseline performs slightly better than random but worse than all the other models.",
            "The Community QA baseline, on the other hand, performs better than the neural baseline (Neural (p, q)), both of which are trained without using the answers.",
            "The neural baselines with answers (Neural(p, q, a) and Neural(p, a)) outperform the neural baseline without answers (Neural(p, q)), showing that answer helps in selecting the right question.",
            "More importantly, EVPI outperforms the Neural (p, q, a) baseline across most metrics.",
            "Both models use the same information regarding the true question and answer and are trained using the same number of model parameters.17.",
            "However, the EVPI model, unlike the neural baseline, additionally makes use of alternate question and answer candidates to compute its loss function.",
            "This shows that when the candidate set consists of questions similar to the original question, summing over their utilities gives us a boost.",
            "5.2.2 Evaluating against the original question.",
            "The last column in Table 2 shows the results when evaluated against the original question paired with the post.",
            "The bag-of-ngrams baseline performs similar to random, unlike when evaluated against human judgments.",
            "The Community QA baseline again outperforms Neural(p, q) model and comes very close to the Neural (p, a) model.",
            "As before, the neural baselines that make use of the answer outperform the one that does not use the answer and the EVPI model performs significantly better than Neural(p, q, a)."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1,
            2,
            2,
            2,
            0,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "B1 \u222a B2",
                "V1 \u2229 V2"
            ],
            [
                "Random",
                "Bag-of-ngrams",
                "Community QA"
            ],
            [
                "Community QA",
                "Neural (p q)"
            ],
            [
                "Neural (p a)",
                "Neural (p q a)",
                "Neural (p q)"
            ],
            [
                "EVPI",
                "Neural (p q a)"
            ],
            [
                "EVPI",
                "Neural (p q a)"
            ],
            [
                "EVPI"
            ],
            null,
            null,
            [
                "p@1"
            ],
            [
                "Bag-of-ngrams",
                "Random"
            ],
            [
                "Community QA",
                "Neural (p q)",
                "Neural (p a)"
            ],
            [
                "Neural (p a)",
                "Neural (p q a)",
                "Neural (p q)",
                "EVPI"
            ]
        ],
        "n_sentence": 14.0,
        "table_id": "table_2",
        "paper_id": "P18-1255",
        "valid": 1
    },
    {
        "table_id_paper": "P18-2002table_1",
        "description": "As shown in table 1, with an equal number of parameters, the r-RNTN with f mapping outperforms the s-RNN with a bigger hidden layer. It appears that heuristically allocating increased model capacity as done by the f based r-RNTN is a better way to increase performance than simply increasing hidden layer size, which also incurs a computational penalty. Although m-RNNs have been successfully employed in character-level language models with small vocabularies, they are seldom used in wordlevel models. The poor results shown in table 1 could explain why.2. For fixed hidden layer sizes, r-RNTNs yield significant improvements to s-RNNs, GRUs, and LSTMs, confirming the advantages of distinct representations.",
        "sentences": [
            "As shown in table 1, with an equal number of parameters, the r-RNTN with f mapping outperforms the s-RNN with a bigger hidden layer.",
            "It appears that heuristically allocating increased model capacity as done by the f based r-RNTN is a better way to increase performance than simply increasing hidden layer size, which also incurs a computational penalty.",
            "Although m-RNNs have been successfully employed in character-level language models with small vocabularies, they are seldom used in wordlevel models.",
            "The poor results shown in table 1 could explain why.2.",
            "For fixed hidden layer sizes, r-RNTNs yield significant improvements to s-RNNs, GRUs, and LSTMs, confirming the advantages of distinct representations."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "# Params",
                "r-RNTN f",
                "s-RNN"
            ],
            [
                "r-RNTN f"
            ],
            [
                "m-RNN"
            ],
            null,
            [
                "r-RNTN f",
                "s-RNN",
                "GRU",
                "LSTM"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_1",
        "paper_id": "P18-2002",
        "valid": 1
    },
    {
        "table_id_paper": "P18-2005table_1",
        "description": "Table 1 shows the results for the TrustPilot dataset. Observe that the disparity for the BASELINE tagger accuracy (the delta column), for AGE is larger than for SEX, consistent with the results of Hovy and Sogaard (2015). Our ADV method leads to a sizeable reduction in the difference in accuracy across both SEX and AGE, showing our model is capturing the bias signal less and more robust to the tagging task. Moreover, our method leads to a substantial improvement in accuracy across all the test cases. We speculate that this is a consequence of the regularising effect of the adversarial loss, leading to a better characterisation of the tagging problem.",
        "sentences": [
            "Table 1 shows the results for the TrustPilot dataset.",
            "Observe that the disparity for the BASELINE tagger accuracy (the delta column), for AGE is larger than for SEX, consistent with the results of Hovy and Sogaard (2015).",
            "Our ADV method leads to a sizeable reduction in the difference in accuracy across both SEX and AGE, showing our model is capturing the bias signal less and more robust to the tagging task.",
            "Moreover, our method leads to a substantial improvement in accuracy across all the test cases.",
            "We speculate that this is a consequence of the regularising effect of the adversarial loss, leading to a better characterisation of the tagging problem."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "BASELINE",
                "delta",
                "AGE",
                "SEX"
            ],
            [
                "ADV",
                "SEX",
                "AGE",
                "delta"
            ],
            [
                "ADV",
                "F",
                "M",
                "O45",
                "U35",
                "SEX",
                "AGE"
            ],
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_1",
        "paper_id": "P18-2005",
        "valid": 1
    },
    {
        "table_id_paper": "P18-2005table_2",
        "description": "Table 2 shows the results for the AAVE heldout domain. Note that we do not have annotations for SEX or AGE, and thus we only report the overall accuracy on this dataset. Note that ADV also significantly outperforms the BASELINE across the three heldout domains.",
        "sentences": [
            "Table 2 shows the results for the AAVE heldout domain.",
            "Note that we do not have annotations for SEX or AGE, and thus we only report the overall accuracy on this dataset.",
            "Note that ADV also significantly outperforms the BASELINE across the three heldout domains."
        ],
        "class_sentence": [
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "ADV",
                "LYRICS",
                "SUBTITLES",
                "TWEETS",
                "BASELINE"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_2",
        "paper_id": "P18-2005",
        "valid": 1
    },
    {
        "table_id_paper": "P18-2010table_4",
        "description": "Table 4 presents results on the second dataset for the best models identified on the first dataset. The LDA-Frames yielded the best results with our approach performing comparably in terms of the F1-score. We attribute the low performance of the Triframes method based on CW clustering to its hard partitioning output, whereas the evaluation dataset contains fuzzy clusters. Different rankings also suggest that frame induction cannot simply be treated as a verb clustering and requires a separate task.",
        "sentences": [
            "Table 4 presents results on the second dataset for the best models identified on the first dataset.",
            "The LDA-Frames yielded the best results with our approach performing comparably in terms of the F1-score.",
            "We attribute the low performance of the Triframes method based on CW clustering to its hard partitioning output, whereas the evaluation dataset contains fuzzy clusters.",
            "Different rankings also suggest that frame induction cannot simply be treated as a verb clustering and requires a separate task."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "LDA-Frames",
                "F1"
            ],
            [
                "Triframes CW"
            ],
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_4",
        "paper_id": "P18-2010",
        "valid": 1
    },
    {
        "table_id_paper": "P18-2012table_2",
        "description": "Table 2 shows performance as a function of the number of RNN units with a fixed unit size. The number of units is clearly a hyperparameter which\nmust be optimized for. We find good performance across the board (there is no catastrophic collapse in results) however when using 16 units we do outperform other models substantially.",
        "sentences": [
            "Table 2 shows performance as a function of the number of RNN units with a fixed unit size.",
            "The number of units is clearly a hyperparameter which\nmust be optimized for.",
            "We find good performance across the board (there is no catastrophic collapse in results) however when using 16 units we do outperform other models substantially."
        ],
        "class_sentence": [
            1,
            2,
            1
        ],
        "header_mention": [
            [
                "# RNN units"
            ],
            null,
            [
                "16",
                "F1"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_2",
        "paper_id": "P18-2012",
        "valid": 1
    },
    {
        "table_id_paper": "P18-2013table_3",
        "description": "Table 3 shows that TypeDM and TypeComplex dominate across all data sets. E by itself is understandably weak, and DM+E does not lift it much. Each typed model improves upon the corresponding base model on all measures, underscoring the value of type compatibility scores.3 . To the best of our knowledge, the results of our typed models are competitive with various reported results for models of similar sizes that do not use any additional information, e.g., soft rules (Guo et al., 2018), or textual corpora (Toutanova et al., 2015).",
        "sentences": [
            "Table 3 shows that TypeDM and TypeComplex dominate across all data sets.",
            "E by itself is understandably weak, and DM+E does not lift it much.",
            "Each typed model improves upon the corresponding base model on all measures, underscoring the value of type compatibility scores.3 .",
            "To the best of our knowledge, the results of our typed models are competitive with various reported results for models of similar sizes that do not use any additional information, e.g., soft rules (Guo et al., 2018), or textual corpora (Toutanova et al., 2015)."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "TypeDM",
                "TypeComplex"
            ],
            [
                "E",
                "DM+E"
            ],
            [
                "TypeDM",
                "TypeComplex",
                "DM",
                "Complex"
            ],
            [
                "TypeDM",
                "TypeComplex"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "P18-2013",
        "valid": 1
    },
    {
        "table_id_paper": "P18-2014table_1",
        "description": "Table 1 illustrates the performance of our proin comparison with SPTree sysposed model tem Miwa and Bansal (2016) on ACE 2005. We use the same data split with SPTree to compare with their model. We retrained their model with gold entities in order to compare the performances on the relation extraction task. The Baseline corresponds to a model that classifies relations by using only the representations of entities in a target pair. As it can be observed from the table, the Baseline model achieves the lowest F1 score between the proposed models. By incorporating attention we can further improve the performance by 1.3 percent point (pp). The addition of 2-length walks further improves performance (0.9 pp). The best results among the proposed models are achieved for maximum 4-length walks. By using up-to 8-length walks the performance drops almost by 2 pp.",
        "sentences": [
            "Table 1 illustrates the performance of our proin comparison with SPTree sysposed model tem Miwa and Bansal (2016) on ACE 2005.",
            "We use the same data split with SPTree to compare with their model.",
            "We retrained their model with gold entities in order to compare the performances on the relation extraction task.",
            "The Baseline corresponds to a model that classifies relations by using only the representations of entities in a target pair.",
            "As it can be observed from the table, the Baseline model achieves the lowest F1 score between the proposed models.",
            "By incorporating attention we can further improve the performance by 1.3 percent point (pp).",
            "The addition of 2-length walks further improves performance (0.9 pp).",
            "The best results among the proposed models are achieved for maximum 4-length walks.",
            "By using up-to 8-length walks the performance drops almost by 2 pp."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "SPTree",
                "No walks l = 1",
                "+ Walks l = 2",
                "+ Walks l = 4",
                "+ Walks l = 8"
            ],
            [
                "SPTree",
                "No walks l = 1",
                "+ Walks l = 2",
                "+ Walks l = 4",
                "+ Walks l = 8"
            ],
            null,
            [
                "Baseline"
            ],
            [
                "Baseline",
                "F1 (%)"
            ],
            [
                "No walks l = 1",
                "F1 (%)"
            ],
            [
                "+ Walks l = 2",
                "F1 (%)"
            ],
            [
                "+ Walks l = 4"
            ],
            [
                "+ Walks l = 8"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_1",
        "paper_id": "P18-2014",
        "valid": 1
    },
    {
        "table_id_paper": "P18-2014table_2",
        "description": "Finally, we show the performance of the proposed model as a function of the number of entities in a sentence. Results in Table 2 reveal that for multi-pair sentences the model performs significantly better compared to the no-walks models, proving the effectiveness of the method. Additionally, it is observed that for more entity pairs, longer walks seem to be required. However, very long walks result to reduced performance (l = 8).",
        "sentences": [
            "Finally, we show the performance of the proposed model as a function of the number of entities in a sentence.",
            "Results in Table 2 reveal that for multi-pair sentences the model performs significantly better compared to the no-walks models, proving the effectiveness of the method.",
            "Additionally, it is observed that for more entity pairs, longer walks seem to be required.",
            "However, very long walks result to reduced performance (l = 8)."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "# Entities"
            ],
            [
                "2",
                "3"
            ],
            [
                "l = 1",
                "l = 2",
                "l = 4"
            ],
            [
                "l = 8"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "P18-2014",
        "valid": 1
    },
    {
        "table_id_paper": "P18-2015table_2",
        "description": "The performances of the seed selection methods are presented in Table 2. For the HITS-based and HITS+K-means-based methods, we display the P@50 with three types of graph representation as shown in Section 4.2. We use random seed selection as the baseline for comparison. As Table 2 shows, the random method achieved a precision of 0.75. The relation extraction system that uses the random method has the worst average P@50 among all seed selection strategies. The HITS-based  method P@50 when using Graph1 and Graph3 are confirmed to be better than when using Graph2. This indicates that relying on reliable instances is better than reasoning over patterns (recall that for the Graph2, we first choose the patterns, then select the instances associated with those patterns), as there is a possibility that a pattern can be ambiguous, and therefore, instances linked to that pattern can be incorrect. The Kmeans-based seed selection method provides the best average P@50 with a performance of 0.96. The HITS+K-means-based method performs better than using only the HITS strategy, while the LSA-based and NMF-based methods have a comparable performance.",
        "sentences": [
            "The performances of the seed selection methods are presented in Table 2.",
            "For the HITS-based and HITS+K-means-based methods, we display the P@50 with three types of graph representation as shown in Section 4.2.",
            "We use random seed selection as the baseline for comparison.",
            "As Table 2 shows, the random method achieved a precision of 0.75.",
            "The relation extraction system that uses the random method has the worst average P@50 among all seed selection strategies.",
            "The HITS-based  method P@50 when using Graph1 and Graph3 are confirmed to be better than when using Graph2.",
            "This indicates that relying on reliable instances is better than reasoning over patterns (recall that for the Graph2, we first choose the patterns, then select the instances associated with those patterns), as there is a possibility that a pattern can be ambiguous, and therefore, instances linked to that pattern can be incorrect.",
            "The Kmeans-based seed selection method provides the best average P@50 with a performance of 0.96.",
            "The HITS+K-means-based method performs better than using only the HITS strategy, while the LSA-based and NMF-based methods have a comparable performance."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "HITS Graph1",
                "HITS Graph2",
                "HITS Graph3",
                "HITS+K-means Graph1",
                "HITS+K-means Graph2",
                "HITS+K-means Graph3",
                "Average P@50"
            ],
            [
                "Random"
            ],
            [
                "Random",
                "Average P@50"
            ],
            [
                "Random",
                "Average P@50"
            ],
            [
                "HITS Graph1",
                "HITS Graph2",
                "HITS Graph3"
            ],
            [
                "HITS Graph1",
                "HITS Graph2",
                "HITS Graph3"
            ],
            [
                "K-means",
                "Average P@50"
            ],
            [
                "HITS+K-means Graph1",
                "HITS+K-means Graph2",
                "HITS+K-means Graph3",
                "HITS Graph1",
                "HITS Graph2",
                "HITS Graph3",
                "LSA",
                "NMF"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_2",
        "paper_id": "P18-2015",
        "valid": 1
    },
    {
        "table_id_paper": "P18-2016table_3",
        "description": "Table 3 shows the ranking results using Mean Average Precision (MAP) and Precision at K as the metrics. Accumulative scores (f1 and f3) generally do better. Thus, we choose f = f3 with a MAP score of 0.59 as the scoring function.",
        "sentences": [
            "Table 3 shows the ranking results using Mean Average Precision (MAP) and Precision at K as the metrics.",
            "Accumulative scores (f1 and f3) generally do better.",
            "Thus, we choose f = f3 with a MAP score of 0.59 as the scoring function."
        ],
        "class_sentence": [
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "MAP",
                "P@50",
                "P@100",
                "P@200",
                "P@300"
            ],
            [
                "f1",
                "f3"
            ],
            [
                "f3"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_3",
        "paper_id": "P18-2016",
        "valid": 1
    },
    {
        "table_id_paper": "P18-2020table_1",
        "description": "Table 1 shows results on GermEval using the official metric (metric 1) for the best performing systems. This measure considers both outer and inner span annotations. Within the challenge, the ExB (Hanig et al., 2015) ensemble classifier achieved the best result with an F1 score of 76.38, followed by the RNN-based method from UKP (Reimers et al., 2014) with 75.09. GermaNER achieves high precision, but cannot compete in terms of recall. Our BiLSTM with Wikipedia word embeddings, scores highest (79.99) and outperforms the shared task winner ExB significantly, based on a bootstrap\nresampling test (Efron and Tibshirani, 1994). Using Europeana embeddings, the performance drops to an F1 score of 73.03 ? due to the difference in vocabulary.",
        "sentences": [
            "Table 1 shows results on GermEval using the official metric (metric 1) for the best performing systems.",
            "This measure considers both outer and inner span annotations.",
            "Within the challenge, the ExB (Hanig et al., 2015) ensemble classifier achieved the best result with an F1 score of 76.38, followed by the RNN-based method from UKP (Reimers et al., 2014) with 75.09.",
            "GermaNER achieves high precision, but cannot compete in terms of recall.",
            "Our BiLSTM with Wikipedia word embeddings, scores highest (79.99) and outperforms the shared task winner ExB significantly, based on a bootstrap\nresampling test (Efron and Tibshirani, 1994).",
            "Using Europeana embeddings, the performance drops to an F1 score of 73.03 ? due to the difference in vocabulary."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "ExB",
                "F1",
                "RNN",
                "UKP"
            ],
            [
                "GermaNER",
                "Pr",
                "R"
            ],
            [
                "BiLSTM-WikiEmb",
                "ExB",
                "F1"
            ],
            [
                "BiLSTM-EuroEmb",
                "F1"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_1",
        "paper_id": "P18-2020",
        "valid": 1
    },
    {
        "table_id_paper": "P18-2020table_5",
        "description": "The results in Table 5 show significant improvements for the CoNLL dataset but performance drops for GermEval. Combining contemporary sources with historic target corpora yields to consistent benefits. Performance on LFT increases from 69.62 to 74.33 and on ONB from 73.31 to 78.56. Cross-domain classification scores are also improved consistently. The GermEval corpus is more appropriate as a source corpus, presumably because it is both larger and drawn from encyclopaedic text, more varied than newswire. We conclude that transfer learning is beneficial for BiLSTMs, especially when training data for the target domain is scarce.",
        "sentences": [
            "The results in Table 5 show significant improvements for the CoNLL dataset but performance drops for GermEval.",
            "Combining contemporary sources with historic target corpora yields to consistent benefits.",
            "Performance on LFT increases from 69.62 to 74.33 and on ONB from 73.31 to 78.56.",
            "Cross-domain classification scores are also improved consistently.",
            "The GermEval corpus is more appropriate as a source corpus, presumably because it is both larger and drawn from encyclopaedic text, more varied than newswire.",
            "We conclude that transfer learning is beneficial for BiLSTMs, especially when training data for the target domain is scarce."
        ],
        "class_sentence": [
            1,
            2,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            [
                "CoNLL",
                "GermEval"
            ],
            null,
            [
                "BiLSTM-WikiEmb",
                "LFT",
                "BiLSTM-EuroEmb",
                "ONB"
            ],
            null,
            [
                "GermEval"
            ],
            [
                "BiLSTM-WikiEmb",
                "BiLSTM-EuroEmb"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_5",
        "paper_id": "P18-2020",
        "valid": 1
    },
    {
        "table_id_paper": "P18-2021table_2",
        "description": "Table 2 shows the average precision, recall, F1-measure, and AUC. The classifiers trained on the linguistic features, while performing near the baselines on the first three measures, substantially outperform the baselines on AUC, with all three yielding values over 0.8. Given these results and the imbalanced nature of the dataset, it seems that the classifiers trained on the linguistic features are able to identify both classes of comments with high accuracy, while the baseline classifiers perform only marginally better than a majority class baseline.",
        "sentences": [
            "Table 2 shows the average precision, recall, F1-measure, and AUC.",
            "The classifiers trained on the linguistic features, while performing near the baselines on the first three measures, substantially outperform the baselines on AUC, with all three yielding values over 0.8.",
            "Given these results and the imbalanced nature of the dataset, it seems that the classifiers trained on the linguistic features are able to identify both classes of comments with high accuracy, while the baseline classifiers perform only marginally better than a majority class baseline."
        ],
        "class_sentence": [
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "Precision",
                "Recall",
                "F1",
                "AUC"
            ],
            [
                "All",
                "Significant",
                "Relevant",
                "Precision",
                "Recall",
                "F1",
                "AUC"
            ],
            [
                "All",
                "Significant",
                "Relevant",
                "# Tokens",
                "# Sentences"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_2",
        "paper_id": "P18-2021",
        "valid": 1
    },
    {
        "table_id_paper": "P18-2023table_4",
        "description": "Table 4 lists the performance of them on CA_translated and CA8 datasets under different configurations. We can observe that on CA8 dataset, SGNS representations perform better in analogical reasoning of morphological relations and PPMI representations show great advantages in semantic relations. However, Table 4 shows that there is only a slight increase on CA_translated dataset with ngram features, and the accuracies in most cases decrease after integrating character features. In contrast, on CA8 dataset, the introduction of ngram and character features brings significant and consistent improvements on almost all the categories. Furthermore, character features are especially advantageous for reasoning of morphological relations. SGNS model integrating with character features even doubles the accuracy in morphological questions.",
        "sentences": [
            "Table 4 lists the performance of them on CA_translated and CA8 datasets under different configurations.",
            "We can observe that on CA8 dataset, SGNS representations perform better in analogical reasoning of morphological relations and PPMI representations show great advantages in semantic relations.",
            "However, Table 4 shows that there is only a slight increase on CA_translated dataset with ngram features, and the accuracies in most cases decrease after integrating character features.",
            "In contrast, on CA8 dataset, the introduction of ngram and character features brings significant and consistent improvements on almost all the categories.",
            "Furthermore, character features are especially advantageous for reasoning of morphological relations.",
            "SGNS model integrating with character features even doubles the accuracy in morphological questions."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            [
                "CA_translated",
                "CA8"
            ],
            [
                "CA8",
                "SGNS",
                "Mor.",
                "PPMI",
                "Sem."
            ],
            [
                "CA_translated",
                "word+ngram",
                "word"
            ],
            [
                "CA8",
                "word+ngram",
                "word+char"
            ],
            [
                "word+char",
                "Mor."
            ],
            [
                "SGNS",
                "word+char",
                "Mor."
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_4",
        "paper_id": "P18-2023",
        "valid": 1
    },
    {
        "table_id_paper": "P18-2023table_5",
        "description": "Table 5 shows that accuracies increase with the growth in corpus size, e.g .Baidubaike (an online Chinese encyclopedia) has a clear advantage over Wikipedia. Also, the domain of a corpus plays an important role in the experiments. We can observe that vectors trained on news data are beneficial to geography relations, especially on People\u2019s Daily which has a focus on political news. Another example is Zhihu QA, an online questionanswering corpus which contains more informal data than others. It is helpful to reduplication relations since many reduplication words appear frequently in spoken language. With the largest size and varied domains, Combinationcorpus performs much better than others in both morphological and semantic relations.",
        "sentences": [
            "Table 5 shows that accuracies increase with the growth in corpus size, e.g .Baidubaike (an online Chinese encyclopedia) has a clear advantage over Wikipedia.",
            "Also, the domain of a corpus plays an important role in the experiments.",
            "We can observe that vectors trained on news data are beneficial to geography relations, especially on People\u2019s Daily which has a focus on political news.",
            "Another example is Zhihu QA, an online questionanswering corpus which contains more informal data than others.",
            "It is helpful to reduplication relations since many reduplication words appear frequently in spoken language.",
            "With the largest size and varied domains, Combinationcorpus performs much better than others in both morphological and semantic relations."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            [
                "Baidubaike 4.3G",
                "Wikipedia 1.2G"
            ],
            [
                "Cap.",
                "Sta.",
                "Fam.",
                "A",
                "AB",
                "Pre.",
                "Suf.",
                "Mor.",
                "Geo.",
                "His.",
                "Nat.",
                "Peo.",
                "Sem."
            ],
            [
                "People Daily 4.2G",
                "Geo."
            ],
            [
                "Zhihu QA 2.2G"
            ],
            null,
            [
                "Combination 15.9G",
                "Mor.",
                "Sem."
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_5",
        "paper_id": "P18-2023",
        "valid": 1
    },
    {
        "table_id_paper": "P18-2051table_4",
        "description": "Our plain BPE baseline (Table 4) outperforms the current best system on WAT Ja-En, an 8-model ensemble (Morishita et al., 2017). Our syntax models achieve similar results despite producing much longer sequences.",
        "sentences": [
            "Our plain BPE baseline (Table 4) outperforms the current best system on WAT Ja-En, an 8-model ensemble (Morishita et al., 2017).",
            "Our syntax models achieve similar results despite producing much longer sequences."
        ],
        "class_sentence": [
            1,
            2
        ],
        "header_mention": [
            [
                "Transformer",
                "Plain BPE",
                "Seq2seq (8-model ensemble)",
                "Best WAT17 result (Morishita et al. 2017)"
            ],
            null
        ],
        "n_sentence": 2.0,
        "table_id": "table_4",
        "paper_id": "P18-2051",
        "valid": 1
    },
    {
        "table_id_paper": "P18-2058table_2",
        "description": "To compare with additional previous systems, we also conduct experiments with gold predicates by constraining our predicate beam to be gold predicates only. As shown in Table 2, our model significantly out-performs He et al. (2017), but falls short of Tan et al. (2018).",
        "sentences": [
            "To compare with additional previous systems, we also conduct experiments with gold predicates by constraining our predicate beam to be gold predicates only.",
            "As shown in Table 2, our model significantly out-performs He et al. (2017), but falls short of Tan et al. (2018)."
        ],
        "class_sentence": [
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "Ours",
                "He et al. (2017)",
                "Tan et al. (2018)"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_2",
        "paper_id": "P18-2058",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1001table_2",
        "description": "6.4 Evaluation Results . Table 2 report evaluation results on the three data sets where IoI-global and IoI-local represent models learned with Objective (17) and Objective (18) respectively. We can see that both IoI-local and IoI-global outperform the best performing baseline, and improvements from IoI-local on all metrics and from IoI-global on a few metrics are statistically significant (t-test with p-value < 0.05). IoI-local is consistently better than IoI-global over all metrics on all the three data sets, demonstrating that directly supervising each block in learning can lead to a more optimal deep structure than optimizing the final matching model.",
        "sentences": [
            "6.4 Evaluation Results .",
            "Table 2 report evaluation results on the three data sets where IoI-global and IoI-local represent models learned with Objective (17) and Objective (18) respectively.",
            "We can see that both IoI-local and IoI-global outperform the best performing baseline, and improvements from IoI-local on all metrics and from IoI-global on a few metrics are statistically significant (t-test with p-value < 0.05).",
            "IoI-local is consistently better than IoI-global over all metrics on all the three data sets, demonstrating that directly supervising each block in learning can lead to a more optimal deep structure than optimizing the final matching model."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "IoI-global",
                "IoI-local"
            ],
            [
                "IoI-local",
                "IoI-global"
            ],
            [
                "IoI-local",
                "IoI-global"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "P19-1001",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1006table_1",
        "description": "3.5  Ablation Study. As it is shown in Table 1, we conduct an ablation study on the testset of the Ubuntu Corpus, where we aim to examine the effect of each part in our proposed model. Firstly, we verify the effectiveness of dual multi-turn encoder by comparing Baseline and DME in Table 1. Thanks to dual multi-turn en-coder,  DME  achieves  0.725  at R100@10 which is  0.366  better  than  the  Baseline  (Lowe  et  al.,2015b). Secondly, we study the ability of representation module by testing LSTM, GRU and Transformer with the default hyperparameter in Tensorflow. We note  that  GRU  is  better  for  this  task.   . After  removing spatio-temporal matching block, the per-formance degrades significantly. In  order  to  verify  the  effectiveness  of  STM block further, we design a DME-SMN which uses 2D convolution for extracting spatial attention information and employ GRU for modeling temporal information. The STM block makes a 10.54% improvement at R100@1. Next,  we  replace  GRU  with  Transformer  in STM. Supposed  the  data  has  maximal m turns and n candidates,  the  time  complexity  of  cross-attention  (Zhou  et  al.,  2018), O(mn), is  much higher than that of the Dual-Encoder based model,O(m+n). Thus, cross-attention is an impractical operation when the candidate set is large. So we remove cross-attention operations in DAM and extend it with Dual-Encoder architecture. The result in Table 1 shows that using self-attention only may not be enough for representation. As BERT (Devlin et al., 2018) has been shown to be a powerful feature extractor for various tasks, we  employ  BERT  as  a  feature-based  approach to generate ELMo-like pre-trained contextual representations  (Peters  et  al.,  2018b). It  succeed  the highest results and outperforms other methods by a significant margin.",
        "sentences": [
            "3.5  Ablation Study.",
            "As it is shown in Table 1, we conduct an ablation study on the testset of the Ubuntu Corpus, where we aim to examine the effect of each part in our proposed model.",
            "Firstly, we verify the effectiveness of dual multi-turn encoder by comparing Baseline and DME in Table 1.",
            "Thanks to dual multi-turn en-coder,  DME  achieves  0.725  at R100@10 which is  0.366  better  than  the  Baseline  (Lowe  et  al.,2015b).",
            "Secondly, we study the ability of representation module by testing LSTM, GRU and Transformer with the default hyperparameter in Tensorflow.",
            "We note  that  GRU  is  better  for  this  task.   .",
            "After  removing spatio-temporal matching block, the per-formance degrades significantly.",
            "In  order  to  verify  the  effectiveness  of  STM block further, we design a DME-SMN which uses 2D convolution for extracting spatial attention information and employ GRU for modeling temporal information.",
            "The STM block makes a 10.54% improvement at R100@1.",
            "Next,  we  replace  GRU  with  Transformer  in STM.",
            "Supposed  the  data  has  maximal m turns and n candidates,  the  time  complexity  of  cross-attention  (Zhou  et  al.,  2018), O(mn), is  much higher than that of the Dual-Encoder based model,O(m+n).",
            "Thus, cross-attention is an impractical operation when the candidate set is large.",
            "So we remove cross-attention operations in DAM and extend it with Dual-Encoder architecture.",
            "The result in Table 1 shows that using self-attention only may not be enough for representation.",
            "As BERT (Devlin et al., 2018) has been shown to be a powerful feature extractor for various tasks, we  employ  BERT  as  a  feature-based  approach to generate ELMo-like pre-trained contextual representations  (Peters  et  al.,  2018b).",
            "It  succeed  the highest results and outperforms other methods by a significant margin."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2,
            2,
            2,
            2,
            1,
            1,
            2,
            2,
            2,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Baseline",
                "DME"
            ],
            [
                "DME",
                "Baseline",
                "R100@10"
            ],
            [
                "STM(Transform)",
                "STM(GRU)"
            ],
            [
                "STM(GRU)"
            ],
            [
                "STM(GRU)"
            ],
            [
                "STM(GRU)",
                "DME-SMN"
            ],
            [
                "STM(GRU)",
                "R100@1"
            ],
            [
                "STM(Transform)"
            ],
            null,
            null,
            [
                "DAM"
            ],
            null,
            [
                "STM(BERT)"
            ],
            [
                "STM(BERT)"
            ]
        ],
        "n_sentence": 16.0,
        "table_id": "table_1",
        "paper_id": "P19-1006",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1013table_3",
        "description": "Table 3 shows the results of the parsing experiment, where the scores of revious work (C&C (Clark and Curran, 2007) and EasySRL (Lewis et al., 2016)) are included for reference. The plain depccg already achieves higher scores than these methods, and boosts when combined with ELMo (improvement of 2.73 points in terms of F1). Fine-tuning the parser on GENIA1000 results in a mixed result, with slightly lower scores. This is presumably because the automatically annotated Head First dependencies are not accurate. Finally, by fine-tuning on the Genia CCGbank, we observe another improvement, resulting in the highest 86.52 F1 score.",
        "sentences": [
            "Table 3 shows the results of the parsing experiment, where the scores of revious work (C&C (Clark and Curran, 2007) and EasySRL (Lewis et al., 2016)) are included for reference.",
            "The plain depccg already achieves higher scores than these methods, and boosts when combined with ELMo (improvement of 2.73 points in terms of F1). Fine-tuning the parser on GENIA1000 results in a mixed result, with slightly lower scores.",
            "This is presumably because the automatically annotated Head First dependencies are not accurate.",
            "Finally, by fine-tuning on the Genia CCGbank, we observe another improvement, resulting in the highest 86.52 F1 score."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "depccg"
            ],
            null,
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "P19-1013",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1013table_4",
        "description": "Table 4 compares the performance of depccg fine-tuned on the QuestionBank, along with other baselines. Contrary to our expectation, the plain depccg retrained on Questions data performs the best, with neither ELMo nor the proposed method taking any effect. We hypothesize that, since the evaluation set contains sentences with similar constructions, the contributions of the latter two methods are less observable on top of Questions data. Inspection of the output trees reveals that this is actually the case; the majority of differences among parser's configurations are irrelevant to question constructions, suggesting that the models capture well the syntax of question in the data.11.",
        "sentences": [
            "Table 4 compares the performance of depccg fine-tuned on the QuestionBank, along with other baselines.",
            "Contrary to our expectation, the plain depccg retrained on Questions data performs the best, with neither ELMo nor the proposed method taking any effect.",
            "We hypothesize that, since the evaluation set contains sentences with similar constructions, the contributions of the latter two methods are less observable on top of Questions data.",
            "Inspection of the output trees reveals that this is actually the case; the majority of differences among parser's configurations are irrelevant to question constructions, suggesting that the models capture well the syntax of question in the data.11."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "depccg",
                "depccg+elmo",
                "depccg+proposed"
            ],
            null,
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_4",
        "paper_id": "P19-1013",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1019table_1",
        "description": "Table 1 reports the results of the proposed system in comparison to previous work. As it can be seen, our full system obtains the best published results in all cases, outperforming the previous state-of-the-art by 5-7 BLEU points in all datasets and translation directions.",
        "sentences": [
            "Table 1 reports the results of the proposed system in comparison to previous work.",
            "As it can be seen, our full system obtains the best published results in all cases, outperforming the previous state-of-the-art by 5-7 BLEU points in all datasets and translation directions."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Proposed system"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_1",
        "paper_id": "P19-1019",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1019table_3",
        "description": "So as to put our results into perspective, Table 3 reports the results of different supervised systems in the same WMT 2014 test set. More concretely, we include the best results from the shared task itself, which reflect the state-of-the-art in machine translation back in 2014; those of Vaswani et al. (2017), who introduced the now predominant transformer architecture; and those of Edunov et al. (2018), who apply back-translation at a large scale and, to the best of our knowledge, hold the current best results in the test set. As it can be seen, our unsupervised system outperforms the WMT 2014 shared task winner in English-to-German, and is around 2 BLEU points behind it in the other translation directions. This shows that unsupervised machine translation is already competitive with the state-of-the-art in supervised machine translation in 2014.",
        "sentences": [
            "So as to put our results into perspective, Table 3 reports the results of different supervised systems in the same WMT 2014 test set.",
            "More concretely, we include the best results from the shared task itself, which reflect the state-of-the-art in machine translation back in 2014; those of Vaswani et al. (2017), who introduced the now predominant transformer architecture; and those of Edunov et al. (2018), who apply back-translation at a large scale and, to the best of our knowledge, hold the current best results in the test set.",
            "As it can be seen, our unsupervised system outperforms the WMT 2014 shared task winner in English-to-German, and is around 2 BLEU points behind it in the other translation directions.",
            "This shows that unsupervised machine translation is already competitive with the state-of-the-art in supervised machine translation in 2014."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Vaswani et al. (2017)",
                "Edunov et al. (2018)"
            ],
            [
                "Unsupervised",
                "Proposed system",
                "en-de"
            ],
            [
                "Unsupervised",
                "Supervised"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "P19-1019",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1021table_4",
        "description": "Table 4 shows results for Korean - English, using the same configurations (1, 2 and 8) as for German - English. Our results confirm that the techniques we apply are successful across datasets, and result in stronger systems than previously reported on this dataset, achieving 10.37 BLEU as compared to 5.97 BLEU reported by Gu et al. (2018b).",
        "sentences": [
            "Table 4 shows results for Korean - English, using the same configurations (1, 2 and 8) as for German - English.",
            "Our results confirm that the techniques we apply are successful across datasets, and result in stronger systems than previously reported on this dataset, achieving 10.37 BLEU as compared to 5.97 BLEU reported by Gu et al. (2018b)."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "NMT optimized (8)",
                "BLEU",
                "(Gu et al., 2018b) (supervised Transformer)"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_4",
        "paper_id": "P19-1021",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1023table_3",
        "description": "4.3 Result. Table 3 shows that the end-to-end models outper-form  the  existing  model. In particular, our proposed n-gram attention model achieves the best results in terms of precision, recall, and F1 score. Our proposed model outperforms the best existing model (MinIE) by 33.39% and 34.78% in terms of F1 score on the WIKI and GEO test dataset re-spectively. These results are expected since the existing models are affected by the error propagation of the NED. As expected, the combination of the existing models with AIDA achieves higher F1 scores than the combination with NeuralEL as AIDA achieves a higher precision than NeuralEL. To further show the effect of error propagation, we  set  up  an  experiment  without  the  canonicalization task (i.e., the objective is predicting a relationship  between  known  entities). We  removethe NED pre-processing step by allowing the CNN model to access the correct entities.  Meanwhile,we provide the correct entities to the decoder of our proposed model. In this setup, our proposed model achieves 86.34% and 79.11%, while CNN achieves 81.92% and 75.82% in precision over the WIKI and GEO test datasets, respectively. Our  proposed  n-gram  attention  model  outper-forms  the  end-to-end  models  by 15.51% and 8.38% in terms of F1 score on the WIKI and GEO test datasets, respectively. The Transformer model also  only  yields  similar  performance  to  that  of the Single Attention model,  which is worse than ours.  These results indicate that our model captures  multi-word  entity  name  (in  both  datasets, 82.9% of the entities have multi-word entity name) in the input sentence better than the other models. Table 3 also shows that the pre-trained embeddings improve the performance of the model in all measures. Moreover, the pre-trained embeddings help the model to converge faster. In our experiments, the models that use the pre-trained embeddings converge in 20 epochs on average, while the models that do not use the pre-trained embeddings converge in 30 - 40 epochs. Our triple classifier combined with the modified beam search boost the performance of the model. The modified beam search provides a high recall by extracting the correct entities based on the surface form in the input sentence while the triple classifier provides a high precision by filtering the invalid triples.",
        "sentences": [
            "4.3 Result.",
            "Table 3 shows that the end-to-end models outper-form  the  existing  model.",
            "In particular, our proposed n-gram attention model achieves the best results in terms of precision, recall, and F1 score.",
            "Our proposed model outperforms the best existing model (MinIE) by 33.39% and 34.78% in terms of F1 score on the WIKI and GEO test dataset re-spectively.",
            "These results are expected since the existing models are affected by the error propagation of the NED.",
            "As expected, the combination of the existing models with AIDA achieves higher F1 scores than the combination with NeuralEL as AIDA achieves a higher precision than NeuralEL.",
            "To further show the effect of error propagation, we  set  up  an  experiment  without  the  canonicalization task (i.e., the objective is predicting a relationship  between  known  entities).",
            "We  removethe NED pre-processing step by allowing the CNN model to access the correct entities.",
            " Meanwhile,we provide the correct entities to the decoder of our proposed model.",
            "In this setup, our proposed model achieves 86.34% and 79.11%, while CNN achieves 81.92% and 75.82% in precision over the WIKI and GEO test datasets, respectively.",
            "Our  proposed  n-gram  attention  model  outper-forms  the  end-to-end  models  by 15.51% and 8.38% in terms of F1 score on the WIKI and GEO test datasets, respectively.",
            "The Transformer model also  only  yields  similar  performance  to  that  of the Single Attention model,  which is worse than ours.",
            " These results indicate that our model captures  multi-word  entity  name  (in  both  datasets, 82.9% of the entities have multi-word entity name) in the input sentence better than the other models.",
            "Table 3 also shows that the pre-trained embeddings improve the performance of the model in all measures.",
            "Moreover, the pre-trained embeddings help the model to converge faster.",
            "In our experiments, the models that use the pre-trained embeddings converge in 20 epochs on average, while the models that do not use the pre-trained embeddings converge in 30 - 40 epochs.",
            "Our triple classifier combined with the modified beam search boost the performance of the model.",
            "The modified beam search provides a high recall by extracting the correct entities based on the surface form in the input sentence while the triple classifier provides a high precision by filtering the invalid triples."
        ],
        "class_sentence": [
            0,
            1,
            1,
            1,
            2,
            1,
            2,
            2,
            2,
            1,
            1,
            1,
            1,
            1,
            2,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Proposed"
            ],
            [
                "Proposed",
                "N-gram Attention",
                "Precision",
                "Recall",
                "F1"
            ],
            [
                "Proposed",
                "N-gram Attention",
                "MinIE (+AIDA)",
                "MinIE (+NeuralEL)",
                "F1",
                "WIKI",
                "GEO"
            ],
            [
                "MinIE (+AIDA)",
                "MinIE (+NeuralEL)"
            ],
            [
                "MinIE (+AIDA)",
                "F1",
                "Precision",
                "MinIE (+NeuralEL)"
            ],
            null,
            [
                "CNN (+AIDA)",
                "CNN (+NeuralEL)"
            ],
            [
                "N-gram Attention"
            ],
            [
                "N-gram Attention",
                "CNN (+AIDA)",
                "CNN (+NeuralEL)",
                "Precision",
                "WIKI",
                "GEO"
            ],
            [
                "N-gram Attention",
                "F1",
                "WIKI",
                "GEO"
            ],
            [
                "Transformer",
                "Single Attention",
                "N-gram Attention"
            ],
            [
                "N-gram Attention"
            ],
            [
                "N-gram Attention (+pre-trained)"
            ],
            null,
            [
                "N-gram Attention (+pre-trained)"
            ],
            [
                "N-gram Attention (+triple classifier)"
            ],
            [
                "N-gram Attention (+beam)",
                "Recall",
                "N-gram Attention (+triple classifier)"
            ]
        ],
        "n_sentence": 18.0,
        "table_id": "table_3",
        "paper_id": "P19-1023",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1029table_3",
        "description": "A.3 Offline Evaluation on IWSLT.  Table 3 reports the offline held-out set evaluations for the early stopping points selected on the dev set for all feedback modes. All models notably improve over the baseline, only using full feedback leads to the overall best model on IWSLT (+0.6 BLEU / -0.6 TER), but costs a massive amounts of edits (417k characters). Self regulating models still achieve improvements of 0.4-0.5 BLEU/TER with costs reduced up to a factor of 23. The reduction in cost is enabled by the use of cheaper feedback, here markings and self-supervision, which in isolation are successful as well. Self-supervision works surprisingly well, which makes it attractive for cheap but effective unsupervised domain adaptation. It has to be noted that both weak and self-supervision worked only  well  when  targets  were  pre-computed  with the baseline model and held fixed during training. We suspect that the strong reward signal (ft= 1)for  non-reference  outputs  leads  otherwise  to  undesired local overfitting effects that a learner with online-generated targets cannot recover from.",
        "sentences": [
            "A.3 Offline Evaluation on IWSLT.",
            " Table 3 reports the offline held-out set evaluations for the early stopping points selected on the dev set for all feedback modes.",
            "All models notably improve over the baseline, only using full feedback leads to the overall best model on IWSLT (+0.6 BLEU / -0.6 TER), but costs a massive amounts of edits (417k characters).",
            "Self regulating models still achieve improvements of 0.4-0.5 BLEU/TER with costs reduced up to a factor of 23.",
            "The reduction in cost is enabled by the use of cheaper feedback, here markings and self-supervision, which in isolation are successful as well.",
            "Self-supervision works surprisingly well, which makes it attractive for cheap but effective unsupervised domain adaptation.",
            "It has to be noted that both weak and self-supervision worked only  well  when  targets  were  pre-computed  with the baseline model and held fixed during training.",
            "We suspect that the strong reward signal (ft= 1)for  non-reference  outputs  leads  otherwise  to  undesired local overfitting effects that a learner with online-generated targets cannot recover from."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "Full",
                "Baseline",
                "BLEU",
                "TER",
                "Cost"
            ],
            [
                "Self",
                "BLEU",
                "TER"
            ],
            null,
            [
                "Self"
            ],
            [
                "Weak",
                "Self",
                "Baseline"
            ],
            null
        ],
        "n_sentence": 8.0,
        "table_id": "table_3",
        "paper_id": "P19-1029",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1035table_1",
        "description": "The right-hand side of table 1 shows the performance of our SVM and the two neural methods. The results indicate that the SVM setup is wellsuited for the difficulty prediction task and that it successfully generalizes to new data.",
        "sentences": [
            "The right-hand side of table 1 shows the performance of our SVM and the two neural methods.",
            "The results indicate that the SVM setup is wellsuited for the difficulty prediction task and that it successfully generalizes to new data."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "SVM (original)",
                "SVM (reproduced)"
            ],
            [
                "SVM (original)",
                "SVM (reproduced)"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_1",
        "paper_id": "P19-1035",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1036table_4",
        "description": "5.2 Result . For the purpose of experiment, operational teams (not experts) were asked to provide manual tags for a sample of 989 operational incidents. Table 4 provide the classification results of our approach when compared to those manual annotations, considering all three levels of the taxonomy. In a second step in the evaluation, an expert was given the difficult task to challenge each time they disagreed the computer and human annotation and determine which was ultimately correct. This exercise indicated that in 32 cases out of 989 operational incidents under consideration for the Level 1 classification, the machine generated category were more relevant (hence correct) than those identified by the operational team.",
        "sentences": [
            "5.2 Result .",
            "For the purpose of experiment, operational teams (not experts) were asked to provide manual tags for a sample of 989 operational incidents.",
            "Table 4 provide the classification results of our approach when compared to those manual annotations, considering all three levels of the taxonomy.",
            "In a second step in the evaluation, an expert was given the difficult task to challenge each time they disagreed the computer and human annotation and determine which was ultimately correct.",
            "This exercise indicated that in 32 cases out of 989 operational incidents under consideration for the Level 1 classification, the machine generated category were more relevant (hence correct) than those identified by the operational team."
        ],
        "class_sentence": [
            2,
            2,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Taxonomy level"
            ],
            null,
            [
                "Level 1"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_4",
        "paper_id": "P19-1036",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1041table_3",
        "description": "Table 3 presents the results of human evaluation on selected methods. Again, we see that the style embedding model (Fu et al., 2018) is ineffective as it has a very low transfer strength, and that our method outperforms other baselines in all aspects. The results are consistent with Table 2. This also implies that the automatic metrics we used are reasonable, and could be extrapolated to different models;  it also shows consistent evidence of the effectiveness of our approach.",
        "sentences": [
            "Table 3 presents the results of human evaluation on selected methods.",
            "Again, we see that the style embedding model (Fu et al., 2018) is ineffective as it has a very low transfer strength, and that our method outperforms other baselines in all aspects.",
            "The results are consistent with Table 2.",
            "This also implies that the automatic metrics we used are reasonable, and could be extrapolated to different models;  it also shows consistent evidence of the effectiveness of our approach."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "Fu et al. (2018)",
                "Ours (DAE)",
                "Ours (VAE)"
            ],
            null,
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "P19-1041",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1042table_4",
        "description": "5.1 Modeling Cross-Sentence Context . We investigate different mechanisms of integrating cross-sentence context. Table 4 shows the average single model results of our sentence-level BASELINE compared to two different strategies of integrating cross-sentence context. concat' refers to simply prepending the previous source sentences to the current source sentence. The context and the current source sentence is separated by a special token (<CONCAT>). This model does not have an auxiliary encoder. aux (no gate)' uses an auxiliary encoder similar to our CROSENT model except for gating. aux (+gate)' is our CROSENT model (Section 2.2) which employs the auxiliary encoder with the gating mechanism. The first two variants perform comparably to our sentence-level BASELINE and shows no notable gains from using cross-sentence context. When the  gating  mechanism  is  added,  results  improve substantially. Using the gating mechanism is crucial in our CROSENTmodel, as it has the ability to selectively pass information through. This shows that  properly  modeling  cross-sentence  context  is essential to improve overall performance.",
        "sentences": [
            "5.1 Modeling Cross-Sentence Context .",
            "We investigate different mechanisms of integrating cross-sentence context.",
            "Table 4 shows the average single model results of our sentence-level BASELINE compared to two different strategies of integrating cross-sentence context.",
            "concat' refers to simply prepending the previous source sentences to the current source sentence.",
            "The context and the current source sentence is separated by a special token (<CONCAT>).",
            "This model does not have an auxiliary encoder.",
            "aux (no gate)' uses an auxiliary encoder similar to our CROSENT model except for gating.",
            "aux (+gate)' is our CROSENT model (Section 2.2) which employs the auxiliary encoder with the gating mechanism.",
            "The first two variants perform comparably to our sentence-level BASELINE and shows no notable gains from using cross-sentence context.",
            "When the  gating  mechanism  is  added,  results  improve substantially.",
            "Using the gating mechanism is crucial in our CROSENTmodel, as it has the ability to selectively pass information through.",
            "This shows that  properly  modeling  cross-sentence  context  is essential to improve overall performance."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            2,
            2,
            2,
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "BASELINE"
            ],
            [
                "concat"
            ],
            null,
            null,
            [
                "aux (no gate)"
            ],
            [
                "aux (+gate)"
            ],
            [
                "BASELINE",
                "aux (no gate)",
                "concat"
            ],
            [
                "BASELINE"
            ],
            null,
            null
        ],
        "n_sentence": 12.0,
        "table_id": "table_4",
        "paper_id": "P19-1042",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1046table_5",
        "description": "Table 5 shows that in terms of the number of parameters, TFN is around 511 times larger than our HFFN, even under the situation where we adopt a more complex module after tensor fusion, demonstrating the high efficiency of HFFN. Note that if TFN adopts the original setting as stated in (Zadeh et al., 2017) where the FC layers have 128 units, it would even have more parameters than our version of TFN. Compared to BC-LSTM, HFFN has about 166 times fewer parameters and the FLOPs of HFFN is over 79 times fewer than that of BCLSTM. Moreover, BC-LSTM is over 6 times faster than TFN in time complexity measured by FLOPs and the number of parameters is over 3 times smaller. These results demonstrate that outer product applied in TFN results in heavy computational complexity and a substantial number of parameters  compared with  other methods  such as  BC-LSTM, while HFFN can avoid these two problems and is even more efficient than other approaches adopting low-complexity fusion methods.",
        "sentences": [
            "Table 5 shows that in terms of the number of parameters, TFN is around 511 times larger than our HFFN, even under the situation where we adopt a more complex module after tensor fusion, demonstrating the high efficiency of HFFN.",
            "Note that if TFN adopts the original setting as stated in (Zadeh et al., 2017) where the FC layers have 128 units, it would even have more parameters than our version of TFN.",
            "Compared to BC-LSTM, HFFN has about 166 times fewer parameters and the FLOPs of HFFN is over 79 times fewer than that of BCLSTM.",
            "Moreover, BC-LSTM is over 6 times faster than TFN in time complexity measured by FLOPs and the number of parameters is over 3 times smaller.",
            "These results demonstrate that outer product applied in TFN results in heavy computational complexity and a substantial number of parameters  compared with  other methods  such as  BC-LSTM, while HFFN can avoid these two problems and is even more efficient than other approaches adopting low-complexity fusion methods."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "TFN",
                "HFFN"
            ],
            [
                "TFN"
            ],
            [
                "HFFN",
                "FLOPs"
            ],
            [
                "TFN",
                "FLOPs"
            ],
            [
                "TFN",
                "BC-LSTM",
                "HFFN"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_5",
        "paper_id": "P19-1046",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1048table_4",
        "description": "Ablation study. To investigate the impact of different components, we start with a vanilla model which consists of f \u03b8s , f \u03b8ae , and f \u03b8as only without any informative message passing, and add other components one at a time. Table 4 shows the results of different model variants. +Opinion transmission denotes the operation of providing additional information P op to the self-attention layer j as shown in Eq.(1). +Message passing-a denotes propagating the outputs from aspect-level tasks only at each Message passing iteration. +DS and +DD denote adding DS and DD with parameter sharing only. +Message passing-d denotes involving the document-level information for message passing. We observe that +Message passing-a and +Message passing-d contribute to the performance gains the most, which demonstrates the effectiveness of the proposed message passing mechanism. We  also  observe  that  simply  adding  document-level tasks (+DS/DD) with parameter sharing only marginally improves the performance of IMN ?d. This again indicates that domain-specific knowledge has already been captured by domain embeddings, while knowledge obtained from DD and DS via parameter sharing could be redundant in this case. However, +Message passing-d is still help-ful with considerable performance gains, showingthat aspect-level tasks can benefit from knowingpredictions of the relevant document-level tasks.",
        "sentences": [
            "Ablation study.",
            "To investigate the impact of different components, we start with a vanilla model which consists of f \u03b8s , f \u03b8ae , and f \u03b8as only without any informative message passing, and add other components one at a time.",
            "Table 4 shows the results of different model variants.",
            "+Opinion transmission denotes the operation of providing additional information P op to the self-attention layer j as shown in Eq.(1).",
            "+Message passing-a denotes propagating the outputs from aspect-level tasks only at each Message passing iteration.",
            "+DS and +DD denote adding DS and DD with parameter sharing only.",
            "+Message passing-d denotes involving the document-level information for message passing.",
            "We observe that +Message passing-a and +Message passing-d contribute to the performance gains the most, which demonstrates the effectiveness of the proposed message passing mechanism.",
            "We  also  observe  that  simply  adding  document-level tasks (+DS/DD) with parameter sharing only marginally improves the performance of IMN ?d.",
            "This again indicates that domain-specific knowledge has already been captured by domain embeddings, while knowledge obtained from DD and DS via parameter sharing could be redundant in this case.",
            "However, +Message passing-d is still help-ful with considerable performance gains, showingthat aspect-level tasks can benefit from knowingpredictions of the relevant document-level tasks."
        ],
        "class_sentence": [
            2,
            2,
            1,
            2,
            2,
            2,
            2,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "+Opinion transmission"
            ],
            [
                "+Message passing-a (IMN -d)"
            ],
            [
                "+DS",
                "+DD"
            ],
            [
                "+Message passing-d (IMN)"
            ],
            [
                "+Message passing-a (IMN -d)",
                "+Message passing-d (IMN)"
            ],
            [
                "+DS",
                "+DD"
            ],
            [
                "+DS",
                "+DD"
            ],
            [
                "+Message passing-d (IMN)"
            ]
        ],
        "n_sentence": 11.0,
        "table_id": "table_4",
        "paper_id": "P19-1048",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1048table_7",
        "description": "Both IMN -d and IMN still significantly outperform other baselines in most cases under this setting. In addition, when compare the results in Table 7 and Table 3, we observe that IMN -d and IMN consistently yield better F1-I scores on all datasets in Table 3, when opinion term extraction is also considered. Consistent improvements are not observed in other baseline methods when trained with opinion term labels. These findings suggest that knowledge obtained from learning opinion term extraction is indeed beneficial, however, a carefully-designed network structure is needed to utilize such information. IMN is designed to exploit task correlations by explicitly modeling interactions between tasks, and thus it better integrates knowledge obtained from training different tasks.",
        "sentences": [
            "Both IMN -d and IMN still significantly outperform other baselines in most cases under this setting.",
            "In addition, when compare the results in Table 7 and Table 3, we observe that IMN -d and IMN consistently yield better F1-I scores on all datasets in Table 3, when opinion term extraction is also considered.",
            "Consistent improvements are not observed in other baseline methods when trained with opinion term labels.",
            "These findings suggest that knowledge obtained from learning opinion term extraction is indeed beneficial, however, a carefully-designed network structure is needed to utilize such information.",
            "IMN is designed to exploit task correlations by explicitly modeling interactions between tasks, and thus it better integrates knowledge obtained from training different tasks."
        ],
        "class_sentence": [
            2,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            [
                "IMN -d",
                "IMN"
            ],
            [
                "F1-a",
                "F1-s",
                "F1-I"
            ],
            null,
            null,
            [
                "IMN"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_7",
        "paper_id": "P19-1048",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1056table_3",
        "description": "Results on ATE. Table 3 shows the results of aspect term extraction only. DE-CNN is the current state-of-the-art model on ATE as mentioned above. Comparing with it, DOER achieves new state-of-the-art scores. DOER* denotes the DOER without ASC part. As the table shows, DOER achieves better performance than DOER*, which indicates the interaction between ATE and ASC can yield better performance for ATE than only conduct a single task.",
        "sentences": [
            "Results on ATE.",
            "Table 3 shows the results of aspect term extraction only.",
            "DE-CNN is the current state-of-the-art model on ATE as mentioned above.",
            "Comparing with it, DOER achieves new state-of-the-art scores.",
            "DOER* denotes the DOER without ASC part.",
            "As the table shows, DOER achieves better performance than DOER*, which indicates the interaction between ATE and ASC can yield better performance for ATE than only conduct a single task."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "DE-CNN"
            ],
            [
                "DOER"
            ],
            [
                "DOER*"
            ],
            [
                "DOER",
                "DOER*"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_3",
        "paper_id": "P19-1056",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1069table_5",
        "description": "In Table 5 we compare the results of IMS when trained on different corpora. As one can see, OneSeC achieves the best results on ALL when compared to automatic and semi-automatic approaches, and ranks second only with respect to SemCor. Interestingly enough, OneSeC beats its manual competitor on SemEval-2013 by 1 point and on SemEval-2015 by 4.7 points, an impressive result considering that OneSeC does not involve any human intervention during the generation of the corpus. In Table 5 we also report the statistical significance between OneSeC and its competitors on the ALL dataset by juxtaposing a \u2020\u00a0symbol next to the score. In order to do this, we computed the McNemar\u2019s chi-square test (McNemar, 1947) with significance level alpha = 0.01 between OneSeC and SemCor. It resulted in no statistical significance, meaning that IMS trained on OneSeC is in the same ballpark as when trained on SemCor. We note that the goal of this work was not to achieve state-of-the-art results on English WSD compared to manually-annotated corpora. However, performing competitively on standard benchmarks represents one step further towards getting rid of the limitation imposed by resources like SemCor. Moreover, our approach outperforms Train-O-Matic, our direct competitor, on all the datasets, with the highest increment of 3.7 points on SemEval-2007, while scoring almost 2 points higher than TOM overall.",
        "sentences": [
            "In Table 5 we compare the results of IMS when trained on different corpora.",
            "As one can see, OneSeC achieves the best results on ALL when compared to automatic and semi-automatic approaches, and ranks second only with respect to SemCor.",
            "Interestingly enough, OneSeC beats its manual competitor on SemEval-2013 by 1 point and on SemEval-2015 by 4.7 points, an impressive result considering that OneSeC does not involve any human intervention during the generation of the corpus.",
            "In Table 5 we also report the statistical significance between OneSeC and its competitors on the ALL dataset by juxtaposing a \u2020\u00a0symbol next to the score.",
            "In order to do this, we computed the McNemar\u2019s chi-square test (McNemar, 1947) with significance level alpha = 0.01 between OneSeC and SemCor.",
            "It resulted in no statistical significance, meaning that IMS trained on OneSeC is in the same ballpark as when trained on SemCor.",
            "We note that the goal of this work was not to achieve state-of-the-art results on English WSD compared to manually-annotated corpora.",
            "However, performing competitively on standard benchmarks represents one step further towards getting rid of the limitation imposed by resources like SemCor.",
            "Moreover, our approach outperforms Train-O-Matic, our direct competitor, on all the datasets, with the highest increment of 3.7 points on SemEval-2007, while scoring almost 2 points higher than TOM overall."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            2,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "OneSeC",
                "ALL",
                "SemCor"
            ],
            [
                "OneSeC",
                "SemEval-13",
                "SemEval-15"
            ],
            [
                "OneSeC",
                "ALL"
            ],
            [
                "OneSeC",
                "SemCor"
            ],
            [
                "OneSeC",
                "SemCor"
            ],
            null,
            [
                "SemCor"
            ],
            [
                "TOM"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_5",
        "paper_id": "P19-1069",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1074table_7",
        "description": "Supporting Evidence Prediction. We propose a new task to predict the supporting evidence for relation instances. On the one hand, jointly predicting the evidence provides better explainability. On the  other  hand,  identifying  supporting  evidence and  reasoning  relational  facts  from  text  are  naturally dual tasks with potential mutual enhancement. We design two supporting evidence prediction methods: (1) Heuristic predictor. We implement a simple heuristic-based model that considers all sentences containing the head or tail entity as supporting evidence. (2) Neural predictor. We also design a neural supporting evidence predictor. Given an entity pair and a predicted relation, sentences are first transformed into input representations by the concatenation of word embeddings and position embeddings, and then fed into a BiLSTM encoder for contextual representations. Inspired by Yang et al. (2018), we concatenate the output of the BiLSTM at the first and last positions with a trainable relation embedding to obtain a sentence\u00e2\u20ac\u2122s representation, which is used to predict whether the sentence is adopted as supporting evidence for the given relation instance. As Table 7 shows, the neural predictor significantly outperforms heuristic-based baseline in predicting supporting evidence, which indicates the potential of RE models in joint relation and supporting evidence prediction.",
        "sentences": [
            "Supporting Evidence Prediction.",
            "We propose a new task to predict the supporting evidence for relation instances.",
            "On the one hand, jointly predicting the evidence provides better explainability.",
            "On the  other  hand,  identifying  supporting  evidence and  reasoning  relational  facts  from  text  are  naturally dual tasks with potential mutual enhancement.",
            "We design two supporting evidence prediction methods: (1) Heuristic predictor.",
            "We implement a simple heuristic-based model that considers all sentences containing the head or tail entity as supporting evidence.",
            "(2) Neural predictor.",
            "We also design a neural supporting evidence predictor.",
            "Given an entity pair and a predicted relation, sentences are first transformed into input representations by the concatenation of word embeddings and position embeddings, and then fed into a BiLSTM encoder for contextual representations.",
            "Inspired by Yang et al. (2018), we concatenate the output of the BiLSTM at the first and last positions with a trainable relation embedding to obtain a sentence\u00e2\u20ac\u2122s representation, which is used to predict whether the sentence is adopted as supporting evidence for the given relation instance.",
            "As Table 7 shows, the neural predictor significantly outperforms heuristic-based baseline in predicting supporting evidence, which indicates the potential of RE models in joint relation and supporting evidence prediction."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            [
                "Heuristic predictor"
            ],
            null,
            [
                "Neural predictor"
            ],
            null,
            null,
            null,
            [
                "Neural predictor",
                "Heuristic predictor"
            ]
        ],
        "n_sentence": 11.0,
        "table_id": "table_7",
        "paper_id": "P19-1074",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1079table_5",
        "description": "Table 5 shows results on ATIS and our split version of Snips. We now have four tasks: ATIS, Snips-location, Snips-music, and Snips-creative. JOINT-SF-IC is our baseline that treats these four tasks independently. All other models process the four tasks together in the MTL setup. For the models introduced in this paper, we define two task groups: ATIS and Snips-location as one group, and Snips-music and Snips-creative as another. Our models, which use these groups, generally outperform the other MTL models (PARALLEL[UNIV] and PARALLEL[UNIV+TASK]); especially the serial MTL architectures perform well.",
        "sentences": [
            "Table 5 shows results on ATIS and our split version of Snips.",
            "We now have four tasks: ATIS, Snips-location, Snips-music, and Snips-creative.",
            "JOINT-SF-IC is our baseline that treats these four tasks independently.",
            "All other models process the four tasks together in the MTL setup.",
            "For the models introduced in this paper, we define two task groups: ATIS and Snips-location as one group, and Snips-music and Snips-creative as another.",
            "Our models, which use these groups, generally outperform the other MTL models (PARALLEL[UNIV] and PARALLEL[UNIV+TASK]); especially the serial MTL architectures perform well."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "ATIS",
                "Snips-location",
                "Snips-music",
                "Snips-creative"
            ],
            [
                "ATIS",
                "Snips-location",
                "Snips-music",
                "Snips-creative"
            ],
            [
                "JOINT-SF-IC"
            ],
            null,
            [
                "ATIS",
                "Snips-location",
                "Snips-music",
                "Snips-creative"
            ],
            [
                "SERIAL",
                "SERIAL+HIGHWAY",
                "SERIAL+HIGHWAY+SWAP",
                "PARALLEL[UNIV]",
                "PARALLEL[UNIV+TASK]"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_5",
        "paper_id": "P19-1079",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1079table_6",
        "description": "4.2 Alexa data. Table 6 shows the results of the single-domain model and the MTL models on the Alexa dataset. The trend is clearly visible in these results compared to the results on the benchmark data. As Alexa data has more domains, there might not be many features that are common across all the domains. Capturing those features that are only common across a group became possible by incorporating task group encoders. SERIAL+HIGHWAY+SWAP yields the best mean intent accuracy. PARALLEL+UNIV+GROUP+TASK and SERIAL+HIGHWAY show statistically indistinguishable results. For slot filling, all MTL architectures achieve competitive results on mean Slot F1.",
        "sentences": [
            "4.2 Alexa data.",
            "Table 6 shows the results of the single-domain model and the MTL models on the Alexa dataset.",
            "The trend is clearly visible in these results compared to the results on the benchmark data.",
            "As Alexa data has more domains, there might not be many features that are common across all the domains.",
            "Capturing those features that are only common across a group became possible by incorporating task group encoders.",
            "SERIAL+HIGHWAY+SWAP yields the best mean intent accuracy.",
            "PARALLEL+UNIV+GROUP+TASK and SERIAL+HIGHWAY show statistically indistinguishable results.",
            "For slot filling, all MTL architectures achieve competitive results on mean Slot F1."
        ],
        "class_sentence": [
            2,
            1,
            2,
            2,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null,
            [
                "SERIAL+HIGHWAY+SWAP"
            ],
            [
                "PARALLEL[UNIV+GROUP+TASK]",
                "SERIAL+HIGHWAY"
            ],
            [
                "Mean",
                "Slot F1"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_6",
        "paper_id": "P19-1079",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1081table_3",
        "description": "Cross-domain evaluation: Table 3 demonstrates that the DialKG Walker model can generalize to multiple domains better than the baseline approaches (train: movie & test: book / train: movie & test: music). This result indicates that our method also allows for zeroshot pruning by relations based on their proximity in the KG embeddings space, thus effective in cross-domain cases as well. For example, relations 'scenario by' and 'author' are close neighbors in the KG embeddings space, thus allowing for zeroshot prediction in cross-domain tests, although their training examples usually appear in two separate domains: movie and book.",
        "sentences": [
            "Cross-domain evaluation: Table 3 demonstrates that the DialKG Walker model can generalize to multiple domains better than the baseline approaches (train: movie & test: book / train: movie & test: music).",
            "This result indicates that our method also allows for zeroshot pruning by relations based on their proximity in the KG embeddings space, thus effective in cross-domain cases as well.",
            "For example, relations 'scenario by' and 'author' are close neighbors in the KG embeddings space, thus allowing for zeroshot prediction in cross-domain tests, although their training examples usually appear in two separate domains: movie and book."
        ],
        "class_sentence": [
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "DialKG Walker (proposed)"
            ],
            null,
            null
        ],
        "n_sentence": 3.0,
        "table_id": "table_3",
        "paper_id": "P19-1081",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1085table_4",
        "description": "The rightmost column of Table 4 shows the results of our GEAR frameworks with different sentence selection thresholds. We choose the model with threshold 10^-3, which has the highest label accuracy, as our final model. When the threshold increases from 0 to 10^-3, the label accuracy increases due to less noisy information. However, when the threshold increases from 10^-3 to 10^-1, the label accuracy decreases because informative evidence is filtered out, and the model can not obtain sufficient evidence to make the right inference.",
        "sentences": [
            "The rightmost column of Table 4 shows the results of our GEAR frameworks with different sentence selection thresholds.",
            "We choose the model with threshold 10^-3, which has the highest label accuracy, as our final model.",
            "When the threshold increases from 0 to 10^-3, the label accuracy increases due to less noisy information.",
            "However, when the threshold increases from 10^-3 to 10^-1, the label accuracy decreases because informative evidence is filtered out, and the model can not obtain sufficient evidence to make the right inference."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "GEAR LA"
            ],
            [
                "threshold",
                "10^-3"
            ],
            [
                "threshold",
                "0",
                "10^-3"
            ],
            [
                "threshold",
                "10^-3",
                "10^-1"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_4",
        "paper_id": "P19-1085",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1085table_7",
        "description": "Table 7 presents the evaluations of the full pipeline. We find the test FEVER score of BERT fine-tuning systems outperform other shared task models by nearly 1%. Furthermore, our full pipeline outperforms the BERT-Concat baseline by 1.46% and achieves significant improvements.",
        "sentences": [
            "Table 7 presents the evaluations of the full pipeline.",
            "We find the test FEVER score of BERT fine-tuning systems outperform other shared task models by nearly 1%.",
            "Furthermore, our full pipeline outperforms the BERT-Concat baseline by 1.46% and achieves significant improvements."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "FEVER",
                "BERT Pair",
                "BERT Concat"
            ],
            null
        ],
        "n_sentence": 3.0,
        "table_id": "table_7",
        "paper_id": "P19-1085",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1087table_4",
        "description": "Next, the Transformer encoder was compared against the LSTM encoder, using pre-training in both cases. Based on the performance on the development set, the best encoder was chosen which consists of two layers, each with 1024 hidden dimension and 16 attention heads. The results in Table 4 show that the LSTM-encoder outperforms the Transformer-encoder consistently in this task, when both are pre-trained. Therefore, for the rest of the experiments, we only report results using the LSTM-encoder.",
        "sentences": [
            "Next, the Transformer encoder was compared against the LSTM encoder, using pre-training in both cases.",
            "Based on the performance on the development set, the best encoder was chosen which consists of two layers, each with 1024 hidden dimension and 16 attention heads.",
            "The results in Table 4 show that the LSTM-encoder outperforms the Transformer-encoder consistently in this task, when both are pre-trained.",
            "Therefore, for the rest of the experiments, we only report results using the LSTM-encoder."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "Xformer",
                "LSTM"
            ],
            null,
            [
                "LSTM",
                "Xformer"
            ],
            [
                "LSTM"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_4",
        "paper_id": "P19-1087",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1088table_5",
        "description": "Table 5 shows the classification performance obtained when using each feature set at the time. We measure the performance of the classifiers in terms of accuracy and F-score, which provide overall and class-specific performance assessments. Compared to the majority baseline, all the feature sets demonstrate a clear improvement in the classification of counseling quality. Among all feature sets, N-grams attain the best performance, followed by discourse topics and the semantic feature sets. Furthermore, the combination of all the features sets achieve the best accuracy values.",
        "sentences": [
            "Table 5 shows the classification performance obtained when using each feature set at the time.",
            "We measure the performance of the classifiers in terms of accuracy and F-score, which provide overall and class-specific performance assessments.",
            "Compared to the majority baseline, all the feature sets demonstrate a clear improvement in the classification of counseling quality.",
            "Among all feature sets, N-grams attain the best performance, followed by discourse topics and the semantic feature sets.",
            "Furthermore, the combination of all the features sets achieve the best accuracy values."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Acc.",
                "F-score"
            ],
            [
                "Baseline"
            ],
            [
                "N-grams"
            ],
            [
                "All features",
                "Acc."
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_5",
        "paper_id": "P19-1088",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1109table_1",
        "description": "The results presented in Table 1 show that the SEQ system outperforms the CAMB system on all three genres on the task of binary complex word identification. The largest performance increase for words is on the WIKIPEDIA test set (+3.60%). Table 1 also shows that on the combined set of words and phrases (words+phrases) the two systems achieve similar results: the SEQ model beats the CAMB model only marginally, with the largest difference of +1.05% on the WIKINEWS data. However, it its worth highlighting that the CAMB system does not perform any phrase classification per se and simply marks all phrases as complex. Using the dataset statistics, we estimate that CAMB system achieves precision of 0.64. The SEQ model outperforms the CAMB system, achieving precision of 0.71.",
        "sentences": [
            "The results presented in Table 1 show that the SEQ system outperforms the CAMB system on all three genres on the task of binary complex word identification.",
            "The largest performance increase for words is on the WIKIPEDIA test set (+3.60%).",
            "Table 1 also shows that on the combined set of words and phrases (words+phrases) the two systems achieve similar results: the SEQ model beats the CAMB model only marginally, with the largest difference of +1.05% on the WIKINEWS data.",
            "However, it its worth highlighting that the CAMB system does not perform any phrase classification per se and simply marks all phrases as complex.",
            "Using the dataset statistics, we estimate that CAMB system achieves precision of 0.64.",
            "The SEQ model outperforms the CAMB system, achieving precision of 0.71."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "SEQ",
                "CAMB"
            ],
            [
                "WIKIPEDIA"
            ],
            [
                "Words+Phrases",
                "SEQ",
                "CAMB"
            ],
            [
                "CAMB"
            ],
            [
                "CAMB"
            ],
            [
                "SEQ"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_1",
        "paper_id": "P19-1109",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1112table_3",
        "description": "We are organizing a SemEval shared task on emphasis selection called \"Task 10: Emphasis Selection for Written Text in Visual Media\". In order to set out a comparable baseline for this shared task, in this section, we report results of our models according to the SemEval setting defined for the task. After the submission of this paper, we continued to improve the quality of the annotated data by cleaning the data and fixing the annotations of some noisy instances. The SemEval version of Spark dataset contains 1,200 instances with a different split: 70% training, 10% development and 20% test sets. We choose Matchm as the evaluation metric for this shared task as it provides a comprehensive evaluation compared to MAX, as one can choose the value of m. Furthermore, compared to TopK, the Matchm metric can better handle cases where multiple tokens have the same label distribution according to the annotators in the ground truth. Table 3 shows the results of all nine models under the SemEval setting, using the Matchm evaluation metric. Similar to the results we showed in Table 2, M3 and M4 both perform competitively and outperform the other models.",
        "sentences": [
            "We are organizing a SemEval shared task on emphasis selection called \"Task 10: Emphasis Selection for Written Text in Visual Media\".",
            "In order to set out a comparable baseline for this shared task, in this section, we report results of our models according to the SemEval setting defined for the task.",
            "After the submission of this paper, we continued to improve the quality of the annotated data by cleaning the data and fixing the annotations of some noisy instances.",
            "The SemEval version of Spark dataset contains 1,200 instances with a different split: 70% training, 10% development and 20% test sets.",
            "We choose Matchm as the evaluation metric for this shared task as it provides a comprehensive evaluation compared to MAX, as one can choose the value of m. Furthermore, compared to TopK, the Matchm metric can better handle cases where multiple tokens have the same label distribution according to the annotators in the ground truth.",
            "Table 3 shows the results of all nine models under the SemEval setting, using the Matchm evaluation metric.",
            "Similar to the results we showed in Table 2, M3 and M4 both perform competitively and outperform the other models."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null,
            null,
            [
                "M3",
                "M4"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_3",
        "paper_id": "P19-1112",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1119table_1",
        "description": "3.3 Analysis.  The empirical results in this section show that the quality of pre-trained UBWE is important to UNMT. However, the quality of UBWE decreases significantly during UNMT training. We hypothesize that maintaining the quality of UBWE may enhance the performance of UNMT. In this subsection, we analyze some possible solutions to this issue. Use fixed embedding?. As Figure 2 shows, the UBWE performance decreases significantly during the UNMT training process. Therefore, we try to fix the embedding of the encoder and decoder on the basis of the original baseline system (Baseline-fix). Table 1 shows that the performance of the Baseline-fix system is quite similar to that of the original baseline system. In other words, Baseline-fix prevents the degradation of UBWE accuracy; however, the fixed embedding also prevents UBWE from further improving UNMT training. Therefore, the fixed UBWE does not enhance the performance of UNMT.",
        "sentences": [
            "3.3 Analysis.",
            " The empirical results in this section show that the quality of pre-trained UBWE is important to UNMT.",
            "However, the quality of UBWE decreases significantly during UNMT training.",
            "We hypothesize that maintaining the quality of UBWE may enhance the performance of UNMT.",
            "In this subsection, we analyze some possible solutions to this issue.",
            "Use fixed embedding?.",
            "As Figure 2 shows, the UBWE performance decreases significantly during the UNMT training process.",
            "Therefore, we try to fix the embedding of the encoder and decoder on the basis of the original baseline system (Baseline-fix).",
            "Table 1 shows that the performance of the Baseline-fix system is quite similar to that of the original baseline system.",
            "In other words, Baseline-fix prevents the degradation of UBWE accuracy; however, the fixed embedding also prevents UBWE from further improving UNMT training.",
            "Therefore, the fixed UBWE does not enhance the performance of UNMT."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            [
                "Baseline-fix"
            ],
            [
                "Baseline-fix",
                "Baseline"
            ],
            [
                "Baseline-fix"
            ],
            null
        ],
        "n_sentence": 11.0,
        "table_id": "table_1",
        "paper_id": "P19-1119",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1120table_2",
        "description": "Table 2 presents the results. Plain transfer learning already gives a boost but is still far from a satisfying quality, especially for Basque\u00ae-English and Azerbaijani\u00aeEnglish. On top of that, each of our three techniques offers clear, incremental improvements in all child language pairs with a maximum of 5.1% BLEU in total.",
        "sentences": [
            "Table 2 presents the results.",
            "Plain transfer learning already gives a boost but is still far from a satisfying quality, especially for Basque\u00ae-English and Azerbaijani\u00aeEnglish.",
            "On top of that, each of our three techniques offers clear, incremental improvements in all child language pairs with a maximum of 5.1% BLEU in total."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Transfer (Zoph et al. 2016)",
                "be-en",
                "az-en"
            ],
            [
                " + Cross-lingual word embedding",
                " + Artificial noises",
                " + Synthetic data",
                "BLEU (%)"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_2",
        "paper_id": "P19-1120",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1120table_5",
        "description": "Table 5 estimates how large the vocabulary should be for the language-switching side in NMT transfer. We varied the number of BPE merges on the source side, fixing the target vocabulary to 50k merges. The best results are with 10k or 20k of BPE merges, which shows that the source vocabulary should be reasonably small to maximize the transfer performance. Less BPE merges lead to more language-independent tokens; it is easier for the cross-lingual embedding to find the overlaps in the shared semantic space.",
        "sentences": [
            "Table 5 estimates how large the vocabulary should be for the language-switching side in NMT transfer.",
            "We varied the number of BPE merges on the source side, fixing the target vocabulary to 50k merges.",
            "The best results are with 10k or 20k of BPE merges, which shows that the source vocabulary should be reasonably small to maximize the transfer performance.",
            "Less BPE merges lead to more language-independent tokens; it is easier for the cross-lingual embedding to find the overlaps in the shared semantic space."
        ],
        "class_sentence": [
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "50k"
            ],
            [
                "10k",
                "20k"
            ],
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_5",
        "paper_id": "P19-1120",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1127table_1",
        "description": "Same as (Su et al., 2018), we use PCNN+ATT (Lin et al., 2016) as our base model. GloRE++ improves its best F1-score from 42.7% to 45.2%, slightly outperforming the previous state-of-theart (GloRE, 44.7%). As shown in previous work (Su et al., 2018), on NYT dataset, due to a significant amount of false negatives, the PR curve on the held-out set may not be an accurate measure of performance. Therefore, we mainly employ manual evaluation. We invite graduate students to check top 1000 predictions of each method. They are present with the entity pair, the prediction, and all the contextual sentences of the entity pair. Each prediction is examined by two students until reaching an agreement after discussion. Besides, the students are not aware of the source of the predictions. Table 1 shows the manual evaluation results. Both GloRE+ and GloRE++ get improvements over GloRE. GloRE++ obtains the best results for top 700, 900 and 1000 predictions.",
        "sentences": [
            "Same as (Su et al., 2018), we use PCNN+ATT (Lin et al., 2016) as our base model.",
            "GloRE++ improves its best F1-score from 42.7% to 45.2%, slightly outperforming the previous state-of-theart (GloRE, 44.7%).",
            "As shown in previous work (Su et al., 2018), on NYT dataset, due to a significant amount of false negatives, the PR curve on the held-out set may not be an accurate measure of performance.",
            "Therefore, we mainly employ manual evaluation.",
            "We invite graduate students to check top 1000 predictions of each method.",
            "They are present with the entity pair, the prediction, and all the contextual sentences of the entity pair.",
            "Each prediction is examined by two students until reaching an agreement after discussion.",
            "Besides, the students are not aware of the source of the predictions.",
            "Table 1 shows the manual evaluation results.",
            "Both GloRE+ and GloRE++ get improvements over GloRE.",
            "GloRE++ obtains the best results for top 700, 900 and 1000 predictions."
        ],
        "class_sentence": [
            2,
            1,
            2,
            2,
            2,
            2,
            2,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "PCNN+ATT"
            ],
            [
                "PCNN+ATT+GloRE++",
                "PCNN+ATT+GloRE"
            ],
            null,
            null,
            [
                "1000"
            ],
            null,
            null,
            null,
            null,
            [
                "PCNN+ATT+GloRE+",
                "PCNN+ATT+GloRE++",
                "PCNN+ATT+GloRE"
            ],
            [
                "PCNN+ATT+GloRE++",
                "700",
                "900",
                "1000"
            ]
        ],
        "n_sentence": 11.0,
        "table_id": "table_1",
        "paper_id": "P19-1127",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1130table_1",
        "description": "Results. From Table 1, we can see both BIO tag embeddings and multi-task learning can improve the performance of the baseline model. Baseline+Tag can outperform the state-ofthe-art models on both the Chinese and English corpus. Compared to the baseline model, BIO tag embeddings lead to an absolute increase of about 10% in F1-score, which indicates that BIO tag embeddings are very effective. Multi-task learning can yield further improvement in addition to BIO tag embeddings: Baseline+MTL+Tag achieves the highest F1-score on both corpora.",
        "sentences": [
            "Results.",
            "From Table 1, we can see both BIO tag embeddings and multi-task learning can improve the performance of the baseline model.",
            "Baseline+Tag can outperform the state-ofthe-art models on both the Chinese and English corpus.",
            "Compared to the baseline model, BIO tag embeddings lead to an absolute increase of about 10% in F1-score, which indicates that BIO tag embeddings are very effective.",
            "Multi-task learning can yield further improvement in addition to BIO tag embeddings: Baseline+MTL+Tag achieves the highest F1-score on both corpora."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Baseline+MTL+Tag"
            ],
            [
                "Baseline+Tag"
            ],
            null,
            [
                "Baseline+MTL+Tag",
                "F1%"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_1",
        "paper_id": "P19-1130",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1136table_1",
        "description": "5.3 Quantitative Results . Table 1 presents the precision, recall, and F1 score of NovelTagging, MultiDecoder, and GraphRel for both the NYT and WebNLG datasets. OneDecoder, proposed in MultiDecoder\u00d5s original paper, uses only a single decoder to extract relation triplets. GraphRel1p is the proposed method but only 1st-phase, and GraphRel2p is the complete version, which predicts relations and entities after the 2nd-phase. For the NYT dataset, we see that GraphRel1-hop outperforms NovelTagging by 18.0%, OneDecoder by 4.0%, and MultiDecoder by 1.3% in terms of F1.  As it acquires both sequential and regional dependency word features, GraphRel1-hop performs better on both precision and recall, resulting in a higher F1 score.  With relation weighted GCN in 2nd-phase, GraphRel2p, which considers interaction between name entities and relations, further surpasses MultiDecoder by 3.2% and yields a 1.9% improvement in comparison with GraphRel1p.",
        "sentences": [
            "5.3 Quantitative Results .",
            "Table 1 presents the precision, recall, and F1 score of NovelTagging, MultiDecoder, and GraphRel for both the NYT and WebNLG datasets.",
            "OneDecoder, proposed in MultiDecoder\u00d5s original paper, uses only a single decoder to extract relation triplets.",
            "GraphRel1p is the proposed method but only 1st-phase, and GraphRel2p is the complete version, which predicts relations and entities after the 2nd-phase.",
            "For the NYT dataset, we see that GraphRel1-hop outperforms NovelTagging by 18.0%, OneDecoder by 4.0%, and MultiDecoder by 1.3% in terms of F1.",
            " As it acquires both sequential and regional dependency word features, GraphRel1-hop performs better on both precision and recall, resulting in a higher F1 score.",
            " With relation weighted GCN in 2nd-phase, GraphRel2p, which considers interaction between name entities and relations, further surpasses MultiDecoder by 3.2% and yields a 1.9% improvement in comparison with GraphRel1p."
        ],
        "class_sentence": [
            2,
            1,
            2,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "NovelTagging",
                "MultiDecoder",
                "GraphRel1p",
                "GraphRel2p",
                "Precision",
                "Recall",
                "F1"
            ],
            [
                "OneDecoder",
                "MultiDecoder"
            ],
            [
                "GraphRel1p",
                "GraphRel2p"
            ],
            [
                "NYT",
                "GraphRel1p",
                "NovelTagging",
                "OneDecoder",
                "MultiDecoder"
            ],
            [
                "GraphRel1p",
                "Precision",
                "Recall",
                "F1"
            ],
            [
                "GraphRel2p",
                "MultiDecoder",
                "GraphRel1p"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_1",
        "paper_id": "P19-1136",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1137table_3",
        "description": "4.3 Pattern-based Diagnostic Results . Besides for improving the extraction performance, DIAG-NRE can interpret different noise effects caused by DS via refined patterns, as Table 3 shows. Next, we elaborate these diagnostic results and the corresponding performance degradation of NRE models from two perspectives: false negatives (FN) and false positives (FP).",
        "sentences": [
            "4.3 Pattern-based Diagnostic Results .",
            "Besides for improving the extraction performance, DIAG-NRE can interpret different noise effects caused by DS via refined patterns, as Table 3 shows.",
            "Next, we elaborate these diagnostic results and the corresponding performance degradation of NRE models from two perspectives: false negatives (FN) and false positives (FP)."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null
        ],
        "n_sentence": 3.0,
        "table_id": "table_3",
        "paper_id": "P19-1137",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1140table_3",
        "description": "5.2 Overall Performance. Results on Table 3 shows DBP15K and DWY100K. In general, MuGNN significantly outperforms all baselines regarding all metrics, mainly because it reconciles the structural differences by two different schemes for KG completion and pruning, which are thus well modeled in multi-channel GNN.",
        "sentences": [
            "5.2 Overall Performance.",
            "Results on Table 3 shows DBP15K and DWY100K.",
            "In general, MuGNN significantly outperforms all baselines regarding all metrics, mainly because it reconciles the structural differences by two different schemes for KG completion and pruning, which are thus well modeled in multi-channel GNN."
        ],
        "class_sentence": [
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "MuGNN"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_3",
        "paper_id": "P19-1140",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1147table_3",
        "description": "The human accuracy metric is the percentage of human responses that matches the true label. Table 3 is a comparison of LSTM and BERT models using the GS-EC attack. It shows that the distance in embeddings space of BERT can better reflect semantic similarity and contribute to more natural adversarial examples. And, in Table 4, we compare using GS-GR and GS-EC method on BERT model. Again, we see that the GS-EC method, which restricts the distance between sentence embeddings of original and adversarial inputs, can produce superior adversaries.",
        "sentences": [
            "The human accuracy metric is the percentage of human responses that matches the true label.",
            "Table 3 is a comparison of LSTM and BERT models using the GS-EC attack.",
            "It shows that the distance in embeddings space of BERT can better reflect semantic similarity and contribute to more natural adversarial examples.",
            "And, in Table 4, we compare using GS-GR and GS-EC method on BERT model.",
            "Again, we see that the GS-EC method, which restricts the distance between sentence embeddings of original and adversarial inputs, can produce superior adversaries."
        ],
        "class_sentence": [
            2,
            1,
            1,
            0,
            0
        ],
        "header_mention": [
            null,
            [
                "LSTM",
                "BERT"
            ],
            [
                "BERT"
            ],
            null,
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_3",
        "paper_id": "P19-1147",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1153table_2",
        "description": "We present in Table 2 results for fine-grained sentiment analysis on all nodes as well as comparison with recent state-of-the-art methods on binary sentiment classification of the root node. For the five class sentiment task, we compare our model with the original Sentiment Treebank results and beat all the models. In order to compare our approach with state-of-the-art methods, we also trained our model on the binary classification task with sole classification of the root node. Other presented models are GenSen (Subramanian et al., 2018) and BERTBASE (Devlin et al., 2018). Both these recent methods perform extremely well on multiple natural language processing tasks. We set the RAE embedding size demb to 1024. Larger embedding sizes did not improve the accuracy of our model for this task. In this setting, the RAE has 11M parameters, while the models we compare with, GenSen and BERTBASE, have respectively 100M and 110M parameters. Both our model and GenSen fail to beat the RNTN model for the SST-2 task. We see an improvement in accuracy when combining both methods' embeddings, surpassing every model in the SST paper, while being close to BERTBASE's performance.",
        "sentences": [
            "We present in Table 2 results for fine-grained sentiment analysis on all nodes as well as comparison with recent state-of-the-art methods on binary sentiment classification of the root node.",
            "For the five class sentiment task, we compare our model with the original Sentiment Treebank results and beat all the models.",
            "In order to compare our approach with state-of-the-art methods, we also trained our model on the binary classification task with sole classification of the root node.",
            "Other presented models are GenSen (Subramanian et al., 2018) and BERTBASE (Devlin et al., 2018).",
            "Both these recent methods perform extremely well on multiple natural language processing tasks.",
            "We set the RAE embedding size demb to 1024.",
            "Larger embedding sizes did not improve the accuracy of our model for this task.",
            "In this setting, the RAE has 11M parameters, while the models we compare with, GenSen and BERTBASE, have respectively 100M and 110M parameters.",
            "Both our model and GenSen fail to beat the RNTN model for the SST-2 task.",
            "We see an improvement in accuracy when combining both methods' embeddings, surpassing every model in the SST paper, while being close to BERTBASE's performance."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            1,
            2,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            [
                "GenSen",
                "BERTBASE"
            ],
            [
                "RAE"
            ],
            null,
            [
                "RAE",
                "GenSen",
                "BERTBASE"
            ],
            [
                "GenSen",
                "RNTN",
                "SST-2(Root)"
            ],
            [
                "BERTBASE"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_2",
        "paper_id": "P19-1153",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1173table_3",
        "description": "Table 3 shows the results. Multitask learning with MSA consistently outperforms the models that use EGY data only. The accuracy almost doubles in the 2K model. We also notice that the accuracy gap increases as the EGY training dataset size decreases, highlighting the importance of joint modeling with MSA in low-resource DA settings. The adversarial adaptation results in the learning curve further show a significant increase in accuracy with decreasing training data size, compared to the multitask learning results. The model seems to be facilitating more efficient knowledgetransfer, especially for the lower-resource EGY experiments. We can also observe that for the extreme low-resource setting, we can double the accuracy through adversarial multitask learning, achieving about 58% relative error reduction.",
        "sentences": [
            "Table 3 shows the results.",
            "Multitask learning with MSA consistently outperforms the models that use EGY data only.",
            "The accuracy almost doubles in the 2K model.",
            "We also notice that the accuracy gap increases as the EGY training dataset size decreases, highlighting the importance of joint modeling with MSA in low-resource DA settings.",
            "The adversarial adaptation results in the learning curve further show a significant increase in accuracy with decreasing training data size, compared to the multitask learning results.",
            "The model seems to be facilitating more efficient knowledgetransfer, especially for the lower-resource EGY experiments.",
            "We can also observe that for the extreme low-resource setting, we can double the accuracy through adversarial multitask learning, achieving about 58% relative error reduction."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "MTL",
                "MSA-EGY",
                "EGY"
            ],
            [
                "2K (1.5%)"
            ],
            [
                "EGY Train Size",
                "MSA-EGY"
            ],
            [
                "ADV"
            ],
            null,
            [
                "ADV"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_3",
        "paper_id": "P19-1173",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1175table_6",
        "description": "5.3 Test set evaluation Table 6 contains the results for EN-NL for the entire test set (3207 sentences). The dedicated NFR + NMT backoff approach outperforms all baseline systems, scoring +3.19 BLEU, -3.6 TER and +1.87 METEOR points compared to the best baseline (TM-SMT). Compared to the NMT baseline, the difference is 7.46 BLEU points. The best unified NFR system (Unified F3) scores only slightly worse than the approach with a dedicated NFR system and NMT backoff. Both NFR systems score significantly higher than the best baseline in terms of BLEU (p < 0.001). We note that the baseline SMT outperforms the baseline NMT, which in turn obtains better scores than Google Translate on this data set.",
        "sentences": [
            "5.3 Test set evaluation Table 6 contains the results for EN-NL for the entire test set (3207 sentences).",
            "The dedicated NFR + NMT backoff approach outperforms all baseline systems, scoring +3.19 BLEU, -3.6 TER and +1.87 METEOR points compared to the best baseline (TM-SMT).",
            "Compared to the NMT baseline, the difference is 7.46 BLEU points.",
            "The best unified NFR system (Unified F3) scores only slightly worse than the approach with a dedicated NFR system and NMT backoff.",
            "Both NFR systems score significantly higher than the best baseline in terms of BLEU (p < 0.001).",
            "We note that the baseline SMT outperforms the baseline NMT, which in turn obtains better scores than Google Translate on this data set."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Best NFR + NMT backoff",
                "BLEU",
                "TER",
                "MET."
            ],
            [
                "Baseline NMT",
                "BLEU"
            ],
            [
                "Best NFR unified",
                "Best NFR + NMT backoff"
            ],
            [
                "Best NFR + NMT backoff",
                "Best NFR unified"
            ],
            [
                "Baseline SMT",
                "Baseline NMT"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_6",
        "paper_id": "P19-1175",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1183table_4",
        "description": "Table 4 shows the results. Similarly, the proposed attentive interactor model (without the diversity loss) outperforms all the baselines. Moreover, the diversity loss further improves the performance. Note that the improvement of our model on this dataset is more significant than that on the VID-sentence dataset. The reason might be that the upper bound performance of the Personsentence is much higher than that of the VIDsentence (77.9 for Person-sentence versus 47.6 for VID-sentence on average). This also suggests that the created VID-sentence dataset is more challenging and more suitable as a benchmark dataset.",
        "sentences": [
            "Table 4 shows the results.",
            "Similarly, the proposed attentive interactor model (without the diversity loss) outperforms all the baselines.",
            "Moreover, the diversity loss further improves the performance.",
            "Note that the improvement of our model on this dataset is more significant than that on the VID-sentence dataset.",
            "The reason might be that the upper bound performance of the Personsentence is much higher than that of the VIDsentence (77.9 for Person-sentence versus 47.6 for VID-sentence on average).",
            "This also suggests that the created VID-sentence dataset is more challenging and more suitable as a benchmark dataset."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Ours"
            ],
            [
                "Ours w/o L div"
            ],
            [
                "Ours"
            ],
            [
                "Proposal upper bound"
            ],
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_4",
        "paper_id": "P19-1183",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1185table_1",
        "description": "7.1 Continual Learning on GLUE Tasks Baseline Models: We use bidirectional LSTMRNN encoders with max-pooling (Conneau et al., 2017) as our baseline. Further, we used the ELMo embeddings (Peters et al., 2018) as input to the encoders, where we allowed to train the weights on each layer of ELMo to get a final representation. Table 1 shows that our baseline models achieve strong results when compared with GLUE benchmark baselines (Wang et al., 2018). On top of these strong baselines, we add ENAS approach. ENAS Models: Next, Table 1 shows that our ENAS models (for all three tasks QNLI, RTE, WNLI) perform better or equal than the nonarchitecture search based models. Note that we only replace the LSTM-RNN cell with our ENAS cell, rest of the model architecture in ENAS model is same as our baseline model. CAS Models: Next, we apply our continual architecture search (CAS) approach on QNLI, RTE, and WNLI, where we sequentially allow the model to learn QNLI, RTE, and WNLI (in the order of decreasing dataset size, following standard transfer setup practice) and the results are as shown in Table 1. We train on QNLI task, RTE task, and WNLI task in step-1, step-2, and step-3, respectively. We observe that even though we learn the models sequentially, we are able to maintain performance on the previously-learned QNLI task in step-2 (74.1 vs. 74.2 on validation set which is statistically equal, and 73.6 vs. 73.8 on test). Note that if we remove our sparsity and orthogonality conditions (Sec. 4), the step-2 QNLI performance drops from 74.1 to 69.1 on validation set, demonstrating the importance of our conditions for CAS (see next paragraph on \u2018CAS Condition Ablation\u2019 for more details). Next, we observe a similar pattern when we extend CAS to the WNLI dataset (see step-3 in Table 1), i.e, we are still able to maintain the performance on QNLI (as well as RTE now) from step-2 to step-3 (scores are statistically equal on validation set). Further, if we compare the performance of QNLI from step-1 to step-3, we see that they are also stat. equal on val set (73.9 vs. 74.2). This shows that our CAS method can maintain the performance of a task in a continual learning setting with several steps.",
        "sentences": [
            "7.1 Continual Learning on GLUE Tasks Baseline Models: We use bidirectional LSTMRNN encoders with max-pooling (Conneau et al., 2017) as our baseline.",
            "Further, we used the ELMo embeddings (Peters et al., 2018) as input to the encoders, where we allowed to train the weights on each layer of ELMo to get a final representation.",
            "Table 1 shows that our baseline models achieve strong results when compared with GLUE benchmark baselines (Wang et al., 2018).",
            "On top of these strong baselines, we add ENAS approach.",
            "ENAS Models: Next, Table 1 shows that our ENAS models (for all three tasks QNLI, RTE, WNLI) perform better or equal than the nonarchitecture search based models.",
            "Note that we only replace the LSTM-RNN cell with our ENAS cell, rest of the model architecture in ENAS model is same as our baseline model.",
            "CAS Models: Next, we apply our continual architecture search (CAS) approach on QNLI, RTE, and WNLI, where we sequentially allow the model to learn QNLI, RTE, and WNLI (in the order of decreasing dataset size, following standard transfer setup practice) and the results are as shown in Table 1.",
            "We train on QNLI task, RTE task, and WNLI task in step-1, step-2, and step-3, respectively.",
            "We observe that even though we learn the models sequentially, we are able to maintain performance on the previously-learned QNLI task in step-2 (74.1 vs. 74.2 on validation set which is statistically equal, and 73.6 vs. 73.8 on test).",
            "Note that if we remove our sparsity and orthogonality conditions (Sec. 4), the step-2 QNLI performance drops from 74.1 to 69.1 on validation set, demonstrating the importance of our conditions for CAS (see next paragraph on \u2018CAS Condition Ablation\u2019 for more details).",
            "Next, we observe a similar pattern when we extend CAS to the WNLI dataset (see step-3 in Table 1), i.e, we are still able to maintain the performance on QNLI (as well as RTE now) from step-2 to step-3 (scores are statistically equal on validation set).",
            "Further, if we compare the performance of QNLI from step-1 to step-3, we see that they are also stat. equal on val set (73.9 vs. 74.2).",
            "This shows that our CAS method can maintain the performance of a task in a continual learning setting with several steps."
        ],
        "class_sentence": [
            2,
            2,
            1,
            2,
            1,
            2,
            1,
            2,
            1,
            2,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "Baseline (with ELMo)"
            ],
            [
                "ENAS (Architecture Search)"
            ],
            [
                "ENAS (Architecture Search)",
                "Baseline (with ELMo)"
            ],
            [
                "ENAS (Architecture Search)"
            ],
            [
                "CAS Step-1 (QNLI training)",
                "CAS Step-2 (RTE training)",
                "CAS Step-3 (WNLI training)"
            ],
            [
                "CAS Step-1 (QNLI training)",
                "CAS Step-2 (RTE training)",
                "CAS Step-3 (WNLI training)"
            ],
            [
                "CAS Step-2 (RTE training)"
            ],
            null,
            null,
            null,
            null
        ],
        "n_sentence": 13.0,
        "table_id": "table_1",
        "paper_id": "P19-1185",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1186table_3",
        "description": "Next, we consider the impact of using different combinations of F and Y. Table 3 shows the performance of difference configurations. Overall, F + Y gives excellent performance. Interestingly, Y on its own is only a little worse than only F, showing that target labels y are more important for learning than the domain d. The Y configuration fully domain unsupervised training still results in decent performance, boding well for application to very messy and heterogenous datasets with no domain metadata.",
        "sentences": [
            "Next, we consider the impact of using different combinations of F and Y.",
            "Table 3 shows the performance of difference configurations.",
            "Overall, F + Y gives excellent performance.",
            "Interestingly, Y on its own is only a little worse than only F, showing that target labels y are more important for learning than the domain d.",
            "The Y configuration fully domain unsupervised training still results in decent performance, boding well for application to very messy and heterogenous datasets with no domain metadata."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "F+Y"
            ],
            null,
            [
                "F+Y"
            ],
            [
                "Y",
                "F"
            ],
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_3",
        "paper_id": "P19-1186",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1193table_2",
        "description": "Table 2 presents the human evaluation results, from which we can draw similar conclusions. It is obvious that our approach can outperform the baselines by a large margin, especially in terms of diversity and topic-consistency. For example, the proposed model achieves improvements of 15.33% diversity score and 12.28% consistency score over the best baseline. The main reason for this increase in diversity is that we integrate commonsense knowledge into the generator through the memory mechanism. This external commonsense knowledge provides additional background information, making the generated essays more novel and diverse. In addition, the adversarial training is employed to increase the coverage of the output on the target topics, which further enhances the topic-consistency.",
        "sentences": [
            "Table 2 presents the human evaluation results, from which we can draw similar conclusions.",
            "It is obvious that our approach can outperform the baselines by a large margin, especially in terms of diversity and topic-consistency.",
            "For example, the proposed model achieves improvements of 15.33% diversity score and 12.28% consistency score over the best baseline.",
            "The main reason for this increase in diversity is that we integrate commonsense knowledge into the generator through the memory mechanism.",
            "This external commonsense knowledge provides additional background information, making the generated essays more novel and diverse.",
            "In addition, the adversarial training is employed to increase the coverage of the output on the target topics, which further enhances the topic-consistency."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "Proposal",
                "Diversity",
                "Consistency"
            ],
            [
                "Proposal",
                "Diversity",
                "Consistency"
            ],
            null,
            null,
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_2",
        "paper_id": "P19-1193",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1193table_3",
        "description": "Memory mechanism. We find that the memory mechanism can significantly improve the novelty and diversity. As is shown in Table 3, compared to the removal of the adversarial training, the model exhibits larger degradation in terms of novelty and diversity when the memory mechanism is removed. This shows that with the help of external commonsense knowledge, the source information can be enriched, leading to the outputs that are more novel and diverse.",
        "sentences": [
            "Memory mechanism.",
            "We find that the memory mechanism can significantly improve the novelty and diversity.",
            "As is shown in Table 3, compared to the removal of the adversarial training, the model exhibits larger degradation in terms of novelty and diversity when the memory mechanism is removed.",
            "This shows that with the help of external commonsense knowledge, the source information can be enriched, leading to the outputs that are more novel and diverse."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "w/o Adversarial Training",
                "w/o Memory",
                "Novelty",
                "Dist-1",
                "Dist-2"
            ],
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "P19-1193",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1195table_7",
        "description": "Results on the Development Set . Table 7 (top) shows results on the ROTOWIRE development set for our dynamic entity memory model (ENT), the best system of Wiseman et al. (2017) (WS-2017) which is an encoder-decoder model with conditional copy, the template generator (TEMPL), our implementation of encoder-decoder model with conditional copy (ED+CC), and NCP+CC (Puduppully et al., 2019). We see that ENT achieves scores comparable to NCP+CC, but performs better on the metrics of RG precision, CS precision, and CO. Table 7 (bottom) also presents our results on MLB. ENT achieves highest BLEU amongst all models and highest CS recall and RG count amongst neural models.",
        "sentences": [
            "Results on the Development Set .",
            "Table 7 (top) shows results on the ROTOWIRE development set for our dynamic entity memory model (ENT), the best system of Wiseman et al. (2017) (WS-2017) which is an encoder-decoder model with conditional copy, the template generator (TEMPL), our implementation of encoder-decoder model with conditional copy (ED+CC), and NCP+CC (Puduppully et al., 2019).",
            "We see that ENT achieves scores comparable to NCP+CC, but performs better on the metrics of RG precision, CS precision, and CO. Table 7 (bottom) also presents our results on MLB. ENT achieves highest BLEU amongst all models and highest CS recall and RG count amongst neural models."
        ],
        "class_sentence": [
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "TEMPL",
                "WS-2017",
                "ED+CC",
                "NCP+CC",
                "ENT"
            ],
            [
                "ENT"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_7",
        "paper_id": "P19-1195",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1197table_1",
        "description": "3.5 Results. We compare our PIVOT model with the above baseline models. Table 1 summarizes the results of these models. It shows that our PIVOT model achieves 87.92% F1 score, 92.59% precision, and 83.70% recall at the stage of key fact prediction, which provides a good foundation for the stage of surface realization. Based on the selected key facts, our models achieve the scores of 20.09 BLEU, 6.5130 NIST, and 18.31 ROUGE under the vanilla Seq2Seq framework, and 27.34 BLEU, 6.8763 NIST, and 19.30 ROUGE under the Transformer framework, which significantly outperform all the baseline models in terms of all metrics. Furthermore, it shows that the implementation with the Transformer can obtain higher scores than that with the vanilla Seq2Seq.",
        "sentences": [
            "3.5 Results.",
            "We compare our PIVOT model with the above baseline models.",
            "Table 1 summarizes the results of these models.",
            "It shows that our PIVOT model achieves 87.92% F1 score, 92.59% precision, and 83.70% recall at the stage of key fact prediction, which provides a good foundation for the stage of surface realization.",
            "Based on the selected key facts, our models achieve the scores of 20.09 BLEU, 6.5130 NIST, and 18.31 ROUGE under the vanilla Seq2Seq framework, and 27.34 BLEU, 6.8763 NIST, and 19.30 ROUGE under the Transformer framework, which significantly outperform all the baseline models in terms of all metrics.",
            "Furthermore, it shows that the implementation with the Transformer can obtain higher scores than that with the vanilla Seq2Seq."
        ],
        "class_sentence": [
            2,
            0,
            1,
            0,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "PIVOT-Vanilla",
                "PIVOT-Trans"
            ],
            null,
            null,
            [
                "PIVOT-Vanilla",
                "BLEU",
                "NIST",
                "ROUGE",
                "PIVOT-Trans"
            ],
            [
                "PIVOT-Trans",
                "PIVOT-Vanilla"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_1",
        "paper_id": "P19-1197",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1204table_2",
        "description": "Automatic Evaluation . Table 2 summarizes the results: both GCN CITED TEXT SPANS and TALKSUMM-ONLY models, are not able to obtain better performance than ABSTRACT8. However, for the Hybrid approach, where the abstract is augmented with sentences from the summaries emitted by the models, our TALKSUMM-HYBRID outperforms both GCN HYBRID 2 and ABSTRACT. Importantly, our model, trained on automaticallygenerated summaries, performs on par with models trained over SCISUMMNET, in which training data was created manually.",
        "sentences": [
            "Automatic Evaluation .",
            "Table 2 summarizes the results: both GCN CITED TEXT SPANS and TALKSUMM-ONLY models, are not able to obtain better performance than ABSTRACT8.",
            "However, for the Hybrid approach, where the abstract is augmented with sentences from the summaries emitted by the models, our TALKSUMM-HYBRID outperforms both GCN HYBRID 2 and ABSTRACT.",
            "Importantly, our model, trained on automaticallygenerated summaries, performs on par with models trained over SCISUMMNET, in which training data was created manually."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "GCN CITED TEXT SPANS*",
                "TALKSUMM-ONLY"
            ],
            [
                "TALKSUMM-HYBRID",
                "GCN HYBRID2*",
                "ABSTRACT*"
            ],
            [
                "TALKSUMM-HYBRID",
                "TALKSUMM-ONLY"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "P19-1204",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1206table_2",
        "description": "4.4 Evaluation of Summary Generation . Table 2 shows the ROUGE scores of our models and the baselines for the evaluation sets. With regards to Toys & Games and Sports & Outdoors, our full model (StrSum + DiscourseRank) achieves the best ROUGE scores among the unsupervised approaches.",
        "sentences": [
            "4.4 Evaluation of Summary Generation .",
            "Table 2 shows the ROUGE scores of our models and the baselines for the evaluation sets.",
            "With regards to Toys & Games and Sports & Outdoors, our full model (StrSum + DiscourseRank) achieves the best ROUGE scores among the unsupervised approaches."
        ],
        "class_sentence": [
            0,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "R-1",
                "R-2",
                "R-L"
            ],
            [
                "StrSum+DiscourseRank",
                "R-1",
                "R-2",
                "R-L"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_2",
        "paper_id": "P19-1206",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1209table_2",
        "description": "Extraction Results . In Table 2 we present intance selection results for the CNN/DM, XSum, and DUC-04 datasets. Our method builds representations for instances using either BERT or VSM (\u00a73.1). To ensure a thorough comparison, we experiment with selecting a mixed set of singletons and pairs (\u201cSingPairMix\u201d) as well as selecting singletons only (\u201cSingOnly\u201d). On the CNN/DM and XSum datasets, we observe that selecting a mixed set of singletons and pairs based on BERT representations (BERT+SingPairMix) demonstrates the most competitive results. It outperforms a number of strong baselines when evaluated on a full set of ground-truth sentences. The method also performs superiorly on identifying secondary sentences. For example, it increases recall scores for identifying secondary sentences from 33.8% to 69.8% (CNN/DM) and from 16.7% to 65.3% (XSum). Our method is able to achieve strong performance on instance selection owing to BERT\u2019s capability of building effective representations for both singletons and pairs. It learns to identify salient source content based on token and position embeddings and it encodes sentential semantic compatibility using the pretraining task of predicting the next sentence; both are valuable additions to summary instance selection. Further, we observe that identifying summaryworthy singletons and pairs from multi-document inputs (DUC-04) appears to be more challenging than that of single-document inputs (XSum and CNN/DM). This distinction is not surprising given that for multi-document inputs, the system has a large and diverse search space where candidate singletons and pairs are gathered from a set of documents written by different authors.6 . We find that the BERT model performs consistently on identifying secondary sentences, and VSM yields considerable performance gain on selecting primary sentences. Both BERT and VSM models are trained on the CNN/DM dataset and applied to DUC-04 as the latter data are only used for testing. Our findings suggest that the TF-IDF features of the VSM model are effective for multi-document inputs, as important topic words are usually repeated across documents and TF-IDF scores can reflect topical importance of words. This analysis further reveals that extending BERT to incorporate topical salience of words can be a valuable line of research for future work.",
        "sentences": [
            "Extraction Results .",
            "In Table 2 we present intance selection results for the CNN/DM, XSum, and DUC-04 datasets.",
            "Our method builds representations for instances using either BERT or VSM (\u00a73.1).",
            "To ensure a thorough comparison, we experiment with selecting a mixed set of singletons and pairs (\u201cSingPairMix\u201d) as well as selecting singletons only (\u201cSingOnly\u201d).",
            "On the CNN/DM and XSum datasets, we observe that selecting a mixed set of singletons and pairs based on BERT representations (BERT+SingPairMix) demonstrates the most competitive results.",
            "It outperforms a number of strong baselines when evaluated on a full set of ground-truth sentences.",
            "The method also performs superiorly on identifying secondary sentences.",
            "For example, it increases recall scores for identifying secondary sentences from 33.8% to 69.8% (CNN/DM) and from 16.7% to 65.3% (XSum).",
            "Our method is able to achieve strong performance on instance selection owing to BERT\u2019s capability of building effective representations for both singletons and pairs.",
            "It learns to identify salient source content based on token and position embeddings and it encodes sentential semantic compatibility using the pretraining task of predicting the next sentence; both are valuable additions to summary instance selection.",
            "Further, we observe that identifying summaryworthy singletons and pairs from multi-document inputs (DUC-04) appears to be more challenging than that of single-document inputs (XSum and CNN/DM).",
            "This distinction is not surprising given that for multi-document inputs, the system has a large and diverse search space where candidate singletons and pairs are gathered from a set of documents written by different authors.6 .",
            "We find that the BERT model performs consistently on identifying secondary sentences, and VSM yields considerable performance gain on selecting primary sentences.",
            "Both BERT and VSM models are trained on the CNN/DM dataset and applied to DUC-04 as the latter data are only used for testing.",
            "Our findings suggest that the TF-IDF features of the VSM model are effective for multi-document inputs, as important topic words are usually repeated across documents and TF-IDF scores can reflect topical importance of words.",
            "This analysis further reveals that extending BERT to incorporate topical salience of words can be a valuable line of research for future work."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            2,
            2,
            1,
            2,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "CNN/Daily Mail",
                "XSum",
                "DUC-04"
            ],
            [
                "VSM-SingOnly (This work)",
                "VSM-SingPairMix (This work)",
                "BERT-SingOnly (This work)",
                "BERT-SingPairMix (This work)"
            ],
            [
                "VSM-SingOnly (This work)",
                "VSM-SingPairMix (This work)",
                "BERT-SingOnly (This work)",
                "BERT-SingPairMix (This work)"
            ],
            [
                "CNN/Daily Mail",
                "XSum",
                "BERT-SingPairMix (This work)"
            ],
            [
                "BERT-SingPairMix (This work)",
                "Primary"
            ],
            [
                "BERT-SingPairMix (This work)",
                "Secondary"
            ],
            [
                "BERT-SingPairMix (This work)",
                "XSum",
                "R",
                "Secondary"
            ],
            [
                "BERT-SingPairMix (This work)"
            ],
            [
                "BERT-SingPairMix (This work)"
            ],
            [
                "DUC-04",
                "XSum",
                "CNN/Daily Mail"
            ],
            null,
            [
                "VSM-SingOnly (This work)",
                "VSM-SingPairMix (This work)",
                "BERT-SingOnly (This work)",
                "BERT-SingPairMix (This work)"
            ],
            [
                "VSM-SingOnly (This work)",
                "VSM-SingPairMix (This work)",
                "BERT-SingOnly (This work)",
                "BERT-SingPairMix (This work)",
                "CNN/Daily Mail",
                "DUC-04"
            ],
            [
                "VSM-SingOnly (This work)",
                "VSM-SingPairMix (This work)"
            ],
            [
                "BERT-SingOnly (This work)",
                "BERT-SingPairMix (This work)"
            ]
        ],
        "n_sentence": 16.0,
        "table_id": "table_2",
        "paper_id": "P19-1209",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1212table_4",
        "description": "Table 4 reports F1 scores of ROUGE-1, 2, and L (Lin and Hovy, 2003) for all models. For BIGPATENT, almost all models outperform the LEAD-3 baseline due to the more uniform distribution of salient content in BIGPATENT\u2019s input articles. Among extractive models, TEXTRANK and LEXRANK outperform RNN-EXT RL which was trained on only the first 400 words of the input, again suggesting the need for neural models to efficiently handle longer input. Finally, SENTREWRITING, a reinforcement learning model with ROUGE as reward, achieves the best performance on BIGPATENT.",
        "sentences": [
            "Table 4 reports F1 scores of ROUGE-1, 2, and L (Lin and Hovy, 2003) for all models.",
            "For BIGPATENT, almost all models outperform the LEAD-3 baseline due to the more uniform distribution of salient content in BIGPATENT\u2019s input articles.",
            "Among extractive models, TEXTRANK and LEXRANK outperform RNN-EXT RL which was trained on only the first 400 words of the input, again suggesting the need for neural models to efficiently handle longer input.",
            "Finally, SENTREWRITING, a reinforcement learning model with ROUGE as reward, achieves the best performance on BIGPATENT."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "R-1",
                "R-2",
                "R-L"
            ],
            [
                "BIGPATENT",
                "LEAD-3"
            ],
            [
                "TEXTRANK (Mihalcea and Tarau 2004)",
                "LEXRANK (Erkan and Radev 2004)",
                "RNN-EXT RL (Chen and Bansal 2018)"
            ],
            [
                "SENTREWRITING (Chen and Bansal 2018)",
                "BIGPATENT"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_4",
        "paper_id": "P19-1212",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1216table_2",
        "description": "5 Results and Analysis . METEOR metric (n-gram overlap with synonyms) was used for automatic evaluation. The novel ngram rate9 (e.t., NN-1, NN-2, NN-3, and NN-4) was also computed to investigate the number of novel words that could be introduced by the models. Table 2 and Table 3 present the results and below are our observations: (i) keyphrase word graph approach (#2) is a strong baseline according to the METEOR metric. In comparison, the proposed rewriter (#5) yields comparable result on the METEOR metric for the Giga-MSC dataset but lower result for the Cornell dataset. We speculate that it may be due to the difference in the ground-truth compression. 8.6% of novel unigrams exist in the ground-truth compression of the Giga-MSC dataset, while only 5.2% of novel unigrams exist in that of the Cornell dataset, (ii) Hard Para.(#3), Seq2seq (#4), and our rewriter (#5) significantly increase the number of novel n-grams, and the proposed rewriter (#5) seemed to be a better trade-off between the information coverage (measured by METEOR) and the introduction of novel n-grams across all methods, (iii) on comparing with Seq2seq (#4) and our rewriter (#5), we found that adding pseudo data helps to decrease the novel words rate and increase the METEOR score on both datasets.",
        "sentences": [
            "5 Results and Analysis .",
            "METEOR metric (n-gram overlap with synonyms) was used for automatic evaluation.",
            "The novel ngram rate9 (e.t., NN-1, NN-2, NN-3, and NN-4) was also computed to investigate the number of novel words that could be introduced by the models.",
            "Table 2 and Table 3 present the results and below are our observations: (i) keyphrase word graph approach (#2) is a strong baseline according to the METEOR metric.",
            "In comparison, the proposed rewriter (#5) yields comparable result on the METEOR metric for the Giga-MSC dataset but lower result for the Cornell dataset.",
            "We speculate that it may be due to the difference in the ground-truth compression.",
            "8.6% of novel unigrams exist in the ground-truth compression of the Giga-MSC dataset, while only 5.2% of novel unigrams exist in that of the Cornell dataset, (ii) Hard Para.(#3), Seq2seq (#4), and our rewriter (#5) significantly increase the number of novel n-grams, and the proposed rewriter (#5) seemed to be a better trade-off between the information coverage (measured by METEOR) and the introduction of novel n-grams across all methods, (iii) on comparing with Seq2seq (#4) and our rewriter (#5), we found that adding pseudo data helps to decrease the novel words rate and increase the METEOR score on both datasets."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "METEOR"
            ],
            [
                "NN-1",
                "NN-2",
                "NN-3",
                "NN-4"
            ],
            [
                "METEOR",
                "#2 KWG (Boudin+, 13)"
            ],
            [
                "METEOR",
                "#5 Our rewriter (RWT)"
            ],
            null,
            [
                "#3 Hard Para.",
                "#4 Seq2seq with attention",
                "#5 Our rewriter (RWT)",
                "METEOR"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_2",
        "paper_id": "P19-1216",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1216table_4",
        "description": "Human Evaluation . As METEOR metric cannot measure the grammaticality of compression, we asked two human raters10 to assess 50 compressed sentences out of the Giga-MSC test dataset in terms of informativeness and grammaticality. We used 0-2 point scale (2 pts: excellent; 1 pts: good; 0 pts: poor), similar to previous work (we recommend readers to refer to Appendix 2 for the 0-2 scale point evaluation details). Table 4 shows the average ratings for informativeness and readability. From that, we found that our rewriter (RWT) significantly improved the grammaticality of compression in comparison with the keyphrase word graph approach, implying that the pseudo data may contribute to the language modeling of the decoder, thereby improving the grammaticality.",
        "sentences": [
            "Human Evaluation .",
            "As METEOR metric cannot measure the grammaticality of compression, we asked two human raters10 to assess 50 compressed sentences out of the Giga-MSC test dataset in terms of informativeness and grammaticality.",
            "We used 0-2 point scale (2 pts: excellent; 1 pts: good; 0 pts: poor), similar to previous work (we recommend readers to refer to Appendix 2 for the 0-2 scale point evaluation details).",
            "Table 4 shows the average ratings for informativeness and readability.",
            "From that, we found that our rewriter (RWT) significantly improved the grammaticality of compression in comparison with the keyphrase word graph approach, implying that the pseudo data may contribute to the language modeling of the decoder, thereby improving the grammaticality."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "Informativeness",
                "Grammaticality"
            ],
            [
                "RWT",
                "Grammaticality"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_4",
        "paper_id": "P19-1216",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1220table_2",
        "description": "4.2 Results . Does our model achieve state-of-the-art on the two tasks with different styles?. Table 2 shows the performance of our model and competing models on the leaderboard. Our ensemble model of six training runs, where each model was trained with the two answer styles, achieved state-of-theart performance on both tasks in terms of ROUGEL. In particular, for the NLG task, our single model outperformed competing models in terms of both ROUGE-L and BLEU-1.",
        "sentences": [
            "4.2 Results .",
            "Does our model achieve state-of-the-art on the two tasks with different styles?.",
            "Table 2 shows the performance of our model and competing models on the leaderboard.",
            "Our ensemble model of six training runs, where each model was trained with the two answer styles, achieved state-of-theart performance on both tasks in terms of ROUGEL.",
            "In particular, for the NLG task, our single model outperformed competing models in terms of both ROUGE-L and BLEU-1."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Masque (NLG ensemble)",
                "Masque (Q&A ensemble)",
                "Masque (NLG single)",
                "Masque (Q&A single)"
            ],
            [
                "Masque (NLG ensemble)",
                "Masque (Q&A ensemble)"
            ],
            [
                "Masque (NLG single)"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_2",
        "paper_id": "P19-1220",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1220table_5",
        "description": "5.2 Results . Does our model achieve state-of-the-art performance?. Table 5 shows that our single model, trained with two styles and controlled with the NQA style, pushed forward the state-of-the-art by a significant margin. The evaluation scores of the model controlled with the NLG style were low because the two styles are different. Also, our model without multi-style learning (trained with only the NQA style) outperformed the baselines in terms of ROUGE-L. This indicates that our model architecture itself is powerful for natural language understanding in RC.",
        "sentences": [
            "5.2 Results .",
            "Does our model achieve state-of-the-art performance?.",
            "Table 5 shows that our single model, trained with two styles and controlled with the NQA style, pushed forward the state-of-the-art by a significant margin.",
            "The evaluation scores of the model controlled with the NLG style were low because the two styles are different.",
            "Also, our model without multi-style learning (trained with only the NQA style) outperformed the baselines in terms of ROUGE-L.",
            "This indicates that our model architecture itself is powerful for natural language understanding in RC."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Masque (NQA)"
            ],
            [
                "Masque (NLG)"
            ],
            [
                "Masque (NQA)",
                "R-L"
            ],
            [
                "Masque (NQA)",
                "Masque (NLG)"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_5",
        "paper_id": "P19-1220",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1221table_4",
        "description": "We also report the performance on documentlevel SQuAD in Table 4 to assess our approach in single-document setting. We find our approach adapts well: the best model achieves 87.2 F1. Note that the BERTLARGE model has obtained 90.9 F1 on the original SQuAD dataset (single-paragraph setting), which is only 3.7% ahead of us.",
        "sentences": [
            "We also report the performance on documentlevel SQuAD in Table 4 to assess our approach in single-document setting.",
            "We find our approach adapts well: the best model achieves 87.2 F1.",
            "Note that the BERTLARGE model has obtained 90.9 F1 on the original SQuAD dataset (single-paragraph setting), which is only 3.7% ahead of us."
        ],
        "class_sentence": [
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "F1",
                "RE3QALARGE"
            ],
            [
                "F1"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_4",
        "paper_id": "P19-1221",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1225table_5",
        "description": "What are the characteristics of our evidence extraction? . Table 5 shows the evidence extraction performance in the distractor setting. Our model improves both precision and recall, and the improvement in precision is larger. Figure 4 reveals the reason for the high EM and precision scores; QFE rarely extracts too much evidence. That is, it predicts the number of evidence sentences more accurately than the baseline. Table 5 also shows the correlation of our model about the number of evidence sentences is higher than that of the baseline. We consider that the sequential extraction and the adaptive termination help to prevent overextraction. In contrast, the baseline evaluates each sentence independently, so the baseline often extracts too much evidence.",
        "sentences": [
            "What are the characteristics of our evidence extraction? .",
            "Table 5 shows the evidence extraction performance in the distractor setting.",
            "Our model improves both precision and recall, and the improvement in precision is larger.",
            "Figure 4 reveals the reason for the high EM and precision scores; QFE rarely extracts too much evidence.",
            "That is, it predicts the number of evidence sentences more accurately than the baseline.",
            "Table 5 also shows the correlation of our model about the number of evidence sentences is higher than that of the baseline.",
            "We consider that the sequential extraction and the adaptive termination help to prevent overextraction.",
            "In contrast, the baseline evaluates each sentence independently, so the baseline often extracts too much evidence."
        ],
        "class_sentence": [
            2,
            1,
            1,
            0,
            0,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "Precision",
                "Recall",
                "QFE",
                "baseline"
            ],
            null,
            null,
            [
                "Correlation",
                "QFE",
                "baseline"
            ],
            null,
            [
                "baseline"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_5",
        "paper_id": "P19-1225",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1227table_6",
        "description": "5.3 Overall Results. Table 6 shows the overall results for different methods in different languages. There is a large gap between the performance of English and that of other target languages, which implies that the task of cross-lingual OpenQA is difficult. In the English test set, the performance of the multilingual BERT model is worse than that of the monolingual BERT model. In almost all target languages, however, the multilingual model achieves the best result, manifesting its ability in capturing answers for questions across various languages. When we compare DocumentQA to BERT, although they have similar performance in English, BERT consistently outperforms DocumentQA by a large margin in all target languages in both translate-test and translate-train settings. We conjecture that it is because the BERT model, which has been pretrained on large-scale unlabeled text data, has better generalization power, and could better handle the different distributions between the original English training data and the machine translated test data. Translate-train methods outperform translatetest methods in all cases except for DocumentQA in German. This may be due to the fact that DocumentQA uses space-tokenized words as basic units. In German, there is no space between compound words, resulting in countless possible combinations. Therefore, many of the words in translate-train German data do not have pretrained word vectors. On the contrary, using WordPiece tokenizer, BERT is not influenced by this.",
        "sentences": [
            "5.3 Overall Results.",
            "Table 6 shows the overall results for different methods in different languages.",
            "There is a large gap between the performance of English and that of other target languages, which implies that the task of cross-lingual OpenQA is difficult.",
            "In the English test set, the performance of the multilingual BERT model is worse than that of the monolingual BERT model.",
            "In almost all target languages, however, the multilingual model achieves the best result, manifesting its ability in capturing answers for questions across various languages.",
            "When we compare DocumentQA to BERT, although they have similar performance in English, BERT consistently outperforms DocumentQA by a large margin in all target languages in both translate-test and translate-train settings.",
            "We conjecture that it is because the BERT model, which has been pretrained on large-scale unlabeled text data, has better generalization power, and could better handle the different distributions between the original English training data and the machine translated test data.",
            "Translate-train methods outperform translatetest methods in all cases except for DocumentQA in German.",
            "This may be due to the fact that DocumentQA uses space-tokenized words as basic units.",
            "In German, there is no space between compound words, resulting in countless possible combinations.",
            "Therefore, many of the words in translate-train German data do not have pretrained word vectors.",
            "On the contrary, using WordPiece tokenizer, BERT is not influenced by this."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            1,
            2,
            1,
            2,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "Languages"
            ],
            [
                "English"
            ],
            [
                "English",
                "Multilingual BERT",
                "BERT"
            ],
            [
                "Languages",
                "Multilingual BERT"
            ],
            [
                "DocQA",
                "BERT",
                "English",
                "Translate-Train",
                "Translate-Test"
            ],
            null,
            [
                "Translate-Train",
                "Translate-Test",
                "DocQA",
                "German"
            ],
            [
                "DocQA"
            ],
            [
                "German"
            ],
            [
                "Translate-Train",
                "German"
            ],
            [
                "BERT"
            ]
        ],
        "n_sentence": 12.0,
        "table_id": "table_6",
        "paper_id": "P19-1227",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1227table_9",
        "description": "The results in Table 9 verify our assumption. The performance of different languages generally decreases as the genetic distance grows. The exceptions are Chinese and Portuguese since the percentages of \"easy\" questions in them are significantly higher than those in other languages. For languages that have similar genetic distances with English (i.e. Russian, Ukrainian, and Portuguese), the performance increases as the percentage of \"easy\" questions grows.",
        "sentences": [
            "The results in Table 9 verify our assumption.",
            "The performance of different languages generally decreases as the genetic distance grows.",
            "The exceptions are Chinese and Portuguese since the percentages of \"easy\" questions in them are significantly higher than those in other languages.",
            "For languages that have similar genetic distances with English (i.e. Russian, Ukrainian, and Portuguese), the performance increases as the percentage of \"easy\" questions grows."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Pct. of easy",
                "EM",
                "German",
                "French",
                "Polish",
                "Ukrainian",
                "Russian",
                "Tamil",
                "Genetic dist."
            ],
            [
                "Chinese",
                "Portuguese",
                "Pct. of easy"
            ],
            [
                "Genetic dist.",
                "Russian",
                "Ukrainian",
                "Portuguese",
                "Pct. of easy"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_9",
        "paper_id": "P19-1227",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1229table_3",
        "description": "4.1 Single-domain Training Results . Table 3 presents parsing accuracy on the dev data when training each parser on a single-domain training data. We can see that although PB-train is much smaller than BC-train, the PB-trained parser outperforms the BC-trained parser by about 8% on PB-dev, indicating the usefulness and importance of target-domain labeled data especially when two domains are very dissimilar. However, the gap between the ZX-trained parser and the BC-trained is only about 2% in LAS, which we believe has a two-fold reason. First, the size of ZX-train is even smaller, and is only less than one third of that of PB-train. Second, the BC corpus are from the People Daily newspaper and probably contains novel articles, which are more similar to ZX. Overall, it is clear and reasonable that the parser achieves best performance on a given domain when the training data is from the same domain.",
        "sentences": [
            "4.1 Single-domain Training Results .",
            "Table 3 presents parsing accuracy on the dev data when training each parser on a single-domain training data.",
            "We can see that although PB-train is much smaller than BC-train, the PB-trained parser outperforms the BC-trained parser by about 8% on PB-dev, indicating the usefulness and importance of target-domain labeled data especially when two domains are very dissimilar.",
            "However, the gap between the ZX-trained parser and the BC-trained is only about 2% in LAS, which we believe has a two-fold reason.",
            "First, the size of ZX-train is even smaller, and is only less than one third of that of PB-train.",
            "Second, the BC corpus are from the People Daily newspaper and probably contains novel articles, which are more similar to ZX.",
            "Overall, it is clear and reasonable that the parser achieves best performance on a given domain when the training data is from the same domain."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "PB",
                "BC"
            ],
            [
                "ZX",
                "BC"
            ],
            [
                "ZX"
            ],
            [
                "BC"
            ],
            null
        ],
        "n_sentence": 7.0,
        "table_id": "table_3",
        "paper_id": "P19-1229",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1229table_5",
        "description": "4.4 Final Results . Table 5 shows the final results on the test data, which are consistent with the previous observations. First, when constrained on single-domain training data, using the target-domain data is the most effective. Second, using source-domain data as extra training data is helpful, and the DOEMB method performs the best. Third, it is extremely useful and efficient to first train ELMo on very large-scale general-purpose unlabeled data and then fine-tune it on relatively small-scale targetdomain unlabeled data.",
        "sentences": [
            "4.4 Final Results .",
            "Table 5 shows the final results on the test data, which are consistent with the previous observations.",
            "First, when constrained on single-domain training data, using the target-domain data is the most effective.",
            "Second, using source-domain data as extra training data is helpful, and the DOEMB method performs the best.",
            "Third, it is extremely useful and efficient to first train ELMo on very large-scale general-purpose unlabeled data and then fine-tune it on relatively small-scale targetdomain unlabeled data."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Trained on single-domain data"
            ],
            [
                "Trained on source- and target-domain data",
                "DOEMB"
            ],
            [
                "+ ELMo",
                "+ Fine-tuning"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_5",
        "paper_id": "P19-1229",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1231table_3",
        "description": "General Performance. Table 3 shows model performance by entity type and the overall performance on the four tested datasets. From the table, we can observe: 1) The performance of the Matching model is quite poor compared to other models. We found out that it mainly resulted from low recall values.",
        "sentences": [
            "General Performance.",
            "Table 3 shows model performance by entity type and the overall performance on the four tested datasets.",
            "From the table, we can observe: 1) The performance of the Matching model is quite poor compared to other models.",
            "We found out that it mainly resulted from low recall values."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "CoNLL (en)",
                "CoNLL (sp)",
                "MUC",
                "Twitter"
            ],
            [
                "Matching"
            ],
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "P19-1231",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1236table_3",
        "description": "Our method outperforms all baselines significantly, which shows the importance of using rich data. A contrast between our method and MIXDATA shows the effectiveness of using two different language models across domains. Even through MIX-DATA uses more data for training language models on both the source and target domains, it cannot learn a domain contrast since both sides use the same mixed data. In contrast, our model gives significantly better results by gleaning such contrast. (3) Comparison with current state-of-the-art. Finally, Table 3 also shows a comparison with a state-of-the-art method on the 13PC and 13CG datasets (Crichton et al., 2017), which leverages POS tagging for multi-task learning by using cotraining method. Our model outperforms their results, giving the best results in the literature. Discussion. When the number of target-domain NER sentences is 0, the transfer learning setting is unsupervised domain adaptation. As the number of target domain NER sentences increases, they will intuitively play an increasingly important role for target NER. Figure 6 compares the F1-scores of the baseline STM-TARGET and our multi-task model with varying numbers of target-domain NER training data under 100 training epochs. In the nearly unsupervised setting, our method gives the largest improvement of 20.5% F1-scores. As the number of training data increases, the gap between the two methods becomes smaller. But our method still gives a 3.3% F1 score gain when the number of training sentences reach 3,000, show-.",
        "sentences": [
            "Our method outperforms all baselines significantly, which shows the importance of using rich data.",
            "A contrast between our method and MIXDATA shows the effectiveness of using two different language models across domains.",
            "Even through MIX-DATA uses more data for training language models on both the source and target domains, it cannot learn a domain contrast since both sides use the same mixed data.",
            "In contrast, our model gives significantly better results by gleaning such contrast.",
            "(3) Comparison with current state-of-the-art.",
            "Finally, Table 3 also shows a comparison with a state-of-the-art method on the 13PC and 13CG datasets (Crichton et al., 2017), which leverages POS tagging for multi-task learning by using cotraining method.",
            "Our model outperforms their results, giving the best results in the literature.",
            "Discussion.",
            "When the number of target-domain NER sentences is 0, the transfer learning setting is unsupervised domain adaptation.",
            "As the number of target domain NER sentences increases, they will intuitively play an increasingly important role for target NER.",
            "Figure 6 compares the F1-scores of the baseline STM-TARGET and our multi-task model with varying numbers of target-domain NER training data under 100 training epochs.",
            "In the nearly unsupervised setting, our method gives the largest improvement of 20.5% F1-scores.",
            "As the number of training data increases, the gap between the two methods becomes smaller.",
            "But our method still gives a 3.3% F1 score gain when the number of training sentences reach 3,000, show-."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            1,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2
        ],
        "header_mention": [
            [
                "FINAL"
            ],
            [
                "FINAL",
                "MIX-DATA"
            ],
            [
                "MIX-DATA"
            ],
            [
                "FINAL"
            ],
            null,
            [
                "13PC",
                "13CG",
                "MULTITASK(NER+LM)",
                "MULTITASK(NER)"
            ],
            [
                "FINAL"
            ],
            null,
            null,
            null,
            null,
            null,
            null,
            null
        ],
        "n_sentence": 14.0,
        "table_id": "table_3",
        "paper_id": "P19-1236",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1240table_6",
        "description": "5.3 Further Discussions . Ablation Study. We compare the results of our full model and its four ablated variants to analyze the relative contributions of topics on different components. The results in Table 6 indicate the competitive effect of topics on decoder attention and that on hidden states, but combining them both help our full model achieve the best performance. We also observe that pre-trained topics only bring a small boost, indicated by the close scores yielded by our model (separate train) and SEQ2SEQ-COPY. This suggests that the joint training is crucial to better absorb latent topics.",
        "sentences": [
            "5.3 Further Discussions .",
            "Ablation Study.",
            "We compare the results of our full model and its four ablated variants to analyze the relative contributions of topics on different components.",
            "The results in Table 6 indicate the competitive effect of topics on decoder attention and that on hidden states, but combining them both help our full model achieve the best performance.",
            "We also observe that pre-trained topics only bring a small boost, indicated by the close scores yielded by our model (separate train) and SEQ2SEQ-COPY.",
            "This suggests that the joint training is crucial to better absorb latent topics."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "Our full model"
            ],
            [
                "Our model (separate train)",
                "SEQ2SEQ-COPY"
            ],
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_6",
        "paper_id": "P19-1240",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1241table_3",
        "description": "6.1 Performance . Table 3 describes the performance of the baseline classifiers as well as the deep learning models based on four evaluation metrics. The Social Media Language Model outperforms all baseline models, including RNNs, LSTMs, CNNs, and the linear DAD and SDA models. The A-CNN-LSTM and the Hierarchical Attention Model has a high recall due to its ability to better capture long term dependencies. The attention mechanism allows the model to retain some important hidden information when the sentences are quite long.",
        "sentences": [
            "6.1 Performance .",
            "Table 3 describes the performance of the baseline classifiers as well as the deep learning models based on four evaluation metrics.",
            "The Social Media Language Model outperforms all baseline models, including RNNs, LSTMs, CNNs, and the linear DAD and SDA models.",
            "The A-CNN-LSTM and the Hierarchical Attention Model has a high recall due to its ability to better capture long term dependencies.",
            "The attention mechanism allows the model to retain some important hidden information when the sentences are quite long."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "SMLM",
                "RNN",
                "LSTM",
                "DAD Model",
                "SDA Model"
            ],
            [
                "A-CNN-LSTM",
                "HATT",
                "Recall"
            ],
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_3",
        "paper_id": "P19-1241",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1244table_5",
        "description": "Table 5 shows that HAN-nli* is much better than the two baselines in terms of label accuracy and evidence F1 score. There are two reasons: 1) apart from the retrieval module, our model optimizes all the parameters end-to-end, while the two pipeline systems may result in error propagation; and 2) our evidence embedding method considers more complex facets such as topical coherence and semantic entailment, while NSMN just focuses on similarity matching between the claim and each sentence. HAN-nli seem already a decent model given its much better performance than Fever-base. This confirms the advantage of our evidence embedding method on the FEVER task.",
        "sentences": [
            "Table 5 shows that HAN-nli* is much better than the two baselines in terms of label accuracy and evidence F1 score.",
            "There are two reasons: 1) apart from the retrieval module, our model optimizes all the parameters end-to-end, while the two pipeline systems may result in error propagation; and 2) our evidence embedding method considers more complex facets such as topical coherence and semantic entailment, while NSMN just focuses on similarity matching between the claim and each sentence.",
            "HAN-nli seem already a decent model given its much better performance than Fever-base.",
            "This confirms the advantage of our evidence embedding method on the FEVER task."
        ],
        "class_sentence": [
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            [
                "HAN-nli*",
                "Fever-base",
                "NSMN",
                "Acc.",
                "F1"
            ],
            null,
            [
                "HAN-nli",
                "Fever-base"
            ],
            [
                "FEVER"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_5",
        "paper_id": "P19-1244",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1249table_5",
        "description": "Table 5 shows all models' transfer performance between populations on gender. In general, all models generalize well to the respectively unseen datasets but perform best on the data they have been specifically trained for. The largest difference can be observed on the sub-1,000 author dataset PAN15, where the model of \u00c3lvarez-Carmona et al. (2015) suffers a significant performance loss, and PAN16, where the model of Busger op Vollenbroek et al.(2016) performs notably better on the celebrity data. This was a surprise to us that may be explained by the longer samples of writing per profile in our corpus. This hypothesis is also supported by the large increase in accuracy of the baseline model after retraining for two epochs with the PAN15 and PAN16 training datasets, respectively. The occupation model achieved a 0.7111 accuracy.",
        "sentences": [
            "Table 5 shows all models' transfer performance between populations on gender.",
            "In general, all models generalize well to the respectively unseen datasets but perform best on the data they have been specifically trained for.",
            "The largest difference can be observed on the sub-1,000 author dataset PAN15, where the model of \u00c3lvarez-Carmona et al. (2015) suffers a significant performance loss, and PAN16, where the model of Busger op Vollenbroek et al.(2016) performs notably better on the celebrity data.",
            "This was a surprise to us that may be explained by the longer samples of writing per profile in our corpus.",
            "This hypothesis is also supported by the large increase in accuracy of the baseline model after retraining for two epochs with the PAN15 and PAN16 training datasets, respectively.",
            "The occupation model achieved a 0.7111 accuracy."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "PAN15",
                "PAN16",
                "nissim16 (2016)"
            ],
            null,
            [
                "PAN15",
                "PAN16"
            ],
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_5",
        "paper_id": "P19-1249",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1251table_3",
        "description": "3.2 Experimental Results. For evaluation, micro and macro-F1 scores are selected the evaluation metrics. Table 3 demonstrates the performance of the three methods. Micro-F1 scores are generally better than macroF1 scores because the trivial cases like the class of good air quality are the majority of datasets with higher weights in micro-F1 scores. PAQI is better than BOW although BOW uses the knowledge of social media. It is because BOW features involve all irrelevant words so that the actual essential knowledge cannot be recognized. Our approach significantly outperforms all baseline methods in almost all metrics. More precisely, our approach improves the air quality prediction over PAQI from 6.92% to 17.71% in macro-F1 scores. The results demonstrate that social media and NLP can benefit air quality prediction.",
        "sentences": [
            "3.2 Experimental Results.",
            "For evaluation, micro and macro-F1 scores are selected the evaluation metrics.",
            "Table 3 demonstrates the performance of the three methods.",
            "Micro-F1 scores are generally better than macroF1 scores because the trivial cases like the class of good air quality are the majority of datasets with higher weights in micro-F1 scores.",
            "PAQI is better than BOW although BOW uses the knowledge of social media.",
            "It is because BOW features involve all irrelevant words so that the actual essential knowledge cannot be recognized.",
            "Our approach significantly outperforms all baseline methods in almost all metrics.",
            "More precisely, our approach improves the air quality prediction over PAQI from 6.92% to 17.71% in macro-F1 scores.",
            "The results demonstrate that social media and NLP can benefit air quality prediction."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "MicroAverage",
                "F1",
                "MacroAverage"
            ],
            [
                "PAQI",
                "BOW"
            ],
            [
                "BOW"
            ],
            [
                "Ours"
            ],
            [
                "PAQI",
                "MacroAverage",
                "F1"
            ],
            null
        ],
        "n_sentence": 9.0,
        "table_id": "table_3",
        "paper_id": "P19-1251",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1254table_3",
        "description": "We compare three models ability to fill (using coreferenceentities based on context anonymization): a model that does not receive the story, a model that uses only leftward context (as in Clark et al. (2018)), and a model with access to the full story. We show in Table 3 that having access to the full story provides the best performance. Having no access to any of the story decreases ranking accuracy, even though the model still receives the local context window of the entity as input. The left story context model performs better, but looking at the complete story provides additional gains. We note that full-story context can only be provided in a multi-stage generation approach.",
        "sentences": [
            "We compare three models ability to fill (using coreferenceentities based on context anonymization): a model that does not receive the story, a model that uses only leftward context (as in Clark et al. (2018)), and a model with access to the full story.",
            "We show in Table 3 that having access to the full story provides the best performance.",
            "Having no access to any of the story decreases ranking accuracy, even though the model still receives the local context window of the entity as input.",
            "The left story context model performs better, but looking at the complete story provides additional gains.",
            "We note that full-story context can only be provided in a multi-stage generation approach."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Full story"
            ],
            [
                "No story"
            ],
            [
                "Left story context"
            ],
            [
                "Full story"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_3",
        "paper_id": "P19-1254",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1256table_2",
        "description": "3.2 Main Results . NLU Results. One challenge in E2E dataset is the need to account for the noise in the corpus as some of the MR-text pairs are not semantically equivalent due to the data collection process (Dusek et al., 2018). We examine the performance of the NLU module by comparing noise reduction of the reconstructed MR-text pairs with the original ones in both training and test sets. Table 2 shows the automatic results. Applying our NLU model with iterative data refinement, the error rates of refined MR-text pairs yields 23.33% absolute error reduction on test set.",
        "sentences": [
            "3.2 Main Results .",
            "NLU Results.",
            "One challenge in E2E dataset is the need to account for the noise in the corpus as some of the MR-text pairs are not semantically equivalent due to the data collection process (Dusek et al., 2018).",
            "We examine the performance of the NLU module by comparing noise reduction of the reconstructed MR-text pairs with the original ones in both training and test sets.",
            "Table 2 shows the automatic results.",
            "Applying our NLU model with iterative data refinement, the error rates of refined MR-text pairs yields 23.33% absolute error reduction on test set."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null,
            [
                "NLU refined data",
                "Original data",
                "Test Err (%)"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_2",
        "paper_id": "P19-1256",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1256table_3",
        "description": "Human evaluation in Table 3 shows that our proposed method achieves 16.69% improvement on information equivalence between MR-text pairs. These results confirm the effectiveness of our method in reducing the unaligned data noise, and the large improvement (i.e, 15.09%) on exact match when applying self-training algorithm suggests the importance of iterative data refinement.",
        "sentences": [
            "Human evaluation in Table 3 shows that our proposed method achieves 16.69% improvement on information equivalence between MR-text pairs.",
            "These results confirm the effectiveness of our method in reducing the unaligned data noise, and the large improvement (i.e, 15.09%) on exact match when applying self-training algorithm suggests the importance of iterative data refinement."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "NLU refined data",
                "Original data"
            ],
            [
                "NLU refined data",
                "w/o self-training",
                "Original data"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_3",
        "paper_id": "P19-1256",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1257table_2",
        "description": "Data Analysis . High-quality testing set is necessary for faithful automatic evaluation. Therefore, we randomly selected 200 samples from the testing set for quality evaluation. Three annotators with linguistic background are required to score comments and readers can refer to Section 4.3 for the evaluation details. Table 2 shows the evaluation results. The average score for overall quality is 7.6, showing that the testing set is satisfactory.",
        "sentences": [
            "Data Analysis .",
            "High-quality testing set is necessary for faithful automatic evaluation.",
            "Therefore, we randomly selected 200 samples from the testing set for quality evaluation.",
            "Three annotators with linguistic background are required to score comments and readers can refer to Section 4.3 for the evaluation details.",
            "Table 2 shows the evaluation results.",
            "The average score for overall quality is 7.6, showing that the testing set is satisfactory."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null,
            [
                "Overall"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_2",
        "paper_id": "P19-1257",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1266table_4",
        "description": "Results: Summary. We now turn to a summary of our analysis across the 510 comparisons of Reimers and Gurevych (2017a). Table 4 presents the percentage of comparisons that fall into each category, along with the average and std of the e value of ASO for each case (all ASO results are significant with p <= 0.01). Figure 1 presents the histogram of these e values in each case. The number of comparisons that fall into case A is only 0.98%, indicating that it is rare that a decision about stochastic dominance of one algorithm can be reached when comparing DNNs. We consider this a strong indication that the Mann Whitney U test is not suitable for DNN comparison as it has very little statistical power (criterion (b)).",
        "sentences": [
            "Results: Summary.",
            "We now turn to a summary of our analysis across the 510 comparisons of Reimers and Gurevych (2017a).",
            "Table 4 presents the percentage of comparisons that fall into each category, along with the average and std of the e value of ASO for each case (all ASO results are significant with p <= 0.01).",
            "Figure 1 presents the histogram of these e values in each case.",
            "The number of comparisons that fall into case A is only 0.98%, indicating that it is rare that a decision about stochastic dominance of one algorithm can be reached when comparing DNNs.",
            "We consider this a strong indication that the Mann Whitney U test is not suitable for DNN comparison as it has very little statistical power (criterion (b))."
        ],
        "class_sentence": [
            2,
            2,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "Avg. e"
            ],
            null,
            [
                "% of comparisons",
                "Case A"
            ],
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_4",
        "paper_id": "P19-1266",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1276table_4",
        "description": "Schemas Matching. Table  4  shows  the  overall performance of schema matching on GNBusiness-Test. From the table, we can see that ODEE-FER achieves the best F1 scores among all the methods. By  comparing Nguyen  et  al.  (2015) and ODEE-F (p= 0.01), we can see that using continuous contextual features gives better performance than discrete  features. This  demonstrates  the  advantages of continuous contextual features for alleviating the sparsity of discrete features in texts. We can also see from the result of Clustering that using  only  the  contextual  features  is  not  sufficient for ODEE, while combining with our neural latent variable model in ODEE-F can achieve strong results (p= 6\u00d710^6). This shows that the neural latent variable model can better explain the observed data. These results demonstrate the effectivenesses of our method in incorporating with contextual features,  latent  event  types  and  redundancy  information. Among ODEE models, ODEE-Fe gives a 2% gain in F1 score  against ODEE-F,  which shows that the latent event type modeling is beneficial and the slot distribution relies on the latent event type. Additionally, there is a 1% gain in F1 score  by  comparing ODEE-FER and ODEE-FE (p= 2\u00d710?6), which confirms that leveraging redundancy is also beneficial in exploring which slot an entity should be assigned.",
        "sentences": [
            "Schemas Matching.",
            "Table  4  shows  the  overall performance of schema matching on GNBusiness-Test.",
            "From the table, we can see that ODEE-FER achieves the best F1 scores among all the methods.",
            "By  comparing Nguyen  et  al.  (2015) and ODEE-F (p= 0.01), we can see that using continuous contextual features gives better performance than discrete  features.",
            "This  demonstrates  the  advantages of continuous contextual features for alleviating the sparsity of discrete features in texts.",
            "We can also see from the result of Clustering that using  only  the  contextual  features  is  not  sufficient for ODEE, while combining with our neural latent variable model in ODEE-F can achieve strong results (p= 6\u00d710^6).",
            "This shows that the neural latent variable model can better explain the observed data.",
            "These results demonstrate the effectivenesses of our method in incorporating with contextual features,  latent  event  types  and  redundancy  information.",
            "Among ODEE models, ODEE-Fe gives a 2% gain in F1 score  against ODEE-F,  which shows that the latent event type modeling is beneficial and the slot distribution relies on the latent event type.",
            "Additionally, there is a 1% gain in F1 score  by  comparing ODEE-FER and ODEE-FE (p= 2\u00d710?6), which confirms that leveraging redundancy is also beneficial in exploring which slot an entity should be assigned."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2,
            1,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "ODEE-FER",
                "F1"
            ],
            [
                "Nguyen et al. (2015)",
                "ODEE-F"
            ],
            null,
            [
                "ODEE-F",
                "ODEE-FE",
                "ODEE-FER"
            ],
            null,
            null,
            [
                "ODEE-F",
                "F1"
            ],
            [
                "F1",
                "ODEE-FER",
                "ODEE-FE"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_4",
        "paper_id": "P19-1276",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1284table_3",
        "description": "Results. With a target rate of 10%, the HardKuma model achieved 8.5% non-zero attention. Table 3 shows that, even with so many zeros in the attention matrices, it only does about 1% worse compared to the DA baseline. Figure 6 shows an example of HardKuma attention, with additional examples in Appendix B. We leave further explorations with HardKuma attention for future work.",
        "sentences": [
            "Results.",
            "With a target rate of 10%, the HardKuma model achieved 8.5% non-zero attention.",
            "Table 3 shows that, even with so many zeros in the attention matrices, it only does about 1% worse compared to the DA baseline.",
            "Figure 6 shows an example of HardKuma attention, with additional examples in Appendix B.",
            "We leave further explorations with HardKuma attention for future work."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                " DA with HardKuma attention"
            ],
            [
                " DA with HardKuma attention",
                " DA (reimplementation)"
            ],
            null,
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_3",
        "paper_id": "P19-1284",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1296table_1",
        "description": "4.2 Performance . Table 1 shows the performances measured in terms of BLEU score. On ZH-EN task, Transformer(Base) existing systems EDR (Tu et al., 2017) and DB (Kuang et al., 2018) by 11.5 and 6.5 BLEU points. With respect to BLEU scores, all the proposed models consistently outperform Transformer(base) by 0.96 and 1.23 BLEU points. The big models (Row 7-8) also achieve similar improvement by 0.73 and 0.82 BLEU points on a larger parameters model. These findings suggest a sentence-level agreement between source-side and target-side is helpful for NMT. Further, we to enhance the source representation is use it an effective way to improve the translation. In addition, the proposed methods gain similar improvements on EN-DE task.",
        "sentences": [
            "4.2 Performance .",
            "Table 1 shows the performances measured in terms of BLEU score.",
            "On ZH-EN task, Transformer(Base) existing systems EDR (Tu et al., 2017) and DB (Kuang et al., 2018) by 11.5 and 6.5 BLEU points.",
            "With respect to BLEU scores, all the proposed models consistently outperform Transformer(base) by 0.96 and 1.23 BLEU points.",
            "The big models (Row 7-8) also achieve similar improvement by 0.73 and 0.82 BLEU points on a larger parameters model.",
            "These findings suggest a sentence-level agreement between source-side and target-side is helpful for NMT.",
            "Further, we to enhance the source representation is use it an effective way to improve the translation.",
            "In addition, the proposed methods gain similar improvements on EN-DE task."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "EDR (Tu et al., 2017)",
                "(Kuang et al., 2018)"
            ],
            [
                "Our NMT Systems",
                "Transformer(Base)"
            ],
            [
                "+lossmse",
                "+lossmse + enhanced"
            ],
            null,
            null,
            null
        ],
        "n_sentence": 8.0,
        "table_id": "table_1",
        "paper_id": "P19-1296",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1305table_3",
        "description": "Our Systems VS. the Baselines . The bottom part of Table 3 lists the performances of our methods. It manifests that both teaching summary word generation and teaching attention weights are able to improve the performance over the baselines. When the summary word generation and attention weights are taught simultaneously (denoted by Teaching Generation+Attention), the performance is further improved, surpassing the best baseline by more than two points on Gigaword evaluation set and more than one point on DUC2004.",
        "sentences": [
            "Our Systems VS. the Baselines .",
            "The bottom part of Table 3 lists the performances of our methods.",
            "It manifests that both teaching summary word generation and teaching attention weights are able to improve the performance over the baselines.",
            "When the summary word generation and attention weights are taught simultaneously (denoted by Teaching Generation+Attention), the performance is further improved, surpassing the best baseline by more than two points on Gigaword evaluation set and more than one point on DUC2004."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Teaching Generation",
                "Teaching Attention",
                "Teaching Generation+Attention"
            ],
            [
                "Teaching Generation",
                "Teaching Attention"
            ],
            [
                "Teaching Generation+Attention",
                "DUC2004"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "P19-1305",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1306table_2",
        "description": "Table 2 shows the result on EN->SW and EN>TL where we train and test on the same language pair. Performance of Baselines. For query translation, PSQ is better than DBQT because PSQ uses a weighted alternative to translate query terms and does not limit to the fixed translation from the dictionary as in DBQT. For document translation, we find that both SMT and NMT have a similar performance which is close to PSQ. The effectiveness of different approaches depends on the language pair (PSQ for EN->SW and SMT for EN->TL), which is a similar finding with McCarley (1999) and Franz et al. (1999). In our experiments with deep relevance ranking models, we all use SMT and PSQ because they have strong performances in both language pairs and it is fair to compare. Effect of Extra Features and Bilingual Representation. While deep relevance ranking can achieve decent performance, the extra features are critical to achieve better results. Because the extra features include the Indri score, the deep neural model essentially learns to rerank the document by effectively using a small number of training examples. Furthermore, our models with bilingual representations achieve better results in both language pairs, giving additional 1-3 MAP improvements over their counterparts. To compare language pairs, EN->TL has larger improvements over EN->SW. This is because EN->TL has better query translation, document translation, and query likelihood retrieval results from the baselines, and thus it enjoys more benefits from our model. We also found POSIT-DRMM works better than the other two, suggesting term-gating is useful especially when the query translation can provide more alternatives. We then perform ensembling of POSIT-DRMM to further improve th results.",
        "sentences": [
            "Table 2 shows the result on EN->SW and EN>TL where we train and test on the same language pair.",
            "Performance of Baselines.",
            "For query translation, PSQ is better than DBQT because PSQ uses a weighted alternative to translate query terms and does not limit to the fixed translation from the dictionary as in DBQT.",
            "For document translation, we find that both SMT and NMT have a similar performance which is close to PSQ.",
            "The effectiveness of different approaches depends on the language pair (PSQ for EN->SW and SMT for EN->TL), which is a similar finding with McCarley (1999) and Franz et al. (1999).",
            "In our experiments with deep relevance ranking models, we all use SMT and PSQ because they have strong performances in both language pairs and it is fair to compare.",
            "Effect of Extra Features and Bilingual Representation.",
            "While deep relevance ranking can achieve decent performance, the extra features are critical to achieve better results.",
            "Because the extra features include the Indri score, the deep neural model essentially learns to rerank the document by effectively using a small number of training examples.",
            "Furthermore, our models with bilingual representations achieve better results in both language pairs, giving additional 1-3 MAP improvements over their counterparts.",
            "To compare language pairs, EN->TL has larger improvements over EN->SW. This is because EN->TL has better query translation, document translation, and query likelihood retrieval results from the baselines, and thus it enjoys more benefits from our model.",
            "We also found POSIT-DRMM works better than the other two, suggesting term-gating is useful especially when the query translation can provide more alternatives.",
            "We then perform ensembling of POSIT-DRMM to further improve th results."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            2,
            1,
            2,
            2,
            2,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "EN->SW",
                "EN->TL"
            ],
            null,
            [
                "Probabilistic Structured Query (PSQ)",
                "Dictionary-Based Query Translation (DBQT)"
            ],
            [
                "Statistical MT (SMT)",
                "Neural MT (NMT)",
                "Probabilistic Structured Query (PSQ)"
            ],
            [
                "Probabilistic Structured Query (PSQ)",
                "EN->SW",
                "EN->TL"
            ],
            [
                "Probabilistic Structured Query (PSQ)",
                "Statistical MT (SMT)"
            ],
            null,
            null,
            null,
            [
                "Probabilistic Structured Query (PSQ)"
            ],
            [
                "EN->SW",
                "EN->TL"
            ],
            [
                "POSIT-DRMM"
            ],
            [
                "POSIT-DRMM"
            ]
        ],
        "n_sentence": 13.0,
        "table_id": "table_2",
        "paper_id": "P19-1306",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1308table_2",
        "description": "3.2 Experimental Results . Table 2 presents the results of different systems, showing that our proposed model achieves the best performance on all test language pairs under unsupervised settings. In addition, our approach is able to achieve completely comparable or even better performance than supervised systems. This illustrates that the quality of word alignment can be improved by introducing grammar information from the pre-trained denoising language model. Our denoising evaluator encourages the model to retrieve the correct translation with appropriate morphological by assessing the fluency of sentences obtained by word-to-word translation. This alleviates the adverse effect of morphological variation.",
        "sentences": [
            "3.2 Experimental Results .",
            "Table 2 presents the results of different systems, showing that our proposed model achieves the best performance on all test language pairs under unsupervised settings.",
            "In addition, our approach is able to achieve completely comparable or even better performance than supervised systems.",
            "This illustrates that the quality of word alignment can be improved by introducing grammar information from the pre-trained denoising language model.",
            "Our denoising evaluator encourages the model to retrieve the correct translation with appropriate morphological by assessing the fluency of sentences obtained by word-to-word translation.",
            "This alleviates the adverse effect of morphological variation."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "Ours",
                "Unsupervised"
            ],
            [
                "Ours",
                "Supervised"
            ],
            null,
            null,
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_2",
        "paper_id": "P19-1308",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1309table_2",
        "description": "4.1 BUCC mining task. The shared task of the workshop on Building and Using Comparable Corpora (BUCC) is a wellestablished evaluation framework for bitext mining (Zweigenbaum et al., 2017, 2018). The task is to mine for parallel sentences between English and four foreign languages: German, French, Russian and Chinese. There are 150K to 1.2M sentences for each language, split into a sample, training and test set. About 2-3% of the sentences are parallel. Table 2 reports precision, recall and F1 scores on the training set.8 . Our results show that multilingual sentence embeddings already achieve competitive performance using standard forward retrieval over cosine similarity, which is in line with Schwenk (2018). Both of our bidirectional retrieval strategies achieve substantial improvements over this baseline while still relying on cosine similarity, with intersection giving the best results. Moreover, our proposed margin-based scoring brings large improvements when using either the distance or the ratio functions, outperforming cosine similarity by more than 10 points in all cases. The best results are achieved by ratio, which outperforms distance by 0.3-0.5 points. Interestingly, the retrieval strategy has a very small effect in both cases, suggesting that the proposed scoring is more robust than cosine.",
        "sentences": [
            "4.1 BUCC mining task.",
            "The shared task of the workshop on Building and Using Comparable Corpora (BUCC) is a wellestablished evaluation framework for bitext mining (Zweigenbaum et al., 2017, 2018).",
            "The task is to mine for parallel sentences between English and four foreign languages: German, French, Russian and Chinese.",
            "There are 150K to 1.2M sentences for each language, split into a sample, training and test set.",
            "About 2-3% of the sentences are parallel.",
            "Table 2 reports precision, recall and F1 scores on the training set.8 .",
            "Our results show that multilingual sentence embeddings already achieve competitive performance using standard forward retrieval over cosine similarity, which is in line with Schwenk (2018).",
            "Both of our bidirectional retrieval strategies achieve substantial improvements over this baseline while still relying on cosine similarity, with intersection giving the best results.",
            "Moreover, our proposed margin-based scoring brings large improvements when using either the distance or the ratio functions, outperforming cosine similarity by more than 10 points in all cases.",
            "The best results are achieved by ratio, which outperforms distance by 0.3-0.5 points.",
            "Interestingly, the retrieval strategy has a very small effect in both cases, suggesting that the proposed scoring is more robust than cosine."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            2,
            1,
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null,
            [
                "P",
                "R",
                "F1"
            ],
            null,
            [
                "Retrieval",
                "Abs. (cos)",
                "Intersection"
            ],
            [
                "Ratio",
                "Dist.",
                "Max. score",
                "Abs. (cos)"
            ],
            [
                "Ratio",
                "Dist."
            ],
            null
        ],
        "n_sentence": 11.0,
        "table_id": "table_2",
        "paper_id": "P19-1309",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1317table_3",
        "description": "Different from the lexical (Jaccard) and semantic matching (WMD and SGW) baselines, BNE obtains high scores in accuracy  metric (see Table 3). The result indicates that BNE has encoded both lexical and semantic information of names into their embeddings. Table 3 also includes performances of other state-of-the-art baselines in biomedical name normalization, such as sieve-based (D'Souza and Ng, 2015), supervised semantic indexing (Leaman and Lu, 2016), and coherence-based neural network Wright et al. (2019) approaches. Note that all these baselines require human annotated labels, and the models are specifically tuned for each dataset. On the other hand, BNE utilizes only the existing synonym sets in UMLS for training. When the dataset-specific annotations are utilized, even the simple exact matching rule can boost the performance of our model to surpass other baselines (see the last two rows in Table 3).",
        "sentences": [
            "Different from the lexical (Jaccard) and semantic matching (WMD and SGW) baselines, BNE obtains high scores in accuracy  metric (see Table 3).",
            "The result indicates that BNE has encoded both lexical and semantic information of names into their embeddings.",
            "Table 3 also includes performances of other state-of-the-art baselines in biomedical name normalization, such as sieve-based (D'Souza and Ng, 2015), supervised semantic indexing (Leaman and Lu, 2016), and coherence-based neural network Wright et al. (2019) approaches.",
            "Note that all these baselines require human annotated labels, and the models are specifically tuned for each dataset.",
            "On the other hand, BNE utilizes only the existing synonym sets in UMLS for training.",
            "When the dataset-specific annotations are utilized, even the simple exact matching rule can boost the performance of our model to surpass other baselines (see the last two rows in Table 3)."
        ],
        "class_sentence": [
            1,
            2,
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            [
                "BNE + SG W",
                "BNE + SG S.C",
                "BNE + SG W + XM",
                "BNE + SG S.C + XM"
            ],
            null,
            [
                "DSouza and Ng (2015)",
                "Leaman and Lu (2016)",
                "Wright et al. (2019)"
            ],
            null,
            [
                "BNE + SG W",
                "BNE + SG S.C",
                "BNE + SG W + XM",
                "BNE + SG S.C + XM"
            ],
            [
                "BNE + SG W + XM",
                "BNE + SG S.C + XM"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_3",
        "paper_id": "P19-1317",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1318table_1",
        "description": "Results . Table 1 shows the results of our relational word vectors, the standard FastText embeddings and other baselines on the two relation classification datasets (i.e. BLESS and DiffVec). Our model consistently outperforms the FastText embeddings baseline and comparison systems, with the only exception being the precision score for DiffVec. Despite being completely unsupervised, it is also surprising that our model manages to outperform the knowledge-enhanced embeddings of  Retrofitting  and  Attract-Repel  in  the  BLESS dataset.  For DiffVec, let us recall that both these approaches  have  the  unfair  advantage  of  having had WordNet as source knowledge base, used both to construct the test set and to enhance the word embeddings. In general, the improvement of RWE over standard word embeddings suggests that our vectors capture relations in a way that is compatible to standard word vectors (which will be further discussed in Section 6.2).",
        "sentences": [
            "Results .",
            "Table 1 shows the results of our relational word vectors, the standard FastText embeddings and other baselines on the two relation classification datasets (i.e. BLESS and DiffVec).",
            "Our model consistently outperforms the FastText embeddings baseline and comparison systems, with the only exception being the precision score for DiffVec.",
            "Despite being completely unsupervised, it is also surprising that our model manages to outperform the knowledge-enhanced embeddings of  Retrofitting  and  Attract-Repel  in  the  BLESS dataset.",
            " For DiffVec, let us recall that both these approaches  have  the  unfair  advantage  of  having had WordNet as source knowledge base, used both to construct the test set and to enhance the word embeddings.",
            "In general, the improvement of RWE over standard word embeddings suggests that our vectors capture relations in a way that is compatible to standard word vectors (which will be further discussed in Section 6.2)."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "FastText",
                "BLESS",
                "DiffVec"
            ],
            [
                "FastText",
                "DiffVec"
            ],
            [
                "BLESS",
                "Retrofitting\u2020",
                "Attract-Repel\u2020"
            ],
            [
                "DiffVec"
            ],
            [
                "RWE"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_1",
        "paper_id": "P19-1318",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1318table_2",
        "description": "Results . Table 2 shows the results on the McRae Feature Norms dataset and QVEC. In the case of the McRae Feature Norms dataset, our relational word embeddings achieve the best overall results, although there is some variation for the individual features. These results suggest that attributional information is encoded well in our relational word embeddings. Interestingly, our results also suggest that Retrofitting and Attract-Repel, which use pairs of related words during training, may be too naive to capture the complex relationships proposed in these benchmarks.  In fact, they perform considerably lower than the baseline Fast-Text model. On the other hand, Pair2Vec, which we recall is the most similar to our model, yields slightly  better results  than  the FastText  baseline, but still worse than our relational word embedding model. This is especially remarkable considering its much lower computational cost.",
        "sentences": [
            "Results .",
            "Table 2 shows the results on the McRae Feature Norms dataset and QVEC.",
            "In the case of the McRae Feature Norms dataset, our relational word embeddings achieve the best overall results, although there is some variation for the individual features.",
            "These results suggest that attributional information is encoded well in our relational word embeddings.",
            "Interestingly, our results also suggest that Retrofitting and Attract-Repel, which use pairs of related words during training, may be too naive to capture the complex relationships proposed in these benchmarks.",
            " In fact, they perform considerably lower than the baseline Fast-Text model.",
            "On the other hand, Pair2Vec, which we recall is the most similar to our model, yields slightly  better results  than  the FastText  baseline, but still worse than our relational word embedding model.",
            "This is especially remarkable considering its much lower computational cost."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "McRae Feature Norms",
                "QVEC"
            ],
            [
                "McRae Feature Norms",
                "RWE"
            ],
            [
                "RWE"
            ],
            [
                "Retrofitting",
                "Attract-Repel"
            ],
            [
                "Retrofitting",
                "Attract-Repel"
            ],
            [
                "Pair2Vec",
                "FastText",
                "RWE"
            ],
            [
                "Pair2Vec"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_2",
        "paper_id": "P19-1318",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1321table_1",
        "description": "Table 1 shows word analogy results for three datasets. First, we show results for the Google  analogy  dataset  (Mikolov  et  al.,  2013a) which  is  available from the GloVe  project and covers a mix of semantic and syntactic relations. These results are shown separately in Table 1 as Gsem and Gsyn respectively. Second, we considerthe Microsoft  syntactic  word  analogy  dataset, which  only  covers  syntactic  relations  and  is  re-ferred to as MSR. Finally, we show results for the BATS analogy dataset4, which covers four categories of relations: in\u00ef\u00ac\u201aectional morphology (IM), derivational morphology (DM), encyclopedic semantics (ES) and lexicographic semantics (LS). The results in Table 1 clearly show that our model behaves substantially differently from the baselines: for the syntactic/morphological relationships (Gsyn, MSR, IM, DM), our model outperforms the baselines in a very substantial way. On the other hand, for the remaining, semanticallyoriented categories, the performance is less strong, with particularly weak results for Gsem. For ES and IS, it needs to be emphasized that the results are weak for all models, which is partially due to a relatively high number of out-of-vocabulary words. In Figure 1 we show the impact of the number of mixture components K on the performance for Gsem and Gsyn (for the NIG variant). This shows that the under-performance on Gsem is not due to the choice of K. Among others, we can also see that a relatively high number of mixture components is needed to achieve the best results.",
        "sentences": [
            "Table 1 shows word analogy results for three datasets.",
            "First, we show results for the Google  analogy  dataset  (Mikolov  et  al.,  2013a) which  is  available from the GloVe  project and covers a mix of semantic and syntactic relations.",
            "These results are shown separately in Table 1 as Gsem and Gsyn respectively.",
            "Second, we considerthe Microsoft  syntactic  word  analogy  dataset, which  only  covers  syntactic  relations  and  is  re-ferred to as MSR.",
            "Finally, we show results for the BATS analogy dataset4, which covers four categories of relations: in\u00ef\u00ac\u201aectional morphology (IM), derivational morphology (DM), encyclopedic semantics (ES) and lexicographic semantics (LS).",
            "The results in Table 1 clearly show that our model behaves substantially differently from the baselines: for the syntactic/morphological relationships (Gsyn, MSR, IM, DM), our model outperforms the baselines in a very substantial way.",
            "On the other hand, for the remaining, semanticallyoriented categories, the performance is less strong, with particularly weak results for Gsem.",
            "For ES and IS, it needs to be emphasized that the results are weak for all models, which is partially due to a relatively high number of out-of-vocabulary words.",
            "In Figure 1 we show the impact of the number of mixture components K on the performance for Gsem and Gsyn (for the NIG variant).",
            "This shows that the under-performance on Gsem is not due to the choice of K. Among others, we can also see that a relatively high number of mixture components is needed to achieve the best results."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "Gsem",
                "GSyn"
            ],
            [
                "MSR"
            ],
            [
                "IM",
                "DM",
                "ES",
                "LS"
            ],
            [
                "CvMF(NIG)",
                "GSyn",
                "MSR",
                "IM",
                "DM"
            ],
            [
                "CvMF(NIG)",
                "Gsem"
            ],
            [
                "ES"
            ],
            null,
            null
        ],
        "n_sentence": 10.0,
        "table_id": "table_1",
        "paper_id": "P19-1321",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1321table_7",
        "description": "Table 7 summarizes our document classification results. It can be seen that our model outperforms all baselines, except for the TechTC dataset, where the results are very close. Among the baselines, InterestsHDP achieves the best performance. Interestingly, this model also uses von Mishes-Fisher mixtures, but relies on a pre-trained word embedding.",
        "sentences": [
            "Table 7 summarizes our document classification results.",
            "It can be seen that our model outperforms all baselines, except for the TechTC dataset, where the results are very close.",
            "Among the baselines, InterestsHDP achieves the best performance.",
            "Interestingly, this model also uses von Mishes-Fisher mixtures, but relies on a pre-trained word embedding."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "CvMF(NIG)",
                "TechTC"
            ],
            [
                "sHDP"
            ],
            [
                "sHDP"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_7",
        "paper_id": "P19-1321",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1328table_2",
        "description": "For understanding the improvement, we conduct an ablation test and show the result in Table 2. According to Table 2, we observe that the original BERT cannot perform as well as the previous state-of-the-art approaches by its own. When we further add our candidate valuation method in Section 2.2 to validate the candidates, its performance is significantly improved. Furthermore, it is clear that our substitute candidate proposal method is much better than WordNet for candidate proposal when we compare our approach to the -w/o sp (WordNet) baseline where candidates are obtained by WordNet and validated by our validation approach.",
        "sentences": [
            "For understanding the improvement, we conduct an ablation test and show the result in Table 2.",
            "According to Table 2, we observe that the original BERT cannot perform as well as the previous state-of-the-art approaches by its own.",
            "When we further add our candidate valuation method in Section 2.2 to validate the candidates, its performance is significantly improved.",
            "Furthermore, it is clear that our substitute candidate proposal method is much better than WordNet for candidate proposal when we compare our approach to the -w/o sp (WordNet) baseline where candidates are obtained by WordNet and validated by our validation approach."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "BERT (Keep)",
                "BERT (Mask)"
            ],
            [
                "our approach"
            ],
            [
                "our approach",
                " - w/o sp (WordNet)"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "P19-1328",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1335table_5",
        "description": "To analyze the impact of unseen entities and domain shift in zero-shot entity linking, we evaluate performance on a more standard in-domain entity linking setting by making predictions on held out mentions from the training worlds. Table 5 compares entity linking performance for different entity splits. Seen entities from the training worlds are unsurprisingly the easiest to link to. For unseen entities from the training world, we observe a 5-point drop in performance.",
        "sentences": [
            "To analyze the impact of unseen entities and domain shift in zero-shot entity linking, we evaluate performance on a more standard in-domain entity linking setting by making predictions on held out mentions from the training worlds.",
            "Table 5 compares entity linking performance for different entity splits.",
            "Seen entities from the training worlds are unsurprisingly the easiest to link to.",
            "For unseen entities from the training world, we observe a 5-point drop in performance."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Training worlds seen"
            ],
            [
                "Training worlds unseen"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_5",
        "paper_id": "P19-1335",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1338table_1",
        "description": "Table 1 shows the parsing F -scores against the Stanford Parser. The ST-Gumbel Tree-LSTM model and the PRPN were run five times with different initializations, each known as a trajectory. For imitation learning, given a PRPN trajectory, we perform SbS training once and then policy refinement for five runs. Left-/right-branching and balanced trees are also included as baselines. We break down the performance of latent tree induction across constituent types in the setting of keeping punctuation. We see that, among the six most common ones, our imitation approach outperforms the PRPN on four types. However, we also notice that for the most frequent type (NP), our approach is worse than the PRPN. This shows that the strengths of the two approaches complement each other, and in future work ensemble methods could be employed to combine them.",
        "sentences": [
            "Table 1 shows the parsing F -scores against the Stanford Parser.",
            "The ST-Gumbel Tree-LSTM model and the PRPN were run five times with different initializations, each known as a trajectory.",
            "For imitation learning, given a PRPN trajectory, we perform SbS training once and then policy refinement for five runs.",
            "Left-/right-branching and balanced trees are also included as baselines.",
            "We break down the performance of latent tree induction across constituent types in the setting of keeping punctuation.",
            "We see that, among the six most common ones, our imitation approach outperforms the PRPN on four types.",
            "However, we also notice that for the most frequent type (NP), our approach is worse than the PRPN.",
            "This shows that the strengths of the two approaches complement each other, and in future work ensemble methods could be employed to combine them."
        ],
        "class_sentence": [
            1,
            2,
            2,
            2,
            2,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "ST-Gumbel",
                "PRPN"
            ],
            [
                "Imitation (SbS only)"
            ],
            [
                "Left-Branching",
                "Right-Branching",
                "Balanced-Tree"
            ],
            null,
            [
                "Imitation (SbS only)",
                "Imitation (SbS + refine)",
                "PRPN"
            ],
            [
                "Imitation (SbS only)",
                "Imitation (SbS + refine)",
                "PRPN"
            ],
            [
                "Imitation (SbS only)",
                "Imitation (SbS + refine)",
                "PRPN"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_1",
        "paper_id": "P19-1338",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1341table_6",
        "description": "Columns (i) and (ii) of Table 6 show that REG (\u00a73.4) delivers results comparable to Densifier (ORTH) when using the same set of generic training words (GEN) in lexicon induction. However, our method is more efficient - no need to compute the expensive SVD after every batch update.",
        "sentences": [
            "Columns (i) and (ii) of Table 6 show that REG (\u00a73.4) delivers results comparable to Densifier (ORTH) when using the same set of generic training words (GEN) in lexicon induction.",
            "However, our method is more efficient - no need to compute the expensive SVD after every batch update."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "GEN",
                "ORTH",
                "REG"
            ],
            [
                "REG",
                "GEN"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_6",
        "paper_id": "P19-1341",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1342table_6",
        "description": "Table 6 shows the sentence-level phrase accuracy (SPAcc) and phrase error deviation (PEDev) comparison on SST-5 between bi-tree-LSTM and TCM, respectively. TCM outperforms bi-treeLSTM on all the metrics, which demonstrates that TCM gives more consistent predictions of sentiments over different phrases in a tree, compared to top-down communication. This shows the benefit of rich node communication.",
        "sentences": [
            "Table 6 shows the sentence-level phrase accuracy (SPAcc) and phrase error deviation (PEDev) comparison on SST-5 between bi-tree-LSTM and TCM, respectively.",
            "TCM outperforms bi-treeLSTM on all the metrics, which demonstrates that TCM gives more consistent predictions of sentiments over different phrases in a tree, compared to top-down communication.",
            "This shows the benefit of rich node communication."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "BTL",
                "TCM"
            ],
            [
                "BTL",
                "TCM"
            ],
            null
        ],
        "n_sentence": 3.0,
        "table_id": "table_6",
        "paper_id": "P19-1342",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1346table_3",
        "description": "6.1 Overview of Model Performance . Full answer ROUGE. Table 3 shows that the nearest neighbor baseline performs similarly to simply returning the support document which indicates that memorizing answers from the training set is insufficient. For extractive models, the oracle provides an approximate upper bound of 27.4 ROUGE-1. The BidAF model is the strongest (23.5), better than TFIDF between the question and the support document to select sentences. However, these approaches are limited by the support document, as an oracle computed on the full web sources achieves 54.8. Abstractive methods achieve higher ROUGE, likely because they can adapt to the domain shift between the web sources and the ELI5 subreddit. In general, Seq2Seq models perform better than language models and the various Seq2Seq settings do not show large ROUGE differences. Figure 3 shows an example of generation for the language model and the best Seq2Seq and extractive settings (see Appendix F for additional random examples).",
        "sentences": [
            "6.1 Overview of Model Performance .",
            "Full answer ROUGE.",
            "Table 3 shows that the nearest neighbor baseline performs similarly to simply returning the support document which indicates that memorizing answers from the training set is insufficient.",
            "For extractive models, the oracle provides an approximate upper bound of 27.4 ROUGE-1.",
            "The BidAF model is the strongest (23.5), better than TFIDF between the question and the support document to select sentences.",
            "However, these approaches are limited by the support document, as an oracle computed on the full web sources achieves 54.8.",
            "Abstractive methods achieve higher ROUGE, likely because they can adapt to the domain shift between the web sources and the ELI5 subreddit.",
            "In general, Seq2Seq models perform better than language models and the various Seq2Seq settings do not show large ROUGE differences.",
            "Figure 3 shows an example of generation for the language model and the best Seq2Seq and extractive settings (see Appendix F for additional random examples)."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            0
        ],
        "header_mention": [
            null,
            null,
            [
                "Nearest Neighbor"
            ],
            [
                "Oracle support doc",
                "ROUGE-1"
            ],
            [
                "Extractive (BidAF)",
                "Extractive (TFIDF)"
            ],
            [
                "Oracle web sources"
            ],
            null,
            [
                "Seq2Seq Q to A",
                "Seq2Seq Q + D to A"
            ],
            null
        ],
        "n_sentence": 9.0,
        "table_id": "table_3",
        "paper_id": "P19-1346",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1359table_5",
        "description": "The bottom half of Table 5 shows the results of ablation tests. As we can see, after removing the emotion classification term (EmoDS-MLE), the performance decreased most significantly. Our interpretation is that without the emotion classification term, the model can only express the desired emotion explicitly in the generated responses and can not capture the emotional sequences not containing any emotional word. Applying an external emotion lexicon (EmoDS-EV) also brought performance decline, especially on emotion-w. This makes sense because an external emotion lexicon shares fewer words with the corpus, causing the generation process to focus on generic vocabulary and more commonplace responses to be generated. Additionally, the distinct-1/distinct-2 decreased most when using the original beam search (EmoDS-BS), indicating that the diverse decoding can promote diversity in response generation.",
        "sentences": [
            "The bottom half of Table 5 shows the results of ablation tests.",
            "As we can see, after removing the emotion classification term (EmoDS-MLE), the performance decreased most significantly.",
            "Our interpretation is that without the emotion classification term, the model can only express the desired emotion explicitly in the generated responses and can not capture the emotional sequences not containing any emotional word.",
            "Applying an external emotion lexicon (EmoDS-EV) also brought performance decline, especially on emotion-w.",
            "This makes sense because an external emotion lexicon shares fewer words with the corpus, causing the generation process to focus on generic vocabulary and more commonplace responses to be generated.",
            "Additionally, the distinct-1/distinct-2 decreased most when using the original beam search (EmoDS-BS), indicating that the diverse decoding can promote diversity in response generation."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "EmoDS-MLE"
            ],
            null,
            [
                "EmoDS-EV",
                "emotion-w"
            ],
            null,
            [
                "EmoDS-BS",
                "distinct-1",
                "distinct-2"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_5",
        "paper_id": "P19-1359",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1359table_6",
        "description": "It is shown in Table 6 that EmoDS achieved the highest performance in most cases (Sign Test, with p-value < 0.05). Specifically, for content coherence, there was no obvious difference among most models, but for emotional expression, the EmoDS yielded a significant performance boost. As we can see from Table 6, EmoDS performed well on all categories with an overall emotion score of 0.608, while EmoEmb and ECM performed poorly on categories with less training data, e.g., disgust, anger and sadness. Note that all emotion scores of Seq2Seq were the lowest, indicating that Seq2Seq is bad at emotional expression when generating responses. To sum up, EmoDS can generate meaningful responses with better emotional expression, due to the fact that EmoDS is capable of expressing the desired emotion either explicitly or implicitly.",
        "sentences": [
            "It is shown in Table 6 that EmoDS achieved the highest performance in most cases (Sign Test, with p-value < 0.05).",
            "Specifically, for content coherence, there was no obvious difference among most models, but for emotional expression, the EmoDS yielded a significant performance boost.",
            "As we can see from Table 6, EmoDS performed well on all categories with an overall emotion score of 0.608, while EmoEmb and ECM performed poorly on categories with less training data, e.g., disgust, anger and sadness.",
            "Note that all emotion scores of Seq2Seq were the lowest, indicating that Seq2Seq is bad at emotional expression when generating responses.",
            "To sum up, EmoDS can generate meaningful responses with better emotional expression, due to the fact that EmoDS is capable of expressing the desired emotion either explicitly or implicitly."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "EmoDS"
            ],
            [
                "EmoDS"
            ],
            [
                "EmoDS"
            ],
            [
                "Seq2Seq"
            ],
            [
                "EmoDS"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_6",
        "paper_id": "P19-1359",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1367table_2",
        "description": "Comparison Results. Table 2 demonstrates performances on whether using multi-level vocabularies. We can observe that incorporating multilevel vocabularies could improve performances on almost all of the metrics. For example, enc3-dec3 (MVs) improves relative performance up to 25.73% in BLEU score compared with enc3-dec3 (SV) on the Weibo dataset. Only on the Twitter dataset, enc1-dec3 (MVs) is slightly worse than \u201cenc1-dec3 (SV)\u201d in the BLEU score.",
        "sentences": [
            "Comparison Results.",
            "Table 2 demonstrates performances on whether using multi-level vocabularies.",
            "We can observe that incorporating multilevel vocabularies could improve performances on almost all of the metrics.",
            "For example, enc3-dec3 (MVs) improves relative performance up to 25.73% in BLEU score compared with enc3-dec3 (SV) on the Weibo dataset.",
            "Only on the Twitter dataset, enc1-dec3 (MVs) is slightly worse than \u201cenc1-dec3 (SV)\u201d in the BLEU score."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "enc3-dec1 (MVs)",
                "enc1-dec3 (MVs)",
                "enc3-dec3 (MVs)"
            ],
            [
                "enc3-dec1 (MVs)",
                "enc1-dec3 (MVs)",
                "enc3-dec3 (MVs)",
                "ROUGE",
                "BLEU"
            ],
            [
                "enc3-dec3 (MVs)",
                "enc3-dec3 (SV)",
                "Weibo",
                "BLEU"
            ],
            [
                "enc1-dec3 (MVs)",
                "enc1-dec3 (SV)",
                "Twitter",
                "BLEU"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_2",
        "paper_id": "P19-1367",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1368table_2",
        "description": "Taking these major differences into consideration, we still compare results against prior non-ondevice state-of-art neural networks. As shown in Table 2 only (Khanpour et al., 2016; Ortega and Vu, 2017; Lee and Dernoncourt, 2016) have evaluated on more than one task, while the rest of the methods target specific one. We denote with ? models that do not have results for the task. SGNN++ is the only approach spanning across multiple NLP tasks and languages. On the Dialog Act MRDA and SWDA tasks, SGNN++ outperformed deep learning methods like CNN (Lee and Dernoncourt, 2016), RNN (Khanpour et al., 2016) and RNN with gated attention (Tran et al., 2017) and reached the best results of 87.3% and 88.43% accuracy. For Intent Prediction, SGNN++ also improved with 0.13% 1.13% and 2.63% over the gated attention (Goo et al., 2018), the joint slot and intent biLSTM model (Hakkani-Tur et al., 2016) and the attention slot and intent RNN (Liu and Lane, 2016) on the ATIS task. This is very significant, given that (Goo et al., 2018; Hakkani-Tur et al., 2016; Liu and Lane, 2016) used a joint model to learn the slot entities and types, and used this information to better guide the intent prediction, while SGNN++ does not have any additional information about slots, entities and entity types. On Customer Feedback, SGNN++ reached better performance than Logistic regression models (Elfardy et al., 2017; Dzendzik et al., 2017). Overall, SGNN++ achieves impressive results given the small memory footprint and the fact that it did not rely on pre-trained word embeddings like (Hakkani-Tur et al., 2016; Liu and Lane, 2016) and used the same architecture and model parameters across all tasks and languages. We believe that the dimensionality-reduction techniques like locality sensitive context projections jointly coupled with deep, non-linear functions are effective at dynamically capturing low dimensional semantic text representations that are useful for text classification applications.",
        "sentences": [
            "Taking these major differences into consideration, we still compare results against prior non-ondevice state-of-art neural networks.",
            "As shown in Table 2 only (Khanpour et al., 2016; Ortega and Vu, 2017; Lee and Dernoncourt, 2016) have evaluated on more than one task, while the rest of the methods target specific one. We denote with ? models that do not have results for the task.",
            "SGNN++ is the only approach spanning across multiple NLP tasks and languages.",
            "On the Dialog Act MRDA and SWDA tasks, SGNN++ outperformed deep learning methods like CNN (Lee and Dernoncourt, 2016), RNN (Khanpour et al., 2016) and RNN with gated attention (Tran et al., 2017) and reached the best results of 87.3% and 88.43% accuracy.",
            "For Intent Prediction, SGNN++ also improved with 0.13% 1.13% and 2.63% over the gated attention (Goo et al., 2018), the joint slot and intent biLSTM model (Hakkani-Tur et al., 2016) and the attention slot and intent RNN (Liu and Lane, 2016) on the ATIS task.",
            "This is very significant, given that (Goo et al., 2018; Hakkani-Tur et al., 2016; Liu and Lane, 2016) used a joint model to learn the slot entities and types, and used this information to better guide the intent prediction, while SGNN++ does not have any additional information about slots, entities and entity types.",
            "On Customer Feedback, SGNN++ reached better performance than Logistic regression models (Elfardy et al., 2017; Dzendzik et al., 2017).",
            "Overall, SGNN++ achieves impressive results given the small memory footprint and the fact that it did not rely on pre-trained word embeddings like (Hakkani-Tur et al., 2016; Liu and Lane, 2016) and used the same architecture and model parameters across all tasks and languages.",
            "We believe that the dimensionality-reduction techniques like locality sensitive context projections jointly coupled with deep, non-linear functions are effective at dynamically capturing low dimensional semantic text representations that are useful for text classification applications."
        ],
        "class_sentence": [
            0,
            1,
            2,
            1,
            1,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "SGNN++ (our on-device)"
            ],
            [
                "SGNN++ (our on-device)",
                "CNN(Lee and Dernoncourt, 2016)",
                "RNN(Khanpour et al., 2016)",
                "JointBiLSTM(Hakkani-Tur et al., 2016)",
                "Atten.RNN(Liu and Lane, 2016)",
                "RNN+Attention(Ortega and Vu, 2017)",
                "ADAPT-Run1(Dzendzik et al., 2017)",
                "Bingo-logistic-reg(Elfardy et al., 2017)"
            ],
            [
                "SGNN++ (our on-device)"
            ],
            [
                "SGNN++ (our on-device)"
            ],
            [
                "SGNN++ (our on-device)"
            ],
            [
                "SGNN++ (our on-device)"
            ],
            null
        ],
        "n_sentence": 9.0,
        "table_id": "table_2",
        "paper_id": "P19-1368",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1372table_1",
        "description": "5.1 Comparison against Baselines. Table 1 shows our main experimental results, with baselines shown in the top and our models at the bottom. The results show that our model (Ours) outperforms competitive baselines on various evaluation metrics. The Seq2seq based models (S2S, S2S-DB and MMS) tend to generate fluent utterances and can share some overlapped words with the references, as the high BLEU-2 scores show.",
        "sentences": [
            "5.1 Comparison against Baselines.",
            "Table 1 shows our main experimental results, with baselines shown in the top and our models at the bottom.",
            "The results show that our model (Ours) outperforms competitive baselines on various evaluation metrics.",
            "The Seq2seq based models (S2S, S2S-DB and MMS) tend to generate fluent utterances and can share some overlapped words with the references, as the high BLEU-2 scores show."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Ours-First",
                "Ours-Disc",
                "Ours-MBOW",
                "Ours",
                "Ours+GMP"
            ],
            [
                "Ours-First",
                "Ours-Disc",
                "Ours-MBOW",
                "Ours",
                "Ours+GMP"
            ],
            [
                "S2S",
                "MMS",
                "BLEU-2"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_1",
        "paper_id": "P19-1372",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1374table_4",
        "description": "Conversations: Table 4 presents results on the metrics defined in Section 4.3. There are three regions of performance. First, the baseline has consistently low scores since it forms a single conversation containing all messages. Second, Elsner and Charniak (2008) and Lowe et al. (2017) perform similarly, with one doing better on VI and the other on 1-1, though Elsner and Charniak (2008) do consistently better across the exact conversation extraction metrics. Third, our methods do best, with x10 vote best in all cases except precision, where the intersect approach is much better.",
        "sentences": [
            "Conversations: Table 4 presents results on the metrics defined in Section 4.3.",
            "There are three regions of performance.",
            "First, the baseline has consistently low scores since it forms a single conversation containing all messages.",
            "Second, Elsner and Charniak (2008) and Lowe et al. (2017) perform similarly, with one doing better on VI and the other on 1-1, though Elsner and Charniak (2008) do consistently better across the exact conversation extraction metrics.",
            "Third, our methods do best, with x10 vote best in all cases except precision, where the intersect approach is much better."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "VI",
                "1-1",
                "Elsner (2008)",
                "Lowe (2017)"
            ],
            [
                "x10 vote"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_4",
        "paper_id": "P19-1374",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1374table_5",
        "description": "Dataset Variations: Table 5 shows results for the feedforward model with several modifications to the training set, designed to test corpus design decisions. Removing context does not substantially impact results. Decreasing the data size to match Elsner and Charniak (2008)\u2019s training set leads to worse results, both if the sentences are from diverse contexts (3rd row), and if they are from just two contexts (bottom row). We also see a substantial increase in the standard deviation when only two samples are used, indicating that performance is not robust when the data is not widely sampled.",
        "sentences": [
            "Dataset Variations: Table 5 shows results for the feedforward model with several modifications to the training set, designed to test corpus design decisions.",
            "Removing context does not substantially impact results.",
            "Decreasing the data size to match Elsner and Charniak (2008)\u2019s training set leads to worse results, both if the sentences are from diverse contexts (3rd row), and if they are from just two contexts (bottom row).",
            "We also see a substantial increase in the standard deviation when only two samples are used, indicating that performance is not robust when the data is not widely sampled."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "No context"
            ],
            [
                "1k random msg",
                "2x 500 msg samples"
            ],
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_5",
        "paper_id": "P19-1374",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1389table_2",
        "description": "We randomly sample 10,000 query and response segments respectively from the STCSeFun dataset for testing. Results on test query is summarized in Table 2. As stated in Section 4.1, we train different models with query/response data only (denoted as separated), as well as query and response data jointly (denoted as joint) and try two sentence encoders: CNN-based and RNN-based. From the results, we can see that the RNN-based encoder is better than the CNN-based encoder on test query consistently on all metrics.",
        "sentences": [
            "We randomly sample 10,000 query and response segments respectively from the STCSeFun dataset for testing.",
            "Results on test query is summarized in Table 2.",
            "As stated in Section 4.1, we train different models with query/response data only (denoted as separated), as well as query and response data jointly (denoted as joint) and try two sentence encoders: CNN-based and RNN-based.",
            "From the results, we can see that the RNN-based encoder is better than the CNN-based encoder on test query consistently on all metrics."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "RNN-encoder (joint)",
                "RNN-encoder (separated)",
                "CNN-encoder (separated)"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "P19-1389",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1389table_4",
        "description": "We utilize classifiers for this task to estimate the proper response sentence function given the query with/without the query sentence functions. We also implement the RNN-based and CNN-based encoders for the query representation for comparison. Table 4 shows the results on 5,000 test queries by comparing the predicted response sentence function with its annotated groundtrue response sentence function. We can observe that encoding query sentence functions is useful to improve the performance for both CNN-based and RNN-based encoders.",
        "sentences": [
            "We utilize classifiers for this task to estimate the proper response sentence function given the query with/without the query sentence functions.",
            "We also implement the RNN-based and CNN-based encoders for the query representation for comparison.",
            "Table 4 shows the results on 5,000 test queries by comparing the predicted response sentence function with its annotated groundtrue response sentence function.",
            "We can observe that encoding query sentence functions is useful to improve the performance for both CNN-based and RNN-based encoders."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "CNN-encoder (with query SeFun)",
                "RNN-encoder (with query SeFun)"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_4",
        "paper_id": "P19-1389",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1402table_2",
        "description": "Results. Table 2 illustrates the results evaluated on the downstream tasks. HiCE outperforms the baselines in all the settings. Compared to the best baseline `a la carte, the relative improvements are 12.4%, 2.9% and 5.1% for Rare-NER, BioNER, and Twitter POS, respectively. As aforementioned, the ratio of OOV words in Rare-NER is high. As a result, all the systems perform worse on Rare-NER than Bio-NER, while HiCE reaches the largest improvement than all the other baselines. Besides, our baseline embedding is trained on Wikipedia corpus (WikiText-103), which is quite different from the bio-medical texts and social media domain. The experiment demonstrates that HiCE trained on DT is already able to leverage the general language knowledge which can be transferred through different domains, and adaptation with MAML can further reduce the domain gap and enhance the performance.",
        "sentences": [
            "Results.",
            "Table 2 illustrates the results evaluated on the downstream tasks.",
            "HiCE outperforms the baselines in all the settings.",
            "Compared to the best baseline `a la carte, the relative improvements are 12.4%, 2.9% and 5.1% for Rare-NER, BioNER, and Twitter POS, respectively.",
            "As aforementioned, the ratio of OOV words in Rare-NER is high.",
            "As a result, all the systems perform worse on Rare-NER than Bio-NER, while HiCE reaches the largest improvement than all the other baselines.",
            "Besides, our baseline embedding is trained on Wikipedia corpus (WikiText-103), which is quite different from the bio-medical texts and social media domain.",
            "The experiment demonstrates that HiCE trained on DT is already able to leverage the general language knowledge which can be transferred through different domains, and adaptation with MAML can further reduce the domain gap and enhance the performance."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "HiCE w/o Morph",
                "HiCE + Morph",
                "HiCE + Morph + MAML"
            ],
            [
                "HiCE w/o Morph",
                "HiCE + Morph",
                "HiCE + Morph + MAML",
                "Rare-NER",
                "Twitter POS"
            ],
            null,
            [
                "Rare-NER",
                "HiCE w/o Morph",
                "HiCE + Morph",
                "HiCE + Morph + MAML"
            ],
            null,
            [
                "HiCE w/o Morph",
                "HiCE + Morph",
                "HiCE + Morph + MAML"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_2",
        "paper_id": "P19-1402",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1403table_5",
        "description": "Table 5 shows the absolute percentage improvement in classification performance when using each diachronic embedding compared to a classifier without diachronic embeddings. Overall, diachronic embeddings improve classification models. The diachronic embedding appears to be particularly important for NTAM, improving performance on all 6 datasets with an average increase in performance up to 2.53 points. The RCNN also benefits from diachronic embeddings, but to a lesser extent, with an improvement on 4 of the 6 datasets. Comparing the different methods for constructing diachronic embeddings, we find that our proposed subword method works the best on average for both classifiers. The incremental training method also provides improved performance for both classifiers, while the linear regression and Procrustes approaches have mixed results.",
        "sentences": [
            "Table 5 shows the absolute percentage improvement in classification performance when using each diachronic embedding compared to a classifier without diachronic embeddings.",
            "Overall, diachronic embeddings improve classification models.",
            "The diachronic embedding appears to be particularly important for NTAM, improving performance on all 6 datasets with an average increase in performance up to 2.53 points.",
            "The RCNN also benefits from diachronic embeddings, but to a lesser extent, with an improvement on 4 of the 6 datasets.",
            "Comparing the different methods for constructing diachronic embeddings, we find that our proposed subword method works the best on average for both classifiers.",
            "The incremental training method also provides improved performance for both classifiers, while the linear regression and Procrustes approaches have mixed results."
        ],
        "class_sentence": [
            1,
            2,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "NTAM"
            ],
            [
                "RCNN",
                "Incre",
                "Linear",
                "Procrustes",
                "Subword"
            ],
            [
                "NTAM",
                "Incre",
                "Linear",
                "Procrutes",
                "Subword"
            ],
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_5",
        "paper_id": "P19-1403",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1407table_1",
        "description": "4.1 Translation . We evaluate on the IWLST 14 English to German and Spanish to English translation datasets (Cettolo et al., 2015) as well as the IWSLT 15 (Cettolo et al., 2015) English to Vietnamese translation dataset. For IWSLT14 (Cettolo et al., 2015), we compare to the models evaluated by He et al. (2018), which includes a transformer (Vaswani et al., 2017) and RNN-based models (Bahdanau et al., 2014). For IWSLT15, we primarily compare to GNMT (Wu et al., 2016), which incorporates Coverage (Tu et al., 2016). Table 1 shows BLEU scores of our approach on 3 IWSLT translation tasks along with reported results from previous work. Our approach achieves state-of-the-art or comparable results on all datasets.",
        "sentences": [
            "4.1 Translation .",
            "We evaluate on the IWLST 14 English to German and Spanish to English translation datasets (Cettolo et al., 2015) as well as the IWSLT 15 (Cettolo et al., 2015) English to Vietnamese translation dataset.",
            "For IWSLT14 (Cettolo et al., 2015), we compare to the models evaluated by He et al. (2018), which includes a transformer (Vaswani et al., 2017) and RNN-based models (Bahdanau et al., 2014).",
            "For IWSLT15, we primarily compare to GNMT (Wu et al., 2016), which incorporates Coverage (Tu et al., 2016).",
            "Table 1 shows BLEU scores of our approach on 3 IWSLT translation tasks along with reported results from previous work.",
            "Our approach achieves state-of-the-art or comparable results on all datasets."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "IWSLT14",
                "IWSLT15"
            ],
            [
                "IWSLT14"
            ],
            [
                "IWSLT15"
            ],
            null,
            [
                "Scratchpad (3 layer)"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_1",
        "paper_id": "P19-1407",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1408table_5",
        "description": "A Appendix . Table 5 shows CoNLL scores and the LEA F1 values of the participating systems in the CoNLL2012 shared task (closed task with predicted syntax and mentions) based on both maximum and minimum span evaluations. Minimum spans are detected using both MINA and Collins\u00e2\u20ac\u2122 head finding rules using gold parse trees. Based on the results of Tables 5 and  6:  (1) the use  of  minimum  spans  reduces  the  gap  between the performance on gold vs. system mentions by about two percent, (2) the use of minimum instead of  maximum  spans  results  in  a  different  ordering for some of the coreference resolvers, and (3) when gold mentions are used, there are no boundary detection errors, and consequently the results using MINA are the same as those of using maximum spans. Due to recognizing the same head for  distinct  overlapping  mentions,  the  scores  using the head of gold mentions are not the same as using their maximum span, which in turn indicates MINA is suited better for detecting minimum spans compared to head words.",
        "sentences": [
            "A Appendix .",
            "Table 5 shows CoNLL scores and the LEA F1 values of the participating systems in the CoNLL2012 shared task (closed task with predicted syntax and mentions) based on both maximum and minimum span evaluations.",
            "Minimum spans are detected using both MINA and Collins\u00e2\u20ac\u2122 head finding rules using gold parse trees.",
            "Based on the results of Tables 5 and  6:  (1) the use  of  minimum  spans  reduces  the  gap  between the performance on gold vs. system mentions by about two percent, (2) the use of minimum instead of  maximum  spans  results  in  a  different  ordering for some of the coreference resolvers, and (3) when gold mentions are used, there are no boundary detection errors, and consequently the results using MINA are the same as those of using maximum spans.",
            "Due to recognizing the same head for  distinct  overlapping  mentions,  the  scores  using the head of gold mentions are not the same as using their maximum span, which in turn indicates MINA is suited better for detecting minimum spans compared to head words."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "CoNLL",
                "LEA"
            ],
            [
                "MINA"
            ],
            [
                "MINA",
                "max"
            ],
            [
                "MINA",
                "head"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_5",
        "paper_id": "P19-1408",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1409table_3",
        "description": "Table 3 presents the results on event coreference. Our joint model outperforms all the baseines with a gap of 10.5 CoNLL F1 points from the last published results (KCP), while surpassing our strong lemma baseline by 3 points.",
        "sentences": [
            "Table 3 presents the results on event coreference.",
            "Our joint model outperforms all the baseines with a gap of 10.5 CoNLL F1 points from the last published results (KCP), while surpassing our strong lemma baseline by 3 points."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Variants JOINT",
                " CoNLL",
                "F1",
                "CV (Cybulska and Vossen 2015a),71,75,73,71,78,74,-,-,64,73\nModel,Baselines,KCP (Kenyon-Dean et al. 2018)",
                "CLUSTER+KCP",
                "Baselines"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_3",
        "paper_id": "P19-1409",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1411table_2",
        "description": "4.2 Comparing to the State of the Art . This section compares our proposed model with the current state-of-the-art models for IDRR. In particular, Table 2 shows the performance of the models for the multi-class classification settings (i.e., 4-way and 11-way with PDTB-Lin and PDTB-Ji) on the corresponding test sets. The  first  observation  from  these  tables  is  that the proposed model is significantly better than the model in (Bai and Zhao, 2018) over all the dataset settings (with p <0.05) with large performance gap.  As the proposed model is developed on topof the model in (Bai and Zhao, 2018), this is a direct  comparison  and  demonstrates  the  benefit  ofthe  embeddings  for  relations  and  connectives  as well as the transfer learning mechanisms for IDRR in this work. Second, the proposed model achieves the state-of-the-art performance on the multi-class classification settings (i.e.,  Table 2) and two set-tings  for  binary  classification  (i.e.,Comparison and Expansion). The performance gaps between the  proposed  method  and  the  other  methods  on the  multiclass  classification  datasets  (i.e.,  Table2) are large and clearly testify to the advantage ofthe proposed model for IDRR.",
        "sentences": [
            "4.2 Comparing to the State of the Art .",
            "This section compares our proposed model with the current state-of-the-art models for IDRR.",
            "In particular, Table 2 shows the performance of the models for the multi-class classification settings (i.e., 4-way and 11-way with PDTB-Lin and PDTB-Ji) on the corresponding test sets.",
            "The  first  observation  from  these  tables  is  that the proposed model is significantly better than the model in (Bai and Zhao, 2018) over all the dataset settings (with p <0.05) with large performance gap.",
            " As the proposed model is developed on topof the model in (Bai and Zhao, 2018), this is a direct  comparison  and  demonstrates  the  benefit  ofthe  embeddings  for  relations  and  connectives  as well as the transfer learning mechanisms for IDRR in this work.",
            "Second, the proposed model achieves the state-of-the-art performance on the multi-class classification settings (i.e.,  Table 2) and two set-tings  for  binary  classification  (i.e.,Comparison and Expansion).",
            "The performance gaps between the  proposed  method  and  the  other  methods  on the  multiclass  classification  datasets  (i.e.,  Table2) are large and clearly testify to the advantage ofthe proposed model for IDRR."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "4-way",
                "PDTB-Lin",
                "PDTB-Ji"
            ],
            [
                "This work",
                "(Bai and Zhao 2018)"
            ],
            [
                "This work",
                "(Bai and Zhao 2018)"
            ],
            [
                "This work"
            ],
            [
                "This work"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_2",
        "paper_id": "P19-1411",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1411table_3",
        "description": "4.3 Ablation Study . The multi-task learning framework in this work involves three penalization terms (i.e., L1, L2 and L3 in Equations 2, 3 and 4). In order to illustrate the contribution of these terms, Table 3 presents the test set performance of the proposed model when different combinations of the terms are employed for the multi-class classification settings. The row with \u201cNone\u201d in the table corresponds to the proposed model where none of the penalization terms (L1, L2 and L3) is used, reducing to the model in (Bai and Zhao, 2018) that is augmented with the connective and relation embeddings.  As we can see from the table, the embeddings of connectives  and  relations  can  only  slightly  improve the performance of the model in (Bai and Zhao, 2018), necessitating the penalization terms L1, L2 and L3 to facilitate the knowledge transfer and further improve the performance. From the table, it is also clear that each penalization term is important for the proposed model as eliminating any of them would worsen the performance.  Combining the three penalization terms results in the best performance for IDRR in this work.",
        "sentences": [
            "4.3 Ablation Study .",
            "The multi-task learning framework in this work involves three penalization terms (i.e., L1, L2 and L3 in Equations 2, 3 and 4).",
            "In order to illustrate the contribution of these terms, Table 3 presents the test set performance of the proposed model when different combinations of the terms are employed for the multi-class classification settings.",
            "The row with \u201cNone\u201d in the table corresponds to the proposed model where none of the penalization terms (L1, L2 and L3) is used, reducing to the model in (Bai and Zhao, 2018) that is augmented with the connective and relation embeddings.",
            " As we can see from the table, the embeddings of connectives  and  relations  can  only  slightly  improve the performance of the model in (Bai and Zhao, 2018), necessitating the penalization terms L1, L2 and L3 to facilitate the knowledge transfer and further improve the performance.",
            "From the table, it is also clear that each penalization term is important for the proposed model as eliminating any of them would worsen the performance.",
            " Combining the three penalization terms results in the best performance for IDRR in this work."
        ],
        "class_sentence": [
            0,
            2,
            1,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "L1",
                "L2",
                "L3"
            ],
            null,
            [
                "L1",
                "L2",
                "L3"
            ],
            [
                "L1 + L2 + L3"
            ],
            [
                "L1",
                "L2",
                "L3",
                "None"
            ],
            [
                "L1 + L2 + L3"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_3",
        "paper_id": "P19-1411",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1412table_3",
        "description": "Focusing on the restricted set, we perform detailed error analysis of the outputs of the rule-based and hybrid biLSTM models, which achieved the best correlation. Table 3 shows performance for the following linguistic features. The rule-based model can only capture inferences involving negation (r = 0.45), while the hybrid model performs more consistently across negation, modal, and question (r \u00e2\u02c6\u00bc 0.25). Both models cannot handle inferences with conditionals.",
        "sentences": [
            "Focusing on the restricted set, we perform detailed error analysis of the outputs of the rule-based and hybrid biLSTM models, which achieved the best correlation.",
            "Table 3 shows performance for the following linguistic features.",
            "The rule-based model can only capture inferences involving negation (r = 0.45), while the hybrid model performs more consistently across negation, modal, and question (r \u00e2\u02c6\u00bc 0.25).",
            "Both models cannot handle inferences with conditionals."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "r",
                "Rule",
                "Negation",
                "Modal",
                "Question"
            ],
            [
                "r",
                "Rule",
                "Cond."
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "P19-1412",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1414table_3",
        "description": "4.4 Results . Table 3 shows the performances of all the methods in the Precision of the top answer (P@1) and the Mean Average Precision (MAP) (Oh et al., 2013). Note that the Oracle method indicates the performance of a fictional method that ranks the answer passages perfectly, i.e., it locates all the m correct answers to a question in the top-m ranks, based on the gold-standard labels. This performance is the upper bound of those of all the implementable methods. Our proposed method, Ours(OP), outperformed all the other methods. Our starting point,i.e.,BASE, was already superior to the methods in the previous works. Compared with BASE and BASE+AddTr, neither of which used compact-answer representations or fake-representation generator F, Ours(OP) gave 3.4% and 2.8% improvement in P@1, respectively. It also outperformed BASE+CAns and BASE+CEnc, which generated compact-answer representations in away different from the proposed method, and BASE+Enc, which trained the fake-representation generator without adversarial learning. These performance differences were statistically significant (p <0.01by the McNemar\u2019s test). Ours (OP)also outperformed all the BERT-based models but an interesting point is that fake-representation generatorFboosted the performance of the BERT-based models (statistically significant with p <0.01by the McNemar\u2019s test). These results suggest that AGR is effective in both our why-QA model and our BERT-based model.",
        "sentences": [
            "4.4 Results .",
            "Table 3 shows the performances of all the methods in the Precision of the top answer (P@1) and the Mean Average Precision (MAP) (Oh et al., 2013).",
            "Note that the Oracle method indicates the performance of a fictional method that ranks the answer passages perfectly, i.e., it locates all the m correct answers to a question in the top-m ranks, based on the gold-standard labels.",
            "This performance is the upper bound of those of all the implementable methods.",
            "Our proposed method, Ours(OP), outperformed all the other methods.",
            "Our starting point,i.e.,BASE, was already superior to the methods in the previous works.",
            "Compared with BASE and BASE+AddTr, neither of which used compact-answer representations or fake-representation generator F, Ours(OP) gave 3.4% and 2.8% improvement in P@1, respectively.",
            "It also outperformed BASE+CAns and BASE+CEnc, which generated compact-answer representations in away different from the proposed method, and BASE+Enc, which trained the fake-representation generator without adversarial learning.",
            "These performance differences were statistically significant (p <0.01by the McNemar\u2019s test).",
            "Ours (OP)also outperformed all the BERT-based models but an interesting point is that fake-representation generatorFboosted the performance of the BERT-based models (statistically significant with p <0.01by the McNemar\u2019s test).",
            "These results suggest that AGR is effective in both our why-QA model and our BERT-based model."
        ],
        "class_sentence": [
            2,
            1,
            2,
            2,
            1,
            1,
            1,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "P@1",
                "MAP"
            ],
            [
                "Oracle"
            ],
            null,
            [
                "Ours (OP)"
            ],
            [
                "BASE"
            ],
            [
                "Ours (OP)",
                "BASE",
                "BASE+AddTr",
                "P@1"
            ],
            [
                "Ours (OP)",
                "BASE+CAns",
                "BASE+CEnc",
                "BASE+Enc"
            ],
            null,
            [
                "Ours (OP)",
                "BERT"
            ],
            null
        ],
        "n_sentence": 11.0,
        "table_id": "table_3",
        "paper_id": "P19-1414",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1415table_2",
        "description": "Table 2 shows the exact match and F1 scores of multiple reading comprehension models with and without data augmentation. We can see that the generated unanswerable questions can improve both specifically designed reading comprehension models and strong BERT fine-tuning models, yielding 1.9 absolute F1 improvement with BERTbase model and 1.7 absolute F1 improvement with BERT-large model. Our submitted model obtains an EM score of 80.75 and an F1 score of 83.85 on the hidden test set.",
        "sentences": [
            "Table 2 shows the exact match and F1 scores of multiple reading comprehension models with and without data augmentation.",
            "We can see that the generated unanswerable questions can improve both specifically designed reading comprehension models and strong BERT fine-tuning models, yielding 1.9 absolute F1 improvement with BERTbase model and 1.7 absolute F1 improvement with BERT-large model.",
            "Our submitted model obtains an EM score of 80.75 and an F1 score of 83.85 on the hidden test set."
        ],
        "class_sentence": [
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "EM",
                "F1"
            ],
            [
                "F1",
                "BERTBase + UNANSQ",
                "BERT Large+ UNANSQ"
            ],
            null
        ],
        "n_sentence": 3.0,
        "table_id": "table_2",
        "paper_id": "P19-1415",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1415table_3",
        "description": "Table 3 shows the human evaluation results of generated unanswerable questions. We compare with the baseline method TFIDF, which uses the input answerable question to retrieve similar questions towards other articles as outputs. The retrieved questions are mostly unanswerable and readable, but they are not quite relevant to the question answering pair. Notice that being relevant is demonstrated to be important for data augmentation in further experiments on machine reading comprehension. Here pair-to-sequence model still outperforms sequence-to-sequence model in terms of all three metrics. But the differences in human evaluation are not as notable as in the automatic metrics.",
        "sentences": [
            "Table 3 shows the human evaluation results of generated unanswerable questions.",
            "We compare with the baseline method TFIDF, which uses the input answerable question to retrieve similar questions towards other articles as outputs.",
            "The retrieved questions are mostly unanswerable and readable, but they are not quite relevant to the question answering pair.",
            "Notice that being relevant is demonstrated to be important for data augmentation in further experiments on machine reading comprehension.",
            "Here pair-to-sequence model still outperforms sequence-to-sequence model in terms of all three metrics.",
            "But the differences in human evaluation are not as notable as in the automatic metrics."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "TFIDF"
            ],
            [
                "UNANS",
                " READ"
            ],
            [
                " RELA"
            ],
            [
                "PAIR2SEQ",
                "SEQ2SEQ"
            ],
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_3",
        "paper_id": "P19-1415",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1425table_2",
        "description": "Table 2 shows the comparisons to the above five baseline methods. Among all methods trained without extra corpora, our approach achieves the best result across datasets. After incorporating the back-translated corpus, our method yields an additional gain of 1-3 points over (Sennrich et al., 2016b) trained on the same back-translated corpus. Since all methods are built on top of the same backbone, the result substantiates the efficacy of our method on the standard benchmarks that contain natural noise. Compared to (Miyato et al., 2017), we found that continuous gradientbased perturbations to word embeddings can be absorbed quickly, often resulting in a worse BLEU score than the proposed discrete perturbations by word replacement.",
        "sentences": [
            "Table 2 shows the comparisons to the above five baseline methods.",
            "Among all methods trained without extra corpora, our approach achieves the best result across datasets.",
            "After incorporating the back-translated corpus, our method yields an additional gain of 1-3 points over (Sennrich et al., 2016b) trained on the same back-translated corpus.",
            "Since all methods are built on top of the same backbone, the result substantiates the efficacy of our method on the standard benchmarks that contain natural noise.",
            "Compared to (Miyato et al., 2017), we found that continuous gradientbased perturbations to word embeddings can be absorbed quickly, often resulting in a worse BLEU score than the proposed discrete perturbations by word replacement."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "Ours",
                "Trans.-Base"
            ],
            [
                "Ours + BackTranslation*",
                "Trans.-Base"
            ],
            null,
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_2",
        "paper_id": "P19-1425",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1425table_3",
        "description": "Table 3 shows the BLEU scores on the NIST Chinese-English translation task. We first compare our approach with the Transformer model (Vaswani et al., 2017) on which our model is built. As we see, the introduction of our method to the standard backbone model (Trans.-Base) leads to substantial improvements across the validation and test sets. Specifically, our approach achieves an average gain of 2.25 BLEU points and up to 2.8 BLEU points on NIST03.",
        "sentences": [
            "Table 3 shows the BLEU scores on the NIST Chinese-English translation task.",
            "We first compare our approach with the Transformer model (Vaswani et al., 2017) on which our model is built.",
            "As we see, the introduction of our method to the standard backbone model (Trans.-Base) leads to substantial improvements across the validation and test sets.",
            "Specifically, our approach achieves an average gain of 2.25 BLEU points and up to 2.8 BLEU points on NIST03."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Ours",
                "Vaswani et al. (2017)"
            ],
            [
                "Ours"
            ],
            [
                "Ours"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "P19-1425",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1429table_2",
        "description": "4.2 Overall Performance . Table 2 shows the overall ACE2005 results of all baselines and our approach. For our approach, we show the results of four settings: our approach using word embedding as its word representation rw \u2013 \u2206w2v; our approach using ELMo as rw \u2206ELM o; our approach simply concatenating [rd, rg, rw] as instance representation \u2206concat . From Table 2, we can see that by distilling both discrimination and generalization knowledge, our method achieves state-of-the-art performance. Compared with the best feature system, \u2206w2v and \u2206ELMo gain 2.8 and 4.6 F1-score improvements. Compared to the representation learning based baselines, both \u2206w2v and \u2206ELMo outperform all of them. Notably, \u2206ELMo outperforms all the baselines using external resources.",
        "sentences": [
            "4.2 Overall Performance .",
            "Table 2 shows the overall ACE2005 results of all baselines and our approach.",
            "For our approach, we show the results of four settings: our approach using word embedding as its word representation rw \u2013 \u2206w2v; our approach using ELMo as rw \u2206ELM o; our approach simply concatenating [rd, rg, rw] as instance representation \u2206concat .",
            "From Table 2, we can see that by distilling both discrimination and generalization knowledge, our method achieves state-of-the-art performance.",
            "Compared with the best feature system, \u2206w2v and \u2206ELMo gain 2.8 and 4.6 F1-score improvements.",
            "Compared to the representation learning based baselines, both \u2206w2v and \u2206ELMo outperform all of them.",
            "Notably, \u2206ELMo outperforms all the baselines using external resources."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "Our Approach"
            ],
            [
                "Feature based Approaches",
                "\u2206w2v",
                "\u2206ELM o",
                " F1"
            ],
            [
                "Representation Learning based Approaches",
                "\u2206w2v",
                "\u2206ELM o"
            ],
            [
                "\u2206ELM o",
                "External Resource based Approaches"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_2",
        "paper_id": "P19-1429",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1435table_1",
        "description": "Predicting semantic relationships without using sentence contents seems impossible. However, we find that the graph-based features (Leakage and Advanced) make the problem feasible on a wide range of datasets. Specifically, on the datasets like QuoraQP and ByteDance, the leakage features are even more effective than the unlexicalized features. One exception is that on MultiNLI, Majority outperforms Leakage and Advanced significantly. Another interesting finding is that on SNLI and ByteDance, advanced graph-based features improve a lot over the leakage features, while on QuoraQP, the difference is very small. Among all the tested datasets, only MSRP and SICKNLI are almost neutral to the leakage features. Note that their sizes are relatively small with only less than 10k samples. Results in Table 1 raise concerns about the impact of selection bias on the models and evaluation results.",
        "sentences": [
            "Predicting semantic relationships without using sentence contents seems impossible.",
            "However, we find that the graph-based features (Leakage and Advanced) make the problem feasible on a wide range of datasets.",
            "Specifically, on the datasets like QuoraQP and ByteDance, the leakage features are even more effective than the unlexicalized features.",
            "One exception is that on MultiNLI, Majority outperforms Leakage and Advanced significantly.",
            "Another interesting finding is that on SNLI and ByteDance, advanced graph-based features improve a lot over the leakage features, while on QuoraQP, the difference is very small.",
            "Among all the tested datasets, only MSRP and SICKNLI are almost neutral to the leakage features.",
            "Note that their sizes are relatively small with only less than 10k samples.",
            "Results in Table 1 raise concerns about the impact of selection bias on the models and evaluation results."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "Leakage",
                "Advanced"
            ],
            [
                " QuoraQP",
                " ByteDance",
                "Leakage",
                "Unlexicalized"
            ],
            [
                " MultiNLI Matched",
                "MultiNLI Mismatched",
                "Majority",
                "Leakage",
                "Advanced"
            ],
            [
                " SNLI",
                " ByteDance",
                " QuoraQP",
                "Advanced",
                "Leakage"
            ],
            [
                " MSRP",
                "SICK NLI",
                "Leakage"
            ],
            null,
            null
        ],
        "n_sentence": 8.0,
        "table_id": "table_1",
        "paper_id": "P19-1435",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1435table_4",
        "description": "Table 4 reports the results on the datasets that are not biased to the leakage pattern of QuoraQP. We find that the Debiased Model significantly outperforms the Biased Model on all three datasets. This indicates that the Debiased Model better captures the true semantic similarities of the input sentences. We further visualize the predictions on the synthetic dataset in Figure 6. As illustrated, the predictions are more neutral to the leakage feature. From the experimental results, we can see that the proposed leakage-neutral training method is effective, as the Debiased Model performs significantly better with Synthetic dataset, MSRP and SICK, showing a better generalization strength.",
        "sentences": [
            "Table 4 reports the results on the datasets that are not biased to the leakage pattern of QuoraQP.",
            "We find that the Debiased Model significantly outperforms the Biased Model on all three datasets.",
            "This indicates that the Debiased Model better captures the true semantic similarities of the input sentences.",
            "We further visualize the predictions on the synthetic dataset in Figure 6.",
            "As illustrated, the predictions are more neutral to the leakage feature.",
            "From the experimental results, we can see that the proposed leakage-neutral training method is effective, as the Debiased Model performs significantly better with Synthetic dataset, MSRP and SICK, showing a better generalization strength."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "Debiased Model",
                "Biased Model"
            ],
            [
                "Debiased Model"
            ],
            null,
            null,
            [
                "Debiased Model",
                "Synthetic",
                "MSRP",
                "SICK STS"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_4",
        "paper_id": "P19-1435",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1436table_1",
        "description": "Results . Table 1 compares the performance of our system with different baselines in terms of efficiency and accuracy. We note the following observations from the result table. (1) DENSPI outperforms the query-agnostic baseline (Seo et al., 2018) by a large margin, 20.1% EM and 18.5% F1. This is largely credited towards the usage of BERT encoder with an effective phrase embedding mechanism on the top. (2) DENSPI outperforms DrQA by 3.3% EM. This signifies that phrase-indexed models can now outperform early (unconstrained) state-of-the-art models in SQuAD. (3) DENSPI is 9.2% below the current state of the art. The difference, which we call decomposability gap5, is now within 10% and future work will involve further closing the gap. (4) Query-agnostic models can process (read) words much faster than query-dependent representation models. In a controlled environment where all information is in memory and the documents are pre-indexed, DENSPI can process 28.7 million words per second, which is 6,000 times faster than DrQA and 563,000 times faster than BERT without any approximation.",
        "sentences": [
            "Results .",
            "Table 1 compares the performance of our system with different baselines in terms of efficiency and accuracy.",
            "We note the following observations from the result table.",
            "(1) DENSPI outperforms the query-agnostic baseline (Seo et al., 2018) by a large margin, 20.1% EM and 18.5% F1.",
            "This is largely credited towards the usage of BERT encoder with an effective phrase embedding mechanism on the top.",
            "(2) DENSPI outperforms DrQA by 3.3% EM.",
            "This signifies that phrase-indexed models can now outperform early (unconstrained) state-of-the-art models in SQuAD.",
            "(3) DENSPI is 9.2% below the current state of the art.",
            "The difference, which we call decomposability gap5, is now within 10% and future work will involve further closing the gap.",
            "(4) Query-agnostic models can process (read) words much faster than query-dependent representation models.",
            "In a controlled environment where all information is in memory and the documents are pre-indexed, DENSPI can process 28.7 million words per second, which is 6,000 times faster than DrQA and 563,000 times faster than BERT without any approximation."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2,
            1,
            2,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "DENSPI (dense only)",
                "LSTM+SA+ELMo"
            ],
            null,
            [
                "DENSPI (dense only)",
                "DrQA"
            ],
            null,
            [
                "DENSPI (dense only)",
                "BERT-Large"
            ],
            null,
            [
                "Query-Agnostic"
            ],
            [
                "DENSPI (dense only)",
                "DrQA"
            ]
        ],
        "n_sentence": 11.0,
        "table_id": "table_1",
        "paper_id": "P19-1436",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1441table_2",
        "description": "MT-DNNno-fine-tune. Since the MTL of MT-DNN uses all GLUE tasks, it is possible to directly apply MT-DNN to each GLUE task without finetuning. The results in Table 2 show that MTDNNno-fine-tune still outperforms BERTLARGE consistently among all tasks but CoLA. Our analysis shows that CoLA is a challenge task with much smaller in-domain data than other tasks, and its task definition and dataset are unique among all GLUE tasks, making it difficult to benefit from the knowledge learned from other tasks. As a result, MTL tends to underfit the CoLA dataset. In such a case, fine-tuning is necessary to boost the performance. As shown in Table 2, the accuracy improves from 58.9% to 62.5% after finetuning, even though only a very small amount of in-domain data is available for adaptation. This, together with the fact that the fine-tuned MT-DNN significantly outperforms the fine-tuned BERTLARGE on CoLA (62.5% vs. 60.5%), reveals that the learned MT-DNN representation allows much more effective domain adaptation than the pre-trained BERT representation. We will revisit this topic with more experiments in Section 4.4.",
        "sentences": [
            "MT-DNNno-fine-tune.",
            "Since the MTL of MT-DNN uses all GLUE tasks, it is possible to directly apply MT-DNN to each GLUE task without finetuning.",
            "The results in Table 2 show that MTDNNno-fine-tune still outperforms BERTLARGE consistently among all tasks but CoLA.",
            "Our analysis shows that CoLA is a challenge task with much smaller in-domain data than other tasks, and its task definition and dataset are unique among all GLUE tasks, making it difficult to benefit from the knowledge learned from other tasks.",
            "As a result, MTL tends to underfit the CoLA dataset.",
            "In such a case, fine-tuning is necessary to boost the performance.",
            "As shown in Table 2, the accuracy improves from 58.9% to 62.5% after finetuning, even though only a very small amount of in-domain data is available for adaptation.",
            "This, together with the fact that the fine-tuned MT-DNN significantly outperforms the fine-tuned BERTLARGE on CoLA (62.5% vs. 60.5%), reveals that the learned MT-DNN representation allows much more effective domain adaptation than the pre-trained BERT representation.",
            "We will revisit this topic with more experiments in Section 4.4."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0
        ],
        "header_mention": [
            [
                "MT-DNNno-fine-tune"
            ],
            [
                "MT-DNN"
            ],
            [
                "MT-DNNno-fine-tune",
                "BERT LARGE 4",
                " CoLA"
            ],
            [
                " CoLA"
            ],
            [
                " CoLA",
                "MT-DNN"
            ],
            null,
            [
                "MT-DNNno-fine-tune",
                "MT-DNN"
            ],
            [
                "MT-DNN",
                "BERT LARGE 4",
                " CoLA"
            ],
            null
        ],
        "n_sentence": 9.0,
        "table_id": "table_2",
        "paper_id": "P19-1441",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1443table_8",
        "description": "Performance stratified by SQL difficulty We group individual questions in SParC into different difficulty levels based on the complexity of their corresponding SQL representations using the criteria proposed in Yu et al.(2018c). As shown in Figure 3, the questions turned to get harder as interaction proceeds, more questions with hard and extra hard difficulties appear in late turns. Table 8 shows the performance of the two models across each difficulty level. As we expect, the models perform better when the user request is easy. Both models fail on most hard and extra hard questions. Considering that the size and question types of SParC are very close to Spider, the relatively lower performances of SyntaxSQLNet on medium, hard and extra hard questions in Table 8 comparing to its performances on Spider (17.6%, 16.3%, and 4.9% respectively) indicates that SParC introduces additional challenge by introducing context dependencies, which is absent from Spider.",
        "sentences": [
            "Performance stratified by SQL difficulty We group individual questions in SParC into different difficulty levels based on the complexity of their corresponding SQL representations using the criteria proposed in Yu et al.(2018c).",
            "As shown in Figure 3, the questions turned to get harder as interaction proceeds, more questions with hard and extra hard difficulties appear in late turns.",
            "Table 8 shows the performance of the two models across each difficulty level.",
            "As we expect, the models perform better when the user request is easy.",
            "Both models fail on most hard and extra hard questions.",
            "Considering that the size and question types of SParC are very close to Spider, the relatively lower performances of SyntaxSQLNet on medium, hard and extra hard questions in Table 8 comparing to its performances on Spider (17.6%, 16.3%, and 4.9% respectively) indicates that SParC introduces additional challenge by introducing context dependencies, which is absent from Spider."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                " CD-Seq2Seq",
                " SyntaxSQL-con",
                "Goal Difficulty"
            ],
            [
                "Easy (483)"
            ],
            [
                "Hard (145)",
                "Extra hard (134)"
            ],
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_8",
        "paper_id": "P19-1443",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1446table_4",
        "description": "3.4 Evaluating with SEMBLEU . Table 4 shows the SEMBLEU and SMATCH scores several recent models. In particular, we asked for the outputs of Lyu (Lyu and Titov, 2018), Gros (Groschwitz et al., 2018), van Nood (van Noord and Bos, 2017) and Guo (Guo and Lu, 2018) to evaluate on our SEMBLEU. For CAMR and JAMR, we obtain their outputs by running the released systems. SEMBLEU is mostly consistent with SMATCH, except for the order between Guo and Gros. It is probably because Guo has more highorder correspondences with the reference.",
        "sentences": [
            "3.4 Evaluating with SEMBLEU .",
            "Table 4 shows the SEMBLEU and SMATCH scores several recent models.",
            "In particular, we asked for the outputs of Lyu (Lyu and Titov, 2018), Gros (Groschwitz et al., 2018), van Nood (van Noord and Bos, 2017) and Guo (Guo and Lu, 2018) to evaluate on our SEMBLEU.",
            "For CAMR and JAMR, we obtain their outputs by running the released systems.",
            "SEMBLEU is mostly consistent with SMATCH, except for the order between Guo and Gros.",
            "It is probably because Guo has more highorder correspondences with the reference."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                " SEMBLEU"
            ],
            [
                " SEMBLEU",
                " SMATCH"
            ],
            [
                " Gros",
                " Guo",
                " SEMBLEU"
            ],
            [
                " CAMR",
                " JAMR"
            ],
            [
                " SEMBLEU",
                " SMATCH",
                " Guo"
            ],
            [
                " Guo"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_4",
        "paper_id": "P19-1446",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1453table_1",
        "description": "Results in Table 1 show two observations. First, models trained on En-En, in contrast to those trained on En-CS, have higher correlation for all encoders except SP. However, when the same number of English sentences is used, models trained on bitext have greater than or equal performance across all encoders. Second, SP has the best performance in the En-CS setting. It also has fewer parameters and is therefore faster to train than LSTM-SP and TRIGRAM. Further, it is much faster at encoding new sentences at test time.",
        "sentences": [
            "Results in Table 1 show two observations.",
            "First, models trained on En-En, in contrast to those trained on En-CS, have higher correlation for all encoders except SP.",
            "However, when the same number of English sentences is used, models trained on bitext have greater than or equal performance across all encoders.",
            "Second, SP has the best performance in the En-CS setting.",
            "It also has fewer parameters and is therefore faster to train than LSTM-SP and TRIGRAM.",
            "Further, it is much faster at encoding new sentences at test time."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "En-En",
                "En-Cs (1M)",
                "En-Cs (2M)",
                "Model",
                "SP (20k)"
            ],
            [
                "En-Cs (2M)",
                "En-En"
            ],
            [
                "LSTM-SP (20k)",
                "SP (20k)",
                "En-Cs (1M)",
                "En-Cs (2M)"
            ],
            [
                "LSTM-SP (20k)",
                "SP (20k)",
                "TRIGRAM"
            ],
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_1",
        "paper_id": "P19-1453",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1457table_2",
        "description": "We re-implement constituent Tree-LSTM (ConTree) of Tai et al. (2015) and obtain better results than their original implementation. We then integrate ConTree with Weighted Grammars (ConTree+WG), Latent Variable Grammars with a subtype number of 4 (ConTree+LVG4), and Latent Variable Grammars (ConTree+LVeG), respectively. Table 2 shows the experimental results for sentiment classification on both SST-5 and SST-2 at the sentence level (Root) and all nodes (Phrase).",
        "sentences": [
            "We re-implement constituent Tree-LSTM (ConTree) of Tai et al. (2015) and obtain better results than their original implementation.",
            "We then integrate ConTree with Weighted Grammars (ConTree+WG), Latent Variable Grammars with a subtype number of 4 (ConTree+LVG4), and Latent Variable Grammars (ConTree+LVeG), respectively.",
            "Table 2 shows the experimental results for sentiment classification on both SST-5 and SST-2 at the sentence level (Root) and all nodes (Phrase)."
        ],
        "class_sentence": [
            1,
            2,
            1
        ],
        "header_mention": [
            [
                "ConTree (Our implementation)",
                "ConTree (Tai et al., 2015)",
                "ConTree (Le and Zuidema, 2015)",
                "ConTree (Zhu et al., 2015)",
                "ConTree (Li et al., 2015)"
            ],
            [
                "ConTree + WG",
                "ConTree + LVG4",
                "ConTree + LVeG"
            ],
            [
                " SST-5 Root",
                " SST-5 Phrase",
                " SST-2 Root",
                " SST-2 Phrase"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_2",
        "paper_id": "P19-1457",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1457table_3",
        "description": "There has also been work using large-scale external datasets to improve performances of sentiment classification. Peters et al. (2018) combined bi-attentive classification network (BCN, McCann et al. (2017)) with a pretrained language modelwith character convolutions on a large-scale corpus (ELMo) and reported an accuracy of 54.7 on sentence-level SST-5. For fair comparison, we also augment our model with ELMo. Table 3 shows that our methods beat the baseline on every task. BCN+WG improves accuracies on all task slightly by modeling sentiment composition explicitly. The obvious promotion of BCN+LVG4 and BCN+LVeG shows that explicitly modeling sentiment composition with fine-grained sentiment subtypes is useful. Particularly, BCN+LVeG improves the sentence level classification accurracies by 1.4 points (fine-grained) and 0.7 points (binary) compared to BCN (our implementation), respectively. To our knowledge, we achieve the best results on the SST dataset.",
        "sentences": [
            "There has also been work using large-scale external datasets to improve performances of sentiment classification.",
            "Peters et al. (2018) combined bi-attentive classification network (BCN, McCann et al. (2017)) with a pretrained language modelwith character convolutions on a large-scale corpus (ELMo) and reported an accuracy of 54.7 on sentence-level SST-5.",
            "For fair comparison, we also augment our model with ELMo.",
            "Table 3 shows that our methods beat the baseline on every task.",
            "BCN+WG improves accuracies on all task slightly by modeling sentiment composition explicitly.",
            "The obvious promotion of BCN+LVG4 and BCN+LVeG shows that explicitly modeling sentiment composition with fine-grained sentiment subtypes is useful.",
            "Particularly, BCN+LVeG improves the sentence level classification accurracies by 1.4 points (fine-grained) and 0.7 points (binary) compared to BCN (our implementation), respectively.",
            "To our knowledge, we achieve the best results on the SST dataset."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "BCN(P)",
                " SST-5"
            ],
            null,
            [
                "BCN+WG",
                "BCN+LVG4",
                "BCN+LVeG"
            ],
            [
                "BCN+WG"
            ],
            [
                "BCN+LVG4",
                "BCN+LVeG"
            ],
            [
                "BCN+LVeG",
                "BCN(O)"
            ],
            [
                " SST-5",
                " SST-2"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_3",
        "paper_id": "P19-1457",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1458table_6",
        "description": "7.3 Size of Pre-Training Corpus.  We also investigate whether the size of the source language model affects the sentiment analysis performance on the Yahoo dataset. This is especially important for low-resource languages that do not usually have large amounts of data available for training. We used the ja.text816 small text corpus (100MB) from the Japanese Wikipedia to compare with the whole Wikipedia (2.9GB) used in our previous experiments. Table 6 shows slightly lower performance for BCN+ELMo and ULMFiT while BERT performed much worse. Thus, for effective sentiment analysis, a large corpus is required for pre-training BERT.",
        "sentences": [
            "7.3 Size of Pre-Training Corpus.",
            " We also investigate whether the size of the source language model affects the sentiment analysis performance on the Yahoo dataset.",
            "This is especially important for low-resource languages that do not usually have large amounts of data available for training.",
            "We used the ja.text816 small text corpus (100MB) from the Japanese Wikipedia to compare with the whole Wikipedia (2.9GB) used in our previous experiments.",
            "Table 6 shows slightly lower performance for BCN+ELMo and ULMFiT while BERT performed much worse.",
            "Thus, for effective sentiment analysis, a large corpus is required for pre-training BERT."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            [
                "BCN+ELMo",
                "BCN+ELMo [100MB]",
                "ULMFiT",
                "ULMFiT Adapted [100MB]",
                "BERTBASE",
                "BERTBASE [100MB]"
            ],
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_6",
        "paper_id": "P19-1458",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1465table_3",
        "description": "Results on Quora dataset are listed in Table 3. Since paraphrase identification is a symmetric task where two input sequences can be swapped with no effect to the label of the text pair, in hyperparameter tuning we validate between two symmetric versions of the prediction layer (Equation 6 and Equation 7) and use no additional data augmentation. The performance of RE2 is on par with the state-of-the-art on this dataset.",
        "sentences": [
            "Results on Quora dataset are listed in Table 3.",
            "Since paraphrase identification is a symmetric task where two input sequences can be swapped with no effect to the label of the text pair, in hyperparameter tuning we validate between two symmetric versions of the prediction layer (Equation 6 and Equation 7) and use no additional data augmentation.",
            "The performance of RE2 is on par with the state-of-the-art on this dataset."
        ],
        "class_sentence": [
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "RE2 (ours)"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_3",
        "paper_id": "P19-1465",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1469table_1",
        "description": "Table 1 summarizes the result of experiments. We can clearly see that the proposed CS-LVM architecture substantially outperforms other models based on auto-encoding. Also, the semantic constraints brought additional boost in performance, achieving the new state of the art in semisupervised classification of the SNLI dataset. When all training data are used as labeled data (? 550k), CS-LVM also improves performance by achieving accuracy of 82.8%, compared to the supervised LSTM (81.5%), LSTM-AE (81.6%), LSTM-VAE (80.8%), DeConv-VAE (80.9%).",
        "sentences": [
            "Table 1 summarizes the result of experiments.",
            "We can clearly see that the proposed CS-LVM architecture substantially outperforms other models based on auto-encoding.",
            "Also, the semantic constraints brought additional boost in performance, achieving the new state of the art in semisupervised classification of the SNLI dataset.",
            "When all training data are used as labeled data (? 550k), CS-LVM also improves performance by achieving accuracy of 82.8%, compared to the supervised LSTM (81.5%), LSTM-AE (81.6%), LSTM-VAE (80.8%), DeConv-VAE (80.9%)."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "CS-LVM (ours)"
            ],
            [
                "CS-LVM (ours)"
            ],
            [
                "CS-LVM (ours)",
                "LSTM(a)",
                "LSTM-AE(a)",
                "LSTM-VAE(b)",
                "DeConv-VAE(b)"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_1",
        "paper_id": "P19-1469",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1470table_1",
        "description": "4.2 Results. The BLEU-2 results in Table 1 indicate that COMET exceeds the performance of all baselines, achieving a 51% relative improvement over the top performing model of Sap et al. (2019). More interesting, however, is the result of the human evaluation, where COMET reported a statistically significant relative Avg performance increase of 18% over the top baseline,Event2IN(VOLUN). This performance increase is consistent, as well, with an improvement being observed across every relation type. In addition to the quality improvements, Table 1 shows that COMET produces more novel tuple objects than the baselines, as well.",
        "sentences": [
            "4.2 Results.",
            "The BLEU-2 results in Table 1 indicate that COMET exceeds the performance of all baselines, achieving a 51% relative improvement over the top performing model of Sap et al. (2019).",
            "More interesting, however, is the result of the human evaluation, where COMET reported a statistically significant relative Avg performance increase of 18% over the top baseline,Event2IN(VOLUN).",
            "This performance increase is consistent, as well, with an improvement being observed across every relation type.",
            "In addition to the quality improvements, Table 1 shows that COMET produces more novel tuple objects than the baselines, as well."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "COMET",
                "BLEU-2"
            ],
            [
                "COMET",
                "Event2(IN)VOLUN (Sap et al., 2019)"
            ],
            [
                "COMET",
                "N/T sro6",
                "N/T o",
                "N/U o"
            ],
            [
                "COMET"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_1",
        "paper_id": "P19-1470",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1470table_4",
        "description": "Efficiency of learning from seed tuples . Because not all domains will have large available commonsense KBs on which to train, we explore how varying the amount of training data available for learning affects the quality and novelty of the knowledge that is produced. Our results in Table 4 indicate that even with only 10% of the available training data, the model is still able to produce generations that are coherent, adequate, and novel. Using only 1% of the training data clearly diminishes the quality of the produced generations, with significantly lower observed results across both quality and novelty metrics. Interestingly, we note that training the model without pretrained weights performs comparably to training with 10% of the seed tuples, quantifying the impact of using pre-trained language representations.",
        "sentences": [
            "Efficiency of learning from seed tuples .",
            "Because not all domains will have large available commonsense KBs on which to train, we explore how varying the amount of training data available for learning affects the quality and novelty of the knowledge that is produced.",
            "Our results in Table 4 indicate that even with only 10% of the available training data, the model is still able to produce generations that are coherent, adequate, and novel.",
            "Using only 1% of the training data clearly diminishes the quality of the produced generations, with significantly lower observed results across both quality and novelty metrics.",
            "Interestingly, we note that training the model without pretrained weights performs comparably to training with 10% of the seed tuples, quantifying the impact of using pre-trained language representations."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "1% train"
            ],
            [
                "10% train"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_4",
        "paper_id": "P19-1470",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1470table_6",
        "description": "5.2 Results . Quality . Our results indicate that high-quality knowledge can be generated by the model: the low perplexity scores in Table 6 indicate high model confidence in its predictions, while the high classifier score (95.25%) indicates that the KB completion model of Li et al. (2016) scores the generated tuples as correct in most of the cases. While adversarial generations could be responsible for this high score, a human evaluation (following the same design as for ATOMIC) scores 91.7% of greedily decoded tuples as correct. Randomly selected examples provided in Table 7 also point to the quality of knowledge produced by the model.",
        "sentences": [
            "5.2 Results .",
            "Quality .",
            "Our results indicate that high-quality knowledge can be generated by the model: the low perplexity scores in Table 6 indicate high model confidence in its predictions, while the high classifier score (95.25%) indicates that the KB completion model of Li et al. (2016) scores the generated tuples as correct in most of the cases.",
            "While adversarial generations could be responsible for this high score, a human evaluation (following the same design as for ATOMIC) scores 91.7% of greedily decoded tuples as correct.",
            "Randomly selected examples provided in Table 7 also point to the quality of knowledge produced by the model."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            0
        ],
        "header_mention": [
            null,
            null,
            [
                "COMET",
                "Score"
            ],
            [
                "COMET",
                "Human"
            ],
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_6",
        "paper_id": "P19-1470",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1478table_1",
        "description": "We evaluate all models on WSC273 and the WNLI test dataset, as well as the various subsets of WSC273, as described in Section 2. The results are reported in Table 1 and will be discussed next. We note that models that are fine-tuned on the WSCR dataset consistently outperform their non-fine-tuned counterparts. The BERT_WIKI_WSCR model outperforms other language models on 5 out of 6 sets that they are compared on. In comparison to the LM ensemble by Trinh and Le (2018), the accuracy is more consistent between associative and non-associative subsets and less affected by the switched parties. However, it remains fairly inconsistent, which is a general property of LMs.",
        "sentences": [
            "We evaluate all models on WSC273 and the WNLI test dataset, as well as the various subsets of WSC273, as described in Section 2.",
            "The results are reported in Table 1 and will be discussed next.",
            "We note that models that are fine-tuned on the WSCR dataset consistently outperform their non-fine-tuned counterparts.",
            "The BERT_WIKI_WSCR model outperforms other language models on 5 out of 6 sets that they are compared on.",
            "In comparison to the LM ensemble by Trinh and Le (2018), the accuracy is more consistent between associative and non-associative subsets and less affected by the switched parties.",
            "However, it remains fairly inconsistent, which is a general property of LMs."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "WSC273",
                "WNLI"
            ],
            null,
            [
                "BERT_WIKI_WSCR",
                "BERT_WSCR",
                "BERT-base_WSCR",
                "GPT_WSCR",
                "BERT_WIKI_WSCR_no_pairs",
                "BERT_WIKI_WSCR_pairs"
            ],
            [
                "BERT_WIKI_WSCR"
            ],
            [
                "LM ensemble",
                "assoc.",
                "non-assoc.",
                "switched"
            ],
            [
                "LM ensemble"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_1",
        "paper_id": "P19-1478",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1479table_4",
        "description": "4.5 Results. In Table 4, we show the results of different baseline models and our graph2seq model for the topic of entertainment. From the results we can see that our proposed graph2seq model beats all the baselines in both coherence and informativeness. Our model receives much higher scores in coherence compared with all other baseline models.",
        "sentences": [
            "4.5 Results.",
            "In Table 4, we show the results of different baseline models and our graph2seq model for the topic of entertainment.",
            "From the results we can see that our proposed graph2seq model beats all the baselines in both coherence and informativeness.",
            "Our model receives much higher scores in coherence compared with all other baseline models."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "graph2seq (proposed)"
            ],
            [
                "graph2seq (proposed)",
                "Coherence",
                "Informativeness"
            ],
            [
                "graph2seq (proposed)",
                "Coherence"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_4",
        "paper_id": "P19-1479",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1481table_2",
        "description": "CLQG+parallel: The CLQG model undergoes further training using a parallel corpus (with primary language as source and secondary language as target). After unsupervised pretraining, the encoder and decoder weights are fine-tuned using the parallel corpus. This fine-tuning further refines the language models for both languages and helps enforce the shared latent space across both languages. We observe in Table 2 that CLQG+parallel outperforms all the other models for Hindi. For Chinese, parallel fine-tuning does not give significant improvements over CLQG; this could be attributed to the parallel corpus being smaller in size (when compared to Hindi) and domain-specific (i.e.the news domain).",
        "sentences": [
            "CLQG+parallel: The CLQG model undergoes further training using a parallel corpus (with primary language as source and secondary language as target).",
            "After unsupervised pretraining, the encoder and decoder weights are fine-tuned using the parallel corpus.",
            "This fine-tuning further refines the language models for both languages and helps enforce the shared latent space across both languages.",
            "We observe in Table 2 that CLQG+parallel outperforms all the other models for Hindi.",
            "For Chinese, parallel fine-tuning does not give significant improvements over CLQG; this could be attributed to the parallel corpus being smaller in size (when compared to Hindi) and domain-specific (i.e.the news domain)."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            2
        ],
        "header_mention": [
            [
                "CLQG+parallel"
            ],
            null,
            null,
            [
                "CLQG+parallel"
            ],
            [
                "CLQG+parallel",
                "CLQG"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_2",
        "paper_id": "P19-1481",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1482table_4",
        "description": "5.4 Evaluation Results . Table 4 shows the results of automatic evaluation. It should be noted that the classification accuracy for human reference is relatively low (74.7% on Yelp and 43.2% on Amazon); thus, we do not consider it as a valid metric for comparison. For BLEU score, our method outperforms recent systems by a large margin, which shows that our outputs have higher overlap with reference sentences provided by humans.",
        "sentences": [
            "5.4 Evaluation Results .",
            "Table 4 shows the results of automatic evaluation.",
            "It should be noted that the classification accuracy for human reference is relatively low (74.7% on Yelp and 43.2% on Amazon); thus, we do not consider it as a valid metric for comparison.",
            "For BLEU score, our method outperforms recent systems by a large margin, which shows that our outputs have higher overlap with reference sentences provided by humans."
        ],
        "class_sentence": [
            0,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Acc",
                "Human",
                "Yelp",
                "Amazon"
            ],
            [
                "BLEU",
                "Point-Then-Operate"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_4",
        "paper_id": "P19-1482",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1487table_3",
        "description": "Table 3 shows the results obtained on the CQA test split. We report our two best models that represent using human explanations (CoS-E-openended) for training only and using language model explanations (CAGE-reasoning) during both train and test. We compare our approaches to the best reported models for the CQA task (Talmor et al.,2019). We observe that using CoS-E-open-ended during training improves the state-of-the-art by approximately 6%. Talmor et al. (2019) experimented with using Google search of question + answer choice for each example in the dataset and collected 100 top snippets per answer choice to be used as context for their Reading Comprehension (RC) model. They found that providing such extra data does not improve accuracy. On the other hand, using CAGE-reasoning resulted in a gain of 10% accuracy over the previous state-of-the-art. This suggests that our CoS-E-open-ended and CAGEreasoning explanations provide far more useful information than what can be achieved through simple heuristics like using Google search to find relevant snippets.",
        "sentences": [
            "Table 3 shows the results obtained on the CQA test split.",
            "We report our two best models that represent using human explanations (CoS-E-openended) for training only and using language model explanations (CAGE-reasoning) during both train and test.",
            "We compare our approaches to the best reported models for the CQA task (Talmor et al.,2019).",
            "We observe that using CoS-E-open-ended during training improves the state-of-the-art by approximately 6%.",
            "Talmor et al. (2019) experimented with using Google search of question + answer choice for each example in the dataset and collected 100 top snippets per answer choice to be used as context for their Reading Comprehension (RC) model.",
            "They found that providing such extra data does not improve accuracy.",
            "On the other hand, using CAGE-reasoning resulted in a gain of 10% accuracy over the previous state-of-the-art.",
            "This suggests that our CoS-E-open-ended and CAGEreasoning explanations provide far more useful information than what can be achieved through simple heuristics like using Google search to find relevant snippets."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "CoS-E-open-ended",
                "CAGE-reasoning"
            ],
            null,
            [
                "CoS-E-open-ended"
            ],
            [
                "RC (Talmor et al., 2019)",
                "GPT (Talmor et al., 2019)",
                "Human (Talmor et al., 2019)"
            ],
            [
                "RC (Talmor et al., 2019)",
                "GPT (Talmor et al., 2019)",
                "Human (Talmor et al., 2019)"
            ],
            [
                "CAGE-reasoning"
            ],
            [
                "CoS-E-open-ended",
                "CAGE-reasoning"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_3",
        "paper_id": "P19-1487",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1487table_4",
        "description": "Table 4 also contains results that use only the explanation and exclude the original question from CQA denoted by \u2018w/o question\u2019. These variants also use explanation during both train and validation. For these experiments we give the explanation in place of the question followed by the answer choices as input to the model. When the explanation consists of words humans selected as justification for the answer (CoS-E-selected), the model was able to obtain 53% in contrast to the 85% achieved by the open-ended human explanations (CoS-E-open-ended). Adding the question boosts performance for CoS-E-selected to 70%, again falling short of almost 90% achieved by CoS-E-open-ended. We conclude then that our full, open-ended CoS-E thus supply a significant source of information beyond simply directing the model towards the most useful information already in the question.",
        "sentences": [
            "Table 4 also contains results that use only the explanation and exclude the original question from CQA denoted by \u2018w/o question\u2019.",
            "These variants also use explanation during both train and validation.",
            "For these experiments we give the explanation in place of the question followed by the answer choices as input to the model.",
            "When the explanation consists of words humans selected as justification for the answer (CoS-E-selected), the model was able to obtain 53% in contrast to the 85% achieved by the open-ended human explanations (CoS-E-open-ended).",
            "Adding the question boosts performance for CoS-E-selected to 70%, again falling short of almost 90% achieved by CoS-E-open-ended.",
            "We conclude then that our full, open-ended CoS-E thus supply a significant source of information beyond simply directing the model towards the most useful information already in the question."
        ],
        "class_sentence": [
            1,
            2,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "CoS-E-selected",
                "CoS-E-open-ended*"
            ],
            [
                "CoS-E-selected",
                "CoS-E-open-ended*"
            ],
            [
                "CoS-E-open-ended*"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_4",
        "paper_id": "P19-1487",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1493table_6",
        "description": "We test M-BERT on the CS Hindi/English UD corpus from Bhat et al. (2018), which provides texts in two formats: transliterated, where Hindi words are written in Latin script, and corrected, where annotators have converted them back to Devanagari script. Table 6 shows the results for models fine-tuned using a combination of monolingual Hindi and English, and using the CS training set (both fine-tuning on the script-corrected version of the corpus as well as the transliterated version).",
        "sentences": [
            "We test M-BERT on the CS Hindi/English UD corpus from Bhat et al. (2018), which provides texts in two formats: transliterated, where Hindi words are written in Latin script, and corrected, where annotators have converted them back to Devanagari script.",
            "Table 6 shows the results for models fine-tuned using a combination of monolingual Hindi and English, and using the CS training set (both fine-tuning on the script-corrected version of the corpus as well as the transliterated version)."
        ],
        "class_sentence": [
            2,
            1
        ],
        "header_mention": [
            [
                "M-BERT",
                "Train on code-switched HI/EN",
                "Transliterated",
                "Corrected"
            ],
            [
                "Train on monolingual HI+EN",
                "Train on code-switched HI/EN",
                "Corrected",
                "Transliterated"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_6",
        "paper_id": "P19-1493",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1495table_7",
        "description": "Results presented in Table 7 show that the domain adaptation approach further boosts F1 by 1 point to 79 (t-test, p<0.5) and ROC AUC by 0.012. However, simply pooling the data actually hurts predictive performance leading to a drop of more than 2 points in F1.",
        "sentences": [
            "Results presented in Table 7 show that the domain adaptation approach further boosts F1 by 1 point to 79 (t-test, p<0.5) and ROC AUC by 0.012.",
            "However, simply pooling the data actually hurts predictive performance leading to a drop of more than 2 points in F1."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "Dist. Supervision + EasyAdapt",
                "F1",
                "AUC"
            ],
            [
                "Dist. Supervision + Pooling",
                "F1"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_7",
        "paper_id": "P19-1495",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1503table_1",
        "description": "The automatic evaluation scores are presented in Table 1. For abstractive sentence summarization, we report the ROUGE F1 scores compared with baselines and previous unsupervised methods. Our method outperforms commonly used prefix baselines for this task which take the first 75 characters or 8 words of the source as a summary. Our system achieves comparable results to Wang and Lee (2018) a system based on both GANs and reinforcement training. Note that the GAN-based system needs both source and target sentences for training (they are unpaired), whereas our method only needs the target domain sentences for a simple language model. In Table 1, we also list scores of the state-of-the-art supervised model, an attention based seq-to-seq model of our own implementation, as well as the oracle scores of our method obtained by choosing the best summary among all finished hypothesis from beam search.",
        "sentences": [
            "The automatic evaluation scores are presented in Table 1.",
            "For abstractive sentence summarization, we report the ROUGE F1 scores compared with baselines and previous unsupervised methods.",
            "Our method outperforms commonly used prefix baselines for this task which take the first 75 characters or 8 words of the source as a summary.",
            "Our system achieves comparable results to Wang and Lee (2018) a system based on both GANs and reinforcement training.",
            "Note that the GAN-based system needs both source and target sentences for training (they are unpaired), whereas our method only needs the target domain sentences for a simple language model.",
            "In Table 1, we also list scores of the state-of-the-art supervised model, an attention based seq-to-seq model of our own implementation, as well as the oracle scores of our method obtained by choosing the best summary among all finished hypothesis from beam search."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "R1",
                "R2",
                "RL"
            ],
            [
                "Contextual Match",
                "Lead-75C",
                "Lead-8"
            ],
            [
                "Contextual Match",
                "Wang and Lee (2018)"
            ],
            null,
            [
                "seq2seq",
                "Contextual Oracle"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_1",
        "paper_id": "P19-1503",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1514table_3",
        "description": "5.1 Results on Frequent Attributes . The first experiment is conducted on four frequent attributes (i.e., with sufficient data) on AE-110k and AE-650k datasets. Table 3 reports the comparison results of our two models (on AE-110k and AE-650k datasets) and three baselines. It is observed that our models are consistently ranked the best over all competing baselines. This indicates that our idea of regarding 'attribute' as 'query' successfully models the semantic information embedded in attribute which has been ignored by previous sequence tagging models. Besides, different from the self-attention mechanism only inside title adopted by OpenTag, our interacted similarity between attribute and title does attend to words which are more relevant to current extraction.",
        "sentences": [
            "5.1 Results on Frequent Attributes .",
            "The first experiment is conducted on four frequent attributes (i.e., with sufficient data) on AE-110k and AE-650k datasets.",
            "Table 3 reports the comparison results of our two models (on AE-110k and AE-650k datasets) and three baselines.",
            "It is observed that our models are consistently ranked the best over all competing baselines.",
            "This indicates that our idea of regarding 'attribute' as 'query' successfully models the semantic information embedded in attribute which has been ignored by previous sequence tagging models.",
            "Besides, different from the self-attention mechanism only inside title adopted by OpenTag, our interacted similarity between attribute and title does attend to words which are more relevant to current extraction."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "BiLSTM",
                "BiLSTM-CRF",
                "OpenTag",
                "Our model-110k",
                "Our model-650k"
            ],
            [
                "Our model-110k",
                "Our model-650k"
            ],
            null,
            [
                "OpenTag",
                "Our model-110k",
                "Our model-650k"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_3",
        "paper_id": "P19-1514",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1516table_1",
        "description": "The extensive results of our proposed model with comparisons to the state-of-the-art baselines techniques are reported in Table 1. Our proposed model outperforms the state-of-the-art baselines techniques by fair margins in terms of precision, recall and F1-Score for all the datasets. In our first experiment, we train two models (i.e. SingleTask BLSTM and Multi-Task BLSTM) to analyze the effect of the multi-task model (MT-BLSTM) over a single task model (ST-BLSTM). On all the three datasets, we can visualize from Table 1 that, the multi-task framework with its sharing scheme can help in boost the performance of the system. We observe the performance improvement of 5.89, 6.52 and 2.09 F1-Score points on Twitter, CADEC, and MEDLINE dataset, respectively. The similar improvement is also observed in terms of precision and recall.",
        "sentences": [
            "The extensive results of our proposed model with comparisons to the state-of-the-art baselines techniques are reported in Table 1.",
            "Our proposed model outperforms the state-of-the-art baselines techniques by fair margins in terms of precision, recall and F1-Score for all the datasets.",
            "In our first experiment, we train two models (i.e. SingleTask BLSTM and Multi-Task BLSTM) to analyze the effect of the multi-task model (MT-BLSTM) over a single task model (ST-BLSTM).",
            "On all the three datasets, we can visualize from Table 1 that, the multi-task framework with its sharing scheme can help in boost the performance of the system.",
            "We observe the performance improvement of 5.89, 6.52 and 2.09 F1-Score points on Twitter, CADEC, and MEDLINE dataset, respectively.",
            "The similar improvement is also observed in terms of precision and recall."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Proposed Model"
            ],
            [
                "ST-BLSTM",
                "MT-BLSTM (Chowdhury et al., 2018)",
                "MT-Atten-BLSTM (Chowdhury et al., 2018)"
            ],
            [
                "MT-BLSTM (Chowdhury et al., 2018)",
                "MT-Atten-BLSTM (Chowdhury et al., 2018)"
            ],
            [
                "MT-BLSTM (Chowdhury et al., 2018)",
                "MT-Atten-BLSTM (Chowdhury et al., 2018)"
            ],
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_1",
        "paper_id": "P19-1516",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1520table_2",
        "description": "The experimental results are shown in Table 2. From the results, we can see that the mined rules alone do not perform well. However, by learning from the data automatically labeled by these rules, all four versions of RINANTE achieves better performances than RINANTE (no rule). This verifies that we can indeed use the results of the mined rules to improve the performance of neural models. Moreover, the improvement over RINANTE (no rule) can be especially significant on SE14-L and SE15-R. We think this is because SE14-L is relatively more difficult and SE15-R has much less manually labeled training data. We can also see from Table 2 that the rules mined with our rule mining algorithm performs much better than Double Propagation. This is because our algorithm is able to mine hundreds of effective rules, while Double Propagation only has eight manually designed rules.",
        "sentences": [
            "The experimental results are shown in Table 2.",
            "From the results, we can see that the mined rules alone do not perform well.",
            "However, by learning from the data automatically labeled by these rules, all four versions of RINANTE achieves better performances than RINANTE (no rule).",
            "This verifies that we can indeed use the results of the mined rules to improve the performance of neural models.",
            "Moreover, the improvement over RINANTE (no rule) can be especially significant on SE14-L and SE15-R.",
            "We think this is because SE14-L is relatively more difficult and SE15-R has much less manually labeled training data.",
            "We can also see from Table 2 that the rules mined with our rule mining algorithm performs much better than Double Propagation.",
            "This is because our algorithm is able to mine hundreds of effective rules, while Double Propagation only has eight manually designed rules."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Mined Rules"
            ],
            [
                "RINANTE-Shared-Alt",
                "RINANTE-Shared-Pre",
                "RINANTE-Double-Alt",
                "RINANTE-Double-Pre",
                "RINANTE (No Rule)"
            ],
            null,
            [
                "RINANTE (No Rule)",
                "SE14-L",
                "SE15-R"
            ],
            [
                "SE14-L",
                "SE15-R"
            ],
            [
                "Mined Rules",
                "DP (Qiu et al. 2011)"
            ],
            [
                "DP (Qiu et al. 2011)"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_2",
        "paper_id": "P19-1520",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1524table_1",
        "description": "3.5 Results . Table 1 shows the results on the CoNLL 2003 dataset and OntoNotes 5.0 dataset respectively. HSCRFs using gazetteer-enhanced sub-tagger outperform the baselines, achieving comparable results with those of more complex or larger models on CoNLL 2003 and new state-of-the-art results on OntoNotes 5.0. We also attached some out-of-domain analysis in the Appendix.",
        "sentences": [
            "3.5 Results .",
            "Table 1 shows the results on the CoNLL 2003 dataset and OntoNotes 5.0 dataset respectively.",
            "HSCRFs using gazetteer-enhanced sub-tagger outperform the baselines, achieving comparable results with those of more complex or larger models on CoNLL 2003 and new state-of-the-art results on OntoNotes 5.0.",
            "We also attached some out-of-domain analysis in the Appendix."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "CoNLL",
                "OntoNotes"
            ],
            [
                "HSCRF + softdict",
                "CoNLL",
                "OntoNotes"
            ],
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_1",
        "paper_id": "P19-1524",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1526table_6",
        "description": "Besides LSTM, there are a few other methods of producing the sentence representation. Table 6 compares the experimental results of these methods. The bag-of-tags method simply computes the average of all the POS tag embeddings and has the lowest accuracy, showing that the word order is informative for sentence encoding in D-NDMV. The anchored words method replaces the POS tag embddings used in the neural network of the neural DMV with the corresponding hidden vectors produced by a LSTM on top of the input sentence, which leads to better accuracy than bag-of-tags but is still worse than LSTM. Replacing LSTM with Bi-LSTM or attention-based LSTM also does not lead to better performance, probably because these models are more powerful and hence more likely to result in degeneration and overfitting.",
        "sentences": [
            "Besides LSTM, there are a few other methods of producing the sentence representation.",
            "Table 6 compares the experimental results of these methods.",
            "The bag-of-tags method simply computes the average of all the POS tag embeddings and has the lowest accuracy, showing that the word order is informative for sentence encoding in D-NDMV.",
            "The anchored words method replaces the POS tag embddings used in the neural network of the neural DMV with the corresponding hidden vectors produced by a LSTM on top of the input sentence, which leads to better accuracy than bag-of-tags but is still worse than LSTM.",
            "Replacing LSTM with Bi-LSTM or attention-based LSTM also does not lead to better performance, probably because these models are more powerful and hence more likely to result in degeneration and overfitting."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "LSTM"
            ],
            null,
            [
                "Bag-of-Tags Method"
            ],
            [
                "LSTM",
                "Bag-of-Tags Method"
            ],
            [
                "LSTM"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_6",
        "paper_id": "P19-1526",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1527table_1",
        "description": "5 Results . Table 1 shows the F1 score for the nested NER. When comparing the results for the nested NER in the baseline models (without the contextual word embeddings) to the previous results in literature, we see that LSTM-CRF reaches comparable, but suboptimal results in three out of four nested NE corpora, while seq2seq clearly outperforms all the known methods by a wide margin. We hypothesize that seq2seq, although more complex (the system must predict multiple labels per token, including the special label), is more suitable for more complex corpora. The gain is most visible in ACE-2004 and ACE-2005, which contain extremely long named entities and the level of \u201cnestedness\u201d is greater than in the other nested corpora.",
        "sentences": [
            "5 Results .",
            "Table 1 shows the F1 score for the nested NER.",
            "When comparing the results for the nested NER in the baseline models (without the contextual word embeddings) to the previous results in literature, we see that LSTM-CRF reaches comparable, but suboptimal results in three out of four nested NE corpora, while seq2seq clearly outperforms all the known methods by a wide margin.",
            "We hypothesize that seq2seq, although more complex (the system must predict multiple labels per token, including the special label), is more suitable for more complex corpora.",
            "The gain is most visible in ACE-2004 and ACE-2005, which contain extremely long named entities and the level of \u201cnestedness\u201d is greater than in the other nested corpora."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "LSTM-CRF",
                "seq2seq"
            ],
            [
                "seq2seq"
            ],
            [
                "ACE-2004",
                "ACE-2005"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_1",
        "paper_id": "P19-1527",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1531table_2",
        "description": "4.2 Results . Table 2 compares single-paradigm models against their double-paradigm MTL versions. On average, MTL models with auxiliary losses achieve the best performance for both parsing abstractions. They gain 1.05 F1 points on average in comparison with the single model for constituency parsing, and 0.62 UAS and 0.15 LAS points for dependency parsing. In comparison to the single-paradigm MTL models, the average gain is smaller: 0.05 F1 points for constituency parsing, and 0.09 UAS and 0.21 LAS points for dependency parsing.",
        "sentences": [
            "4.2 Results .",
            "Table 2 compares single-paradigm models against their double-paradigm MTL versions.",
            "On average, MTL models with auxiliary losses achieve the best performance for both parsing abstractions.",
            "They gain 1.05 F1 points on average in comparison with the single model for constituency parsing, and 0.62 UAS and 0.15 LAS points for dependency parsing.",
            "In comparison to the single-paradigm MTL models, the average gain is smaller: 0.05 F1 points for constituency parsing, and 0.09 UAS and 0.21 LAS points for dependency parsing."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "D-MTL-AUX"
            ],
            [
                "D-MTL-AUX",
                "F1",
                "UAS",
                "LAS"
            ],
            [
                "S-MTL",
                "F1",
                "UAS",
                "LAS"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_2",
        "paper_id": "P19-1531",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1542table_1",
        "description": "3.2 Results.  Table 1 shows both automatic and human evaluation results. PAML achieve consistently better results in term of dialogue consistency in both automatic and human evaluation. The latter also shows that all the experimental settings have comparable fluency scores, where instead perplexity and BLEU score are lower in PAML. This confirms that these measures are not correlated to human judgment (Liu et al., 2016). For completeness, we also show generated responses examples from PAML and baseline models in Appendix.",
        "sentences": [
            "3.2 Results.",
            " Table 1 shows both automatic and human evaluation results.",
            "PAML achieve consistently better results in term of dialogue consistency in both automatic and human evaluation.",
            "The latter also shows that all the experimental settings have comparable fluency scores, where instead perplexity and BLEU score are lower in PAML.",
            "This confirms that these measures are not correlated to human judgment (Liu et al., 2016).",
            "For completeness, we also show generated responses examples from PAML and baseline models in Appendix."
        ],
        "class_sentence": [
            0,
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "PAML"
            ],
            [
                "Fluency",
                "PPL",
                "BLEU",
                "PAML"
            ],
            [
                "Human"
            ],
            [
                "PAML"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_1",
        "paper_id": "P19-1542",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1543table_1",
        "description": "We adopted Exact Match (EM) and F1 score in SQuAD as metrics (Rajpurkar et al., 2016). Results in Table 1 show that while the utterance-based HA network is on par with established baselines, the proposed turn-based HA model obtains more gains, achieving the best EM and F1 scores.",
        "sentences": [
            "We adopted Exact Match (EM) and F1 score in SQuAD as metrics (Rajpurkar et al., 2016).",
            "Results in Table 1 show that while the utterance-based HA network is on par with established baselines, the proposed turn-based HA model obtains more gains, achieving the best EM and F1 scores."
        ],
        "class_sentence": [
            2,
            1
        ],
        "header_mention": [
            [
                "EM Score",
                "F1 Score"
            ],
            [
                "Utterance-based HA",
                "Turn-based HA (Proposed)",
                "EM Score",
                "F1 Score"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_1",
        "paper_id": "P19-1543",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1557table_4",
        "description": "Table 3 shows that the system obtains superior results in the Hate Speech dataset and yields competitive results on the Kaggle data in comparison to some sate-of-the-art baseline systems. Table 4 shows the results of our system on the DBpedia and AG News datasets. Using the same model without any tuning, we managed to obtain competitive results again compared to previous stateof-the-art systems.",
        "sentences": [
            "Table 3 shows that the system obtains superior results in the Hate Speech dataset and yields competitive results on the Kaggle data in comparison to some sate-of-the-art baseline systems.",
            "Table 4 shows the results of our system on the DBpedia and AG News datasets.",
            "Using the same model without any tuning, we managed to obtain competitive results again compared to previous stateof-the-art systems."
        ],
        "class_sentence": [
            0,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "DBpedia(%)",
                "AG News (%)"
            ],
            [
                "This work"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_4",
        "paper_id": "P19-1557",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1564table_2",
        "description": "We trained MTN with the Base parameters on the Visual Dialogue v1.0 2 training data and evaluate on the test-std v1.0 set. The image features are extracted by a pre-trained object detection model (Refer to the appendix Section A.2 for data preprocessing). We evaluate our model with Normalized Discounted Cumulative Gain (NDCG) score by submitting the predicted ranks of the response candidates to the evaluation server (as the groundtruth for the test-std v1.0 split is not published). We keep all the training procedures unchanged from the video-grounded dialogue task. Table 2 shows that our proposed MTN is able to generalize to the visually grounded dialogue setting. It is interesting that our generative model outperforms other retrieval-based approaches in NDCG without any task-specific fine-tuning. There are other submissions with higher NDCG scores from the leaderboard 3 but the approaches of these submissions are not clearly detailed to compare with.",
        "sentences": [
            "We trained MTN with the Base parameters on the Visual Dialogue v1.0 2 training data and evaluate on the test-std v1.0 set.",
            "The image features are extracted by a pre-trained object detection model (Refer to the appendix Section A.2 for data preprocessing).",
            "We evaluate our model with Normalized Discounted Cumulative Gain (NDCG) score by submitting the predicted ranks of the response candidates to the evaluation server (as the groundtruth for the test-std v1.0 split is not published).",
            "We keep all the training procedures unchanged from the video-grounded dialogue task.",
            "Table 2 shows that our proposed MTN is able to generalize to the visually grounded dialogue setting.",
            "It is interesting that our generative model outperforms other retrieval-based approaches in NDCG without any task-specific fine-tuning.",
            "There are other submissions with higher NDCG scores from the leaderboard 3 but the approaches of these submissions are not clearly detailed to compare with."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            1,
            1,
            0
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null,
            [
                "MTN (Base)"
            ],
            [
                "CorefNMN (Kottur et al., 2018)",
                "MN (Das et al., 2017a)",
                "HRE (Das et al., 2017a)",
                "LF (Das et al., 2017a)"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_2",
        "paper_id": "P19-1564",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1565table_3",
        "description": "Results . Table 3 shows the evaluation results. Our system with Kernel transition module outperforms all other systems in terms of all metrics on both two tasks, expect for R20@3 where the system with PMI transition performs best. The Kernel approach can predict the next keywords more precisely. In the task of response selection, our systems that are augmented with predicted keywords significantly outperform the base Retrieval approach, showing predicted keywords are helpful for better retrieving responses by capturing coarsegrained information of the next utterances. Interestingly, the system with Random transition has a close performance to the base Retrieval model, indicating that the erroneous keywords can be ignored by the system after training.",
        "sentences": [
            "Results .",
            "Table 3 shows the evaluation results.",
            "Our system with Kernel transition module outperforms all other systems in terms of all metrics on both two tasks, expect for R20@3 where the system with PMI transition performs best.",
            "The Kernel approach can predict the next keywords more precisely.",
            "In the task of response selection, our systems that are augmented with predicted keywords significantly outperform the base Retrieval approach, showing predicted keywords are helpful for better retrieving responses by capturing coarsegrained information of the next utterances.",
            "Interestingly, the system with Random transition has a close performance to the base Retrieval model, indicating that the erroneous keywords can be ignored by the system after training."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Ours-Kernel",
                "Keyword Prediction",
                "Response Retrieval",
                "Ours-PMI",
                "R20@3"
            ],
            [
                "Ours-Kernel"
            ],
            [
                "Response Retrieval",
                "Ours-PMI",
                "Retrieval"
            ],
            [
                "Response Retrieval",
                "Ours-Random",
                "Retrieval"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_3",
        "paper_id": "P19-1565",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1569table_3",
        "description": "5.1 All-Words Disambiguation . In Table 3 we show our results for all tasks of Raganato et al. (2017a)\u2019s evaluation framework. We used the framework\u2019s scoring scripts to avoid any discrepancies in the scoring methodology. Note that the k-NN referred in Table 3 always refers to the closest neighbor, and relies on MFS fallbacks. The first noteworthy result we obtained was that simply replicating Peters et al. (2018)\u2019s method for WSD using BERT instead of ELMo, we were able to significantly, and consistently, surpass the performance of all previous works. When using our method (LMMS), performance still improves significantly over the previous impressive results (+1.9 F1 on ALL, +3.4 F1 on SemEval 2013). Interestingly, we found that our method using ELMo embeddings didn\u2019t outperform ELMo k-NN with MFS fallback, suggesting that it\u2019s necessary to achieve a minimum competence level of embeddings from sense annotations (and glosses) before the inferred sense embeddings become more useful than MFS.",
        "sentences": [
            "5.1 All-Words Disambiguation .",
            "In Table 3 we show our results for all tasks of Raganato et al. (2017a)\u2019s evaluation framework.",
            "We used the framework\u2019s scoring scripts to avoid any discrepancies in the scoring methodology.",
            "Note that the k-NN referred in Table 3 always refers to the closest neighbor, and relies on MFS fallbacks.",
            "The first noteworthy result we obtained was that simply replicating Peters et al. (2018)\u2019s method for WSD using BERT instead of ELMo, we were able to significantly, and consistently, surpass the performance of all previous works.",
            "When using our method (LMMS), performance still improves significantly over the previous impressive results (+1.9 F1 on ALL, +3.4 F1 on SemEval 2013).",
            "Interestingly, we found that our method using ELMo embeddings didn\u2019t outperform ELMo k-NN with MFS fallback, suggesting that it\u2019s necessary to achieve a minimum competence level of embeddings from sense annotations (and glosses) before the inferred sense embeddings become more useful than MFS."
        ],
        "class_sentence": [
            2,
            1,
            2,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "context2vec k-NN\u2020 (2016)",
                "word2vec k-NN (2016)",
                "ELMo k-NN (2018)",
                "BERT k-NN"
            ],
            [
                "LMMS2348 (BERT)"
            ],
            [
                "LMMS2348 (BERT)",
                "BERT k-NN",
                "ALL",
                "SemEval2013"
            ],
            [
                "LMMS2348 (ELMo)",
                "ELMo k-NN (2018)"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_3",
        "paper_id": "P19-1569",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1570table_6",
        "description": "Results . Table 6 shows the results of clustering on WSI SemEval-2010 dataset. WordCtx2Sense outperforms (Arora et al., 2018) and (Mu et al., 2017) on both F-score and V-measure scores by a considerable margin. We observe similar improvements on the MakeSense-2016 dataset.",
        "sentences": [
            "Results .",
            "Table 6 shows the results of clustering on WSI SemEval-2010 dataset.",
            "WordCtx2Sense outperforms (Arora et al., 2018) and (Mu et al., 2017) on both F-score and V-measure scores by a considerable margin.",
            "We observe similar improvements on the MakeSense-2016 dataset."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "SemEval-2010"
            ],
            [
                "WordCtx2Sense (? = 0.0)",
                "WordCtx2Sense (? = 10^?2)",
                "(Arora et al., 2018)",
                "(Mu et al., 2017)"
            ],
            [
                "MakeSense-2016"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_6",
        "paper_id": "P19-1570",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1584table_2",
        "description": "7 Result. Table 2 shows de-identification performance results for the non-private de-identification classifier in comparison to the state of the art. The results are average values out of five experiment runs. When trained on the raw i2b2 2014 data, our models achieve F1 scores that are comparable to Dernoncourt et al. results. The casing feature improves GloVe by 0.4 percentage points.",
        "sentences": [
            "7 Result.",
            "Table 2 shows de-identification performance results for the non-private de-identification classifier in comparison to the state of the art.",
            "The results are average values out of five experiment runs.",
            "When trained on the raw i2b2 2014 data, our models achieve F1 scores that are comparable to Dernoncourt et al. results.",
            "The casing feature improves GloVe by 0.4 percentage points."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "Our non-private FastText",
                "Our non-private GloVe",
                "Our non-private GloVe + casing",
                "Dernoncourt et al. (LSTM-CRF)"
            ],
            [
                "Our non-private GloVe + casing",
                "Our non-private GloVe"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_2",
        "paper_id": "P19-1584",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1595table_2",
        "description": "We compare against recent work by submitting to the GLUE leaderboard. We use Single\u2192Multi distillation. Following the procedure used by BERT, we train multiple models and submit the one with the highest average dev set score to the test set. BERT trained 10 models for each task (80 total);. we trained 20 multi-task models. Results are shown in Table 2. Our work outperforms or matches existing published results that do not rely on ensembling. However, due to the variance between trials discussed under \u201cReporting Results,\u201d we think these test set numbers should be taken with a grain of salt, as they only show the performance of individual training runs. We believe significance testing over multiple trials would be needed to have a definitive comparison.",
        "sentences": [
            "We compare against recent work by submitting to the GLUE leaderboard.",
            "We use Single\u2192Multi distillation.",
            "Following the procedure used by BERT, we train multiple models and submit the one with the highest average dev set score to the test set.",
            "BERT trained 10 models for each task (80 total);.",
            "we trained 20 multi-task models.",
            "Results are shown in Table 2.",
            "Our work outperforms or matches existing published results that do not rely on ensembling.",
            "However, due to the variance between trials discussed under \u201cReporting Results,\u201d we think these test set numbers should be taken with a grain of salt, as they only show the performance of individual training runs.",
            "We believe significance testing over multiple trials would be needed to have a definitive comparison."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            2,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            [
                "GLUE score"
            ],
            null,
            null,
            null,
            null,
            null,
            [
                "BERT-Large + BAM (ours)"
            ],
            null,
            null
        ],
        "n_sentence": 9.0,
        "table_id": "table_2",
        "paper_id": "P19-1595",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1599table_3",
        "description": "Effect of Position of Word Position Loss. We also study the effect of the position of WPL by (1) using the decoder hidden state, (2) using the concatenation of word embeddings in the syntactic encoder and the syntactic variable, (3) using the concatenation of word embeddings in the decoder and the syntactic variable, or (4) adding it on both the encoder embeddings and decoder word embeddings. Table 3 shows that adding WPL on hidden states can help improve performance slightly but not as good as adding it on word embeddings. In practice, we also observe that the value of WPL tends to vanish when using WPL on hidden states, which is presumably caused by the fact that LSTMs have sequence information, making the optimization of WPL trivial. We also observe that adding WPL to both the encoder and decoder brings the largest improvement.",
        "sentences": [
            "Effect of Position of Word Position Loss.",
            "We also study the effect of the position of WPL by (1) using the decoder hidden state, (2) using the concatenation of word embeddings in the syntactic encoder and the syntactic variable, (3) using the concatenation of word embeddings in the decoder and the syntactic variable, or (4) adding it on both the encoder embeddings and decoder word embeddings.",
            "Table 3 shows that adding WPL on hidden states can help improve performance slightly but not as good as adding it on word embeddings.",
            "In practice, we also observe that the value of WPL tends to vanish when using WPL on hidden states, which is presumably caused by the fact that LSTMs have sequence information, making the optimization of WPL trivial.",
            "We also observe that adding WPL to both the encoder and decoder brings the largest improvement."
        ],
        "class_sentence": [
            0,
            0,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Dec. hidden state"
            ],
            [
                "Dec. hidden state"
            ],
            [
                "Dec. hidden state"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_3",
        "paper_id": "P19-1599",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1599table_7",
        "description": "We also compare the performance of LC by using a single latent code that has 50 classes. The results in Table 7 show that it is better to use smaller number of classes for each cluster instead of using a cluster with a large number of classes.",
        "sentences": [
            "We also compare the performance of LC by using a single latent code that has 50 classes.",
            "The results in Table 7 show that it is better to use smaller number of classes for each cluster instead of using a cluster with a large number of classes."
        ],
        "class_sentence": [
            2,
            1
        ],
        "header_mention": [
            [
                "Single LC"
            ],
            [
                "LC",
                "Single LC"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_7",
        "paper_id": "P19-1599",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1602table_3",
        "description": "Results Table 3 shows the performance of unsupervised paraphrase generation. In the first row of Table 3, simply copying the original sentences yields the highest BLEU-ref, but is meaningless as it has a BLEU-ori score of 100. We see that DSS-VAE outperforms the CGMH and the original VAE in BLEU-ref. Especially, DSS-VAE achieves a closer BLEU-ref compared with supervised paraphrase methods (Gupta et al., 2018). We admit that it is hard to present the trade-off by listing a single score for each model in the Table 3. We therefore have the scatter plot in Figure 4 to further compare these methods. As seen, the trade-off is pretty linear and less noisy compared with Figure 3. It is seen that the line of DSS-VAE is located to the upper-left of the competing methods. In other words, the plain VAE and CGMH are \u201cinadmissible,\u201d meaning that DSSVAE simultaneously outperforms them in both BLEU-ori and BLEU-ref, indicating that DSSVAE outperforms previous state-of-the-art methods in unsupervised paraphrase generation.",
        "sentences": [
            "Results Table 3 shows the performance of unsupervised paraphrase generation.",
            "In the first row of Table 3, simply copying the original sentences yields the highest BLEU-ref, but is meaningless as it has a BLEU-ori score of 100.",
            "We see that DSS-VAE outperforms the CGMH and the original VAE in BLEU-ref.",
            "Especially, DSS-VAE achieves a closer BLEU-ref compared with supervised paraphrase methods (Gupta et al., 2018).",
            "We admit that it is hard to present the trade-off by listing a single score for each model in the Table 3.",
            "We therefore have the scatter plot in Figure 4 to further compare these methods.",
            "As seen, the trade-off is pretty linear and less noisy compared with Figure 3.",
            "It is seen that the line of DSS-VAE is located to the upper-left of the competing methods.",
            "In other words, the plain VAE and CGMH are \u201cinadmissible,\u201d meaning that DSSVAE simultaneously outperforms them in both BLEU-ori and BLEU-ref, indicating that DSSVAE outperforms previous state-of-the-art methods in unsupervised paraphrase generation."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            2,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "Origin Sentence",
                "BLEU-ref",
                "BLEU-ori"
            ],
            [
                "DSS-VAE",
                "CGMH",
                "VAE (unsupervised)",
                "BLEU-ref"
            ],
            [
                "DSS-VAE",
                "BLEU-ref"
            ],
            null,
            null,
            null,
            [
                "DSS-VAE"
            ],
            [
                "VAE (unsupervised)",
                "CGMH",
                "DSS-VAE"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_3",
        "paper_id": "P19-1602",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1603table_2",
        "description": "The automatic results of four generation models are shown in Table 2. We have the following observations: (1) Three models based on our proposed framework do not have obvious performance difference in terms of BLEU. Meanwhile, all of them can largely outperform the Seq2Seq+SentiMod baseline which does not follow our framework. Thus it shows the effectiveness of the proposed framework. (2) HM SentiCons which measures the performance of sentiment analyzer is marginally consistent with the I-O SentiCons and Sentiment which measure the performance of sentimental generator. This accords with our expectations because the sentimental generator takes the sentiment intensity predicted by the sentiment analyzer as the input signal for controlling the sentiment of the output.",
        "sentences": [
            "The automatic results of four generation models are shown in Table 2.",
            "We have the following observations: (1) Three models based on our proposed framework do not have obvious performance difference in terms of BLEU.",
            "Meanwhile, all of them can largely outperform the Seq2Seq+SentiMod baseline which does not follow our framework.",
            "Thus it shows the effectiveness of the proposed framework.",
            "(2) HM SentiCons which measures the performance of sentiment analyzer is marginally consistent with the I-O SentiCons and Sentiment which measure the performance of sentimental generator.",
            "This accords with our expectations because the sentimental generator takes the sentiment intensity predicted by the sentiment analyzer as the input signal for controlling the sentiment of the output."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "SIC-Seq2Seq + RB",
                "SIC-Seq2Seq + RM",
                "SIC-Seq2Seq + DA",
                "BLEU-2"
            ],
            [
                "SIC-Seq2Seq + RB",
                "SIC-Seq2Seq + RM",
                "SIC-Seq2Seq + DA",
                "Seq2Seq + SentiMod"
            ],
            [
                "SIC-Seq2Seq + RB",
                "SIC-Seq2Seq + RM",
                "SIC-Seq2Seq + DA"
            ],
            [
                "I-O SentiCons"
            ],
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_2",
        "paper_id": "P19-1603",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1603table_3",
        "description": "The automatic and human evaluation results of four generation models are shown in Table 2 and Table 3 respectively. We have the following observations: (1) Three models based on our proposed framework do not have obvious performance difference in terms of BLEU, Coherency, and Fluency. Meanwhile, all of them can largely outperform the Seq2Seq+SentiMod baseline which does not follow our framework. Thus it shows the effectiveness of the proposed framework. (2) HM SentiCons which measures the performance of sentiment analyzer is marginally consistent with the I-O SentiCons and Sentiment which measure the performance of sentimental generator. This accords with our expectations because the sentimental generator takes the sentiment intensity predicted by the sentiment analyzer as the input signal for controlling the sentiment of the output.",
        "sentences": [
            "The automatic and human evaluation results of four generation models are shown in Table 2 and Table 3 respectively.",
            "We have the following observations: (1) Three models based on our proposed framework do not have obvious performance difference in terms of BLEU, Coherency, and Fluency.",
            "Meanwhile, all of them can largely outperform the Seq2Seq+SentiMod baseline which does not follow our framework.",
            "Thus it shows the effectiveness of the proposed framework.",
            "(2) HM SentiCons which measures the performance of sentiment analyzer is marginally consistent with the I-O SentiCons and Sentiment which measure the performance of sentimental generator.",
            "This accords with our expectations because the sentimental generator takes the sentiment intensity predicted by the sentiment analyzer as the input signal for controlling the sentiment of the output."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "SIC-Seq2Seq + RB",
                "SIC-Seq2Seq + RM",
                "SIC-Seq2Seq + DA"
            ],
            [
                "SIC-Seq2Seq + RB",
                "SIC-Seq2Seq + RM",
                "SIC-Seq2Seq + DA",
                "Seq2Seq + SentiMod"
            ],
            null,
            null,
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_3",
        "paper_id": "P19-1603",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1607table_3",
        "description": "Table 3 shows a comparison between our models and comparative models. Whereas Dress-LS has a higher SARI score because it directly optimizes SARI using reinforcement learning, our models achieved the best BLEU scores across styles and domains.",
        "sentences": [
            "Table 3 shows a comparison between our models and comparative models.",
            "Whereas Dress-LS has a higher SARI score because it directly optimizes SARI using reinforcement learning, our models achieved the best BLEU scores across styles and domains."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Dress-LS",
                "SARI",
                "Ours (RNN)",
                "Ours (SAN)",
                "BLEU"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_3",
        "paper_id": "P19-1607",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1623table_2",
        "description": "Table 2 shows the results of human evaluation on the Chinese-to-English task. We asked two human evaluators who can read both Chinese and English to evaluate the \ufb02uency and adequacy of the translations generated by MLE, MLE + CP, MLE + data, and CLone. The scores of \ufb02uency and adequacy range from 1 to 5. The translations were shuf\ufb02ed randomly, and the name of each method was anonymous to human evaluators. We find that CLone significantly improves the adequacy over all baselines. This is because omitting important information in source sentences decreases the adequacy of translation. CLone is capable of alleviating this problem by assigning lower probabilities to translations with word omission errors.",
        "sentences": [
            "Table 2 shows the results of human evaluation on the Chinese-to-English task.",
            "We asked two human evaluators who can read both Chinese and English to evaluate the \ufb02uency and adequacy of the translations generated by MLE, MLE + CP, MLE + data, and CLone.",
            "The scores of \ufb02uency and adequacy range from 1 to 5.",
            "The translations were shuf\ufb02ed randomly, and the name of each method was anonymous to human evaluators.",
            "We find that CLone significantly improves the adequacy over all baselines.",
            "This is because omitting important information in source sentences decreases the adequacy of translation.",
            "CLone is capable of alleviating this problem by assigning lower probabilities to translations with word omission errors."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "WordDropout",
                "CLone"
            ],
            [
                "Flu.",
                "Ade."
            ],
            null,
            [
                "CLone",
                "Ade."
            ],
            [
                "Ade."
            ],
            [
                "CLone"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_2",
        "paper_id": "P19-1623",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1628table_2",
        "description": "As can be seen in Table 2,  DEGREE (tf-idf) is very close to  TEXTRANK (tf-idf).   . Due to space limitations,  we  only  show  comparisons  between DEGREE and  TEXTRANK with  tf-idf,  however, we  observed  similar  trends  across  sentence  rep-resentations.  These  results  indicate  that  considering  global  structure  does  not  make  a  difference  when  selecting  salient  sentences  for  NYT and  CNN/Daily  Mail,  possibly  due  to  the  fact that news articles in these datasets are relatively short (see Table 1). The results in Table 2 further show that PACSUM substantially outperforms TEXTRANK across sentence representations, directly confirming our assumption that position information is beneficial for determining sentence centrality in news single-document summarization. In Figure 1 we further show how PACSUM's performance (ROUGE-1 F1) on the NYT validation set varies as \u03bb1 ranges from -2 to 1 (\u03bb2 = 1 and \u03b2 = 0, 0.3, 0.6). The plot highlights that differentially weighting a connection's contribution (via relative position) has a huge impact on performance (ROUGE ranges from 0.30 to 0.40). In addition, the optimal \u03bb1 is negative, suggesting that similarity with the previous content actually hurts centrality in this case.",
        "sentences": [
            "As can be seen in Table 2,  DEGREE (tf-idf) is very close to  TEXTRANK (tf-idf).   .",
            "Due to space limitations,  we  only  show  comparisons  between DEGREE and  TEXTRANK with  tf-idf,  however, we  observed  similar  trends  across  sentence  rep-resentations.",
            " These  results  indicate  that  considering  global  structure  does  not  make  a  difference  when  selecting  salient  sentences  for  NYT and  CNN/Daily  Mail,  possibly  due  to  the  fact that news articles in these datasets are relatively short (see Table 1).",
            "The results in Table 2 further show that PACSUM substantially outperforms TEXTRANK across sentence representations, directly confirming our assumption that position information is beneficial for determining sentence centrality in news single-document summarization.",
            "In Figure 1 we further show how PACSUM's performance (ROUGE-1 F1) on the NYT validation set varies as \u03bb1 ranges from -2 to 1 (\u03bb2 = 1 and \u03b2 = 0, 0.3, 0.6).",
            "The plot highlights that differentially weighting a connection's contribution (via relative position) has a huge impact on performance (ROUGE ranges from 0.30 to 0.40).",
            "In addition, the optimal \u03bb1 is negative, suggesting that similarity with the previous content actually hurts centrality in this case."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            [
                "DEGREE (tf-idf)",
                "TEXTRANK (tf-idf)"
            ],
            [
                "DEGREE (tf-idf)",
                "TEXTRANK (tf-idf)"
            ],
            [
                "NYT",
                "CNN+DM"
            ],
            [
                "PACSUM (tf-idf)",
                "TEXTRANK (tf-idf)"
            ],
            null,
            null,
            null
        ],
        "n_sentence": 7.0,
        "table_id": "table_2",
        "paper_id": "P19-1628",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1629table_3",
        "description": "Parsing Documents . Table 3 presents various ablation studies for the document-level model on the development set. Deep sentence representations when combined with multi-attention bring improvements over shallow representations (+3.68 exact-F1).  Using alignments as features and as a way of highlighting where to copy from yields further performance gains both in terms of exact and partial F1. The best performing variant is Deep-Copy  which  combines  supervised  attention  with copying.   . Table  4  shows  our  results  on  the  testset  (see  the  Appendix  for  an  example  of  model output);  we  compare  the  best  performing  DRTS parser  (DeepCopy)  against  two  baselines  which rely  on  our  sentence-level  parser  (DocSent  and DocTree).  The DRTS parser, which has a global view of the document, outperforms variants which construct  document  representations  by  aggregating individually parsed sentences.",
        "sentences": [
            "Parsing Documents .",
            "Table 3 presents various ablation studies for the document-level model on the development set.",
            "Deep sentence representations when combined with multi-attention bring improvements over shallow representations (+3.68 exact-F1).",
            " Using alignments as features and as a way of highlighting where to copy from yields further performance gains both in terms of exact and partial F1.",
            "The best performing variant is Deep-Copy  which  combines  supervised  attention  with copying.   .",
            "Table  4  shows  our  results  on  the  testset  (see  the  Appendix  for  an  example  of  model output);  we  compare  the  best  performing  DRTS parser  (DeepCopy)  against  two  baselines  which rely  on  our  sentence-level  parser  (DocSent  and DocTree).",
            " The DRTS parser, which has a global view of the document, outperforms variants which construct  document  representations  by  aggregating individually parsed sentences."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            0,
            0
        ],
        "header_mention": [
            null,
            null,
            [
                "exa-F1",
                "Deep"
            ],
            [
                "Deep",
                "par-F1",
                "exa-F1"
            ],
            [
                "DeepCopy"
            ],
            null,
            null
        ],
        "n_sentence": 7.0,
        "table_id": "table_3",
        "paper_id": "P19-1629",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1629table_4",
        "description": "Parsing  Documents. Table  3  presents  various ablation studies for the document-level model on the development  set. Deep  sentence  representations  when  combined  with  multi-attention  bring improvements over shallow representations (+3.68 exact-F1). Using alignments as features and as a way of highlighting where to copy from yields further performance gains both in terms of exact and partial F1. The best performing variant is DeepCopy which combines supervised attention with copying. Table 4 shows our results on the test set (see the Appendix for an example of model output); we compare the best performing DRTS parser (DeepCopy) against two baselines which rely on our sentence-level parser (DocSent and DocTree). The DRTS parser, which has a global view of the document, outperforms variants which construct document representations by aggregating individually parsed sentences.",
        "sentences": [
            "Parsing  Documents.",
            "Table  3  presents  various ablation studies for the document-level model on the development  set.",
            "Deep  sentence  representations  when  combined  with  multi-attention  bring improvements over shallow representations (+3.68 exact-F1).",
            "Using alignments as features and as a way of highlighting where to copy from yields further performance gains both in terms of exact and partial F1.",
            "The best performing variant is DeepCopy which combines supervised attention with copying.",
            "Table 4 shows our results on the test set (see the Appendix for an example of model output); we compare the best performing DRTS parser (DeepCopy) against two baselines which rely on our sentence-level parser (DocSent and DocTree).",
            "The DRTS parser, which has a global view of the document, outperforms variants which construct document representations by aggregating individually parsed sentences."
        ],
        "class_sentence": [
            2,
            0,
            0,
            0,
            0,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null,
            [
                "DeepCopy",
                "DocSent",
                "DocTree"
            ],
            [
                "DeepCopy"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_4",
        "paper_id": "P19-1629",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1631table_5",
        "description": "4.3.1 Evaluation on Original Data . We first verify that the prior loss term does not adversely affect overall classifier performance on the main task using general performance metrics such as accuracy and F-1. Results are shown in Table 4. Unlike previous approaches (Park et al., 2018; Dixon et al., 2018; Madras et al., 2018), our method does not degrade classifier performance (it even improves) in terms of all reported metrics. We also look at samples containing identity terms. Table 5 shows classifier performance metrics for such samples. The importance weighting approach slightly outperforms the baseline classifier. Replacing identity words with a special tokens, on the other hand, hurts the performance on the main task. One of the reasons might be that replacing all identity terms with a token potentially removes other useful information model can rely on. If we were to make an analogy between the token replacement method and hard ablation, then the same analogy can be made between our method and soft ablation. Hence, the information pertaining to identity terms is not completely lost for our method, but come at a cost.",
        "sentences": [
            "4.3.1 Evaluation on Original Data .",
            "We first verify that the prior loss term does not adversely affect overall classifier performance on the main task using general performance metrics such as accuracy and F-1.",
            "Results are shown in Table 4.",
            "Unlike previous approaches (Park et al., 2018; Dixon et al., 2018; Madras et al., 2018), our method does not degrade classifier performance (it even improves) in terms of all reported metrics.",
            "We also look at samples containing identity terms.",
            "Table 5 shows classifier performance metrics for such samples.",
            "The importance weighting approach slightly outperforms the baseline classifier.",
            "Replacing identity words with a special tokens, on the other hand, hurts the performance on the main task.",
            "One of the reasons might be that replacing all identity terms with a token potentially removes other useful information model can rely on.",
            "If we were to make an analogy between the token replacement method and hard ablation, then the same analogy can be made between our method and soft ablation.",
            "Hence, the information pertaining to identity terms is not completely lost for our method, but come at a cost."
        ],
        "class_sentence": [
            2,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null,
            null,
            [
                "Importance",
                "Baseline"
            ],
            [
                "TOK Replace"
            ],
            null,
            null,
            [
                "Our Method"
            ]
        ],
        "n_sentence": 11.0,
        "table_id": "table_5",
        "paper_id": "P19-1631",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1635table_7",
        "description": "In this experiment, we release the boundary limitation, and test the numeracy for all real numbers. For instance, the altered results of 138 with 10% distortion factor are in the same magnitude, and that with 30% distortion factor, 96.6 and 179.4, are in different magnitude. Table 7 lists the experimental results. We find that the model obtained better performance for numerals distorted by more than 50%, with more confusion in the range below that. Furthermore, according to the micro and macro-averaged F1 scores, the performance is similar among the three different cases (i.e., overstated, understated, and correct).",
        "sentences": [
            "In this experiment, we release the boundary limitation, and test the numeracy for all real numbers.",
            "For instance, the altered results of 138 with 10% distortion factor are in the same magnitude, and that with 30% distortion factor, 96.6 and 179.4, are in different magnitude.",
            "Table 7 lists the experimental results.",
            "We find that the model obtained better performance for numerals distorted by more than 50%, with more confusion in the range below that.",
            "Furthermore, according to the micro and macro-averaged F1 scores, the performance is similar among the three different cases (i.e., overstated, understated, and correct)."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "\u00b110%",
                "\u00b130%"
            ],
            null,
            [
                "\u00b170%",
                "\u00b190%"
            ],
            [
                "Micro-F1",
                "Macro-F1"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_7",
        "paper_id": "P19-1635",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1643table_6",
        "description": "Table 6 shows the results obtained using the multimodal model for different sets of input features. The model that uses all the input features available leads to the best results, improving significantly over the text-only and video-only methods.",
        "sentences": [
            "Table 6 shows the results obtained using the multimodal model for different sets of input features.",
            "The model that uses all the input features available leads to the best results, improving significantly over the text-only and video-only methods."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Action E + POS + Context S + Concreteness + Inception + C3D"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_6",
        "paper_id": "P19-1643",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1645table_1",
        "description": "Table 1 summarizes the segmentation results on the widely used BR-phono corpus, comparing it to a variety of baselines. Unigram DP, Bigram HDP, LSTM suprisal and HMLSTM refer to the benchmark models explained in \u00a46. The ablated versions of our model show that without the lexicon (-memory), without the expected length penalty (-length), and without either, our model fails to discover good segmentations.",
        "sentences": [
            "Table 1 summarizes the segmentation results on the widely used BR-phono corpus, comparing it to a variety of baselines.",
            "Unigram DP, Bigram HDP, LSTM suprisal and HMLSTM refer to the benchmark models explained in \u00a46.",
            "The ablated versions of our model show that without the lexicon (-memory), without the expected length penalty (-length), and without either, our model fails to discover good segmentations."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Unigram DP",
                "LSTM suprisal (Elman, 1990)",
                "HMLSTM (Chung et al. 2017)"
            ],
            [
                "SNLM (- memory, - length)",
                "SNLM (- memory, + length)",
                "SNLM (+ memory, + length)",
                "SNLM (+ memory, - length)"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_1",
        "paper_id": "P19-1645",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1645table_2",
        "description": "Table 2 summarizes results on the BR-text (orthographic Brent corpus) and Chinese corpora. As in the previous section, all the models were trained to maximize held-out likelihood. Here we observe a similar pattern, with the SNLM outperforming the baseline models, despite the tasks being quite different from each other and from the BR-phono task.",
        "sentences": [
            "Table 2 summarizes results on the BR-text (orthographic Brent corpus) and Chinese corpora.",
            "As in the previous section, all the models were trained to maximize held-out likelihood.",
            "Here we observe a similar pattern, with the SNLM outperforming the baseline models, despite the tasks being quite different from each other and from the BR-phono task."
        ],
        "class_sentence": [
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "SNLM"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_2",
        "paper_id": "P19-1645",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1645table_4",
        "description": "Table 4 summarizes the results of the language modeling experiments. Again, we see that SNLM outperforms the Bayesian models and a character LSTM.",
        "sentences": [
            "Table 4 summarizes the results of the language modeling experiments.",
            "Again, we see that SNLM outperforms the Bayesian models and a character LSTM."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "SNLM",
                "LSTM"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_4",
        "paper_id": "P19-1645",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1648table_1",
        "description": "Results on VisDial val v1.0. Experimental results on val v1.0 are shown in Table 1. \u201c-D\u201d denotes that a discriminative decoder is used. With only one reasoning step, our ReDAN model already achieves better performance than CoAtt, which is the previous best-performing model. Using two or three reasoning steps further increases the performance. Further increasing the number of reasoning steps does not help, thus results are not shown. We also report results on an ensemble of 4 ReDAN-D models. Significant improvement was observed, boosting NDCG from 59.32 to 60.53, and MRR from 64.21 to 65.30.",
        "sentences": [
            "Results on VisDial val v1.0.",
            "Experimental results on val v1.0 are shown in Table 1. \u201c-D\u201d denotes that a discriminative decoder is used.",
            "With only one reasoning step, our ReDAN model already achieves better performance than CoAtt, which is the previous best-performing model.",
            "Using two or three reasoning steps further increases the performance.",
            "Further increasing the number of reasoning steps does not help, thus results are not shown.",
            "We also report results on an ensemble of 4 ReDAN-D models.",
            "Significant improvement was observed, boosting NDCG from 59.32 to 60.53, and MRR from 64.21 to 65.30."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "ReDAN-G (T=1)",
                "CoAtt-G (Wu et al., 2018)"
            ],
            [
                "ReDAN-G (T=2)",
                "ReDAN-G (T=3)"
            ],
            null,
            [
                "Ensemble of 4"
            ],
            [
                "Ensemble of 4",
                "NDCG",
                "MRR"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_1",
        "paper_id": "P19-1648",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1650table_1",
        "description": "Table 1 shows the performance of these baselines. We observe that the image-only models perform poorly on Covr we because they are unable to identify them from the image pixels alone. On the other hand, the labels-only baseline and the proposal of Lu et al.(2018) has high performance across all three metrics.",
        "sentences": [
            "Table 1 shows the performance of these baselines.",
            "We observe that the image-only models perform poorly on Covr we because they are unable to identify them from the image pixels alone.",
            "On the other hand, the labels-only baseline and the proposal of Lu et al.(2018) has high performance across all three metrics."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                " Image|Label",
                " Y|N",
                " Covr we"
            ],
            [
                " Image|Label",
                "N|Y",
                "Y|Y"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_1",
        "paper_id": "P19-1650",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1653table_3",
        "description": "Table 3 shows the human evaluation results. They are consistent with the automatic evaluation results when it comes to the preference of humans towards the deliberation-based setups, but show a more positive outlook regarding the addition of visual information (del+obj over del) for French.",
        "sentences": [
            "Table 3 shows the human evaluation results.",
            "They are consistent with the automatic evaluation results when it comes to the preference of humans towards the deliberation-based setups, but show a more positive outlook regarding the addition of visual information (del+obj over del) for French."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                " del+obj",
                " del",
                "FR"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_3",
        "paper_id": "P19-1653",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1657table_1",
        "description": "Ablation Study. CMASW has only one writer, which is trained on both normal and abnormal findings. Table 1 shows that CMASW can achieve competitive performances to the state-of-the-art methods. CMASNW, AW is a simple concatenation of two single agent models CMASNW and CMASAW, where CMASNW is trained only on normal findings and CMASAW is trained only on abnormal findings. At test time, the final paragraph of CMASNW, AW is simply a concatenation of normal and abnormal findings generated by CMASNW and CMASAW respectively. Surprisingly, CMASNW, AW performs worse than CMASW on the CX-CHR dataset. We believe the main reason is the missing communication protocol between the two agents, which could cause conflicts when they take actions independently. For example, for an image, NW might think \u0093the heart size is normal\u00e2\u0080\u009d, while AW believes \u00e2\u0080\u009cthe heart is enlarged\u00e2\u0080\u009d. Such con\u00ef\u00ac\u0082ict would negatively affect their joint performances. As evidently shown in Table 1, CMAS-IL achieves higher scores than CMASNW, AW, directly proving the importance of communication between agents and thus the importance of PL. Finally, it can be observed from Table 1 that CMAS-RL consistently outperforms CMAS-IL on all metrics, which demonstrates the effectiveness of reinforcement learning.",
        "sentences": [
            "Ablation Study.",
            "CMASW has only one writer, which is trained on both normal and abnormal findings.",
            "Table 1 shows that CMASW can achieve competitive performances to the state-of-the-art methods.",
            "CMASNW, AW is a simple concatenation of two single agent models CMASNW and CMASAW, where CMASNW is trained only on normal findings and CMASAW is trained only on abnormal findings.",
            "At test time, the final paragraph of CMASNW, AW is simply a concatenation of normal and abnormal findings generated by CMASNW and CMASAW respectively.",
            "Surprisingly, CMASNW, AW performs worse than CMASW on the CX-CHR dataset.",
            "We believe the main reason is the missing communication protocol between the two agents, which could cause conflicts when they take actions independently.",
            "For example, for an image, NW might think \u0093the heart size is normal\u00e2\u0080\u009d, while AW believes \u00e2\u0080\u009cthe heart is enlarged\u00e2\u0080\u009d.",
            "Such con\u00ef\u00ac\u0082ict would negatively affect their joint performances.",
            "As evidently shown in Table 1, CMAS-IL achieves higher scores than CMASNW, AW, directly proving the importance of communication between agents and thus the importance of PL.",
            "Finally, it can be observed from Table 1 that CMAS-RL consistently outperforms CMAS-IL on all metrics, which demonstrates the effectiveness of reinforcement learning."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            1,
            2,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "CMASW"
            ],
            [
                "CMASW"
            ],
            [
                "CMASNWAW"
            ],
            [
                "CMASNWAW"
            ],
            [
                "CMASNWAW",
                "CMASW",
                "CX-CHR"
            ],
            null,
            null,
            null,
            [
                "CMAS-IL",
                "CMASNWAW"
            ],
            [
                "CMAS-RL",
                "CMAS-IL"
            ]
        ],
        "n_sentence": 11.0,
        "table_id": "table_1",
        "paper_id": "P19-1657",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1658table_2",
        "description": "Human Evaluation . Following the evaluation procedure of the first VIST Challenge (Mitchell et al., 2018), for each visual story, we recruit five human judges on MTurk to rate it on six aspects (at $0.1/HIT.). We take the average of the five judgments as the final scores for the story. Table 2 shows the results. The LSTM using text-only input outperforms all other baselines. It improves all six aspects for stories by AREL, and improves \u201cFocus\u201d and \u201cHuman-like\u201d aspects for stories by GLAC. These results demonstrate that a relatively small set of human edits can be used to boost the story quality of an existing large VIST model. Table 2 also suggests that the quality of a post-edited story is heavily decided by its pre-edited version. Even after editing by human editors, AREL\u2019s stories still do not achieve the quality of pre-edited stories by GLAC. The inefficacy of image features and Transformer model might be caused by the small size of VIST-Edit. It also requires further research to develop a post-editing model in a multimodal context.",
        "sentences": [
            "Human Evaluation .",
            "Following the evaluation procedure of the first VIST Challenge (Mitchell et al., 2018), for each visual story, we recruit five human judges on MTurk to rate it on six aspects (at $0.1/HIT.).",
            "We take the average of the five judgments as the final scores for the story.",
            "Table 2 shows the results.",
            "The LSTM using text-only input outperforms all other baselines.",
            "It improves all six aspects for stories by AREL, and improves \u201cFocus\u201d and \u201cHuman-like\u201d aspects for stories by GLAC.",
            "These results demonstrate that a relatively small set of human edits can be used to boost the story quality of an existing large VIST model.",
            "Table 2 also suggests that the quality of a post-edited story is heavily decided by its pre-edited version.",
            "Even after editing by human editors, AREL\u2019s stories still do not achieve the quality of pre-edited stories by GLAC.",
            "The inefficacy of image features and Transformer model might be caused by the small size of VIST-Edit.",
            "It also requires further research to develop a post-editing model in a multimodal context."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            [
                "LSTM (T)"
            ],
            [
                "AREL",
                " GLAC",
                "LSTM (T)"
            ],
            null,
            null,
            [
                "Human",
                "AREL",
                " GLAC"
            ],
            [
                "TF (T+I)"
            ],
            null
        ],
        "n_sentence": 11.0,
        "table_id": "table_2",
        "paper_id": "P19-1658",
        "valid": 1
    }
]