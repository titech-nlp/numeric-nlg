[
    {
        "paper_id": "D16-1007",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1007.pdf",
        "title": "A Position Encoding Convolutional Neural Network Based on Dependency Tree for Relation Classification"
    },
    {
        "paper_id": "D16-1010",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1010.pdf",
        "title": "Comparing Computational Cognitive Models of Generalization in a Language Acquisition Task"
    },
    {
        "paper_id": "D16-1011",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1011.pdf",
        "title": "Rationalizing Neural Predictions"
    },
    {
        "paper_id": "D16-1018",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1018.pdf",
        "title": "Context-Dependent Sense Embedding"
    },
    {
        "paper_id": "D16-1021",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1021.pdf",
        "title": "Aspect Level Sentiment Classification with Deep Memory Network"
    },
    {
        "paper_id": "D16-1025",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1025.pdf",
        "title": "Neural versus Phrase-Based Machine Translation Quality: a Case Study"
    },
    {
        "paper_id": "D16-1032",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1032.pdf",
        "title": "Globally Coherent Text Generation with Neural Checklist Models"
    },
    {
        "paper_id": "D16-1035",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1035.pdf",
        "title": "Discourse Parsing with Attention-based Hierarchical Neural Networks"
    },
    {
        "paper_id": "D16-1038",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1038.pdf",
        "title": "Event Detection and Co-reference with Minimal Supervision"
    },
    {
        "paper_id": "D16-1039",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1039.pdf",
        "title": "Learning Term Embeddings for Taxonomic Relation Identification Using Dynamic Weighting Neural Network"
    },
    {
        "paper_id": "D16-1043",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1043.pdf",
        "title": "Comparing Data Sources and Architectures for Deep Visual Representation Learning in Semantics"
    },
    {
        "paper_id": "D16-1044",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1044.pdf",
        "title": "Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding"
    },
    {
        "paper_id": "D16-1045",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1045.pdf",
        "title": "The Structured Weighted Violations Perceptron Algorithm"
    },
    {
        "paper_id": "D16-1048",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1048.pdf",
        "title": "Automatic Cross-Lingual Similarization of Dependency Grammars for Tree-based Machine Translation"
    },
    {
        "paper_id": "D16-1050",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1050.pdf",
        "title": "Variational Neural Machine Translation"
    },
    {
        "paper_id": "D16-1051",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1051.pdf",
        "title": "Towards a Convex HMM Surrogate for Word Alignment"
    },
    {
        "paper_id": "D16-1062",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1062.pdf",
        "title": "Building an Evaluation Scale using Item Response Theory"
    },
    {
        "paper_id": "D16-1063",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1063.pdf",
        "title": "WordRank: Learning Word Embeddings via Robust Ranking"
    },
    {
        "paper_id": "D16-1065",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1065.pdf",
        "title": "AMR Parsing with an Incremental Joint Model"
    },
    {
        "paper_id": "D16-1068",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1068.pdf",
        "title": "Effective Greedy Inference for Graph-based Non-Projective Dependency Parsing"
    },
    {
        "paper_id": "D16-1071",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1071.pdf",
        "title": "LAMB: A Good Shepherd of Morphologically Rich Languages"
    },
    {
        "paper_id": "D16-1072",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1072.pdf",
        "title": "Fast Coupled Sequence Labeling on Heterogeneous Annotations via Context-aware Pruning"
    },
    {
        "paper_id": "D16-1075",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1075.pdf",
        "title": "News Stream Summarization using Burst Information Networks"
    },
    {
        "paper_id": "D16-1078",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1078.pdf",
        "title": "Speculation and Negation Scope Detection via Convolutional Neural Networks"
    },
    {
        "paper_id": "D16-1080",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1080.pdf",
        "title": "Keyphrase Extraction Using Deep Recurrent Neural Networks on Twitter"
    },
    {
        "paper_id": "D16-1083",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1083.pdf",
        "title": "Learning to Represent Review with Tensor Decomposition for Spam Detection"
    },
    {
        "paper_id": "D16-1084",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1084.pdf",
        "title": "Stance Detection with Bidirectional Conditional Encoding"
    },
    {
        "paper_id": "D16-1088",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1088.pdf",
        "title": "Extracting Subevents via an Effective Two-phase Approach"
    },
    {
        "paper_id": "D16-1089",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1089.pdf",
        "title": "Gaussian Visual-Linguistic Embedding for Zero-Shot Recognition"
    },
    {
        "paper_id": "D16-1096",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1096.pdf",
        "title": "Coverage Embedding Models for Neural Machine Translation"
    },
    {
        "paper_id": "D16-1099",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1099.pdf",
        "title": "The Effects of Data Size and Frequency Range on Distributional Semantic Models"
    },
    {
        "paper_id": "D16-1102",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1102.pdf",
        "title": "Towards Semi-Automatic Generation of Proposition Banks for Low-Resource Languages"
    },
    {
        "paper_id": "D16-1104",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1104.pdf",
        "title": "Are Word Embedding-based Features Useful for Sarcasm Detection?"
    },
    {
        "paper_id": "D16-1108",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1108.pdf",
        "title": "Characterizing the Language of Online Communities and its Relation to Community Reception"
    },
    {
        "paper_id": "D16-1122",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1122.pdf",
        "title": "Detecting and Characterizing Events"
    },
    {
        "paper_id": "D16-1129",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1129.pdf",
        "title": "What makes a convincing argument? Empirical analysis and detecting attributes of convincingness in Web argumentation"
    },
    {
        "paper_id": "D16-1132",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1132.pdf",
        "title": "Intra-Sentential Subject Zero Anaphora Resolution using Multi-Column Convolutional Neural Network"
    },
    {
        "paper_id": "D16-1136",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1136.pdf",
        "title": "Learning Crosslingual Word Embeddings without Bilingual Corpora"
    },
    {
        "paper_id": "D16-1138",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1138.pdf",
        "title": "Online Segment to Segment Neural Transduction"
    },
    {
        "paper_id": "D16-1144",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1144.pdf",
        "title": "AFET: Automatic Fine-Grained Entity Typing by Hierarchical Partial-Label Embedding"
    },
    {
        "paper_id": "D16-1147",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1147.pdf",
        "title": "Key-Value Memory Networks for Directly Reading Documents"
    },
    {
        "paper_id": "D16-1149",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1149.pdf",
        "title": "The Teams Corpus and Entrainment in Multi-Party Spoken Dialogues"
    },
    {
        "paper_id": "D16-1150",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1150.pdf",
        "title": "Personalized Emphasis Framing for Persuasive Message Generation"
    },
    {
        "paper_id": "D16-1151",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1151.pdf",
        "title": "Cross Sentence Inference for Process Knowledge"
    },
    {
        "paper_id": "D16-1152",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1152.pdf",
        "title": "Toward Socially-Infused Information Extraction: Embedding Authors, Mentions, and Entities"
    },
    {
        "paper_id": "D16-1153",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1153.pdf",
        "title": "Phonologically Aware Neural Model for Named Entity Recognition in Low Resource Transfer Settings"
    },
    {
        "paper_id": "D16-1154",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1154.pdf",
        "title": "Long-Short Range Context Neural Networks for Language Modeling"
    },
    {
        "paper_id": "D16-1156",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1156.pdf",
        "title": "Resolving Language and Vision Ambiguities Together: Joint Segmentation & Prepositional Attachment Resolution in Captioned Scenes"
    },
    {
        "paper_id": "D16-1157",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1157.pdf",
        "title": "Charagram: Embedding Words and Sentences via Character n-grams"
    },
    {
        "paper_id": "D16-1160",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1160.pdf",
        "title": "Exploiting Source-side Monolingual Data in Neural Machine Translation"
    },
    {
        "paper_id": "D16-1161",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1161.pdf",
        "title": "Phrase-based Machine Translation is State-of-the-Art for Automatic Grammatical Error Correction"
    },
    {
        "paper_id": "D16-1163",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1163.pdf",
        "title": "Transfer Learning for Low-Resource Neural Machine Translation"
    },
    {
        "paper_id": "D16-1165",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1165.pdf",
        "title": "It Takes Three to Tango: Triangulation Approach to Answer Ranking in Community Question Answering"
    },
    {
        "paper_id": "D16-1168",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1168.pdf",
        "title": "A Theme-Rewriting Approach for Generating Algebra Word Problems"
    },
    {
        "paper_id": "D16-1173",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1173.pdf",
        "title": "Deep Neural Networks with Massive Learned Knowledge"
    },
    {
        "paper_id": "D16-1174",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1174.pdf",
        "title": "De-Conflated Semantic Representations"
    },
    {
        "paper_id": "D16-1175",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1175.pdf",
        "title": "Improving Sparse Word Representations with Distributional Inference for Semantic Composition"
    },
    {
        "paper_id": "D16-1179",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1179.pdf",
        "title": "Verb Phrase Ellipsis Resolution Using Discriminative and Margin-Infused Algorithms"
    },
    {
        "paper_id": "D16-1181",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1181.pdf",
        "title": "LSTM Shift-Reduce CCG Parsing"
    },
    {
        "paper_id": "D16-1182",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1182.pdf",
        "title": "An Evaluation of Parser Robustness for Ungrammatical Sentences"
    },
    {
        "paper_id": "D16-1183",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1183.pdf",
        "title": "Neural Shift-Reduce CCG Semantic Parsing"
    },
    {
        "paper_id": "D16-1184",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1184.pdf",
        "title": "Syntactic Parsing of Web Queries"
    },
    {
        "paper_id": "D16-1185",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1185.pdf",
        "title": "Unsupervised Text Recap Extraction for TV Series"
    },
    {
        "paper_id": "D16-1187",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1187.pdf",
        "title": "Deceptive Review Spam Detection via Exploiting Task Relatedness and Unlabeled Data"
    },
    {
        "paper_id": "D16-1194",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1194.pdf",
        "title": "Non-uniform Language Detection in Technical Writing"
    },
    {
        "paper_id": "D16-1196",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1196.pdf",
        "title": "Orthographic Syllable as basic unit for SMT between Related Languages"
    },
    {
        "paper_id": "D16-1200",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1200.pdf",
        "title": "Timeline extraction using distant supervision and joint inference"
    },
    {
        "paper_id": "D16-1204",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1204.pdf",
        "title": "Improving LSTM-based Video Description with Linguistic Knowledge Mined from Text"
    },
    {
        "paper_id": "D16-1207",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1207.pdf",
        "title": "Learning Robust Representations of Text"
    },
    {
        "paper_id": "D16-1210",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1210.pdf",
        "title": "Unsupervised Word Alignment by Agreement Under ITG Constraint"
    },
    {
        "paper_id": "D16-1220",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1220.pdf",
        "title": "Learning to Identify Metaphors from a Corpus of Proverbs"
    },
    {
        "paper_id": "D16-1231",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1231.pdf",
        "title": "Addressee and Response Selection for Multi-Party Conversation"
    },
    {
        "paper_id": "D16-1237",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1237.pdf",
        "title": "Exploiting Sentence Similarities for Better Alignments"
    },
    {
        "paper_id": "D16-1238",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1238.pdf",
        "title": "Bi-directional Attention with Agreement for Dependency Parsing"
    },
    {
        "paper_id": "D16-1242",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1242.pdf",
        "title": "Building compositional semantics and higher-order inference system for a wide-coverage Japanese CCG parser"
    },
    {
        "paper_id": "D16-1243",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1243.pdf",
        "title": "Learning to Generate Compositional Color Descriptions"
    },
    {
        "paper_id": "D16-1247",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1247.pdf",
        "title": "Insertion Position Selection Model for Flexible Non-Terminals in Dependency Tree-to-Tree Machine Translation"
    },
    {
        "paper_id": "D16-1249",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1249.pdf",
        "title": "Supervised Attentions for Neural Machine Translation"
    },
    {
        "paper_id": "D16-1250",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1250.pdf",
        "title": "Learning principled bilingual mappings of word embeddings while preserving monolingual invariance"
    },
    {
        "paper_id": "D16-1253",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1253.pdf",
        "title": "Bilingually-constrained Synthetic Data for Implicit Discourse Relation Recognition"
    },
    {
        "paper_id": "D16-1255",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1255.pdf",
        "title": "Word Ordering Without Syntax"
    },
    {
        "paper_id": "D16-1260",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1260.pdf",
        "title": "Encoding Temporal Information for Time-Aware Link Prediction"
    },
    {
        "paper_id": "D16-1262",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1262.pdf",
        "title": "Global Neural CCG Parsing with Optimality Guarantees"
    },
    {
        "paper_id": "D16-1264",
        "conference": "emnlp",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/D16-1264.pdf",
        "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"
    },
    {
        "paper_id": "D17-1001",
        "conference": "emnlp",
        "year": "2017",
        "abstract": "We propose an efficient method to conduct phrase alignment on parse forests for paraphrase detection. Unlike previous studies, our method identifies syntactic paraphrases under linguistically motivated grammar. In addition, it allows phrases to non-compositionally align to handle paraphrases with non-homographic phrase correspondences. A dataset that provides gold parse trees and their phrase alignments is created. The experimental results confirm that the proposed method conducts highly accurate phrase alignment compared to human performance.",
        "url_pdf": "https://www.aclweb.org/anthology/D17-1001.pdf",
        "title": "Monolingual Phrase Alignment on Parse Forests"
    },
    {
        "paper_id": "D17-1004",
        "conference": "emnlp",
        "year": "2017",
        "abstract": "Organized relational knowledge in the form of \u201cknowledge graphs\u201d is important for many applications. However, the ability to populate knowledge bases with facts automatically extracted from documents has improved frustratingly slowly. This paper simultaneously addresses two issues that have held back prior work. We first propose an effective new model, which combines an LSTM sequence model with a form of entity position-aware attention that is better suited to relation extraction. Then we build TACRED, a large (119,474 examples) supervised relation extraction dataset obtained via crowdsourcing and targeted towards TAC KBP relations. The combination of better supervised data and a more appropriate high-capacity model enables much better relation extraction performance. When the model trained on this new dataset replaces the previous relation extraction component of the best TAC KBP 2015 slot filling system, its F1 score increases markedly from 22.2% to 26.7%.",
        "url_pdf": "https://www.aclweb.org/anthology/D17-1004.pdf",
        "title": "Position-aware Attention and Supervised Data Improve Slot Filling"
    },
    {
        "paper_id": "D17-1006",
        "conference": "emnlp",
        "year": "2017",
        "abstract": "There has been a recent line of work automatically learning scripts from unstructured texts, by modeling narrative event chains. While the dominant approach group events using event pair relations, LSTMs have been used to encode full chains of narrative events. The latter has the advantage of learning long-range temporal orders, yet the former is more adaptive to partial orders. We propose a neural model that leverages the advantages of both methods, by using LSTM hidden states as features for event pair modelling. A dynamic memory network is utilized to automatically induce weights on existing events for inferring a subsequent event. Standard evaluation shows that our method significantly outperforms both methods above, giving the best results reported so far.",
        "url_pdf": "https://www.aclweb.org/anthology/D17-1006.pdf",
        "title": "Integrating Order Information and Event Relation for Script Event Prediction"
    },
    {
        "paper_id": "D17-1214",
        "conference": "emnlp",
        "year": "2017",
        "abstract": "We introduce a hierarchical architecture for machine reading capable of extracting precise information from long documents. The model divides the document into small, overlapping windows and encodes all windows in parallel with an RNN. It then attends over these window encodings, reducing them to a single encoding, which is decoded into an answer using a sequence decoder. This hierarchical approach allows the model to scale to longer documents without increasing the number of sequential steps. In a supervised setting, our model achieves state of the art accuracy of 76.8 on the WikiReading dataset. We also evaluate the model in a semi-supervised setting by downsampling the WikiReading training set to create increasingly smaller amounts of supervision, while leaving the full unlabeled document corpus to train a sequence autoencoder on document windows. We evaluate models that can reuse autoencoder states and outputs without fine-tuning their weights, allowing for more efficient training and inference.",
        "url_pdf": "https://www.aclweb.org/anthology/D17-1214.pdf",
        "title": "Accurate Supervised and Semi-Supervised Machine Reading for Long Documents"
    },
    {
        "paper_id": "D17-1215",
        "conference": "emnlp",
        "year": "2017",
        "abstract": "Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of 75% F1 score to 36%; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to 7%. We hope our insights will motivate the development of new models that understand language more precisely.",
        "url_pdf": "https://www.aclweb.org/anthology/D17-1215.pdf",
        "title": "Adversarial Examples for Evaluating Reading Comprehension Systems"
    },
    {
        "paper_id": "D17-1216",
        "conference": "emnlp",
        "year": "2017",
        "abstract": "Reasoning with commonsense knowledge is critical for natural language understanding. Traditional methods for commonsense machine comprehension mostly only focus on one specific kind of knowledge, neglecting the fact that commonsense reasoning requires simultaneously considering different kinds of commonsense knowledge. In this paper, we propose a multi-knowledge reasoning method, which can exploit heterogeneous knowledge for commonsense machine comprehension. Specifically, we first mine different kinds of knowledge (including event narrative knowledge, entity semantic knowledge and sentiment coherent knowledge) and encode them as inference rules with costs. Then we propose a multi-knowledge reasoning model, which selects inference rules for a specific reasoning context using attention mechanism, and reasons by summarizing all valid inference rules. Experiments on RocStories show that our method outperforms traditional models significantly.",
        "url_pdf": "https://www.aclweb.org/anthology/D17-1216.pdf",
        "title": "Reasoning with Heterogeneous Knowledge for Commonsense Machine Comprehension"
    },
    {
        "paper_id": "D17-1218",
        "conference": "emnlp",
        "year": "2017",
        "abstract": "Argument mining has become a popular research area in NLP. It typically includes the identification of argumentative components, e.g. claims, as the central component of an argument. We perform a qualitative analysis across six different datasets and show that these appear to conceptualize claims quite differently. To learn about the consequences of such different conceptualizations of claim for practical applications, we carried out extensive experiments using state-of-the-art feature-rich and deep learning systems, to identify claims in a cross-domain fashion. While the divergent conceptualization of claims in different datasets is indeed harmful to cross-domain classification, we show that there are shared properties on the lexical level as well as system configurations that can help to overcome these gaps.",
        "url_pdf": "https://www.aclweb.org/anthology/D17-1218.pdf",
        "title": "What is the Essence of a Claim? Cross-Domain Claim Identification"
    },
    {
        "paper_id": "D17-1219",
        "conference": "emnlp",
        "year": "2017",
        "abstract": "A first step in the task of automatically generating questions for testing reading comprehension is to identify question-worthy sentences, i.e. sentences in a text passage that humans find it worthwhile to ask questions about. We propose a hierarchical neural sentence-level sequence tagging model for this task, which existing approaches to question generation have ignored. The approach is fully data-driven \u2014 with no sophisticated NLP pipelines or any hand-crafted rules/features \u2014 and compares favorably to a number of baselines when evaluated on the SQuAD data set. When incorporated into an existing neural question generation system, the resulting end-to-end system achieves state-of-the-art performance for paragraph-level question generation for reading comprehension.",
        "url_pdf": "https://www.aclweb.org/anthology/D17-1219.pdf",
        "title": "Identifying Where to Focus in Reading Comprehension for Neural Question Generation"
    },
    {
        "paper_id": "D17-1224",
        "conference": "emnlp",
        "year": "2017",
        "abstract": "In this paper we investigate a new task of automatically constructing an overview article from a given set of news articles about a news event. We propose a news synthesis approach to address this task based on passage segmentation, ranking, selection and merging. Our proposed approach is compared with several typical multi-document summarization methods on the Wikinews dataset, and achieves the best performance on both automatic evaluation and manual evaluation.",
        "url_pdf": "https://www.aclweb.org/anthology/D17-1224.pdf",
        "title": "Towards Automatic Construction of News Overview Articles by News Synthesis"
    },
    {
        "paper_id": "D17-1226",
        "conference": "emnlp",
        "year": "2017",
        "abstract": "We introduce a novel iterative approach for event coreference resolution that gradually builds event clusters by exploiting inter-dependencies among event mentions within the same chain as well as across event chains. Among event mentions in the same chain, we distinguish within- and cross-document event coreference links by using two distinct pairwise classifiers, trained separately to capture differences in feature distributions of within- and cross-document event clusters. Our event coreference approach alternates between WD and CD clustering and combines arguments from both event clusters after every merge, continuing till no more merge can be made. And then it performs further merging between event chains that are both closely related to a set of other chains of events. Experiments on the ECB+ corpus show that our model outperforms state-of-the-art methods in joint task of WD and CD event coreference resolution.",
        "url_pdf": "https://www.aclweb.org/anthology/D17-1226.pdf",
        "title": "Event Coreference Resolution by Iteratively Unfolding Inter-dependencies among Events"
    },
    {
        "paper_id": "D17-1228",
        "conference": "emnlp",
        "year": "2017",
        "abstract": "We propose simple and flexible training and decoding methods for influencing output style and topic in neural encoder-decoder based language generation. This capability is desirable in a variety of applications, including conversational systems, where successful agents need to produce language in a specific style and generate responses steered by a human puppeteer or external knowledge. We decompose the neural generation process into empirically easier sub-problems: a faithfulness model and a decoding method based on selective-sampling. We also describe training and sampling algorithms that bias the generation process with a specific language style restriction, or a topic restriction. Human evaluation results show that our proposed methods are able to to restrict style and topic without degrading output quality in conversational tasks.",
        "url_pdf": "https://www.aclweb.org/anthology/D17-1228.pdf",
        "title": "Steering Output Style and Topic in Neural Response Generation"
    },
    {
        "paper_id": "D17-1229",
        "conference": "emnlp",
        "year": "2017",
        "abstract": "This paper introduces a novel training/decoding strategy for sequence labeling. Instead of greedily choosing a label at each time step, and using it for the next prediction, we retain the probability distribution over the current label, and pass this distribution to the next prediction. This approach allows us to avoid the effect of label bias and error propagation in sequence learning/decoding. Our experiments on dialogue act classification demonstrate the effectiveness of this approach. Even though our underlying neural network model is relatively simple, it outperforms more complex neural models, achieving state-of-the-art results on the MapTask and Switchboard corpora.",
        "url_pdf": "https://www.aclweb.org/anthology/D17-1229.pdf",
        "title": "Preserving Distributional Information in Dialogue Act Classification"
    },
    {
        "paper_id": "D17-1237",
        "conference": "emnlp",
        "year": "2017",
        "abstract": "Building a dialogue agent to fulfill complex tasks, such as travel planning, is challenging because the agent has to learn to collectively complete multiple subtasks. For example, the agent needs to reserve a hotel and book a flight so that there leaves enough time for commute between arrival and hotel check-in. This paper addresses this challenge by formulating the task in the mathematical framework of options over Markov Decision Processes (MDPs), and proposing a hierarchical deep reinforcement learning approach to learning a dialogue manager that operates at different temporal scales. The dialogue manager consists of: (1) a top-level dialogue policy that selects among subtasks or options, (2) a low-level dialogue policy that selects primitive actions to complete the subtask given by the top-level policy, and (3) a global state tracker that helps ensure all cross-subtask constraints be satisfied. Experiments on a travel planning task with simulated and real users show that our approach leads to significant improvements over three baselines, two based on handcrafted rules and the other based on flat deep reinforcement learning.",
        "url_pdf": "https://www.aclweb.org/anthology/D17-1237.pdf",
        "title": "Composite Task-Completion Dialogue Policy Learning via Hierarchical Deep Reinforcement Learning"
    },
    {
        "paper_id": "D17-1240",
        "conference": "emnlp",
        "year": "2017",
        "abstract": "n this paper, we present a set of computational methods to identify the likeliness of a word being borrowed, based on the signals from social media. In terms of Spearman\u2019s correlation values, our methods perform more than two times better (\u223c 0.62) in predicting the borrowing likeliness compared to the best performing baseline (\u223c 0.26) reported in literature. Based on this likeliness estimate we asked annotators to re-annotate the language tags of foreign words in predominantly native contexts. In 88% of cases the annotators felt that the foreign language tag should be replaced by native language tag, thus indicating a huge scope for improvement of automatic language identification systems.",
        "url_pdf": "https://www.aclweb.org/anthology/D17-1240.pdf",
        "title": "All that is English may be Hindi: Enhancing language identification through automatic ranking of the likeliness of word borrowing in social media"
    },
    {
        "paper_id": "D17-1244",
        "conference": "emnlp",
        "year": "2017",
        "abstract": "This paper presents a corpus and experiments to determine dimensions of interpersonal relationships. We define a set of dimensions heavily inspired by work in social science. We create a corpus by retrieving pairs of people, and then annotating dimensions for their relationships. A corpus analysis shows that dimensions can be annotated reliably. Experimental results show that given a pair of people, values to dimensions can be assigned automatically.",
        "url_pdf": "https://www.aclweb.org/anthology/D17-1244.pdf",
        "title": "Dimensions of Interpersonal Relationships: Corpus and Experiments"
    },
    {
        "paper_id": "D17-1245",
        "conference": "emnlp",
        "year": "2017",
        "abstract": "Social media collect and spread on the Web personal opinions, facts, fake news and all kind of information users may be interested in. Applying argument mining methods to such heterogeneous data sources is a challenging open research issue, in particular considering the peculiarities of the language used to write textual messages on social media. In addition, new issues emerge when dealing with arguments posted on such platforms, such as the need to make a distinction between personal opinions and actual facts, and to detect the source disseminating information about such facts to allow for provenance verification. In this paper, we apply supervised classification to identify arguments on Twitter, and we present two new tasks for argument mining, namely facts recognition and source identification. We study the feasibility of the approaches proposed to address these tasks on a set of tweets related to the Grexit and Brexit news topics.",
        "url_pdf": "https://www.aclweb.org/anthology/D17-1245.pdf",
        "title": "Argument Mining on Twitter: Arguments, Facts and Sources"
    },
    {
        "paper_id": "D17-1260",
        "conference": "emnlp",
        "year": "2017",
        "abstract": "Hand-crafted rules and reinforcement learning (RL) are two popular choices to obtain dialogue policy. The rule-based policy is often reliable within predefined scope but not self-adaptable, whereas RL is evolvable with data but often suffers from a bad initial performance. We employ a companion learning framework to integrate the two approaches for on-line dialogue policy learning, in which a pre-defined rule-based policy acts as a \u201cteacher\u201d and guides a data-driven RL system by giving example actions as well as additional rewards. A novel agent-aware dropout Deep Q-Network (AAD-DQN) is proposed to address the problem of when to consult the teacher and how to learn from the teacher\u2019s experiences. AAD-DQN, as a data-driven student policy, provides (1) two separate experience memories for student and teacher, (2) an uncertainty estimated by dropout to control the timing of consultation and learning. Simulation experiments showed that the proposed approach can significantly improve both safetyand efficiency of on-line policy optimization compared to other companion learning approaches as well as supervised pre-training using static dialogue corpus.",
        "url_pdf": "https://www.aclweb.org/anthology/D17-1260.pdf",
        "title": "Agent-Aware Dropout DQN for Safe and Efficient On-line Dialogue Policy Learning"
    },
    {
        "paper_id": "D17-1267",
        "conference": "emnlp",
        "year": "2017",
        "abstract": "We present a system for identifying cognate sets across dictionaries of related languages. The likelihood of a cognate relationship is calculated on the basis of a rich set of features that capture both phonetic and semantic similarity, as well as the presence of regular sound correspondences. The similarity scores are used to cluster words from different languages that may originate from a common proto-word. When tested on the Algonquian language family, our system detects 63% of cognate sets while maintaining cluster purity of 70%.",
        "url_pdf": "https://www.aclweb.org/anthology/D17-1267.pdf",
        "title": "Identifying Cognate Sets Across Dictionaries of Related Languages"
    },
    {
        "paper_id": "D17-1269",
        "conference": "emnlp",
        "year": "2017",
        "abstract": "Recent work in NLP has attempted to deal with low-resource languages but still assumed a resource level that is not present for most languages, e.g., the availability of Wikipedia in the target language. We propose a simple method for cross-lingual named entity recognition (NER) that works well in settings with very minimal resources. Our approach makes use of a lexicon to \u201ctranslate\u201d annotated data available in one or several high resource language(s) into the target language, and learns a standard monolingual NER model there. Further, when Wikipedia is available in the target language, our method can enhance Wikipedia based methods to yield state-of-the-art NER results; we evaluate on 7 diverse languages, improving the state-of-the-art by an average of 5.5% F1 points. With the minimal resources required, this is an extremely portable cross-lingual NER approach, as illustrated using a truly low-resource language, Uyghur.",
        "url_pdf": "https://www.aclweb.org/anthology/D17-1269.pdf",
        "title": "Cheap Translation for Cross-Lingual Named Entity Recognition"
    },
    {
        "paper_id": "D17-1274",
        "conference": "emnlp",
        "year": "2017",
        "abstract": "Slot Filling (SF) aims to extract the values of certain types of attributes (or slots, such as person:cities_of_residence) for a given entity from a large collection of source documents. In this paper we propose an effective DNN architecture for SF with the following new strategies: (1). Take a regularized dependency graph instead of a raw sentence as input to DNN, to compress the wide contexts between query and candidate filler; (2). Incorporate two attention mechanisms: local attention learned from query and candidate filler, and global attention learned from external knowledge bases, to guide the model to better select indicative contexts to determine slot type. Experiments show that this framework outperforms state-of-the-art on both relation extraction (16% absolute F-score gain) and slot filling validation for each individual system (up to 8.5% absolute F-score gain).",
        "url_pdf": "https://www.aclweb.org/anthology/D17-1274.pdf",
        "title": "Improving Slot Filling Performance with Attentive Neural Networks on Dependency Structures"
    },
    {
        "paper_id": "D17-1275",
        "conference": "emnlp",
        "year": "2017",
        "abstract": "One weakness of machine-learned NLP models is that they typically perform poorly on out-of-domain data. In this work, we study the task of identifying products being bought and sold in online cybercrime forums, which exhibits particularly challenging cross-domain effects. We formulate a task that represents a hybrid of slot-filling information extraction and named entity recognition and annotate data from four different forums. Each of these forums constitutes its own \u201cfine-grained domain\u201d in that the forums cover different market sectors with different properties, even though all forums are in the broad domain of cybercrime. We characterize these domain differences in the context of a learning-based system: supervised models see decreased accuracy when applied to new forums, and standard techniques for semi-supervised learning and domain adaptation have limited effectiveness on this data, which suggests the need to improve these techniques. We release a dataset of 1,938 annotated posts from across the four forums.",
        "url_pdf": "https://www.aclweb.org/anthology/D17-1275.pdf",
        "title": "Identifying Products in Online Cybercrime Marketplaces: A Dataset for Fine-grained Domain Adaptation"
    },
    {
        "paper_id": "D17-1276",
        "conference": "emnlp",
        "year": "2017",
        "abstract": "In this paper, we propose a new model that is capable of recognizing overlapping mentions. We introduce a novel notion of mention separators that can be effectively used to capture how mentions overlap with one another. On top of a novel multigraph representation that we introduce, we show that efficient and exact inference can still be performed. We present some theoretical analysis on the differences between our model and a recently proposed model for recognizing overlapping mentions, and discuss the possible implications of the differences. Through extensive empirical analysis on standard datasets, we demonstrate the effectiveness of our approach.",
        "url_pdf": "https://www.aclweb.org/anthology/D17-1276.pdf",
        "title": "Labeling Gaps Between Words: Recognizing Overlapping Mentions with Mention Separators"
    },
    {
        "paper_id": "D17-1279",
        "conference": "emnlp",
        "year": "2017",
        "abstract": "This paper addresses the problem of extracting keyphrases from scientific articles and categorizing them as corresponding to a task, process, or material. We cast the problem as sequence tagging and introduce semi-supervised methods to a neural tagging model, which builds on recent advances in named entity recognition. Since annotated training data is scarce in this domain, we introduce a graph-based semi-supervised algorithm together with a data selection scheme to leverage unannotated articles. Both inductive and transductive semi-supervised learning strategies outperform state-of-the-art information extraction performance on the 2017 SemEval Task 10 ScienceIE task.",
        "url_pdf": "https://www.aclweb.org/anthology/D17-1279.pdf",
        "title": "Scientific Information Extraction with Semi-supervised Neural Tagging"
    },
    {
        "paper_id": "D17-1284",
        "conference": "emnlp",
        "year": "2017",
        "abstract": "For accurate entity linking, we need to capture various information aspects of an entity, such as its description in a KB, contexts in which it is mentioned, and structured knowledge. Additionally, a linking system should work on texts from different domains without requiring domain-specific training data or hand-engineered features. In this work we present a neural, modular entity linking system that learns a unified dense representation for each entity using multiple sources of information, such as its description, contexts around its mentions, and its fine-grained types. We show that the resulting entity linking system is effective at combining these sources, and performs competitively, sometimes out-performing current state-of-the-art systems across datasets, without requiring any domain-specific training data or hand-engineered features. We also show that our model can effectively \u201cembed\u201d entities that are new to the KB, and is able to link its mentions accurately.",
        "url_pdf": "https://www.aclweb.org/anthology/D17-1284.pdf",
        "title": "Entity Linking via Joint Encoding of Types, Descriptions, and Context"
    },
    {
        "paper_id": "D17-1285",
        "conference": "emnlp",
        "year": "2017",
        "abstract": "Mining biomedical text offers an opportunity to automatically discover important facts and infer associations among them. As new scientific findings appear across a large collection of biomedical publications, our aim is to tap into this literature to automate biomedical knowledge extraction and identify important insights from them. Towards that goal, we develop a system with novel deep neural networks to extract insights on biomedical literature. Evaluation shows our system is able to provide insights with competitive accuracy of human acceptance and its relation extraction component outperforms previous work.",
        "url_pdf": "https://www.aclweb.org/anthology/D17-1285.pdf",
        "title": "An Insight Extraction System on BioMedical Literature with Deep Neural Networks"
    },
    {
        "paper_id": "D17-1291",
        "conference": "emnlp",
        "year": "2017",
        "abstract": "A document outlier is a document that substantially deviates in semantics from the majority ones in a corpus. Automatic identification of document outliers can be valuable in many applications, such as screening health records for medical mistakes. In this paper, we study the problem of mining semantically deviating document outliers in a given corpus. We develop a generative model to identify frequent and characteristic semantic regions in the word embedding space to represent the given corpus, and a robust outlierness measure which is resistant to noisy content in documents. Experiments conducted on two real-world textual data sets show that our method can achieve an up to 135% improvement over baselines in terms of recall at top-1% of the outlier ranking.",
        "url_pdf": "https://www.aclweb.org/anthology/D17-1291.pdf",
        "title": "Identifying Semantically Deviating Outlier Documents"
    },
    {
        "paper_id": "D17-1292",
        "conference": "emnlp",
        "year": "2017",
        "abstract": "Explaining underlying causes or effects about events is a challenging but valuable task. We define a novel problem of generating explanations of a time series event by (1) searching cause and effect relationships of the time series with textual data and (2) constructing a connecting chain between them to generate an explanation. To detect causal features from text, we propose a novel method based on the Granger causality of time series between features extracted from text such as N-grams, topics, sentiments, and their composition. The generation of the sequence of causal entities requires a commonsense causative knowledge base with efficient reasoning. To ensure good interpretability and appropriate lexical usage we combine symbolic and neural representations, using a neural reasoning algorithm trained on commonsense causal tuples to predict the next cause step. Our quantitative and human analysis show empirical evidence that our method successfully extracts meaningful causality relationships between time series with textual features and generates appropriate explanation between them.",
        "url_pdf": "https://www.aclweb.org/anthology/D17-1292.pdf",
        "title": "Detecting and Explaining Causes From Text For a Time Series Event"
    },
    {
        "paper_id": "D17-1293",
        "conference": "emnlp",
        "year": "2017",
        "abstract": "Recent years have witnessed the proliferation of Massive Open Online Courses (MOOCs). With massive learners being offered MOOCs, there is a demand that the forum contents within MOOCs need to be classified in order to facilitate both learners and instructors. Therefore we investigate a significant application, which is to associate forum threads to subtitles of video clips. This task can be regarded as a document ranking problem, and the key is how to learn a distinguishable text representation from word sequences and learners\u2019 behavior sequences. In this paper, we propose a novel cascade model, which can capture both the latent semantics and latent similarity by modeling MOOC data. Experimental results on two real-world datasets demonstrate that our textual representation outperforms state-of-the-art unsupervised counterparts for the application.",
        "url_pdf": "https://www.aclweb.org/anthology/D17-1293.pdf",
        "title": "A Novel Cascade Model for Learning Latent Similarity from Heterogeneous Sequential Data of MOOC"
    },
    {
        "paper_id": "D17-1294",
        "conference": "emnlp",
        "year": "2017",
        "abstract": "Websites\u2019 and mobile apps\u2019 privacy policies, written in natural language, tend to be long and difficult to understand. Information privacy revolves around the fundamental principle of Notice and choice, namely the idea that users should be able to make informed decisions about what information about them can be collected and how it can be used. Internet users want control over their privacy, but their choices are often hidden in long and convoluted privacy policy texts. Moreover, little (if any) prior work has been done to detect the provision of choices in text. We address this challenge of enabling user choice by automatically identifying and extracting pertinent choice language in privacy policies. In particular, we present a two-stage architecture of classification models to identify opt-out choices in privacy policy text, labelling common varieties of choices with a mean F1 score of 0.735. Our techniques enable the creation of systems to help Internet users to learn about their choices, thereby effectuating notice and choice and improving Internet privacy.",
        "url_pdf": "https://www.aclweb.org/anthology/D17-1294.pdf",
        "title": "Identifying the Provision of Choices in Privacy Policy Text"
    },
    {
        "paper_id": "D17-1296",
        "conference": "emnlp",
        "year": "2017",
        "abstract": "In this paper, we model the problem of disfluency detection using a transition-based framework, which incrementally constructs and labels the disfluency chunk of input sentences using a new transition system without syntax information. Compared with sequence labeling methods, it can capture non-local chunk-level features; compared with joint parsing and disfluency detection methods, it is free for noise in syntax. Experiments show that our model achieves state-of-the-art f-score of 87.5% on the commonly used English Switchboard test set, and a set of in-house annotated Chinese data.",
        "url_pdf": "https://www.aclweb.org/anthology/D17-1296.pdf",
        "title": "Transition-Based Disfluency Detection using LSTMs"
    },
    {
        "paper_id": "D17-1297",
        "conference": "emnlp",
        "year": "2017",
        "abstract": "We propose an approach to N-best list reranking using neural sequence-labelling models. We train a compositional model for error detection that calculates the probability of each token in a sentence being correct or incorrect, utilising the full sentence as context. Using the error detection model, we then re-rank the N best hypotheses generated by statistical machine translation systems. Our approach achieves state-of-the-art results on error correction for three different datasets, and it has the additional advantage of only using a small set of easily computed features that require no linguistic input.",
        "url_pdf": "https://www.aclweb.org/anthology/D17-1297.pdf",
        "title": "Neural Sequence-Labelling Models for Grammatical Error Correction"
    },
    {
        "paper_id": "D17-1298",
        "conference": "emnlp",
        "year": "2017",
        "abstract": "In a controlled experiment of sequence-to-sequence approaches for the task of sentence correction, we find that character-based models are generally more effective than word-based models and models that encode subword information via convolutions, and that modeling the output data as a series of diffs improves effectiveness over standard approaches. Our strongest sequence-to-sequence model improves over our strongest phrase-based statistical machine translation model, with access to the same data, by 6 M2 (0.5 GLEU) points. Additionally, in the data environment of the standard CoNLL-2014 setup, we demonstrate that modeling (and tuning against) diffs yields similar or better M2 scores with simpler models and/or significantly less data than previous sequence-to-sequence approaches.",
        "url_pdf": "https://www.aclweb.org/anthology/D17-1298.pdf",
        "title": "Adapting Sequence Models for Sentence Correction"
    },
    {
        "paper_id": "D17-1299",
        "conference": "emnlp",
        "year": "2017",
        "abstract": "Stylistic variations of language, such as formality, carry speakers\u2019 intention beyond literal meaning and should be conveyed adequately in translation. We propose to use lexical formality models to control the formality level of machine translation output. We demonstrate the effectiveness of our approach in empirical evaluations, as measured by automatic metrics and human assessments.",
        "url_pdf": "https://www.aclweb.org/anthology/D17-1299.pdf",
        "title": "A Study of Style in Machine Translation: Controlling the Formality of Machine Translation Output"
    },
    {
        "paper_id": "D17-1301",
        "conference": "emnlp",
        "year": "2017",
        "abstract": "In translation, considering the document as a whole can help to resolve ambiguities and inconsistencies. In this paper, we propose a cross-sentence context-aware approach and investigate the influence of historical contextual information on the performance of neural machine translation (NMT). First, this history is summarized in a hierarchical way. We then integrate the historical representation into NMT in two strategies: 1) a warm-start of encoder and decoder states, and 2) an auxiliary context source for updating decoder states. Experimental results on a large Chinese-English translation task show that our approach significantly improves upon a strong attention-based NMT system by up to +2.1 BLEU points.",
        "url_pdf": "https://www.aclweb.org/anthology/D17-1301.pdf",
        "title": "Exploiting Cross-Sentence Context for Neural Machine Translation"
    },
    {
        "paper_id": "D17-1309",
        "conference": "emnlp",
        "year": "2017",
        "abstract": "We show that small and shallow feed-forward neural networks can achieve near state-of-the-art results on a range of unstructured and structured language processing tasks while being considerably cheaper in memory and computational requirements than deep recurrent models. Motivated by resource-constrained environments like mobile phones, we showcase simple techniques for obtaining such small neural network models, and investigate different tradeoffs when deciding how to allocate a small memory budget.",
        "url_pdf": "https://www.aclweb.org/anthology/D17-1309.pdf",
        "title": "Natural Language Processing with Small Feed-Forward Networks"
    },
    {
        "paper_id": "D17-1317",
        "conference": "emnlp",
        "year": "2017",
        "abstract": "We present an analytic study on the language of news media in the context of political fact-checking and fake news detection. We compare the language of real news with that of satire, hoaxes, and propaganda to find linguistic characteristics of untrustworthy text. To probe the feasibility of automatic political fact-checking, we also present a case study based on PolitiFact.com using their factuality judgments on a 6-point scale. Experiments show that while media fact-checking remains to be an open research question, stylistic cues can help determine the truthfulness of text.",
        "url_pdf": "https://www.aclweb.org/anthology/D17-1317.pdf",
        "title": "Truth of Varying Shades: Analyzing Language in Fake News and Political Fact-Checking"
    },
    {
        "paper_id": "D17-1318",
        "conference": "emnlp",
        "year": "2017",
        "abstract": "We present a topic-based analysis of agreement and disagreement in political manifestos, which relies on a new method for topic detection based on key concept clustering. Our approach outperforms both standard techniques like LDA and a state-of-the-art graph-based method, and provides promising initial results for this new task in computational social science.",
        "url_pdf": "https://www.aclweb.org/anthology/D17-1318.pdf",
        "title": "Topic-Based Agreement and Disagreement in US Electoral Manifestos"
    },
    {
        "paper_id": "D17-1323",
        "conference": "emnlp",
        "year": "2017",
        "abstract": "Language is increasingly being used to de-fine rich visual recognition problems with supporting image collections sourced from the web. Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. In this work, we study data and models associated with multilabel object classification and visual semantic role labeling. We find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias. For example, the activity cooking is over 33% more likely to involve females than males in a training set, and a trained model further amplifies the disparity to 68% at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for collective inference. Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5% and 40.5% for multilabel classification and visual semantic role labeling, respectively\u3002",
        "url_pdf": "https://www.aclweb.org/anthology/D17-1323.pdf",
        "title": "Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints"
    },
    {
        "paper_id": "D18-1001",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "This article deals with adversarial attacks towards deep learning systems for Natural Language Processing (NLP), in the context of privacy protection. We study a specific type of attack: an attacker eavesdrops on the hidden representations of a neural text classifier and tries to recover information about the input text. Such scenario may arise in situations when the computation of a neural network is shared across multiple devices, e.g. some hidden representation is computed by a user\u2019s device and sent to a cloud-based model. We measure the privacy of a hidden representation by the ability of an attacker to predict accurately specific private information from it and characterize the tradeoff between the privacy and the utility of neural representations. Finally, we propose several defense methods based on modified training objectives and show that they improve the privacy of neural representations.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1001.pdf",
        "title": "Privacy-preserving Neural Representations of Text"
    },
    {
        "paper_id": "D18-1002",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Recent advances in Representation Learning and Adversarial Training seem to succeed in removing unwanted features from the learned representation. We show that demographic information of authors is encoded in\u2014and can be recovered from\u2014the intermediate representations learned by text-based neural classifiers. The implication is that decisions of classifiers trained on textual data are not agnostic to\u2014and likely condition on\u2014demographic attributes. When attempting to remove such demographic information using adversarial training, we find that while the adversarial component achieves chance-level development-set accuracy during training, a post-hoc classifier, trained on the encoded sentences from the first part, still manages to reach substantially higher classification accuracies on the same data. This behavior is consistent across several tasks, demographic properties and datasets. We explore several techniques to improve the effectiveness of the adversarial component. Our main conclusion is a cautionary one: do not rely on the adversarial training to achieve invariant representation to sensitive features.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1002.pdf",
        "title": "Adversarial Removal of Demographic Attributes from Text Data"
    },
    {
        "paper_id": "D18-1006",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Comprehending procedural text, e.g., a paragraph describing photosynthesis, requires modeling actions and the state changes they produce, so that questions about entities at different timepoints can be answered. Although several recent systems have shown impressive progress in this task, their predictions can be globally inconsistent or highly improbable. In this paper, we show how the predicted effects of actions in the context of a paragraph can be improved in two ways: (1) by incorporating global, commonsense constraints (e.g., a non-existent entity cannot be destroyed), and (2) by biasing reading with preferences from large-scale corpora (e.g., trees rarely move). Unlike earlier methods, we treat the problem as a neural structured prediction task, allowing hard and soft constraints to steer the model away from unlikely predictions. We show that the new model significantly outperforms earlier systems on a benchmark dataset for procedural text comprehension (+8% relative gain), and that it also avoids some of the nonsensical predictions that earlier systems make.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1006.pdf",
        "title": "Reasoning about Actions and State Changes by Injecting Commonsense Knowledge"
    },
    {
        "paper_id": "D18-1010",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Determining whether a given claim is supported by evidence is a fundamental NLP problem that is best modeled as Textual Entailment. However, given a large collection of text, finding evidence that could support or refute a given claim is a challenge in itself, amplified by the fact that different evidence might be needed to support or refute a claim. Nevertheless, most prior work decouples evidence finding from determining the truth value of the claim given the evidence. We propose to consider these two aspects jointly. We develop TwoWingOS (two-wing optimization strategy), a system that, while identifying appropriate evidence for a claim, also determines whether or not the claim is supported by the evidence. Given the claim, TwoWingOS attempts to identify a subset of the evidence candidates; given the predicted evidence, it then attempts to determine the truth value of the corresponding claim entailment problem. We treat this problem as coupled optimization problems, training a joint model for it. TwoWingOS offers two advantages: (i) Unlike pipeline systems it facilitates flexible-size evidence set, and (ii) Joint training improves both the claim entailment and the evidence identification. Experiments on a benchmark dataset show state-of-the-art performance.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1010.pdf",
        "title": "TwoWingOS: A Two-Wing Optimization Strategy for Evidential Claim Verification"
    },
    {
        "paper_id": "D18-1013",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "The encode-decoder framework has shown recent success in image captioning. Visual attention, which is good at detailedness, and semantic attention, which is good at comprehensiveness, have been separately proposed to ground the caption on the image. In this paper, we propose the Stepwise Image-Topic Merging Network (simNet) that makes use of the two kinds of attention at the same time. At each time step when generating the caption, the decoder adaptively merges the attentive information in the extracted topics and the image according to the generated context, so that the visual information and the semantic information can be effectively combined. The proposed approach is evaluated on two benchmark datasets and reaches the state-of-the-art performances.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1013.pdf",
        "title": "simNet: Stepwise Image-Topic Merging Network for Generating Detailed and Comprehensive Image Captions"
    },
    {
        "paper_id": "D18-1015",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "We introduce an effective and efficient method that grounds (i.e., localizes) natural sentences in long, untrimmed video sequences. Specifically, a novel Temporal GroundNet (TGN) is proposed to temporally capture the evolving fine-grained frame-by-word interactions between video and sentence. TGN sequentially scores a set of temporal candidates ended at each frame based on the exploited frame-by-word interactions, and finally grounds the segment corresponding to the sentence. Unlike traditional methods treating the overlapping segments separately in a sliding window fashion, TGN aggregates the historical information and generates the final grounding result in one single pass. We extensively evaluate our proposed TGN on three public datasets with significant improvements over the state-of-the-arts. We further show the consistent effectiveness and efficiency of TGN through an ablation study and a runtime test.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1015.pdf",
        "title": "Temporally Grounding Natural Sentence in Video"
    },
    {
        "paper_id": "D18-1017",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Named entity recognition (NER) is an important task in natural language processing area, which needs to determine entities boundaries and classify them into pre-defined categories. For Chinese NER task, there is only a very small amount of annotated data available. Chinese NER task and Chinese word segmentation (CWS) task have many similar word boundaries. There are also specificities in each task. However, existing methods for Chinese NER either do not exploit word boundary information from CWS or cannot filter the specific information of CWS. In this paper, we propose a novel adversarial transfer learning framework to make full use of task-shared boundaries information and prevent the task-specific features of CWS. Besides, since arbitrary character can provide important cues when predicting entity type, we exploit self-attention to explicitly capture long range dependencies between two tokens. Experimental results on two different widely used datasets show that our proposed model significantly and consistently outperforms other state-of-the-art methods.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1017.pdf",
        "title": "Adversarial Transfer Learning for Chinese Named Entity Recognition with Self-Attention Mechanism"
    },
    {
        "paper_id": "D18-1020",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "We introduce a family of multitask variational methods for semi-supervised sequence labeling. Our model family consists of a latent-variable generative model and a discriminative labeler. The generative models use latent variables to define the conditional probability of a word given its context, drawing inspiration from word prediction objectives commonly used in learning word embeddings. The labeler helps inject discriminative information into the latent space. We explore several latent variable configurations, including ones with hierarchical structure, which enables the model to account for both label-specific and word-specific information. Our models consistently outperform standard sequential baselines on 8 sequence labeling datasets, and improve further with unlabeled data.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1020.pdf",
        "title": "Variational Sequential Labelers for Semi-Supervised Learning"
    },
    {
        "paper_id": "D18-1023",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "We construct a multilingual common semantic space based on distributional semantics, where words from multiple languages are projected into a shared space via which all available resources and knowledge can be shared across multiple languages. Beyond word alignment, we introduce multiple cluster-level alignments and enforce the word clusters to be consistently distributed across multiple languages. We exploit three signals for clustering: (1) neighbor words in the monolingual word embedding space; (2) character-level information; and (3) linguistic properties (e.g., apposition, locative suffix) derived from linguistic structure knowledge bases available for thousands of languages. We introduce a new cluster-consistent correlational neural network to construct the common semantic space by aligning words as well as clusters. Intrinsic evaluation on monolingual and multilingual QVEC tasks shows our approach achieves significantly higher correlation with linguistic features which are extracted from manually crafted lexical resources than state-of-the-art multi-lingual embedding learning methods do. Using low-resource language name tagging as a case study for extrinsic evaluation, our approach achieves up to 14.6% absolute F-score gain over the state of the art on cross-lingual direct transfer. Our approach is also shown to be robust even when the size of bilingual dictionary is small.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1023.pdf",
        "title": "Multi-lingual Common Semantic Space Construction via Cluster-consistent Word Embedding"
    },
    {
        "paper_id": "D18-1027",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Cross-lingual word embeddings are becoming increasingly important in multilingual NLP. Recently, it has been shown that these embeddings can be effectively learned by aligning two disjoint monolingual vector spaces through linear transformations, using no more than a small bilingual dictionary as supervision. In this work, we propose to apply an additional transformation after the initial alignment step, which moves cross-lingual synonyms towards a middle point between them. By applying this transformation our aim is to obtain a better cross-lingual integration of the vector spaces. In addition, and perhaps surprisingly, the monolingual spaces also improve by this transformation. This is in contrast to the original alignment, which is typically learned such that the structure of the monolingual spaces is preserved. Our experiments confirm that the resulting cross-lingual embeddings outperform state-of-the-art models in both monolingual and cross-lingual evaluation tasks.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1027.pdf",
        "title": "Improving Cross-Lingual Word Embeddings by Meeting in the Middle"
    },
    {
        "paper_id": "D18-1033",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Sememes are defined as the minimum semantic units of human languages. As important knowledge sources, sememe-based linguistic knowledge bases have been widely used in many NLP tasks. However, most languages still do not have sememe-based linguistic knowledge bases. Thus we present a task of cross-lingual lexical sememe prediction, aiming to automatically predict sememes for words in other languages. We propose a novel framework to model correlations between sememes and multi-lingual words in low-dimensional semantic space for sememe prediction. Experimental results on real-world datasets show that our proposed model achieves consistent and significant improvements as compared to baseline methods in cross-lingual sememe prediction. The codes and data of this paper are available at https://github.com/thunlp/CL-SP.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1033.pdf",
        "title": "Cross-lingual Lexical Sememe Prediction"
    },
    {
        "paper_id": "D18-1036",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "One of the weaknesses of Neural Machine Translation (NMT) is in handling lowfrequency and ambiguous words, which we refer as troublesome words. To address this problem, we propose a novel memoryenhanced NMT method. First, we investigate different strategies to define and detect the troublesome words. Then, a contextual memory is constructed to memorize which target words should be produced in what situations. Finally, we design a hybrid model to dynamically access the contextual memory so as to correctly translate the troublesome words. The extensive experiments on Chinese-to-English and English-to-German translation tasks demonstrate that our method significantly outperforms the strong baseline models in translation quality, especially in handling troublesome words.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1036.pdf",
        "title": "Addressing Troublesome Words in Neural Machine Translation"
    },
    {
        "paper_id": "D18-1037",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "The addition of syntax-aware decoding in Neural Machine Translation (NMT) systems requires an effective tree-structured neural network, a syntax-aware attention model and a language generation model that is sensitive to sentence structure. Recent approaches resort to sequential decoding by adding additional neural network units to capture bottom-up structural information, or serialising structured data into sequence. We exploit a top-down tree-structured model called DRNN (Doubly-Recurrent Neural Networks) first proposed by Alvarez-Melis and Jaakola (2017) to create an NMT model called Seq2DRNN that combines a sequential encoder with tree-structured decoding augmented with a syntax-aware attention model. Unlike previous approaches to syntax-based NMT which use dependency parsing models our method uses constituency parsing which we argue provides useful information for translation. In addition, we use the syntactic structure of the sentence to add new connections to the tree-structured decoder neural network (Seq2DRNN+SynC). We compare our NMT model with sequential and state of the art syntax-based NMT models and show that our model produces more fluent translations with better reordering. Since our model is capable of doing translation and constituency parsing at the same time we also compare our parsing accuracy against other neural parsing models.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1037.pdf",
        "title": "Top-down Tree Structured Decoding with Syntactic Connections for Neural Machine Translation and Parsing"
    },
    {
        "paper_id": "D18-1045",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "An effective method to improve neural machine translation with monolingual data is to augment the parallel training corpus with back-translations of target language sentences. This work broadens the understanding of back-translation and investigates a number of methods to generate synthetic source sentences. We find that in all but resource poor settings back-translations obtained via sampling or noised beam outputs are most effective. Our analysis shows that sampling or noisy synthetic data gives a much stronger training signal than data generated by beam or greedy search. We also compare how synthetic data compares to genuine bitext and study various domain effects. Finally, we scale to hundreds of millions of monolingual sentences and achieve a new state of the art of 35 BLEU on the WMT\u201914 English-German test set.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1045.pdf",
        "title": "Understanding Back-Translation at Scale"
    },
    {
        "paper_id": "D18-1046",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Generating the English transliteration of a name written in a foreign script is an important and challenging step in multilingual knowledge acquisition and information extraction. Existing approaches to transliteration generation require a large (>5000) number of training examples. This difficulty contrasts with transliteration discovery, a somewhat easier task that involves picking a plausible transliteration from a given list. In this work, we present a bootstrapping algorithm that uses constrained discovery to improve generation, and can be used with as few as 500 training examples, which we show can be sourced from annotators in a matter of hours. This opens the task to languages for which large number of training examples are unavailable. We evaluate transliteration generation performance itself, as well the improvement it brings to cross-lingual candidate generation for entity linking, a typical downstream task. We present a comprehensive evaluation of our approach on nine languages, each written in a unique script.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1046.pdf",
        "title": "Bootstrapping Transliteration with Constrained Discovery for Low-Resource Languages"
    },
    {
        "paper_id": "D18-1047",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Inducing multilingual word embeddings by learning a linear map between embedding spaces of different languages achieves remarkable accuracy on related languages. However, accuracy drops substantially when translating between distant languages. Given that languages exhibit differences in vocabulary, grammar, written form, or syntax, one would expect that embedding spaces of different languages have different structures especially for distant languages. With the goal of capturing such differences, we propose a method for learning neighborhood sensitive maps, NORMA. Our experiments show that NORMA outperforms current state-of-the-art methods for word translation between distant languages.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1047.pdf",
        "title": "NORMA: Neighborhood Sensitive Maps for Multilingual Word Embeddings"
    },
    {
        "paper_id": "D18-1049",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Although the Transformer translation model (Vaswani et al., 2017) has achieved state-of-the-art performance in a variety of translation tasks, how to use document-level context to deal with discourse phenomena problematic for Transformer still remains a challenge. In this work, we extend the Transformer model with a new context encoder to represent document-level context, which is then incorporated into the original encoder and decoder. As large-scale document-level parallel corpora are usually not available, we introduce a two-step training method to take full advantage of abundant sentence-level parallel corpora and limited document-level parallel corpora. Experiments on the NIST Chinese-English datasets and the IWSLT French-English datasets show that our approach improves over Transformer significantly.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1049.pdf",
        "title": "Improving the Transformer Translation Model with Document-Level Context"
    },
    {
        "paper_id": "D18-1052",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "We formalize a new modular variant of current question answering tasks by enforcing complete independence of the document encoder from the question encoder. This formulation addresses a key challenge in machine comprehension by building a standalone representation of the document discourse. It additionally leads to a significant scalability advantage since the encoding of the answer candidate phrases in the document can be pre-computed and indexed offline for efficient retrieval. We experiment with baseline models for the new task, which achieve a reasonable accuracy but significantly underperform unconstrained QA models. We invite the QA research community to engage in Phrase-Indexed Question Answering (PIQA, pika) for closing the gap. The leaderboard is at: nlp.cs.washington.edu/piqa",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1052.pdf",
        "title": "Phrase-Indexed Question Answering: A New Challenge for Scalable Document Comprehension"
    },
    {
        "paper_id": "D18-1057",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Most models for learning word embeddings are trained based on the context information of words, more precisely first order co-occurrence relations. In this paper, a metric is designed to estimate second order co-occurrence relations based on context overlap. The estimated values are further used as the augmented data to enhance the learning of word embeddings by joint training with existing neural word embedding models. Experimental results show that better word vectors can be obtained for word similarity tasks and some downstream NLP tasks by the enhanced approach.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1057.pdf",
        "title": "Quantifying Context Overlap for Training Word Embeddings"
    },
    {
        "paper_id": "D18-1060",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "We present end-to-end neural models for detecting metaphorical word use in context. We show that relatively standard BiLSTM models which operate on complete sentences work well in this setting, in comparison to previous work that used more restricted forms of linguistic context. These models establish a new state-of-the-art on existing verb metaphor detection benchmarks, and show strong performance on jointly predicting the metaphoricity of all words in a running text.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1060.pdf",
        "title": "Neural Metaphor Detection in Context"
    },
    {
        "paper_id": "D18-1062",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Bilingual lexicon extraction has been studied for decades and most previous methods have relied on parallel corpora or bilingual dictionaries. Recent studies have shown that it is possible to build a bilingual dictionary by aligning monolingual word embedding spaces in an unsupervised way. With the recent advances in generative models, we propose a novel approach which builds cross-lingual dictionaries via latent variable models and adversarial training with no parallel corpora. To demonstrate the effectiveness of our approach, we evaluate our approach on several language pairs and the experimental results show that our model could achieve competitive and even superior performance compared with several state-of-the-art models.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1062.pdf",
        "title": "Unsupervised Bilingual Lexicon Induction via Latent Variable Models"
    },
    {
        "paper_id": "D18-1067",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Identifying optimistic and pessimistic viewpoints and users from Twitter is useful for providing better social support to those who need such support, and for minimizing the negative influence among users and maximizing the spread of positive attitudes and ideas. In this paper, we explore a range of deep learning models to predict optimism and pessimism in Twitter at both tweet and user level and show that these models substantially outperform traditional machine learning classifiers used in prior work. In addition, we show evidence that a sentiment classifier would not be sufficient for accurately predicting optimism and pessimism in Twitter. Last, we study the verb tense usage as well as the presence of polarity words in optimistic and pessimistic tweets.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1067.pdf",
        "title": "Exploring Optimism and Pessimism in Twitter Using Deep Learning"
    },
    {
        "paper_id": "D18-1071",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Traditional neural language models tend to generate generic replies with poor logic and no emotion. In this paper, a syntactically constrained bidirectional-asynchronous approach for emotional conversation generation (E-SCBA) is proposed to address this issue. In our model, pre-generated emotion keywords and topic keywords are asynchronously introduced into the process of decoding. It is much different from most existing methods which generate replies from the first word to the last. Through experiments, the results indicate that our approach not only improves the diversity of replies, but gains a boost on both logic and emotion compared with baselines.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1071.pdf",
        "title": "A Syntactically Constrained Bidirectional-Asynchronous Approach for Emotional Conversation Generation"
    },
    {
        "paper_id": "D18-1074",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Categorizing patient\u2019s intentions in conversational assessment can help decision making in clinical treatments. Many conversation corpora span broaden a series of time stages. However, it is not clear that how the themes shift in the conversation impact on the performance of human intention categorization (eg., patients might show different behaviors during the beginning versus the end). This paper proposes a method that models the temporal factor by using domain adaptation on clinical dialogue corpora, Motivational Interviewing (MI). We deploy Bi-LSTM and topic model jointly to learn language usage change across different time sessions. We conduct experiments on the MI corpora to show the promising improvement after considering temporality in the classification task.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1074.pdf",
        "title": "Modeling Temporality of Human Intentions by Domain Adaptation"
    },
    {
        "paper_id": "D18-1075",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Generating semantically coherent responses is still a major challenge in dialogue generation. Different from conventional text generation tasks, the mapping between inputs and responses in conversations is more complicated, which highly demands the understanding of utterance-level semantic dependency, a relation between the whole meanings of inputs and outputs. To address this problem, we propose an Auto-Encoder Matching (AEM) model to learn such dependency. The model contains two auto-encoders and one mapping module. The auto-encoders learn the semantic representations of inputs and responses, and the mapping module learns to connect the utterance-level representations. Experimental results from automatic and human evaluations demonstrate that our model is capable of generating responses of high coherence and fluency compared to baseline models.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1075.pdf",
        "title": "An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency in Dialogue Generation"
    },
    {
        "paper_id": "D18-1078",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "This paper presents a task for machine listening comprehension in the argumentation domain and a corresponding dataset in English. We recorded 200 spontaneous speeches arguing for or against 50 controversial topics. For each speech, we formulated a question, aimed at confirming or rejecting the occurrence of potential arguments in the speech. Labels were collected by listening to the speech and marking which arguments were mentioned by the speaker. We applied baseline methods addressing the task, to be used as a benchmark for future work over this dataset. All data used in this work is freely available for research.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1078.pdf",
        "title": "Listening Comprehension over Argumentative Content"
    },
    {
        "paper_id": "D18-1084",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Image paragraph captioning models aim to produce detailed descriptions of a source image. These models use similar techniques as standard image captioning models, but they have encountered issues in text generation, notably a lack of diversity between sentences, that have limited their effectiveness. In this work, we consider applying sequence-level training for this task. We find that standard self-critical training produces poor results, but when combined with an integrated penalty on trigram repetition produces much more diverse paragraphs. This simple training approach improves on the best result on the Visual Genome paragraph captioning dataset from 16.9 to 30.6 CIDEr, with gains on METEOR and BLEU as well, without requiring any architectural changes.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1084.pdf",
        "title": "Training for Diversity in Image Paragraph Captioning"
    },
    {
        "paper_id": "D18-1085",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "ROUGE is one of the first and most widely used evaluation metrics for text summarization. However, its assessment merely relies on surface similarities between peer and model summaries. Consequently, ROUGE is unable to fairly evaluate summaries including lexical variations and paraphrasing. We propose a graph-based approach adopted into ROUGE to evaluate summaries based on both lexical and semantic similarities. Experiment results over TAC AESOP datasets show that exploiting the lexico-semantic similarity of the words used in summaries would significantly help ROUGE correlate better with human judgments.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1085.pdf",
        "title": "A Graph-theoretic Summary Evaluation for ROUGE"
    },
    {
        "paper_id": "D18-1086",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Recent work on abstractive summarization has made progress with neural encoder-decoder architectures. However, such models are often challenged due to their lack of explicit semantic modeling of the source document and its summary. In this paper, we extend previous work on abstractive summarization using Abstract Meaning Representation (AMR) with a neural language generation stage which we guide using the source document. We demonstrate that this guidance improves summarization results by 7.4 and 10.5 points in ROUGE-2 using gold standard AMR parses and parses obtained from an off-the-shelf parser respectively. We also find that the summarization performance on later parses is 2 ROUGE-2 points higher than that of a well-established neural encoder-decoder approach trained on a larger dataset.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1086.pdf",
        "title": "Guided Neural Language Generation for Abstractive Summarization using Abstract Meaning Representation"
    },
    {
        "paper_id": "D18-1110",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Existing neural semantic parsers mainly utilize a sequence encoder, i.e., a sequential LSTM, to extract word order features while neglecting other valuable syntactic information such as dependency or constituent trees. In this paper, we first propose to use the syntactic graph to represent three types of syntactic information, i.e., word order, dependency and constituency features; then employ a graph-to-sequence model to encode the syntactic graph and decode a logical form. Experimental results on benchmark datasets show that our model is comparable to the state-of-the-art on Jobs640, ATIS, and Geo880. Experimental results on adversarial examples demonstrate the robustness of the model is also improved by encoding more syntactic information.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1110.pdf",
        "title": "Exploiting Rich Syntactic Information for Semantic Parsing with Graph-to-Sequence Model"
    },
    {
        "paper_id": "D18-1111",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "In models to generate program source code from natural language, representing this code in a tree structure has been a common approach. However, existing methods often fail to generate complex code correctly due to a lack of ability to memorize large and complex structures. We introduce RECODE, a method based on subtree retrieval that makes it possible to explicitly reference existing code examples within a neural code generation model. First, we retrieve sentences that are similar to input sentences using a dynamic-programming-based sentence similarity scoring method. Next, we extract n-grams of action sequences that build the associated abstract syntax tree. Finally, we increase the probability of actions that cause the retrieved n-gram action subtree to be in the predicted code. We show that our approach improves the performance on two code generation tasks by up to +2.6 BLEU.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1111.pdf",
        "title": "Retrieval-Based Neural Code Generation"
    },
    {
        "paper_id": "D18-1112",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Previous work approaches the SQL-to-text generation task using vanilla Seq2Seq models, which may not fully capture the inherent graph-structured information in SQL query. In this paper, we propose a graph-to-sequence model to encode the global structure information into node embeddings. This model can effectively learn the correlation between the SQL query pattern and its interpretation. Experimental results on the WikiSQL dataset and Stackoverflow dataset show that our model outperforms the Seq2Seq and Tree2Seq baselines, achieving the state-of-the-art performance.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1112.pdf",
        "title": "SQL-to-Text Generation with Graph-to-Sequence Model"
    },
    {
        "paper_id": "D18-1114",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "We present a model for semantic proto-role labeling (SPRL) using an adapted bidirectional LSTM encoding strategy that we call NeuralDavidsonian: predicate-argument structure is represented as pairs of hidden states corresponding to predicate and argument head tokens of the input sequence. We demonstrate: (1) state-of-the-art results in SPRL, and (2) that our network naturally shares parameters between attributes, allowing for learning new attribute types with limited added supervision.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1114.pdf",
        "title": "Neural-Davidsonian Semantic Proto-role Labeling"
    },
    {
        "paper_id": "D18-1124",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "It is common that entity mentions can contain other mentions recursively. This paper introduces a scalable transition-based method to model the nested structure of mentions. We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest. Our shift-reduce based system then learns to construct the forest structure in a bottom-up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length. Based on Stack-LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space, our system is further incorporated with a character-based component to capture letter-level patterns. Our model gets the state-of-the-art performances in ACE datasets, showing its effectiveness in detecting nested mentions.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1124.pdf",
        "title": "A Neural Transition-based Model for Nested Mention Recognition"
    },
    {
        "paper_id": "D18-1126",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "To disambiguate between closely related concepts, entity linking systems need to effectively distill cues from their context, which may be quite noisy. We investigate several techniques for using these cues in the context of noisy entity linking on short texts. Our starting point is a state-of-the-art attention-based model from prior work; while this model\u2019s attention typically identifies context that is topically relevant, it fails to identify some of the most indicative surface strings, especially those exhibiting lexical overlap with the true title. Augmenting the model with convolutional networks over characters still leaves it largely unable to pick up on these cues compared to sparse features that target them directly, indicating that automatically learning how to identify relevant character-level context features is a hard problem. Our final system outperforms past work on the WikilinksNED test set by 2.8% absolute.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1126.pdf",
        "title": "Effective Use of Context in Noisy Entity Linking"
    },
    {
        "paper_id": "D18-1130",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Recent work has improved on modeling for reading comprehension tasks with simple approaches such as the Attention Sum-Reader; however, automatic systems still significantly trail human performance. Analysis suggests that many of the remaining hard instances are related to the inability to track entity-references throughout documents. This work focuses on these hard entity tracking cases with two extensions: (1) additional entity features, and (2) training with a multi-task tracking objective. We show that these simple modifications improve performance both independently and in combination, and we outperform the previous state of the art on the LAMBADA dataset by 8 pts, particularly on difficult entity examples. We also effectively match the performance of more complicated models on the named entity portion of the CBT dataset.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1130.pdf",
        "title": "Entity Tracking Improves Cloze-style Reading Comprehension"
    },
    {
        "paper_id": "D18-1138",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "The task of sentiment modification requires reversing the sentiment of the input and preserving the sentiment-independent content. However, aligned sentences with the same content but different sentiments are usually unavailable. Due to the lack of such parallel data, it is hard to extract sentiment independent content and reverse the sentiment in an unsupervised way. Previous work usually can not reconcile sentiment transformation and content preservation. In this paper, motivated by the fact the non-emotional context (e.g., \u201cstaff\u201d) provides strong cues for the occurrence of emotional words (e.g., \u201cfriendly\u201d), we propose a novel method that automatically extracts appropriate sentiment information from learned sentiment memories according to the specific context. Experiments show that our method substantially improves the content preservation degree and achieves the state-of-the-art performance.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1138.pdf",
        "title": "Learning Sentiment Memories for Sentiment Modification without Parallel Data"
    },
    {
        "paper_id": "D18-1139",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "In this work, we propose a new model for aspect-based sentiment analysis. In contrast to previous approaches, we jointly model the detection of aspects and the classification of their polarity in an end-to-end trainable neural network. We conduct experiments with different neural architectures and word representations on the recent GermEval 2017 dataset. We were able to show considerable performance gains by using the joint modeling approach in all settings compared to pipeline approaches. The combination of a convolutional neural network and fasttext embeddings outperformed the best submission of the shared task in 2017, establishing a new state of the art.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1139.pdf",
        "title": "Joint Aspect and Polarity Classification for Aspect-based Sentiment Analysis with End-to-End Neural Networks"
    },
    {
        "paper_id": "D18-1146",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "To what extent could the sommelier profession, or wine stewardship, be displaced by machine leaning algorithms? There are at least three essential skills that make a qualified sommelier: wine theory, blind tasting, and beverage service, as exemplified in the rigorous certification processes of certified sommeliers and above (advanced and master) with the most authoritative body in the industry, the Court of Master Sommelier (hereafter CMS). We propose and train corresponding machine learning models that match these skills, and compare algorithmic results with real data collected from a large group of wine professionals. We find that our machine learning models outperform human sommeliers on most tasks \u2014 most notably in the section of blind tasting, where hierarchically supervised Latent Dirichlet Allocation outperforms sommeliers\u2019 judgment calls by over 6% in terms of F1-score; and in the section of beverage service, especially wine and food pairing, a modified Siamese neural network based on BiLSTM achieves better results than sommeliers by 2%. This demonstrates, contrary to popular opinion in the industry, that the sommelier profession is at least to some extent automatable, barring economic (Kleinberg et al., 2017) and psychological (Dietvorst et al., 2015) complications.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1146.pdf",
        "title": "Somm: Into the Model"
    },
    {
        "paper_id": "D18-1147",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Detecting fine-grained emotions in online health communities provides insightful information about patients\u2019 emotional states. However, current computational approaches to emotion detection from health-related posts focus only on identifying messages that contain emotions, with no emphasis on the emotion type, using a set of handcrafted features. In this paper, we take a step further and propose to detect fine-grained emotion types from health-related posts and show how high-level and abstract features derived from deep neural networks combined with lexicon-based features can be employed to detect emotions.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1147.pdf",
        "title": "Fine-Grained Emotion Detection in Health-Related Online Posts"
    },
    {
        "paper_id": "D18-1148",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Nowcasting based on social media text promises to provide unobtrusive and near real-time predictions of community-level outcomes. These outcomes are typically regarding people, but the data is often aggregated without regard to users in the Twitter populations of each community. This paper describes a simple yet effective method for building community-level models using Twitter language aggregated by user. Results on four different U.S. county-level tasks, spanning demographic, health, and psychological outcomes show large and consistent improvements in prediction accuracies (e.g. from Pearson r=.73 to .82 for median income prediction or r=.37 to .47 for life satisfaction prediction) over the standard approach of aggregating all tweets. We make our aggregated and anonymized community-level data, derived from 37 billion tweets \u2013 over 1 billion of which were mapped to counties, available for research.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1148.pdf",
        "title": "The Remarkable Benefit of User-Level Aggregation for Lexical-based Population-Level Predictions"
    },
    {
        "paper_id": "D18-1152",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Despite the tremendous empirical success of neural models in natural language processing, many of them lack the strong intuitions that accompany classical machine learning approaches. Recently, connections have been shown between convolutional neural networks (CNNs) and weighted finite state automata (WFSAs), leading to new interpretations and insights. In this work, we show that some recurrent neural networks also share this connection to WFSAs. We characterize this connection formally, defining rational recurrences to be recurrent hidden state update functions that can be written as the Forward calculation of a finite set of WFSAs. We show that several recent neural models use rational recurrences. Our analysis provides a fresh view of these models and facilitates devising new neural architectures that draw inspiration from WFSAs. We present one such model, which performs better than two recent baselines on language modeling and text classification. Our results demonstrate that transferring intuitions from classical models like WFSAs can be an effective approach to designing and understanding neural models.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1152.pdf",
        "title": "Rational Recurrences"
    },
    {
        "paper_id": "D18-1156",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Event extraction is of practical utility in natural language processing. In the real world, it is a common phenomenon that multiple events existing in the same sentence, where extracting them are more difficult than extracting a single event. Previous works on modeling the associations between events by sequential modeling methods suffer a lot from the low efficiency in capturing very long-range dependencies. In this paper, we propose a novel Jointly Multiple Events Extraction (JMEE) framework to jointly extract multiple event triggers and arguments by introducing syntactic shortcut arcs to enhance information flow and attention-based graph convolution networks to model graph information. The experiment results demonstrate that our proposed framework achieves competitive results compared with state-of-the-art methods.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1156.pdf",
        "title": "Jointly Multiple Events Extraction via Attention-based Graph Information Aggregation"
    },
    {
        "paper_id": "D18-1158",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Traditional approaches to the task of ACE event detection primarily regard multiple events in one sentence as independent ones and recognize them separately by using sentence-level information. However, events in one sentence are usually interdependent and sentence-level information is often insufficient to resolve ambiguities for some types of events. This paper proposes a novel framework dubbed as Hierarchical and Bias Tagging Networks with Gated Multi-level Attention Mechanisms (HBTNGMA) to solve the two problems simultaneously. Firstly, we propose a hierachical and bias tagging networks to detect multiple events in one sentence collectively. Then, we devise a gated multi-level attention to automatically extract and dynamically fuse the sentence-level and document-level information. The experimental results on the widely used ACE 2005 dataset show that our approach significantly outperforms other state-of-the-art methods.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1158.pdf",
        "title": "Collective Event Detection via a Hierarchical and Bias Tagging Networks with Gated Multi-level Attention Mechanisms"
    },
    {
        "paper_id": "D18-1159",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "We present a complete, automated, and efficient approach for utilizing valency analysis in making dependency parsing decisions. It includes extraction of valency patterns, a probabilistic model for tagging these patterns, and a joint decoding process that explicitly considers the number and types of each token\u2019s syntactic dependents. On 53 treebanks representing 41 languages in the Universal Dependencies data, we find that incorporating valency information yields higher precision and F1 scores on the core arguments (subjects and complements) and functional relations (e.g., auxiliaries) that we employ for valency analysis. Precision on core arguments improves from 80.87 to 85.43. We further show that our approach can be applied to an ostensibly different formalism and dataset, Tree Adjoining Grammar as extracted from the Penn Treebank; there, we outperform the previous state-of-the-art labeled attachment score by 0.7. Finally, we explore the potential of extending valency patterns beyond their traditional domain by confirming their helpfulness in improving PP attachment decisions.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1159.pdf",
        "title": "Valency-Augmented Dependency Parsing"
    },
    {
        "paper_id": "D18-1167",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Recent years have witnessed an increasing interest in image-based question-answering (QA) tasks. However, due to data limitations, there has been much less work on video-based QA. In this paper, we present TVQA, a large-scale video QA dataset based on 6 popular TV shows. TVQA consists of 152,545 QA pairs from 21,793 clips, spanning over 460 hours of video. Questions are designed to be compositional in nature, requiring systems to jointly localize relevant moments within a clip, comprehend subtitle-based dialogue, and recognize relevant visual concepts. We provide analyses of this new dataset as well as several baselines and a multi-stream end-to-end trainable neural network framework for the TVQA task. The dataset is publicly available at http://tvqa.cs.unc.edu.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1167.pdf",
        "title": "TVQA: Localized, Compositional Video Question Answering"
    },
    {
        "paper_id": "D18-1168",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Localizing moments in a longer video via natural language queries is a new, challenging task at the intersection of language and video understanding. Though moment localization with natural language is similar to other language and vision tasks like natural language object retrieval in images, moment localization offers an interesting opportunity to model temporal dependencies and reasoning in text. We propose a new model that explicitly reasons about different temporal segments in a video, and shows that temporal context is important for localizing phrases which include temporal language. To benchmark whether our model, and other recent video localization models, can effectively reason about temporal language, we collect the novel TEMPOral reasoning in video and language (TEMPO) dataset. Our dataset consists of two parts: a dataset with real videos and template sentences (TEMPO - Template Language) which allows for controlled studies on temporal language, and a human language dataset which consists of temporal sentences annotated by humans (TEMPO - Human Language).",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1168.pdf",
        "title": "Localizing Moments in Video with Temporal Language"
    },
    {
        "paper_id": "D18-1173",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Distributional semantic models (DSMs) generally require sufficient examples for a word to learn a high quality representation. This is in stark contrast with human who can guess the meaning of a word from one or a few referents only. In this paper, we propose Mem2Vec, a memory based embedding learning method capable of acquiring high quality word representations from fairly limited context. Our method directly adapts the representations produced by a DSM with a longterm memory to guide its guess of a novel word. Based on a pre-trained embedding space, the proposed method delivers impressive performance on two challenging few-shot word similarity tasks. Embeddings learned with our method also lead to considerable improvements over strong baselines on NER and sentiment classification.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1173.pdf",
        "title": "Memory, Show the Way: Memory Based Few Shot Word Representation Learning"
    },
    {
        "paper_id": "D18-1176",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "While one of the first steps in many NLP systems is selecting what pre-trained word embeddings to use, we argue that such a step is better left for neural networks to figure out by themselves. To that end, we introduce dynamic meta-embeddings, a simple yet effective method for the supervised learning of embedding ensembles, which leads to state-of-the-art performance within the same model class on a variety of tasks. We subsequently show how the technique can be used to shed new light on the usage of word embeddings in NLP systems.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1176.pdf",
        "title": "Dynamic Meta-Embeddings for Improved Sentence Representations"
    },
    {
        "paper_id": "D18-1177",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Several recent studies have shown the benefits of combining language and perception to infer word embeddings. These multimodal approaches either simply combine pre-trained textual and visual representations (e.g. features extracted from convolutional neural networks), or use the latter to bias the learning of textual word embeddings. In this work, we propose a novel probabilistic model to formalize how linguistic and perceptual inputs can work in concert to explain the observed word-context pairs in a text corpus. Our approach learns textual and visual representations jointly: latent visual factors couple together a skip-gram model for co-occurrence in linguistic data and a generative latent variable model for visual data. Extensive experimental studies validate the proposed model. Concretely, on the tasks of assessing pairwise word similarity and image/caption retrieval, our approach attains equally competitive or stronger results when compared to other state-of-the-art multimodal models.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1177.pdf",
        "title": "A Probabilistic Model for Joint Learning of Word Embeddings from Texts and Images"
    },
    {
        "paper_id": "D18-1182",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "We propose Odd-Man-Out, a novel task which aims to test different properties of word representations. An Odd-Man-Out puzzle is composed of 5 (or more) words, and requires the system to choose the one which does not belong with the others. We show that this simple setup is capable of teasing out various properties of different popular lexical resources (like WordNet and pre-trained word embeddings), while being intuitive enough to annotate on a large scale. In addition, we propose a novel technique for training multi-prototype word representations, based on unsupervised clustering of ELMo embeddings, and show that it surpasses all other representations on all Odd-Man-Out collections.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1182.pdf",
        "title": "Spot the Odd Man Out: Exploring the Associative Power of Lexical Resources"
    },
    {
        "paper_id": "D18-1183",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Simile is a special type of metaphor, where comparators such as like and as are used to compare two objects. Simile recognition is to recognize simile sentences and extract simile components, i.e., the tenor and the vehicle. This paper presents a study of simile recognition in Chinese. We construct an annotated corpus for this research, which consists of 11.3k sentences that contain a comparator. We propose a neural network framework for jointly optimizing three tasks: simile sentence classification, simile component extraction and language modeling. The experimental results show that the neural network based approaches can outperform all rule-based and feature-based baselines. Both simile sentence classification and simile component extraction can benefit from multitask learning. The former can be solved very well, while the latter is more difficult.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1183.pdf",
        "title": "Neural Multitask Learning for Simile Recognition"
    },
    {
        "paper_id": "D18-1184",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Many tasks in natural language processing involve comparing two sentences to compute some notion of relevance, entailment, or similarity. Typically this comparison is done either at the word level or at the sentence level, with no attempt to leverage the inherent structure of the sentence. When sentence structure is used for comparison, it is obtained during a non-differentiable pre-processing step, leading to propagation of errors. We introduce a model of structured alignments between sentences, showing how to compare two sentences by matching their latent structures. Using a structured attention mechanism, our model matches candidate spans in the first sentence to candidate spans in the second sentence, simultaneously discovering the tree structure of each sentence. Our model is fully differentiable and trained only on the matching objective. We evaluate this model on two tasks, natural entailment detection and answer sentence selection, and find that modeling latent tree structures results in superior performance. Analysis of the learned sentence structures shows they can reflect some syntactic phenomena.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1184.pdf",
        "title": "Structured Alignment Networks for Matching Sentences"
    },
    {
        "paper_id": "D18-1185",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "This paper presents a new deep learning architecture for Natural Language Inference (NLI). Firstly, we introduce a new architecture where alignment pairs are compared, compressed and then propagated to upper layers for enhanced representation learning. Secondly, we adopt factorization layers for efficient and expressive compression of alignment vectors into scalar features, which are then used to augment the base word representations. The design of our approach is aimed to be conceptually simple, compact and yet powerful. We conduct experiments on three popular benchmarks, SNLI, MultiNLI and SciTail, achieving competitive performance on all. A lightweight parameterization of our model also enjoys a 3 times reduction in parameter size compared to the existing state-of-the-art models, e.g., ESIM and DIIN, while maintaining competitive performance. Additionally, visual analysis shows that our propagated features are highly interpretable.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1185.pdf",
        "title": "Compare, Compress and Propagate: Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference"
    },
    {
        "paper_id": "D18-1186",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Attention-based neural models have achieved great success in natural language inference (NLI). In this paper, we propose the Convolutional Interaction Network (CIN), a general model to capture the interaction between two sentences, which can be an alternative to the attention mechanism for NLI. Specifically, CIN encodes one sentence with the filters dynamically generated based on another sentence. Since the filters may be designed to have various numbers and sizes, CIN can capture more complicated interaction patterns. Experiments on three large datasets demonstrate CIN\u2019s efficacy.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1186.pdf",
        "title": "Convolutional Interaction Network for Natural Language Inference"
    },
    {
        "paper_id": "D18-1194",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "We introduce the task of cross-lingual decompositional semantic parsing: mapping content provided in a source language into a decompositional semantic analysis based on a target language. We present: (1) a form of decompositional semantic analysis designed to allow systems to target varying levels of structural complexity (shallow to deep analysis), (2) an evaluation metric to measure the similarity between system output and reference semantic analysis, (3) an end-to-end model with a novel annotating mechanism that supports intra-sentential coreference, and (4) an evaluation dataset on which our model outperforms strong baselines by at least 1.75 F1 score.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1194.pdf",
        "title": "Cross-lingual Decompositional Semantic Parsing"
    },
    {
        "paper_id": "D18-1199",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Many idiomatic expressions can be interpreted figuratively or literally depending on their contexts. This paper proposes an unsupervised learning method for recognizing the intended usages of idioms. We treat the usages as a latent variable in probabilistic models and train them in a linguistically motivated feature space. Crucially, we show that distributional semantics is a helpful heuristic for distinguishing the literal usage of idioms, giving us a way to formulate a literal usage metric to estimate the likelihood that the idiom is intended literally. This information then serves as a form of distant supervision to guide the unsupervised training process for the probabilistic models. Experiments show that our overall model performs competitively against supervised methods.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1199.pdf",
        "title": "Heuristically Informed Unsupervised Idiom Usage Recognition"
    },
    {
        "paper_id": "D18-1201",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Semantic graphs, such as WordNet, are resources which curate natural language on two distinguishable layers. On the local level, individual relations between synsets (semantic building blocks) such as hypernymy and meronymy enhance our understanding of the words used to express their meanings. Globally, analysis of graph-theoretic properties of the entire net sheds light on the structure of human language as a whole. In this paper, we combine global and local properties of semantic graphs through the framework of Max-Margin Markov Graph Models (M3GM), a novel extension of Exponential Random Graph Model (ERGM) that scales to large multi-relational graphs. We demonstrate how such global modeling improves performance on the local task of predicting semantic relations between synsets, yielding new state-of-the-art results on the WN18RR dataset, a challenging version of WordNet link prediction in which \u201ceasy\u201d reciprocal cases are removed. In addition, the M3GM model identifies multirelational motifs that are characteristic of well-formed lexical semantic ontologies.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1201.pdf",
        "title": "Predicting Semantic Relations using Global Graph Properties"
    },
    {
        "paper_id": "D18-1205",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Information selection is the most important component in document summarization task. In this paper, we propose to extend the basic neural encoding-decoding framework with an information selection layer to explicitly model and optimize the information selection process in abstractive document summarization. Specifically, our information selection layer consists of two parts: gated global information filtering and local sentence selection. Unnecessary information in the original document is first globally filtered, then salient sentences are selected locally while generating each summary sentence sequentially. To optimize the information selection process directly, distantly-supervised training guided by the golden summary is also imported. Experimental results demonstrate that the explicit modeling and optimizing of the information selection process improves document summarization performance significantly, which enables our model to generate more informative and concise summaries, and thus significantly outperform state-of-the-art neural abstractive methods.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1205.pdf",
        "title": "Improving Neural Abstractive Document Summarization with Explicit Information Selection Modeling"
    },
    {
        "paper_id": "D18-1206",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "We introduce \u201cextreme summarization\u201d, a new single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling approach. The idea is to create a short, one-sentence news summary answering the question \u201cWhat is the article about?\u201d. We collect a real-world, large-scale dataset for this task by harvesting online articles from the British Broadcasting Corporation (BBC). We propose a novel abstractive model which is conditioned on the article\u2019s topics and based entirely on convolutional neural networks. We demonstrate experimentally that this architecture captures long-range dependencies in a document and recognizes pertinent content, outperforming an oracle extractive system and state-of-the-art abstractive approaches when evaluated automatically and by humans.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1206.pdf",
        "title": "Don\u2019t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization"
    },
    {
        "paper_id": "D18-1208",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "We carry out experiments with deep learning models of summarization across the domains of news, personal stories, meetings, and medical articles in order to understand how content selection is performed. We find that many sophisticated features of state of the art extractive summarizers do not improve performance over simpler models. These results suggest that it is easier to create a summarizer for a new domain than previous work suggests and bring into question the benefit of deep learning models for summarization for those domains that do have massive datasets (i.e., news). At the same time, they suggest important questions for new research in summarization; namely, new forms of sentence representations or external knowledge sources are needed that are better suited to the sumarization task.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1208.pdf",
        "title": "Content Selection in Deep Learning Models of Summarization"
    },
    {
        "paper_id": "D18-1215",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Deep learning has emerged as a versatile tool for a wide range of NLP tasks, due to its superior capacity in representation learning. But its applicability is limited by the reliance on annotated examples, which are difficult to produce at scale. Indirect supervision has emerged as a promising direction to address this bottleneck, either by introducing labeling functions to automatically generate noisy examples from unlabeled text, or by imposing constraints over interdependent label decisions. A plethora of methods have been proposed, each with respective strengths and limitations. Probabilistic logic offers a unifying language to represent indirect supervision, but end-to-end modeling with probabilistic logic is often infeasible due to intractable inference and learning. In this paper, we propose deep probabilistic logic (DPL) as a general framework for indirect supervision, by composing probabilistic logic with deep learning. DPL models label decisions as latent variables, represents prior knowledge on their relations using weighted first-order logical formulas, and alternates between learning a deep neural network for the end task and refining uncertain formula weights for indirect supervision, using variational EM. This framework subsumes prior indirect supervision methods as special cases, and enables novel combination via infusion of rich domain and linguistic knowledge. Experiments on biomedical machine reading demonstrate the promise of this approach.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1215.pdf",
        "title": "Deep Probabilistic Logic: A Unifying Framework for Indirect Supervision"
    },
    {
        "paper_id": "D18-1218",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "The availability of large scale annotated corpora for coreference is essential to the development of the field. However, creating resources at the required scale via expert annotation would be too expensive. Crowdsourcing has been proposed as an alternative; but this approach has not been widely used for coreference. This paper addresses one crucial hurdle on the way to make this possible, by introducing a new model of annotation for aggregating crowdsourced anaphoric annotations. The model is evaluated along three dimensions: the accuracy of the inferred mention pairs, the quality of the post-hoc constructed silver chains, and the viability of using the silver chains as an alternative to the expert-annotated chains in training a state of the art coreference system. The results suggest that our model can extract from crowdsourced annotations coreference chains of comparable quality to those obtained with expert annotation.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1218.pdf",
        "title": "A Probabilistic Annotation Model for Crowdsourcing Coreference"
    },
    {
        "paper_id": "D18-1219",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Previous work on bridging anaphora resolution (Poesio et al., 2004; Hou et al., 2013) use syntactic preposition patterns to calculate word relatedness. However, such patterns only consider NPs\u2019 head nouns and hence do not fully capture the semantics of NPs. Recently, Hou (2018) created word embeddings (embeddings_PP) to capture associative similarity (i.e., relatedness) between nouns by exploring the syntactic structure of noun phrases. But embeddings_PP only contains word representations for nouns. In this paper, we create new word vectors by combining embeddings_PP with GloVe. This new word embeddings (embeddings_bridging) are a more general lexical knowledge resource for bridging and allow us to represent the meaning of an NP beyond its head easily. We therefore develop a deterministic approach for bridging anaphora resolution, which represents the semantics of an NP based on its head noun and modifications. We show that this simple approach achieves the competitive results compared to the best system in Hou et al. (2013) which explores Markov Logic Networks to model the problem. Additionally, we further improve the results for bridging anaphora resolution reported in Hou (2018) by combining our simple deterministic approach with Hou et al. (2013)\u2019s best system MLN II.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1219.pdf",
        "title": "A Deterministic Algorithm for Bridging Anaphora Resolution"
    },
    {
        "paper_id": "D18-1221",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "This paper addresses the problem of mapping natural language text to knowledge base entities. The mapping process is approached as a composition of a phrase or a sentence into a point in a multi-dimensional entity space obtained from a knowledge graph. The compositional model is an LSTM equipped with a dynamic disambiguation mechanism on the input word embeddings (a Multi-Sense LSTM), addressing polysemy issues. Further, the knowledge base space is prepared by collecting random walks from a graph enhanced with textual features, which act as a set of semantic bridges between text and knowledge base entities. The ideas of this work are demonstrated on large-scale text-to-entity mapping and entity classification tasks, with state of the art results.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1221.pdf",
        "title": "Mapping Text to Knowledge Graph Entities using Multi-Sense LSTMs"
    },
    {
        "paper_id": "D18-1222",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Concepts, which represent a group of different instances sharing common properties, are essential information in knowledge representation. Most conventional knowledge embedding methods encode both entities (concepts and instances) and relations as vectors in a low dimensional semantic space equally, ignoring the difference between concepts and instances. In this paper, we propose a novel knowledge graph embedding model named TransC by differentiating concepts and instances. Specifically, TransC encodes each concept in knowledge graph as a sphere and each instance as a vector in the same semantic space. We use the relative positions to model the relations between concepts and instances (i.e.,instanceOf), and the relations between concepts and sub-concepts (i.e., subClassOf). We evaluate our model on both link prediction and triple classification tasks on the dataset based on YAGO. Experimental results show that TransC outperforms state-of-the-art methods, and captures the semantic transitivity for instanceOf and subClassOf relation. Our codes and datasets can be obtained from https://github.com/davidlvxin/TransC.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1222.pdf",
        "title": "Differentiating Concepts and Instances for Knowledge Graph Embedding"
    },
    {
        "paper_id": "D18-1225",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Knowledge Graph (KG) embedding has emerged as an active area of research resulting in the development of several KG embedding methods. Relational facts in KG often show temporal dynamics, e.g., the fact (Cristiano_Ronaldo, playsFor, Manchester_United) is valid only from 2003 to 2009. Most of the existing KG embedding methods ignore this temporal dimension while learning embeddings of the KG elements. In this paper, we propose HyTE, a temporally aware KG embedding method which explicitly incorporates time in the entity-relation space by associating each timestamp with a corresponding hyperplane. HyTE not only performs KG inference using temporal guidance, but also predicts temporal scopes for relational facts with missing time annotations. Through extensive experimentation on temporal datasets extracted from real-world KGs, we demonstrate the effectiveness of our model over both traditional as well as temporal KG embedding methods.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1225.pdf",
        "title": "HyTE: Hyperplane-based Temporally aware Knowledge Graph Embedding"
    },
    {
        "paper_id": "D18-1227",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "In this paper, we study a new entity linking problem where both the entity mentions and the target entities are within a same social media platform. Compared with traditional entity linking problems that link mentions to a knowledge base, this new problem have less information about the target entities. However, if we can successfully link mentions to entities within a social media platform, we can improve a lot of applications such as comparative study in business intelligence and opinion leader finding. To study this problem, we constructed a dataset called Yelp-EL, where the business mentions in Yelp reviews are linked to their corresponding businesses on the platform. We conducted comprehensive experiments and analysis on this dataset with a learning to rank model that takes different types of features as input, as well as a few state-of-the-art entity linking approaches. Our experimental results show that two types of features that are not available in traditional entity linking: social features and location features, can be very helpful for this task.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1227.pdf",
        "title": "Entity Linking within a Social Media Platform: A Case Study on Yelp"
    },
    {
        "paper_id": "D18-1230",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Recent advances in deep neural models allow us to build reliable named entity recognition (NER) systems without handcrafting features. However, such methods require large amounts of manually-labeled training data. There have been efforts on replacing human annotations with distant supervision (in conjunction with external dictionaries), but the generated noisy labels pose significant challenges on learning effective neural models. Here we propose two neural models to suit noisy distant supervision from the dictionary. First, under the traditional sequence labeling framework, we propose a revised fuzzy CRF layer to handle tokens with multiple possible labels. After identifying the nature of noisy labels in distant supervision, we go beyond the traditional framework and propose a novel, more effective neural model AutoNER with a new Tie or Break scheme. In addition, we discuss how to refine distant supervision for better NER performance. Extensive experiments on three benchmark datasets demonstrate that AutoNER achieves the best performance when only using dictionaries with no additional human effort, and delivers competitive results with state-of-the-art supervised benchmarks.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1230.pdf",
        "title": "Learning Named Entity Tagger using Domain-Specific Dictionary"
    },
    {
        "paper_id": "D18-1231",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "The problem of entity-typing has been studied predominantly as a supervised learning problems, mostly with task-specific annotations (for coarse types) and sometimes with distant supervision (for fine types). While such approaches have strong performance within datasets they often lack the flexibility to transfer across text genres and to generalize to new type taxonomies. In this work we propose a zero-shot entity typing approach that requires no annotated data and can flexibly identify newly defined types. Given a type taxonomy, the entries of which we define as Boolean functions of freebase \u201ctypes,\u201d we ground a given mention to a set of type-compatible Wikipedia entries, and then infer the target mention\u2019s type using an inference algorithm that makes use of the types of these entries. We evaluate our system on a broad range of datasets, including standard fine-grained and coarse-grained entity typing datasets, and on a dataset in the biological domain. Our system is shown to be competitive with state-of-the-art supervised NER systems, and to outperform them on out-of-training datasets. We also show that our system significantly outperforms other zero-shot fine typing systems.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1231.pdf",
        "title": "Zero-Shot Open Entity Typing as Type-Compatible Grounding"
    },
    {
        "paper_id": "D18-1233",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Most work in machine reading focuses on question answering problems where the answer is directly expressed in the text to read. However, many real-world question answering problems require the reading of text not because it contains the literal answer, but because it contains a recipe to derive an answer together with the reader\u2019s background knowledge. One example is the task of interpreting regulations to answer \u201cCan I...?\u201d or \u201cDo I have to...?\u201d questions such as \u201cI am working in Canada. Do I have to carry on paying UK National Insurance?\u201d after reading a UK government website about this topic. This task requires both the interpretation of rules and the application of background knowledge. It is further complicated due to the fact that, in practice, most questions are underspecified, and a human assistant will regularly have to ask clarification questions such as \u201cHow long have you been working abroad?\u201d when the answer cannot be directly derived from the question and text. In this paper, we formalise this task and develop a crowd-sourcing strategy to collect 37k task instances based on real-world rules and crowd-generated questions and scenarios. We analyse the challenges of this task and assess its difficulty by evaluating the performance of rule-based and machine-learning baselines. We observe promising results when no background knowledge is necessary, and substantial room for improvement whenever background knowledge is needed.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1233.pdf",
        "title": "Interpretation of Natural Language Rules in Conversational Machine Reading"
    },
    {
        "paper_id": "D18-1235",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "The task of machine reading comprehension (MRC) has evolved from answering simple questions from well-edited text to answering real questions from users out of web data. In the real-world setting, full-body text from multiple relevant documents in the top search results are provided as context for questions from user queries, including not only questions with a single, short, and factual answer, but also questions about reasons, procedures, and opinions. In this case, multiple answers could be equally valid for a single question and each answer may occur multiple times in the context, which should be taken into consideration when we build MRC system. We propose a multi-answer multi-task framework, in which different loss functions are used for multiple reference answers. Minimum Risk Training is applied to solve the multi-occurrence problem of a single answer. Combined with a simple heuristic passage extraction strategy for overlong documents, our model increases the ROUGE-L score on the DuReader dataset from 44.18, the previous state-of-the-art, to 51.09.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1235.pdf",
        "title": "A Multi-answer Multi-task Framework for Real-world Machine Reading Comprehension"
    },
    {
        "paper_id": "D18-1239",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Answering compositional questions requiring multi-step reasoning is challenging. We introduce an end-to-end differentiable model for interpreting questions about a knowledge graph (KG), which is inspired by formal approaches to semantics. Each span of text is represented by a denotation in a KG and a vector that captures ungrounded aspects of meaning. Learned composition modules recursively combine constituent spans, culminating in a grounding for the complete sentence which answers the question. For example, to interpret \u201cnot green\u201d, the model represents \u201cgreen\u201d as a set of KG entities and \u201cnot\u201d as a trainable ungrounded vector\u2014and then uses this vector to parameterize a composition function that performs a complement operation. For each sentence, we build a parse chart subsuming all possible parses, allowing the model to jointly learn both the composition operators and output structure by gradient descent from end-task supervision. The model learns a variety of challenging semantic operators, such as quantifiers, disjunctions and composed relations, and infers latent syntactic structure. It also generalizes well to longer questions than seen in its training data, in contrast to RNN, its tree-based variants, and semantic parsing baselines.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1239.pdf",
        "title": "Neural Compositional Denotational Semantics for Question Answering"
    },
    {
        "paper_id": "D18-1240",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "High-level semantics tasks, e.g., paraphrasing, textual entailment or question answering, involve modeling of text pairs. Before the emergence of neural networks, this has been mostly performed using intra-pair features, which incorporate similarity scores or rewrite rules computed between the members within the same pair. In this paper, we compute scalar products between vectors representing similarity between members of different pairs, in place of simply using a single vector for each pair. This allows us to obtain a representation specific to any pair of pairs, which delivers the state of the art in answer sentence selection. Most importantly, our approach can outperform much more complex algorithms based on neural networks.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1240.pdf",
        "title": "Cross-Pair Text Representations for Answer Sentence Selection"
    },
    {
        "paper_id": "D18-1244",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Dependency trees help relation extraction models capture long-range relations between words. However, existing dependency-based models either neglect crucial information (e.g., negation) by pruning the dependency trees too aggressively, or are computationally inefficient because it is difficult to parallelize over different tree structures. We propose an extension of graph convolutional networks that is tailored for relation extraction, which pools information over arbitrary dependency structures efficiently in parallel. To incorporate relevant information while maximally removing irrelevant content, we further apply a novel pruning strategy to the input trees by keeping words immediately around the shortest path between the two entities among which a relation might hold. The resulting model achieves state-of-the-art performance on the large-scale TACRED dataset, outperforming existing sequence and dependency-based neural models. We also show through detailed analysis that this model has complementary strengths to sequence models, and combining them further improves the state of the art.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1244.pdf",
        "title": "Graph Convolution over Pruned Dependency Trees Improves Relation Extraction"
    },
    {
        "paper_id": "D18-1259",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Existing question answering (QA) datasets fail to train QA systems to perform complex reasoning and provide explanations for answers. We introduce HotpotQA, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems\u2019 ability to extract relevant facts and perform necessary comparison. We show that HotpotQA is challenging for the latest QA systems, and the supporting facts enable models to improve performance and make explainable predictions.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1259.pdf",
        "title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering"
    },
    {
        "paper_id": "D18-1262",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Semantic role labeling (SRL) aims to recognize the predicate-argument structure of a sentence. Syntactic information has been paid a great attention over the role of enhancing SRL. However, the latest advance shows that syntax would not be so important for SRL with the emerging much smaller gap between syntax-aware and syntax-agnostic SRL. To comprehensively explore the role of syntax for SRL task, we extend existing models and propose a unified framework to investigate more effective and more diverse ways of incorporating syntax into sequential neural networks. Exploring the effect of syntactic input quality on SRL performance, we confirm that high-quality syntactic parse could still effectively enhance syntactically-driven SRL. Using empirically optimized integration strategy, we even enlarge the gap between syntax-aware and syntax-agnostic SRL. Our framework achieves state-of-the-art results on CoNLL-2009 benchmarks both for English and Chinese, substantially outperforming all previous models.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1262.pdf",
        "title": "A Unified Syntax-aware Framework for Semantic Role Labeling"
    },
    {
        "paper_id": "D18-1263",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "We propose a novel approach to semantic dependency parsing (SDP) by casting the task as an instance of multi-lingual machine translation, where each semantic representation is a different foreign dialect. To that end, we first generalize syntactic linearization techniques to account for the richer semantic dependency graph structure. Following, we design a neural sequence-to-sequence framework which can effectively recover our graph linearizations, performing almost on-par with previous SDP state-of-the-art while requiring less parallel training annotations. Beyond SDP, our linearization technique opens the door to integration of graph-based semantic representations as features in neural models for downstream applications.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1263.pdf",
        "title": "Semantics as a Foreign Language"
    },
    {
        "paper_id": "D18-1264",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "In this paper, we propose a new rich resource enhanced AMR aligner which produces multiple alignments and a new transition system for AMR parsing along with its oracle parser. Our aligner is further tuned by our oracle parser via picking the alignment that leads to the highest-scored achievable AMR graph. Experimental results show that our aligner outperforms the rule-based aligner in previous work by achieving higher alignment F1 score and consistently improving two open-sourced AMR parsers. Based on our aligner and transition system, we develop a transition-based AMR parser that parses a sentence into its AMR graph directly. An ensemble of our parsers with only words and POS tags as input leads to 68.4 Smatch F1 score, which outperforms the current state-of-the-art parser.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1264.pdf",
        "title": "An AMR Aligner Tuned by Transition-based Parser"
    },
    {
        "paper_id": "D18-1268",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Cross-lingual transfer of word embeddings aims to establish the semantic mappings among words in different languages by learning the transformation functions over the corresponding word embedding spaces. Successfully solving this problem would benefit many downstream tasks such as to translate text classification models from resource-rich languages (e.g. English) to low-resource languages. Supervised methods for this problem rely on the availability of cross-lingual supervision, either using parallel corpora or bilingual lexicons as the labeled data for training, which may not be available for many low resource languages. This paper proposes an unsupervised learning approach that does not require any cross-lingual labeled data. Given two monolingual word embedding spaces for any language pair, our algorithm optimizes the transformation functions in both directions simultaneously based on distributional matching as well as minimizing the back-translation losses. We use a neural network implementation to calculate the Sinkhorn distance, a well-defined distributional similarity measure, and optimize our objective through back-propagation. Our evaluation on benchmark datasets for bilingual lexicon induction and cross-lingual word similarity prediction shows stronger or competitive performance of the proposed method compared to other state-of-the-art supervised and unsupervised baseline methods over many language pairs.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1268.pdf",
        "title": "Unsupervised Cross-lingual Transfer of Word Embedding Spaces"
    },
    {
        "paper_id": "D18-1270",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Cross-lingual Entity Linking (XEL) aims to ground entity mentions written in any language to an English Knowledge Base (KB), such as Wikipedia. XEL for most languages is challenging, owing to limited availability of resources as supervision. We address this challenge by developing the first XEL approach that combines supervision from multiple languages jointly. This enables our approach to: (a) augment the limited supervision in the target language with additional supervision from a high-resource language (like English), and (b) train a single entity linking model for multiple languages, improving upon individually trained models for each language. Extensive evaluation on three benchmark datasets across 8 languages shows that our approach significantly improves over the current state-of-the-art. We also provide analyses in two limited resource settings: (a) zero-shot setting, when no supervision in the target language is available, and in (b) low-resource setting, when some supervision in the target language is available. Our analysis provides insights into the limitations of zero-shot XEL approaches in realistic scenarios, and shows the value of joint supervision in low-resource settings.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1270.pdf",
        "title": "Joint Multilingual Supervision for Cross-lingual Entity Linking"
    },
    {
        "paper_id": "D18-1273",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Chinese spelling check (CSC) is a challenging yet meaningful task, which not only serves as a preprocessing in many natural language processing(NLP) applications, but also facilitates reading and understanding of running texts in peoples\u2019 daily lives. However, to utilize data-driven approaches for CSC, there is one major limitation that annotated corpora are not enough in applying algorithms and building models. In this paper, we propose a novel approach of constructing CSC corpus with automatically generated spelling errors, which are either visually or phonologically resembled characters, corresponding to the OCR- and ASR-based methods, respectively. Upon the constructed corpus, different models are trained and evaluated for CSC with respect to three standard test sets. Experimental results demonstrate the effectiveness of the corpus, therefore confirm the validity of our approach.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1273.pdf",
        "title": "A Hybrid Approach to Automatic Corpus Generation for Chinese Spelling Check"
    },
    {
        "paper_id": "D18-1277",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "English part-of-speech taggers regularly make egregious errors related to noun-verb ambiguity, despite having achieved 97%+ accuracy on the WSJ Penn Treebank since 2002. These mistakes have been difficult to quantify and make taggers less useful to downstream tasks such as translation and text-to-speech synthesis. This paper creates a new dataset of over 30,000 naturally-occurring non-trivial examples of noun-verb ambiguity. Taggers within 1% of each other when measured on the WSJ have accuracies ranging from 57% to 75% accuracy on this challenge set. Enhancing the strongest existing tagger with contextual word embeddings and targeted training data improves its accuracy to 89%, a 14% absolute (52% relative) improvement. Downstream, using just this enhanced tagger yields a 28% reduction in error over the prior best learned model for homograph disambiguation for textto-speech synthesis.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1277.pdf",
        "title": "A Challenge Set and Methods for Noun-Verb Ambiguity"
    },
    {
        "paper_id": "D18-1278",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "When parsing morphologically-rich languages with neural models, it is beneficial to model input at the character level, and it has been claimed that this is because character-level models learn morphology. We test these claims by comparing character-level models to an oracle with access to explicit morphological analysis on twelve languages with varying morphological typologies. Our results highlight many strengths of character-level models, but also show that they are poor at disambiguating some words, particularly in the face of case syncretism. We then demonstrate that explicitly modeling morphological case improves our best model, showing that character-level models can benefit from targeted forms of explicit morphological modeling.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1278.pdf",
        "title": "What do character-level models learn about morphology? The case of dependency parsing"
    },
    {
        "paper_id": "D18-1279",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Character-based neural models have recently proven very useful for many NLP tasks. However, there is a gap of sophistication between methods for learning representations of sentences and words. While, most character models for learning representations of sentences are deep and complex, models for learning representations of words are shallow and simple. Also, in spite of considerable research on learning character embeddings, it is still not clear which kind of architecture is the best for capturing character-to-word representations. To address these questions, we first investigate the gaps between methods for learning word and sentence representations. We conduct detailed experiments and comparisons on different state-of-the-art convolutional models, and also investigate the advantages and disadvantages of their constituents. Furthermore, we propose IntNet, a funnel-shaped wide convolutional neural architecture with no down-sampling for learning representations of the internal structure of words by composing their characters from limited, supervised training corpora. We evaluate our proposed model on six sequence labeling datasets, including named entity recognition, part-of-speech tagging, and syntactic chunking. Our in-depth analysis shows that IntNet significantly outperforms other character embedding models and obtains new state-of-the-art performance without relying on any external knowledge or resources.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1279.pdf",
        "title": "Learning Better Internal Structure of Words for Sequence Labeling"
    },
    {
        "paper_id": "D18-1280",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Emotion recognition in conversations is crucial for building empathetic machines. Present works in this domain do not explicitly consider the inter-personal influences that thrive in the emotional dynamics of dialogues. To this end, we propose Interactive COnversational memory Network (ICON), a multimodal emotion detection framework that extracts multimodal features from conversational videos and hierarchically models the self- and inter-speaker emotional influences into global memories. Such memories generate contextual summaries which aid in predicting the emotional orientation of utterance-videos. Our model outperforms state-of-the-art networks on multiple classification and regression tasks in two benchmark datasets.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1280.pdf",
        "title": "ICON: Interactive Conversational Memory Network for Multimodal Emotion Detection"
    },
    {
        "paper_id": "D18-1282",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "We address the task of visual semantic role labeling (vSRL), the identification of the participants of a situation or event in a visual scene, and their labeling with their semantic relations to the event or situation. We render candidate participants as image regions of objects, and train a model which learns to ground roles in the regions which depict the corresponding participant. Experimental results demonstrate that we can train a vSRL model without reliance on prohibitive image-based role annotations, by utilizing noisy data which we extract automatically from image captions using a linguistic SRL system. Furthermore, our model induces frame\u2014semantic visual representations, and their comparison to previous work on supervised visual verb sense disambiguation yields overall better results.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1282.pdf",
        "title": "Grounding Semantic Roles in Images"
    },
    {
        "paper_id": "D18-1283",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "To enable collaboration and communication between humans and agents, this paper investigates learning to acquire commonsense evidence for action justification. In particular, we have developed an approach based on the generative Conditional Variational Autoencoder(CVAE) that models object relations/attributes of the world as latent variables and jointly learns a performer that predicts actions and an explainer that gathers commonsense evidence to justify the action. Our empirical results have shown that, compared to a typical attention-based model, CVAE achieves significantly higher performance in both action prediction and justification. A human subject study further shows that the commonsense evidence gathered by CVAE can be communicated to humans to achieve a significantly higher common ground between humans and agents.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1283.pdf",
        "title": "Commonsense Justification for Action Explanation"
    },
    {
        "paper_id": "D18-1289",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "In this paper, we present a crowdsourcing-based approach to model the human perception of sentence complexity. We collect a large corpus of sentences rated with judgments of complexity for two typologically-different languages, Italian and English. We test our approach in two experimental scenarios aimed to investigate the contribution of a wide set of lexical, morpho-syntactic and syntactic phenomena in predicting i) the degree of agreement among annotators independently from the assigned judgment and ii) the perception of sentence complexity.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1289.pdf",
        "title": "Is this Sentence Difficult? Do you Agree?"
    },
    {
        "paper_id": "D18-1292",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "There have been several recent attempts to improve the accuracy of grammar induction systems by bounding the recursive complexity of the induction model. Modern depth-bounded grammar inducers have been shown to be more accurate than early unbounded PCFG inducers, but this technique has never been compared against unbounded induction within the same system, in part because most previous depth-bounding models are built around sequence models, the complexity of which grows exponentially with the maximum allowed depth. The present work instead applies depth bounds within a chart-based Bayesian PCFG inducer, where bounding can be switched on and off, and then samples trees with or without bounding. Results show that depth-bounding is indeed significantly effective in limiting the search space of the inducer and thereby increasing accuracy of resulting parsing model, independent of the contribution of modern Bayesian induction techniques. Moreover, parsing results on English, Chinese and German show that this bounded model is able to produce parse trees more accurately than or competitively with state-of-the-art constituency grammar induction models.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1292.pdf",
        "title": "Depth-bounding is effective: Improvements and evaluation of unsupervised PCFG induction"
    },
    {
        "paper_id": "D18-1296",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "We propose to generalize language models for conversational speech recognition to allow them to operate across utterance boundaries and speaker changes, thereby capturing conversation-level phenomena such as adjacency pairs, lexical entrainment, and topical coherence. The model consists of a long-short-term memory (LSTM) recurrent network that reads the entire word-level history of a conversation, as well as information about turn taking and speaker overlap, in order to predict each next word. The model is applied in a rescoring framework, where the word history prior to the current utterance is approximated with preliminary recognition results. In experiments in the conversational telephone speech domain (Switchboard) we find that such a model gives substantial perplexity reductions over a standard LSTM-LM with utterance scope, as well as improvements in word error rate.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1296.pdf",
        "title": "Session-level Language Modeling for Conversational Speech"
    },
    {
        "paper_id": "D18-1303",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "With the recent rise of #MeToo, an increasing number of personal stories about sexual harassment and sexual abuse have been shared online. In order to push forward the fight against such harassment and abuse, we present the task of automatically categorizing and analyzing various forms of sexual harassment, based on stories shared on the online forum SafeCity. For the labels of groping, ogling, and commenting, our single-label CNN-RNN model achieves an accuracy of 86.5%, and our multi-label model achieves a Hamming score of 82.5%. Furthermore, we present analysis using LIME, first-derivative saliency heatmaps, activation clustering, and embedding visualization to interpret neural model predictions and demonstrate how this helps extract features that can help automatically fill out incident reports, identify unsafe areas, avoid unsafe practices, and \u2018pin the creeps\u2019.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1303.pdf",
        "title": "SafeCity: Understanding Diverse Forms of Sexual Harassment Personal Stories"
    },
    {
        "paper_id": "D18-1309",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "We propose a simple deep neural model for nested named entity recognition (NER). Most NER models focused on flat entities and ignored nested entities, which failed to fully capture underlying semantic information in texts. The key idea of our model is to enumerate all possible regions or spans as potential entity mentions and classify them with deep neural networks. To reduce the computational costs and capture the information of the contexts around the regions, the model represents the regions using the outputs of shared underlying bidirectional long short-term memory. We evaluate our exhaustive model on the GENIA and JNLPBA corpora in biomedical domain, and the results show that our model outperforms state-of-the-art models on nested and flat NER, achieving 77.1% and 78.4% respectively in terms of F-score, without any external knowledge resources.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1309.pdf",
        "title": "Deep Exhaustive Model for Nested Named Entity Recognition"
    },
    {
        "paper_id": "D18-1315",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "The Paradigm Cell Filling Problem in morphology asks to complete word inflection tables from partial ones. We implement novel neural models for this task, evaluating them on 18 data sets in 8 languages, showing performance that is comparable with previous work with far less training data. We also publish a new dataset for this task and code implementing the system described in this paper.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1315.pdf",
        "title": "An Encoder-Decoder Approach to the Paradigm Cell Filling Problem"
    },
    {
        "paper_id": "D18-1316",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Deep neural networks (DNNs) are vulnerable to adversarial examples, perturbations to correctly classified examples which can cause the model to misclassify. In the image domain, these perturbations can often be made virtually indistinguishable to human perception, causing humans and state-of-the-art models to disagree. However, in the natural language domain, small perturbations are clearly perceptible, and the replacement of a single word can drastically alter the semantics of the document. Given these challenges, we use a black-box population-based optimization algorithm to generate semantically and syntactically similar adversarial examples that fool well-trained sentiment analysis and textual entailment models with success rates of 97% and 70%, respectively. We additionally demonstrate that 92.3% of the successful sentiment analysis adversarial examples are classified to their original label by 20 human annotators, and that the examples are perceptibly quite similar. Finally, we discuss an attempt to use adversarial training as a defense, but fail to yield improvement, demonstrating the strength and diversity of our adversarial examples. We hope our findings encourage researchers to pursue improving the robustness of DNNs in the natural language domain.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1316.pdf",
        "title": "Generating Natural Language Adversarial Examples"
    },
    {
        "paper_id": "D18-1321",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Chinese pinyin input method engine (IME) converts pinyin into character so that Chinese characters can be conveniently inputted into computer through common keyboard. IMEs work relying on its core component, pinyin-to-character conversion (P2C). Usually Chinese IMEs simply predict a list of character sequences for user choice only according to user pinyin input at each turn. However, Chinese inputting is a multi-turn online procedure, which can be supposed to be exploited for further user experience promoting. This paper thus for the first time introduces a sequence-to-sequence model with gated-attention mechanism for the core task in IMEs. The proposed neural P2C model is learned by encoding previous input utterance as extra context to enable our IME capable of predicting character sequence with incomplete pinyin input. Our model is evaluated in different benchmark datasets showing great user experience improvement compared to traditional models, which demonstrates the first engineering practice of building Chinese aided IME.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1321.pdf",
        "title": "Chinese Pinyin Aided IME, Input What You Have Not Keystroked Yet"
    },
    {
        "paper_id": "D18-1323",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Recent state-of-the-art neural language models share the representations of words given by the input and output mappings. We propose a simple modification to these architectures that decouples the hidden state from the word embedding prediction. Our architecture leads to comparable or better results compared to previous tied models and models without tying, with a much smaller number of parameters. We also extend our proposal to word2vec models, showing that tying is appropriate for general word prediction tasks.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1323.pdf",
        "title": "How to represent a word and predict it, too: Improving tied architectures for language modelling"
    },
    {
        "paper_id": "D18-1325",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Neural Machine Translation (NMT) can be improved by including document-level contextual information. For this purpose, we propose a hierarchical attention model to capture the context in a structured and dynamic manner. The model is integrated in the original NMT architecture as another level of abstraction, conditioning on the NMT model\u2019s own previous hidden states. Experiments show that hierarchical attention significantly improves the BLEU score over a strong NMT baseline with the state-of-the-art in context-aware methods, and that both the encoder and decoder benefit from context in complementary ways.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1325.pdf",
        "title": "Document-Level Neural Machine Translation with Hierarchical Attention Networks"
    },
    {
        "paper_id": "D18-1326",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Due to the benefits of model compactness, multilingual translation (including many-to-one, many-to-many and one-to-many) based on a universal encoder-decoder architecture attracts more and more attention. However, previous studies show that one-to-many translation based on this framework cannot perform on par with the individually trained models. In this work, we introduce three strategies to improve one-to-many multilingual translation by balancing the shared and unique features. Within the architecture of one decoder for all target languages, we first exploit the use of unique initial states for different target languages. Then, we employ language-dependent positional embeddings. Finally and especially, we propose to divide the hidden cells of the decoder into shared and language-dependent ones. The extensive experiments demonstrate that our proposed methods can obtain remarkable improvements over the strong baselines. Moreover, our strategies can achieve comparable or even better performance than the individually trained translation models.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1326.pdf",
        "title": "Three Strategies to Improve One-to-Many Multilingual Translation"
    },
    {
        "paper_id": "D18-1330",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Continuous word representations learned separately on distinct languages can be aligned so that their words become comparable in a common space. Existing works typically solve a quadratic problem to learn a orthogonal matrix aligning a bilingual lexicon, and use a retrieval criterion for inference. In this paper, we propose an unified formulation that directly optimizes a retrieval criterion in an end-to-end fashion. Our experiments on standard benchmarks show that our approach outperforms the state of the art on word translation, with the biggest improvements observed for distant language pairs such as English-Chinese.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1330.pdf",
        "title": "Loss in Translation: Learning Bilingual Word Mapping with a Retrieval Criterion"
    },
    {
        "paper_id": "D18-1341",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Automated Post-Editing (PE) is the task of automatically correct common and repetitive errors found in machine translation (MT) output. In this paper, we present a neural programmer-interpreter approach to this task, resembling the way that human perform post-editing using discrete edit operations, wich we refer to as programs. Our model outperforms previous neural models for inducing PE programs on the WMT17 APE task for German-English up to +1 BLEU score and -0.7 TER scores.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1341.pdf",
        "title": "Automatic Post-Editing of Machine Translation: A Neural Programmer-Interpreter Approach"
    },
    {
        "paper_id": "D18-1345",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Character-level patterns have been widely used as features in English Named Entity Recognition (NER) systems. However, to date there has been no direct investigation of the inherent differences between name and nonname tokens in text, nor whether this property holds across multiple languages. This paper analyzes the capabilities of corpus-agnostic Character-level Language Models (CLMs) in the binary task of distinguishing name tokens from non-name tokens. We demonstrate that CLMs provide a simple and powerful model for capturing these differences, identifying named entity tokens in a diverse set of languages at close to the performance of full NER systems. Moreover, by adding very simple CLM-based features we can significantly improve the performance of an off-the-shelf NER system for multiple languages.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1345.pdf",
        "title": "On the Strength of Character Language Models for Multilingual Named Entity Recognition"
    },
    {
        "paper_id": "D18-1349",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Prevalent models based on artificial neural network (ANN) for sentence classification often classify sentences in isolation without considering the context in which sentences appear. This hampers the traditional sentence classification approaches to the problem of sequential sentence classification, where structured prediction is needed for better overall classification performance. In this work, we present a hierarchical sequential labeling network to make use of the contextual information within surrounding sentences to help classify the current sentence. Our model outperforms the state-of-the-art results by 2%-3% on two benchmarking datasets for sequential sentence classification in medical scientific abstracts.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1349.pdf",
        "title": "Hierarchical Neural Networks for Sequential Sentence Classification in Medical Scientific Abstracts"
    },
    {
        "paper_id": "D18-1352",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Large multi-label datasets contain labels that occur thousands of times (frequent group), those that occur only a few times (few-shot group), and labels that never appear in the training dataset (zero-shot group). Multi-label few- and zero-shot label prediction is mostly unexplored on datasets with large label spaces, especially for text classification. In this paper, we perform a fine-grained evaluation to understand how state-of-the-art methods perform on infrequent labels. Furthermore, we develop few- and zero-shot methods for multi-label text classification when there is a known structure over the label space, and evaluate them on two publicly available medical text datasets: MIMIC II and MIMIC III. For few-shot labels we achieve improvements of 6.2% and 4.8% in R@10 for MIMIC II and MIMIC III, respectively, over prior efforts; the corresponding R@10 improvements for zero-shot labels are 17.3% and 19%.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1352.pdf",
        "title": "Few-Shot and Zero-Shot Multi-Label Learning for Structured Label Spaces"
    },
    {
        "paper_id": "D18-1353",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Poetry is one of the most beautiful forms of human language art. As a crucial step towards computer creativity, automatic poetry generation has drawn researchers\u2019 attention for decades. In recent years, some neural models have made remarkable progress in this task. However, they are all based on maximum likelihood estimation, which only learns common patterns of the corpus and results in loss-evaluation mismatch. Human experts evaluate poetry in terms of some specific criteria, instead of word-level likelihood. To handle this problem, we directly model the criteria and use them as explicit rewards to guide gradient update by reinforcement learning, so as to motivate the model to pursue higher scores. Besides, inspired by writing theories, we propose a novel mutual reinforcement learning schema. We simultaneously train two learners (generators) which learn not only from the teacher (rewarder) but also from each other to further improve performance. We experiment on Chinese poetry. Based on a strong basic model, our method achieves better results and outperforms the current state-of-the-art method.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1353.pdf",
        "title": "Automatic Poetry Generation with Mutual Reinforcement Learning"
    },
    {
        "paper_id": "D18-1358",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "The rapid development of knowledge graphs (KGs), such as Freebase and WordNet, has changed the paradigm for AI-related applications. However, even though these KGs are impressively large, most of them are suffering from incompleteness, which leads to performance degradation of AI applications. Most existing researches are focusing on knowledge graph embedding (KGE) models. Nevertheless, those models simply embed entities and relations into latent vectors without leveraging the rich information from the relation structure. Indeed, relations in KGs conform to a three-layer hierarchical relation structure (HRS), i.e., semantically similar relations can make up relation clusters and some relations can be further split into several fine-grained sub-relations. Relation clusters, relations and sub-relations can fit in the top, the middle and the bottom layer of three-layer HRS respectively. To this end, in this paper, we extend existing KGE models TransE, TransH and DistMult, to learn knowledge representations by leveraging the information from the HRS. Particularly, our approach is capable to extend other KGE models. Finally, the experiment results clearly validate the effectiveness of the proposed approach against baselines.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1358.pdf",
        "title": "Knowledge Graph Embedding with Hierarchical Relation Structure"
    },
    {
        "paper_id": "D18-1359",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Representing entities and relations in an embedding space is a well-studied approach for machine learning on relational data. Existing approaches, however, primarily focus on simple link structure between a finite set of entities, ignoring the variety of data types that are often used in knowledge bases, such as text, images, and numerical values. In this paper, we propose multimodal knowledge base embeddings (MKBE) that use different neural encoders for this variety of observed data, and combine them with existing relational models to learn embeddings of the entities and multimodal data. Further, using these learned embedings and different neural decoders, we introduce a novel multimodal imputation model to generate missing multimodal values, like text and images, from information in the knowledge base. We enrich existing relational datasets to create two novel benchmarks that contain additional information such as textual descriptions and images of the original entities. We demonstrate that our models utilize this additional information effectively to provide more accurate link prediction, achieving state-of-the-art results with a considerable gap of 5-7% over existing methods. Further, we evaluate the quality of our generated multimodal values via a user study.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1359.pdf",
        "title": "Embedding Multimodal Relational Data for Knowledge Base Completion"
    },
    {
        "paper_id": "D18-1360",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "We introduce a multi-task setup of identifying entities, relations, and coreference clusters in scientific articles. We create SciERC, a dataset that includes annotations for all three tasks and develop a unified framework called SciIE with shared span representations. The multi-task setup reduces cascading errors between tasks and leverages cross-sentence relations through coreference links. Experiments show that our multi-task model outperforms previous models in scientific information extraction without using any domain-specific features. We further show that the framework supports construction of a scientific knowledge graph, which we use to analyze information in scientific literature.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1360.pdf",
        "title": "Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction"
    },
    {
        "paper_id": "D18-1362",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Multi-hop reasoning is an effective approach for query answering (QA) over incomplete knowledge graphs (KGs). The problem can be formulated in a reinforcement learning (RL) setup, where a policy-based agent sequentially extends its inference path until it reaches a target. However, in an incomplete KG environment, the agent receives low-quality rewards corrupted by false negatives in the training data, which harms generalization at test time. Furthermore, since no golden action sequence is used for training, the agent can be misled by spurious search trajectories that incidentally lead to the correct answer. We propose two modeling advances to address both issues: (1) we reduce the impact of false negative supervision by adopting a pretrained one-hop embedding model to estimate the reward of unobserved facts; (2) we counter the sensitivity to spurious paths of on-policy RL by forcing the agent to explore a diverse set of paths using randomly generated edge masks. Our approach significantly improves over existing path-based KGQA models on several benchmark datasets and is comparable or better than embedding-based models.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1362.pdf",
        "title": "Multi-Hop Knowledge Graph Reasoning with Reward Shaping"
    },
    {
        "paper_id": "D18-1363",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Neural state-of-the-art sequence-to-sequence (seq2seq) models often do not perform well for small training sets. We address paradigm completion, the morphological task of, given a partial paradigm, generating all missing forms. We propose two new methods for the minimal-resource setting: (i) Paradigm transduction: Since we assume only few paradigms available for training, neural seq2seq models are able to capture relationships between paradigm cells, but are tied to the idiosyncracies of the training set. Paradigm transduction mitigates this problem by exploiting the input subset of inflected forms at test time. (ii) Source selection with high precision (SHIP): Multi-source models which learn to automatically select one or multiple sources to predict a target inflection do not perform well in the minimal-resource setting. SHIP is an alternative to identify a reliable source if training data is limited. On a 52-language benchmark dataset, we outperform the previous state of the art by up to 9.71% absolute accuracy.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1363.pdf",
        "title": "Neural Transductive Learning and Beyond: Morphological Generation in the Minimal-Resource Setting"
    },
    {
        "paper_id": "D18-1366",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Much work in Natural Language Processing (NLP) has been for resource-rich languages, making generalization to new, less-resourced languages challenging. We present two approaches for improving generalization to low-resourced languages by adapting continuous word representations using linguistically motivated subword units: phonemes, morphemes and graphemes. Our method requires neither parallel corpora nor bilingual dictionaries and provides a significant gain in performance over previous methods relying on these resources. We demonstrate the effectiveness of our approaches on Named Entity Recognition for four languages, namely Uyghur, Turkish, Bengali and Hindi, of which Uyghur and Bengali are low resource languages, and also perform experiments on Machine Translation. Exploiting subwords with transfer learning gives us a boost of +15.2 NER F1 for Uyghur and +9.7 F1 for Bengali. We also show improvements in the monolingual setting where we achieve (avg.) +3 F1 and (avg.) +1.35 BLEU.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1366.pdf",
        "title": "Adapting Word Embeddings to New Languages with Morphological and Phonological Subword Representations"
    },
    {
        "paper_id": "D18-1367",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Several NLP studies address the problem of figurative language, but among non-literal phenomena, they have neglected exaggeration. This paper presents a first computational approach to this figure of speech. We explore the possibility to automatically detect exaggerated sentences. First, we introduce HYPO, a corpus containing overstatements (or hyperboles) collected on the web and validated via crowdsourcing. Then, we evaluate a number of models trained on HYPO, and bring evidence that the task of hyperbole identification can be successfully performed based on a small set of semantic features.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1367.pdf",
        "title": "A Computational Exploration of Exaggeration"
    },
    {
        "paper_id": "D18-1368",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Capabilities to categorize a clause based on the type of situation entity (e.g., events, states and generic statements) the clause introduces to the discourse can benefit many NLP applications. Observing that the situation entity type of a clause depends on discourse functions the clause plays in a paragraph and the interpretation of discourse functions depends heavily on paragraph-wide contexts, we propose to build context-aware clause representations for predicting situation entity types of clauses. Specifically, we propose a hierarchical recurrent neural network model to read a whole paragraph at a time and jointly learn representations for all the clauses in the paragraph by extensively modeling context influences and inter-dependencies of clauses. Experimental results show that our model achieves the state-of-the-art performance for clause-level situation entity classification on the genre-rich MASC+Wiki corpus, which approaches human-level performance.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1368.pdf",
        "title": "Building Context-aware Clause Representations for Situation Entity Type Classification"
    },
    {
        "paper_id": "D18-1369",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "In news and discussions, many articles and posts are provided without their related previous articles or posts. Hence, it is difficult to understand the context from which the articles and posts have occurred. In this paper, we propose the Hierarchical Dirichlet Gaussian Marked Hawkes process (HD-GMHP) for reconstructing the narratives and thread structures of news articles and discussion posts. HD-GMHP unifies three modeling strategies in previous research: temporal characteristics, triggering event relations, and meta information of text in news articles and discussion threads. To show the effectiveness of the model, we perform experiments in narrative reconstruction and thread reconstruction with real world datasets: articles from the New York Times and a corpus of Wikipedia conversations. The experimental results show that HD-GMHP outperforms the baselines of LDA, HDP, and HDHP for both tasks.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1369.pdf",
        "title": "Hierarchical Dirichlet Gaussian Marked Hawkes Process for Narrative Reconstruction in Continuous Time Domain"
    },
    {
        "paper_id": "D18-1371",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "We design and build the first neural temporal dependency parser. It utilizes a neural ranking model with minimal feature engineering, and parses time expressions and events in a text into a temporal dependency tree structure. We evaluate our parser on two domains: news reports and narrative stories. In a parsing-only evaluation setup where gold time expressions and events are provided, our parser reaches 0.81 and 0.70 f-score on unlabeled and labeled parsing respectively, a result that is very competitive against alternative approaches. In an end-to-end evaluation setup where time expressions and events are automatically recognized, our parser beats two strong baselines on both data domains. Our experimental results and discussions shed light on the nature of temporal dependency structures in different domains and provide insights that we believe will be valuable to future research in this area.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1371.pdf",
        "title": "Neural Ranking Models for Temporal Dependency Structure Parsing"
    },
    {
        "paper_id": "D18-1373",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Multimodal learning has shown promising performance in content-based recommendation due to the auxiliary user and item information of multiple modalities such as text and images. However, the problem of incomplete and missing modality is rarely explored and most existing methods fail in learning a recommendation model with missing or corrupted modalities. In this paper, we propose LRMM, a novel framework that mitigates not only the problem of missing modalities but also more generally the cold-start problem of recommender systems. We propose modality dropout (m-drop) and a multimodal sequential autoencoder (m-auto) to learn multimodal representations for complementing and imputing missing modalities. Extensive experiments on real-world Amazon data show that LRMM achieves state-of-the-art performance on rating prediction tasks. More importantly, LRMM is more robust to previous methods in alleviating data-sparsity and the cold-start problem.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1373.pdf",
        "title": "LRMM: Learning to Recommend with Missing Modalities"
    },
    {
        "paper_id": "D18-1374",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Background research is an essential part of document writing. Search engines are great for retrieving information once we know what to look for. However, the bigger challenge is often identifying topics for further research. Automated tools could help significantly in this discovery process and increase the productivity of the writer. In this paper, we formulate the problem of recommending topics to a writer. We consider this as a supervised learning problem and run a user study to validate this approach. We propose an evaluation metric and perform an empirical comparison of state-of-the-art models for extreme multi-label classification on a large data set. We demonstrate how a simple modification of the cross-entropy loss function leads to improved results of the deep learning models.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1374.pdf",
        "title": "Content Explorer: Recommending Novel Entities for a Document Writer"
    },
    {
        "paper_id": "D18-1378",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "We propose Limbic, an unsupervised probabilistic model that addresses the problem of discovering aspects and sentiments and associating them with authors of opinionated texts. Limbic combines three ideas, incorporating authors, discourse relations, and word embeddings. For discourse relations, Limbic adopts a generative process regularized by a Markov Random Field. To promote words with high semantic similarity into the same topic, Limbic captures semantic regularities from word embeddings via a generalized P\u00f3lya Urn process. We demonstrate that Limbic (1) discovers aspects associated with sentiments with high lexical diversity; (2) outperforms state-of-the-art models by a substantial margin in topic cohesion and sentiment classification.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1378.pdf",
        "title": "Limbic: Author-Based Sentiment Aspect Modeling Regularized with Word Embeddings and Discourse Relations"
    },
    {
        "paper_id": "D18-1380",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "We propose a novel multi-grained attention network (MGAN) model for aspect level sentiment classification. Existing approaches mostly adopt coarse-grained attention mechanism, which may bring information loss if the aspect has multiple words or larger context. We propose a fine-grained attention mechanism, which can capture the word-level interaction between aspect and context. And then we leverage the fine-grained and coarse-grained attention mechanisms to compose the MGAN framework. Moreover, unlike previous works which train each aspect with its context separately, we design an aspect alignment loss to depict the aspect-level interactions among the aspects that have the same context. We evaluate the proposed approach on three datasets: laptop and restaurant are from SemEval 2014, and the last one is a twitter dataset. Experimental results show that the multi-grained attention network consistently outperforms the state-of-the-art methods on all three datasets. We also conduct experiments to evaluate the effectiveness of aspect alignment loss, which indicates the aspect-level interactions can bring extra useful information and further improve the performance.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1380.pdf",
        "title": "Multi-grained Attention Network for Aspect-Level Sentiment Classification"
    },
    {
        "paper_id": "D18-1381",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "This paper proposes a new neural architecture that exploits readily available sentiment lexicon resources. The key idea is that that incorporating a word-level prior can aid in the representation learning process, eventually improving model performance. To this end, our model employs two distinctly unique components, i.e., (1) we introduce a lexicon-driven contextual attention mechanism to imbue lexicon words with long-range contextual information and (2), we introduce a contrastive co-attention mechanism that models contrasting polarities between all positive and negative words in a sentence. Via extensive experiments, we show that our approach outperforms many other neural baselines on sentiment classification tasks on multiple benchmark datasets.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1381.pdf",
        "title": "Attentive Gated Lexicon Reader with Contrastive Contextual Co-Attention for Sentiment Classification"
    },
    {
        "paper_id": "D18-1385",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "With the increasing popularity of smart devices, rumors with multimedia content become more and more common on social networks. The multimedia information usually makes rumors look more convincing. Therefore, finding an automatic approach to verify rumors with multimedia content is a pressing task. Previous rumor verification research only utilizes multimedia as input features. We propose not to use the multimedia content but to find external information in other news platforms pivoting on it. We introduce a new features set, cross-lingual cross-platform features that leverage the semantic similarity between the rumors and the external information. When implemented, machine learning methods utilizing such features achieved the state-of-the-art rumor verification results.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1385.pdf",
        "title": "Cross-Lingual Cross-Platform Rumor Verification Pivoting on Multimedia Content"
    },
    {
        "paper_id": "D18-1386",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "We introduce an adversarial method for producing high-recall explanations of neural text classifier decisions. Building on an existing architecture for extractive explanations via hard attention, we add an adversarial layer which scans the residual of the attention for remaining predictive signal. Motivated by the important domain of detecting personal attacks in social media comments, we additionally demonstrate the importance of manually setting a semantically appropriate \u201cdefault\u201d behavior for the model by explicitly manipulating its bias term. We develop a validation set of human-annotated personal attacks to evaluate the impact of these changes.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1386.pdf",
        "title": "Extractive Adversarial Networks: High-Recall Explanations for Identifying Personal Attacks in Social Media Posts"
    },
    {
        "paper_id": "D18-1387",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Website privacy policies represent the single most important source of information for users to gauge how their personal data are collected, used and shared by companies. However, privacy policies are often vague and people struggle to understand the content. Their opaqueness poses a significant challenge to both users and policy regulators. In this paper, we seek to identify vague content in privacy policies. We construct the first corpus of human-annotated vague words and sentences and present empirical studies on automatic vagueness detection. In particular, we investigate context-aware and context-agnostic models for predicting vague words, and explore auxiliary-classifier generative adversarial networks for characterizing sentence vagueness. Our experimental results demonstrate the effectiveness of proposed approaches. Finally, we provide suggestions for resolving vagueness and improving the usability of privacy policies.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1387.pdf",
        "title": "Automatic Detection of Vague Words and Sentences in Privacy Policies"
    },
    {
        "paper_id": "D18-1388",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "A news article\u2019s title, content and link structure often reveal its political ideology. However, most existing works on automatic political ideology detection only leverage textual cues. Drawing inspiration from recent advances in neural inference, we propose a novel attention based multi-view model to leverage cues from all of the above views to identify the ideology evinced by a news article. Our model draws on advances in representation learning in natural language processing and network science to capture cues from both textual content and the network structure of news articles. We empirically evaluate our model against a battery of baselines and show that our model outperforms state of the art by 10 percentage points F1 score.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1388.pdf",
        "title": "Multi-view Models for Political Ideology Detection of News Articles"
    },
    {
        "paper_id": "D18-1389",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "We present a study on predicting the factuality of reporting and bias of news media. While previous work has focused on studying the veracity of claims or documents, here we are interested in characterizing entire news media. This is an under-studied, but arguably important research problem, both in its own right and as a prior for fact-checking systems. We experiment with a large list of news websites and with a rich set of features derived from (i) a sample of articles from the target news media, (ii) its Wikipedia page, (iii) its Twitter account, (iv) the structure of its URL, and (v) information about the Web traffic it attracts. The experimental results show sizable performance gains over the baseline, and reveal the importance of each feature type.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1389.pdf",
        "title": "Predicting Factuality of Reporting and Bias of News Media Sources"
    },
    {
        "paper_id": "D18-1392",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Predictive models over social media language have shown promise in capturing community outcomes, but approaches thus far largely neglect the socio-demographic context (e.g. age, education rates, race) of the community from which the language originates. For example, it may be inaccurate to assume people in Mobile, Alabama, where the population is relatively older, will use words the same way as those from San Francisco, where the median age is younger with a higher rate of college education. In this paper, we present residualized factor adaptation, a novel approach to community prediction tasks which both (a) effectively integrates community attributes, as well as (b) adapts linguistic features to community attributes (factors). We use eleven demographic and socioeconomic attributes, and evaluate our approach over five different community-level predictive tasks, spanning health (heart disease mortality, percent fair/poor health), psychology (life satisfaction), and economics (percent housing price increase, foreclosure rate). Our evaluation shows that residualized factor adaptation significantly improves 4 out of 5 community-level outcome predictions over prior state-of-the-art for incorporating socio-demographic contexts.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1392.pdf",
        "title": "Residualized Factor Adaptation for Community Social Media Prediction Tasks"
    },
    {
        "paper_id": "D18-1395",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "We address the task of native language identification in the context of social media content, where authors are highly-fluent, advanced nonnative speakers (of English). Using both linguistically-motivated features and the characteristics of the social media outlet, we obtain high accuracy on this challenging task. We provide a detailed analysis of the features that sheds light on differences between native and nonnative speakers, and among nonnative speakers with different backgrounds.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1395.pdf",
        "title": "Native Language Identification with User Generated Content"
    },
    {
        "paper_id": "D18-1396",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Neural machine translation usually adopts autoregressive models and suffers from exposure bias as well as the consequent error propagation problem. Many previous works have discussed the relationship between error propagation and the accuracy drop (i.e., the left part of the translated sentence is often better than its right part in left-to-right decoding models) problem. In this paper, we conduct a series of analyses to deeply understand this problem and get several interesting findings. (1) The role of error propagation on accuracy drop is overstated in the literature, although it indeed contributes to the accuracy drop problem. (2) Characteristics of a language play a more important role in causing the accuracy drop: the left part of the translation result in a right-branching language (e.g., English) is more likely to be more accurate than its right part, while the right part is more accurate for a left-branching language (e.g., Japanese). Our discoveries are confirmed on different model structures including Transformer and RNN, and in other sequence generation tasks such as text summarization.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1396.pdf",
        "title": "Beyond Error Propagation in Neural Machine Translation: Characteristics of Language Also Matter"
    },
    {
        "paper_id": "D18-1397",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Recent studies have shown that reinforcement learning (RL) is an effective approach for improving the performance of neural machine translation (NMT) system. However, due to its instability, successfully RL training is challenging, especially in real-world systems where deep models and large datasets are leveraged. In this paper, taking several large-scale translation tasks as testbeds, we conduct a systematic study on how to train better NMT models using reinforcement learning. We provide a comprehensive comparison of several important factors (e.g., baseline reward, reward shaping) in RL training. Furthermore, to fill in the gap that it remains unclear whether RL is still beneficial when monolingual data is used, we propose a new method to leverage RL to further boost the performance of NMT systems trained with source/target monolingual data. By integrating all our findings, we obtain competitive results on WMT14 English-German, WMT17 English-Chinese, and WMT17 Chinese-English translation tasks, especially setting a state-of-the-art performance on WMT17 Chinese-English translation task.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1397.pdf",
        "title": "A Study of Reinforcement Learning for Neural Machine Translation"
    },
    {
        "paper_id": "D18-1399",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "While modern machine translation has relied on large parallel corpora, a recent line of work has managed to train Neural Machine Translation (NMT) systems from monolingual corpora only (Artetxe et al., 2018c; Lample et al., 2018). Despite the potential of this approach for low-resource settings, existing systems are far behind their supervised counterparts, limiting their practical interest. In this paper, we propose an alternative approach based on phrase-based Statistical Machine Translation (SMT) that significantly closes the gap with supervised systems. Our method profits from the modular architecture of SMT: we first induce a phrase table from monolingual corpora through cross-lingual embedding mappings, combine it with an n-gram language model, and fine-tune hyperparameters through an unsupervised MERT variant. In addition, iterative backtranslation improves results further, yielding, for instance, 14.08 and 26.22 BLEU points in WMT 2014 English-German and English-French, respectively, an improvement of more than 7-10 BLEU points over previous unsupervised systems, and closing the gap with supervised SMT (Moses trained on Europarl) down to 2-5 BLEU points. Our implementation is available at https://github.com/artetxem/monoses.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1399.pdf",
        "title": "Unsupervised Statistical Machine Translation"
    },
    {
        "paper_id": "D18-1403",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "We present a neural framework for opinion summarization from online product reviews which is knowledge-lean and only requires light supervision (e.g., in the form of product domain labels and user-provided ratings). Our method combines two weakly supervised components to identify salient opinions and form extractive summaries from multiple reviews: an aspect extractor trained under a multi-task objective, and a sentiment predictor based on multiple instance learning. We introduce an opinion summarization dataset that includes a training set of product reviews from six diverse domains and human-annotated development and test sets with gold standard aspect annotations, salience labels, and opinion summaries. Automatic evaluation shows significant improvements over baselines, and a large-scale study indicates that our opinion summaries are preferred by human judges according to multiple criteria.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1403.pdf",
        "title": "Summarizing Opinions: Aspect Extraction Meets Sentiment Prediction and They Are Both Weakly Supervised"
    },
    {
        "paper_id": "D18-1410",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Current lexical simplification approaches rely heavily on heuristics and corpus level features that do not always align with human judgment. We create a human-rated word-complexity lexicon of 15,000 English words and propose a novel neural readability ranking model with a Gaussian-based feature vectorization layer that utilizes these human ratings to measure the complexity of any given word or phrase. Our model performs better than the state-of-the-art systems for different lexical simplification tasks and evaluation datasets. Additionally, we also produce SimplePPDB++, a lexical resource of over 10 million simplifying paraphrase rules, by applying our model to the Paraphrase Database (PPDB).",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1410.pdf",
        "title": "A Word-Complexity Lexicon and A Neural Readability Ranking Model for Lexical Simplification"
    },
    {
        "paper_id": "D18-1412",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "We introduce the syntactic scaffold, an approach to incorporating syntactic information into semantic tasks. Syntactic scaffolds avoid expensive syntactic processing at runtime, only making use of a treebank during training, through a multitask objective. We improve over strong baselines on PropBank semantics, frame semantics, and coreference resolution, achieving competitive performance on all three tasks.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1412.pdf",
        "title": "Syntactic Scaffolds for Semantic Structures"
    },
    {
        "paper_id": "D18-1414",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "This paper studies semantic parsing for interlanguage (L2), taking semantic role labeling (SRL) as a case task and learner Chinese as a case language. We first manually annotate the semantic roles for a set of learner texts to derive a gold standard for automatic SRL. Based on the new data, we then evaluate three off-the-shelf SRL systems, i.e., the PCFGLA-parser-based, neural-parser-based and neural-syntax-agnostic systems, to gauge how successful SRL for learner Chinese can be. We find two non-obvious facts: 1) the L1-sentence-trained systems performs rather badly on the L2 data; 2) the performance drop from the L1 data to the L2 data of the two parser-based systems is much smaller, indicating the importance of syntactic parsing in SRL for interlanguages. Finally, the paper introduces a new agreement-based model to explore the semantic coherency information in the large-scale L2-L1 parallel data. We then show such information is very effective to enhance SRL for learner texts. Our model achieves an F-score of 72.06, which is a 2.02 point improvement over the best baseline.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1414.pdf",
        "title": "Semantic Role Labeling for Learner Chinese: the Importance of Syntactic Parsing and L2-L1 Parallel Data"
    },
    {
        "paper_id": "D18-1415",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Reinforcement learning (RL) is an attractive solution for task-oriented dialog systems. However, extending RL-based systems to handle new intents and slots requires a system redesign. The high maintenance cost makes it difficult to apply RL methods to practical systems on a large scale. To address this issue, we propose a practical teacher-student framework to extend RL-based dialog systems without retraining from scratch. Specifically, the \u201cstudent\u201d is an extended dialog manager based on a new ontology, and the \u201cteacher\u201d is existing resources used for guiding the learning process of the \u201cstudent\u201d. By specifying constraints held in the new dialog manager, we transfer knowledge of the \u201cteacher\u201d to the \u201cstudent\u201d without additional resources. Experiments show that the performance of the extended system is comparable to the system trained from scratch. More importantly, the proposed framework makes no assumption about the unsupported intents and slots, which makes it possible to improve RL-based systems incrementally.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1415.pdf",
        "title": "A Teacher-Student Framework for Maintainable Dialog Manager"
    },
    {
        "paper_id": "D18-1417",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Spoken Language Understanding (SLU), which typically involves intent determination and slot filling, is a core component of spoken dialogue systems. Joint learning has shown to be effective in SLU given that slot tags and intents are supposed to share knowledge with each other. However, most existing joint learning methods only consider joint learning by sharing parameters on surface level rather than semantic level. In this work, we propose a novel self-attentive model with gate mechanism to fully utilize the semantic correlation between slot and intent. Our model first obtains intent-augmented embeddings based on neural network with self-attention mechanism. And then the intent semantic representation is utilized as the gate for labelling slot tags. The objectives of both tasks are optimized simultaneously via joint learning in an end-to-end way. We conduct experiment on popular benchmark ATIS. The results show that our model achieves state-of-the-art and outperforms other popular methods by a large margin in terms of both intent detection error rate and slot filling F1-score. This paper gives a new perspective for research on SLU.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1417.pdf",
        "title": "A Self-Attentive Model with Gate Mechanism for Spoken Language Understanding"
    },
    {
        "paper_id": "D18-1418",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "In a dialog, there could be multiple valid next utterances at any point. The present end-to-end neural methods for dialog do not take this into account. They learn with the assumption that at any time there is only one correct next utterance. In this work, we focus on this problem in the goal-oriented dialog setting where there are different paths to reach a goal. We propose a new method, that uses a combination of supervised learning and reinforcement learning approaches to address this issue. We also propose a new and more effective testbed, permuted-bAbI dialog tasks, by introducing multiple valid next utterances to the original-bAbI dialog tasks, which allows evaluation of end-to-end goal-oriented dialog systems in a more realistic setting. We show that there is a significant drop in performance of existing end-to-end neural methods from 81.5% per-dialog accuracy on original-bAbI dialog tasks to 30.3% on permuted-bAbI dialog tasks. We also show that our proposed method improves the performance and achieves 47.3% per-dialog accuracy on permuted-bAbI dialog tasks. We also release permuted-bAbI dialog tasks, our proposed testbed, to the community for evaluating dialog systems in a goal-oriented setting.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1418.pdf",
        "title": "Learning End-to-End Goal-Oriented Dialog with Multiple Answers"
    },
    {
        "paper_id": "D18-1421",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Automatic generation of paraphrases from a given sentence is an important yet challenging task in natural language processing (NLP). In this paper, we present a deep reinforcement learning approach to paraphrase generation. Specifically, we propose a new framework for the task, which consists of a generator and an evaluator, both of which are learned from data. The generator, built as a sequence-to-sequence learning model, can produce paraphrases given a sentence. The evaluator, constructed as a deep matching model, can judge whether two sentences are paraphrases of each other. The generator is first trained by deep learning and then further fine-tuned by reinforcement learning in which the reward is given by the evaluator. For the learning of the evaluator, we propose two methods based on supervised learning and inverse reinforcement learning respectively, depending on the type of available training data. Experimental results on two datasets demonstrate the proposed models (the generators) can produce more accurate paraphrases and outperform the state-of-the-art methods in paraphrase generation in both automatic evaluation and human evaluation.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1421.pdf",
        "title": "Paraphrase Generation with Deep Reinforcement Learning"
    },
    {
        "paper_id": "D18-1424",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Question generation, the task of automatically creating questions that can be answered by a certain span of text within a given passage, is important for question-answering and conversational systems in digital assistants such as Alexa, Cortana, Google Assistant and Siri. Recent sequence to sequence neural models have outperformed previous rule-based systems. Existing models mainly focused on using one or two sentences as the input. Long text has posed challenges for sequence to sequence neural models in question generation \u2013 worse performances were reported if using the whole paragraph (with multiple sentences) as the input. In reality, however, it often requires the whole paragraph as context in order to generate high quality questions. In this paper, we propose a maxout pointer mechanism with gated self-attention encoder to address the challenges of processing long text inputs for question generation. With sentence-level inputs, our model outperforms previous approaches with either sentence-level or paragraph-level inputs. Furthermore, our model can effectively utilize paragraphs as inputs, pushing the state-of-the-art result from 13.9 to 16.3 (BLEU_4).",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1424.pdf",
        "title": "Paragraph-level Neural Question Generation with Maxout Pointer and Gated Self-attention Networks"
    },
    {
        "paper_id": "D18-1429",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "There has always been criticism for using n-gram based similarity metrics, such as BLEU, NIST, etc, for evaluating the performance of NLG systems. However, these metrics continue to remain popular and are recently being used for evaluating the performance of systems which automatically generate questions from documents, knowledge graphs, images, etc. Given the rising interest in such automatic question generation (AQG) systems, it is important to objectively examine whether these metrics are suitable for this task. In particular, it is important to verify whether such metrics used for evaluating AQG systems focus on answerability of the generated question by preferring questions which contain all relevant information such as question type (Wh-types), entities, relations, etc. In this work, we show that current automatic evaluation metrics based on n-gram similarity do not always correlate well with human judgments about answerability of a question. To alleviate this problem and as a first step towards better evaluation metrics for AQG, we introduce a scoring function to capture answerability and show that when this scoring function is integrated with existing metrics, they correlate significantly better with human judgments. The scripts and data developed as a part of this work are made publicly available.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1429.pdf",
        "title": "Towards a Better Metric for Evaluating Question Generation Systems"
    },
    {
        "paper_id": "D18-1434",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Generating natural questions from an image is a semantic task that requires using visual and language modality to learn multimodal representations. Images can have multiple visual and language contexts that are relevant for generating questions namely places, captions, and tags. In this paper, we propose the use of exemplars for obtaining the relevant context. We obtain this by using a Multimodal Differential Network to produce natural and engaging questions. The generated questions show a remarkable similarity to the natural questions as validated by a human study. Further, we observe that the proposed approach substantially improves over state-of-the-art benchmarks on the quantitative metrics (BLEU, METEOR, ROUGE, and CIDEr).",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1434.pdf",
        "title": "Multimodal Differential Network for Visual Question Generation"
    },
    {
        "paper_id": "D18-1435",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Current image captioning approaches generate descriptions which lack specific information, such as named entities that are involved in the images. In this paper we propose a new task which aims to generate informative image captions, given images and hashtags as input. We propose a simple but effective approach to tackle this problem. We first train a convolutional neural networks - long short term memory networks (CNN-LSTM) model to generate a template caption based on the input image. Then we use a knowledge graph based collective inference algorithm to fill in the template with specific named entities retrieved via the hashtags. Experiments on a new benchmark dataset collected from Flickr show that our model generates news-style image descriptions with much richer information. Our model outperforms unimodal baselines significantly with various evaluation metrics.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1435.pdf",
        "title": "Entity-aware Image Caption Generation"
    },
    {
        "paper_id": "D18-1438",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Rapid growth of multi-modal documents on the Internet makes multi-modal summarization research necessary. Most previous research summarizes texts or images separately. Recent neural summarization research shows the strength of the Encoder-Decoder model in text summarization. This paper proposes an abstractive text-image summarization model using the attentional hierarchical Encoder-Decoder model to summarize a text document and its accompanying images simultaneously, and then to align the sentences and images in summaries. A multi-modal attentional mechanism is proposed to attend original sentences, images, and captions when decoding. The DailyMail dataset is extended by collecting images and captions from the Web. Experiments show our model outperforms the neural abstractive and extractive text summarization methods that do not consider images. In addition, our model can generate informative summaries of images.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1438.pdf",
        "title": "Abstractive Text-Image Summarization Using Multi-Modal Attentional Hierarchical RNN"
    },
    {
        "paper_id": "D18-1442",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "In this paper, we introduce Iterative Text Summarization (ITS), an iteration-based model for supervised extractive text summarization, inspired by the observation that it is often necessary for a human to read an article multiple times in order to fully understand and summarize its contents. Current summarization approaches read through a document only once to generate a document representation, resulting in a sub-optimal representation. To address this issue we introduce a model which iteratively polishes the document representation on many passes through the document. As part of our model, we also introduce a selective reading mechanism that decides more accurately the extent to which each sentence in the model should be updated. Experimental results on the CNN/DailyMail and DUC2002 datasets demonstrate that our model significantly outperforms state-of-the-art extractive systems when evaluated by machines and by humans.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1442.pdf",
        "title": "Iterative Document Representation Learning Towards Summarization with Polishing"
    },
    {
        "paper_id": "D18-1443",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Neural summarization produces outputs that are fluent and readable, but which can be poor at content selection, for instance often copying full sentences from the source document. This work explores the use of data-efficient content selectors to over-determine phrases in a source document that should be part of the summary. We use this selector as a bottom-up attention step to constrain the model to likely phrases. We show that this approach improves the ability to compress text, while still generating fluent summaries. This two-step process is both simpler and higher performing than other end-to-end content selection models, leading to significant improvements on ROUGE for both the CNN-DM and NYT corpus. Furthermore, the content selector can be trained with as little as 1,000 sentences making it easy to transfer a trained summarizer to a new domain.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1443.pdf",
        "title": "Bottom-Up Abstractive Summarization"
    },
    {
        "paper_id": "D18-1445",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "We propose a method to perform automatic document summarisation without using reference summaries. Instead, our method interactively learns from users\u2019 preferences. The merit of preference-based interactive summarisation is that preferences are easier for users to provide than reference summaries. Existing preference-based interactive learning methods suffer from high sample complexity, i.e. they need to interact with the oracle for many rounds in order to converge. In this work, we propose a new objective function, which enables us to leverage active learning, preference learning and reinforcement learning techniques in order to reduce the sample complexity. Both simulation and real-user experiments suggest that our method significantly advances the state of the art. Our source code is freely available at https://github.com/UKPLab/emnlp2018-april.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1445.pdf",
        "title": "APRIL: Interactively Learning to Summarise by Combining Active Preference Learning and Reinforcement Learning"
    },
    {
        "paper_id": "D18-1447",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "We study the problem of generating keyphrases that summarize the key points for a given document. While sequence-to-sequence (seq2seq) models have achieved remarkable performance on this task (Meng et al., 2017), model training often relies on large amounts of labeled data, which is only applicable to resource-rich domains. In this paper, we propose semi-supervised keyphrase generation methods by leveraging both labeled data and large-scale unlabeled samples for learning. Two strategies are proposed. First, unlabeled documents are first tagged with synthetic keyphrases obtained from unsupervised keyphrase extraction methods or a self-learning algorithm, and then combined with labeled samples for training. Furthermore, we investigate a multi-task learning framework to jointly learn to generate keyphrases as well as the titles of the articles. Experimental results show that our semi-supervised learning-based methods outperform a state-of-the-art model trained with labeled data only.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1447.pdf",
        "title": "Semi-Supervised Learning for Neural Keyphrase Generation"
    },
    {
        "paper_id": "D18-1454",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Reading comprehension QA tasks have seen a recent surge in popularity, yet most works have focused on fact-finding extractive QA. We instead focus on a more challenging multi-hop generative task (NarrativeQA), which requires the model to reason, gather, and synthesize disjoint pieces of information within the context to generate an answer. This type of multi-step reasoning also often requires understanding implicit relations, which humans resolve via external, background commonsense knowledge. We first present a strong generative baseline that uses a multi-attention mechanism to perform multiple hops of reasoning and a pointer-generator decoder to synthesize the answer. This model performs substantially better than previous generative models, and is competitive with current state-of-the-art span prediction models. We next introduce a novel system for selecting grounded multi-hop relational commonsense information from ConceptNet via a pointwise mutual information and term-frequency based scoring function. Finally, we effectively use this extracted commonsense information to fill in gaps of reasoning between context hops, using a selectively-gated attention mechanism. This boosts the model\u2019s performance significantly (also verified via human evaluation), establishing a new state-of-the-art for the task. We also show that our background knowledge enhancements are generalizable and improve performance on QAngaroo-WikiHop, another multi-hop reasoning dataset.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1454.pdf",
        "title": "Commonsense for Generative Multi-Hop Question Answering Tasks"
    },
    {
        "paper_id": "D18-1462",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Narrative story generation is a challenging problem because it demands the generated sentences with tight semantic connections, which has not been well studied by most existing generative models. To address this problem, we propose a skeleton-based model to promote the coherence of generated stories. Different from traditional models that generate a complete sentence at a stroke, the proposed model first generates the most critical phrases, called skeleton, and then expands the skeleton to a complete and fluent sentence. The skeleton is not manually defined, but learned by a reinforcement learning method. Compared to the state-of-the-art models, our skeleton-based model can generate significantly more coherent text according to human evaluation and automatic evaluation. The G-score is improved by 20.1% in human evaluation.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1462.pdf",
        "title": "A Skeleton-Based Model for Promoting Coherence Among Sentences in Narrative Story Generation"
    },
    {
        "paper_id": "D18-1463",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Sequence-to-Sequence (seq2seq) models have become overwhelmingly popular in building end-to-end trainable dialogue systems. Though highly efficient in learning the backbone of human-computer communications, they suffer from the problem of strongly favoring short generic responses. In this paper, we argue that a good response should smoothly connect both the preceding dialogue history and the following conversations. We strengthen this connection by mutual information maximization. To sidestep the non-differentiability of discrete natural language tokens, we introduce an auxiliary continuous code space and map such code space to a learnable prior distribution for generation purpose. Experiments on two dialogue datasets validate the effectiveness of our model, where the generated responses are closely related to the dialogue context and lead to more interactive conversations.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1463.pdf",
        "title": "NEXUS Network: Connecting the Preceding and the Following in Dialogue Generation"
    },
    {
        "paper_id": "D18-1464",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "We propose a local coherence model that captures the flow of what semantically connects adjacent sentences in a text. We represent the semantics of a sentence by a vector and capture its state at each word of the sentence. We model what relates two adjacent sentences based on the two most similar semantic states, each of which is in one of the sentences. We encode the perceived coherence of a text by a vector, which represents patterns of changes in salient information that relates adjacent sentences. Our experiments demonstrate that our approach is beneficial for two downstream tasks: Readability assessment, in which our model achieves new state-of-the-art results; and essay scoring, in which the combination of our coherence vectors and other task-dependent features significantly improves the performance of a strong essay scorer.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1464.pdf",
        "title": "A Neural Local Coherence Model for Text Quality Assessment"
    },
    {
        "paper_id": "D18-1465",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "In this paper, we propose a novel deep attentive sentence ordering network (referred as ATTOrderNet) which integrates self-attention mechanism with LSTMs in the encoding of input sentences. It enables us to capture global dependencies among sentences regardless of their input order and obtains a reliable representation of the sentence set. With this representation, a pointer network is exploited to generate an ordered sequence. The proposed model is evaluated on Sentence Ordering and Order Discrimination tasks. The extensive experimental results demonstrate its effectiveness and superiority to the state-of-the-art methods.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1465.pdf",
        "title": "Deep Attentive Sentence Ordering Network"
    },
    {
        "paper_id": "D18-1483",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Clustering news across languages enables efficient media monitoring by aggregating articles from multilingual sources into coherent stories. Doing so in an online setting allows scalable processing of massive news streams. To this end, we describe a novel method for clustering an incoming stream of multilingual documents into monolingual and crosslingual clusters. Unlike typical clustering approaches that report results on datasets with a small and known number of labels, we tackle the problem of discovering an ever growing number of cluster labels in an online fashion, using real news datasets in multiple languages. In our formulation, the monolingual clusters group together documents while the crosslingual clusters group together monolingual clusters, one per language that appears in the stream. Our method is simple to implement, computationally efficient and produces state-of-the-art results on datasets in German, English and Spanish.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1483.pdf",
        "title": "Multilingual Clustering of Streaming News"
    },
    {
        "paper_id": "D18-1484",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Multi-task learning in text classification leverages implicit correlations among related tasks to extract common features and yield performance gains. However, a large body of previous work treats labels of each task as independent and meaningless one-hot vectors, which cause a loss of potential label information. In this paper, we propose Multi-Task Label Embedding to convert labels in text classification into semantic vectors, thereby turning the original tasks into vector matching tasks. Our model utilizes semantic correlations among tasks and makes it convenient to scale or transfer when new tasks are involved. Extensive experiments on five benchmark datasets for text classification show that our model can effectively improve the performances of related tasks with semantic representations of labels and additional information from each other.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1484.pdf",
        "title": "Multi-Task Label Embedding for Text Classification"
    },
    {
        "paper_id": "D18-1485",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "We propose a novel model for multi-label text classification, which is based on sequence-to-sequence learning. The model generates higher-level semantic unit representations with multi-level dilated convolution as well as a corresponding hybrid attention mechanism that extracts both the information at the word-level and the level of the semantic unit. Our designed dilated convolution effectively reduces dimension and supports an exponential expansion of receptive fields without loss of local information, and the attention-over-attention mechanism is able to capture more summary relevant information from the source context. Results of our experiments show that the proposed model has significant advantages over the baseline models on the dataset RCV1-V2 and Ren-CECps, and our analysis demonstrates that our model is competitive to the deterministic hierarchical models and it is more robust to classifying low-frequency labels",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1485.pdf",
        "title": "Semantic-Unit-Based Dilated Convolution for Multi-Label Text Classification"
    },
    {
        "paper_id": "D18-1486",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Multi-task learning has an ability to share the knowledge among related tasks and implicitly increase the training data. However, it has long been frustrated by the interference among tasks. This paper investigates the performance of capsule network for text, and proposes a capsule-based multi-task learning architecture, which is unified, simple and effective. With the advantages of capsules for feature clustering, proposed task routing algorithm can cluster the features for each task in the network, which helps reduce the interference among tasks. Experiments on six text classification datasets demonstrate the effectiveness of our models and their characteristics for feature clustering.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1486.pdf",
        "title": "MCapsNet: Capsule Network for Text with Multi-Task Learning"
    },
    {
        "paper_id": "D18-1490",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "In recent years, the natural language processing community has moved away from task-specific feature engineering, i.e., researchers discovering ad-hoc feature representations for various tasks, in favor of general-purpose methods that learn the input representation by themselves. However, state-of-the-art approaches to disfluency detection in spontaneous speech transcripts currently still depend on an array of hand-crafted features, and other representations derived from the output of pre-existing systems such as language models or dependency parsers. As an alternative, this paper proposes a simple yet effective model for automatic disfluency detection, called an auto-correlational neural network (ACNN). The model uses a convolutional neural network (CNN) and augments it with a new auto-correlation operator at the lowest layer that can capture the kinds of \u201crough copy\u201d dependencies that are characteristic of repair disfluencies in speech. In experiments, the ACNN model outperforms the baseline CNN on a disfluency detection task with a 5% increase in f-score, which is close to the previous best result on this task.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1490.pdf",
        "title": "Disfluency Detection using Auto-Correlational Neural Networks"
    },
    {
        "paper_id": "D18-1494",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Label-specific topics can be widely used for supporting personality psychology, aspect-level sentiment analysis, and cross-domain sentiment classification. To generate label-specific topics, several supervised topic models which adopt likelihood-driven objective functions have been proposed. However, it is hard for them to get a precise estimation on both topic discovery and supervised learning. In this study, we propose a supervised topic model based on the Siamese network, which can trade off label-specific word distributions with document-specific label distributions in a uniform framework. Experiments on real-world datasets validate that our model performs competitive in topic discovery quantitatively and qualitatively. Furthermore, the proposed model can effectively predict categorical or real-valued labels for new documents by generating word embeddings from a label-specific topical space.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1494.pdf",
        "title": "Siamese Network-Based Supervised Topic Modeling"
    },
    {
        "paper_id": "D18-1495",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Discovering the latent topics within texts has been a fundamental task for many applications. However, conventional topic models suffer different problems in different settings. The Latent Dirichlet Allocation (LDA) may not work well for short texts due to the data sparsity (i.e. the sparse word co-occurrence patterns in short documents). The Biterm Topic Model (BTM) learns topics by modeling the word-pairs named biterms in the whole corpus. This assumption is very strong when documents are long with rich topic information and do not exhibit the transitivity of biterms. In this paper, we propose a novel way called GraphBTM to represent biterms as graphs and design a Graph Convolutional Networks (GCNs) with residual connections to extract transitive features from biterms. To overcome the data sparsity of LDA and the strong assumption of BTM, we sample a fixed number of documents to form a mini-corpus as a sample. We also propose a dataset called All News extracted from 15 news publishers, in which documents are much longer than 20 Newsgroups. We present an amortized variational inference method for GraphBTM. Our method generates more coherent topics compared with previous approaches. Experiments show that the sampling strategy improves performance by a large margin.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1495.pdf",
        "title": "GraphBTM: Graph Enhanced Autoencoded Variational Inference for Biterm Topic Model"
    },
    {
        "paper_id": "D18-1497",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "We propose a method for learning disentangled representations of texts that code for distinct and complementary aspects, with the aim of affording efficient model transfer and interpretability. To induce disentangled embeddings, we propose an adversarial objective based on the (dis)similarity between triplets of documents with respect to specific aspects. Our motivating application is embedding biomedical abstracts describing clinical trials in a manner that disentangles the populations, interventions, and outcomes in a given trial. We show that our method learns representations that encode these clinically salient aspects, and that these can be effectively used to perform aspect-specific retrieval. We demonstrate that the approach generalizes beyond our motivating application in experiments on two multi-aspect review corpora.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1497.pdf",
        "title": "Learning Disentangled Representations of Texts with Application to Biomedical Abstracts"
    },
    {
        "paper_id": "D18-1498",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "We propose a mixture-of-experts approach for unsupervised domain adaptation from multiple sources. The key idea is to explicitly capture the relationship between a target example and different source domains. This relationship, expressed by a point-to-set metric, determines how to combine predictors trained on various domains. The metric is learned in an unsupervised fashion using meta-training. Experimental results on sentiment analysis and part-of-speech tagging demonstrate that our approach consistently outperforms multiple baselines and can robustly handle negative transfer.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1498.pdf",
        "title": "Multi-Source Domain Adaptation with Mixture of Experts"
    },
    {
        "paper_id": "D18-1508",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Human language has evolved towards newer forms of communication such as social media, where emojis (i.e., ideograms bearing a visual meaning) play a key role. While there is an increasing body of work aimed at the computational modeling of emoji semantics, there is currently little understanding about what makes a computational model represent or predict a given emoji in a certain way. In this paper we propose a label-wise attention mechanism with which we attempt to better understand the nuances underlying emoji prediction. In addition to advantages in terms of interpretability, we show that our proposed architecture improves over standard baselines in emoji prediction, and does particularly well when predicting infrequent emojis.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1508.pdf",
        "title": "Interpretable Emoji Prediction via Label-Wise Attention LSTMs"
    },
    {
        "paper_id": "D18-1510",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Neural machine translation (NMT) models are usually trained with the word-level loss using the teacher forcing algorithm, which not only evaluates the translation improperly but also suffers from exposure bias. Sequence-level training under the reinforcement framework can mitigate the problems of the word-level loss, but its performance is unstable due to the high variance of the gradient estimation. On these grounds, we present a method with a differentiable sequence-level training objective based on probabilistic n-gram matching which can avoid the reinforcement framework. In addition, this method performs greedy search in the training which uses the predicted words as context just as at inference to alleviate the problem of exposure bias. Experiment results on the NIST Chinese-to-English translation tasks show that our method significantly outperforms the reinforcement-based algorithms and achieves an improvement of 1.5 BLEU points on average over a strong baseline system.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1510.pdf",
        "title": "Greedy Search with Probabilistic N-gram Matching for Neural Machine Translation"
    },
    {
        "paper_id": "D18-1516",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Research on link prediction in knowledge graphs has mainly focused on static multi-relational data. In this work we consider temporal knowledge graphs where relations between entities may only hold for a time interval or a specific point in time. In line with previous work on static knowledge graphs, we propose to address this problem by learning latent entity and relation type representations. To incorporate temporal information, we utilize recurrent neural networks to learn time-aware representations of relation types which can be used in conjunction with existing latent factorization methods. The proposed approach is shown to be robust to common challenges in real-world KGs: the sparsity and heterogeneity of temporal expressions. Experiments show the benefits of our approach on four temporal KGs. The data sets are available under a permissive BSD-3 license.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1516.pdf",
        "title": "Learning Sequence Encoders for Temporal Knowledge Graph Completion"
    },
    {
        "paper_id": "D18-1525",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "This paper addresses the problem of representation learning. Using an autoencoder framework, we propose and evaluate several loss functions that can be used as an alternative to the commonly used cross-entropy reconstruction loss. The proposed loss functions use similarities between words in the embedding space, and can be used to train any neural model for text generation. We show that the introduced loss functions amplify semantic diversity of reconstructed sentences, while preserving the original meaning of the input. We test the derived autoencoder-generated representations on paraphrase detection and language inference tasks and demonstrate performance improvement compared to the traditional cross-entropy loss.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1525.pdf",
        "title": "Similarity-Based Reconstruction Loss for Meaning Representation"
    },
    {
        "paper_id": "D18-1529",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "A wide variety of neural-network architectures have been proposed for the task of Chinese word segmentation. Surprisingly, we find that a bidirectional LSTM model, when combined with standard deep learning techniques and best practices, can achieve better accuracy on many of the popular datasets as compared to models based on more complex neuralnetwork architectures. Furthermore, our error analysis shows that out-of-vocabulary words remain challenging for neural-network models, and many of the remaining errors are unlikely to be fixed through architecture changes. Instead, more effort should be made on exploring resources for further improvement.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1529.pdf",
        "title": "State-of-the-art Chinese Word Segmentation with Bi-LSTMs"
    },
    {
        "paper_id": "D18-1531",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Previous traditional approaches to unsupervised Chinese word segmentation (CWS) can be roughly classified into discriminative and generative models. The former uses the carefully designed goodness measures for candidate segmentation, while the latter focuses on finding the optimal segmentation of the highest generative probability. However, while there exists a trivial way to extend the discriminative models into neural version by using neural language models, those of generative ones are non-trivial. In this paper, we propose the segmental language models (SLMs) for CWS. Our approach explicitly focuses on the segmental nature of Chinese, as well as preserves several properties of language models. In SLMs, a context encoder encodes the previous context and a segment decoder generates each segment incrementally. As far as we know, we are the first to propose a neural model for unsupervised CWS and achieve competitive performance to the state-of-the-art statistical models on four different datasets from SIGHAN 2005 bakeoff.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1531.pdf",
        "title": "Unsupervised Neural Word Segmentation for Chinese via Segmental Language Modeling"
    },
    {
        "paper_id": "D18-1538",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Neural models have shown several state-of-the-art performances on Semantic Role Labeling (SRL). However, the neural models require an immense amount of semantic-role corpora and are thus not well suited for low-resource languages or domains. The paper proposes a semi-supervised semantic role labeling method that outperforms the state-of-the-art in limited SRL training corpora. The method is based on explicitly enforcing syntactic constraints by augmenting the training objective with a syntactic-inconsistency loss component and uses SRL-unlabeled instances to train a joint-objective LSTM. On CoNLL-2012 English section, the proposed semi-supervised training with 1%, 10% SRL-labeled data and varying amounts of SRL-unlabeled data achieves +1.58, +0.78 F1, respectively, over the pre-trained models that were trained on SOTA architecture with ELMo on the same SRL-labeled data. Additionally, by using the syntactic-inconsistency loss on inference time, the proposed model achieves +3.67, +2.1 F1 over pre-trained model on 1%, 10% SRL-labeled data, respectively.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1538.pdf",
        "title": "Towards Semi-Supervised Learning for Deep Semantic Role Labeling"
    },
    {
        "paper_id": "D18-1544",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "A substantial thread of recent work on latent tree learning has attempted to develop neural network models with parse-valued latent variables and train them on non-parsing tasks, in the hope of having them discover interpretable tree structure. In a recent paper, Shen et al. (2018) introduce such a model and report near-state-of-the-art results on the target task of language modeling, and the first strong latent tree learning result on constituency parsing. In an attempt to reproduce these results, we discover issues that make the original results hard to trust, including tuning and even training on what is effectively the test set. Here, we attempt to reproduce these results in a fair experiment and to extend them to two new datasets. We find that the results of this work are robust: All variants of the model under study outperform all latent tree learning baselines, and perform competitively with symbolic grammar induction systems. We find that this model represents the first empirical success for latent tree learning, and that neural network language modeling warrants further study as a setting for grammar induction.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1544.pdf",
        "title": "Grammar Induction with Neural Language Models: An Unusual Replication"
    },
    {
        "paper_id": "D18-1547",
        "conference": "emnlp",
        "year": "2018",
        "abstract": "Even though machine learning has become the major scene in dialogue research community, the real breakthrough has been blocked by the scale of data available.To address this fundamental obstacle, we introduce the Multi-Domain Wizard-of-Oz dataset (MultiWOZ), a fully-labeled collection of human-human written conversations spanning over multiple domains and topics.At a size of 10k dialogues, it is at least one order of magnitude larger than all previous annotated task-oriented corpora.The contribution of this work apart from the open-sourced dataset is two-fold:firstly, a detailed description of the data collection procedure along with a summary of data structure and analysis is provided. The proposed data-collection pipeline is entirely based on crowd-sourcing without the need of hiring professional annotators;secondly, a set of benchmark results of belief tracking, dialogue act and response generation is reported, which shows the usability of the data and sets a baseline for future studies.",
        "url_pdf": "https://www.aclweb.org/anthology/D18-1547.pdf",
        "title": "MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling"
    },
    {
        "paper_id": "D19-1001",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Neural sequence generation is typically performed token-by-token and left-to-right. Whenever a token is generated only previously produced tokens are taken into consideration. In contrast, for problems such as sequence classification, bidirectional attention, which takes both past and future tokens into consideration, has been shown to perform much better. We propose to make the sequence generation process bidirectional by employing special placeholder tokens. Treated as a node in a fully connected graph, a placeholder token can take past and future tokens into consideration when generating the actual output token. We verify the effectiveness of our approach experimentally on two conversational tasks where the proposed bidirectional model outperforms competitive baselines by a large margin.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1001.pdf",
        "title": "Attending to Future Tokens for Bidirectional Sequence Generation"
    },
    {
        "paper_id": "D19-1005",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Contextual word representations, typically trained on unstructured, unlabeled text, do not contain any explicit grounding to real world entities and are often unable to remember facts about those entities. We propose a general method to embed multiple knowledge bases (KBs) into large scale models, and thereby enhance their representations with structured, human-curated knowledge. For each KB, we first use an integrated entity linker to retrieve relevant entity embeddings, then update contextual word representations via a form of word-to-entity attention. In contrast to previous approaches, the entity linkers and self-supervised language modeling objective are jointly trained end-to-end in a multitask setting that combines a small amount of entity linking supervision with a large amount of raw text. After integrating WordNet and a subset of Wikipedia into BERT, the knowledge enhanced BERT (KnowBert) demonstrates improved perplexity, ability to recall facts as measured in a probing task and downstream performance on relationship extraction, entity typing, and word sense disambiguation. KnowBert\u2019s runtime is comparable to BERT\u2019s and it scales to large KBs.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1005.pdf",
        "title": "Knowledge Enhanced Contextual Word Representations"
    },
    {
        "paper_id": "D19-1009",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Game-theoretic models, thanks to their intrinsic ability to exploit contextual information, have shown to be particularly suited for the Word Sense Disambiguation task. They represent ambiguous words as the players of a non cooperative game and their senses as the strategies that the players can select in order to play the games. The interaction among the players is modeled with a weighted graph and the payoff as an embedding similarity function, that the players try to maximize. The impact of the word and sense embedding representations in the framework has been tested and analyzed extensively: experiments on standard benchmarks show state-of-art performances and different tests hint at the usefulness of using disambiguation to obtain contextualized word representations.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1009.pdf",
        "title": "Game Theory Meets Embeddings: a Unified Framework for Word Sense Disambiguation"
    },
    {
        "paper_id": "D19-1010",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Dialog policy decides what and how a task-oriented dialog system will respond, and plays a vital role in delivering effective conversations. Many studies apply Reinforcement Learning to learn a dialog policy with the reward function which requires elaborate design and pre-specified user goals. With the growing needs to handle complex goals across multiple domains, such manually designed reward functions are not affordable to deal with the complexity of real-world tasks. To this end, we propose Guided Dialog Policy Learning, a novel algorithm based on Adversarial Inverse Reinforcement Learning for joint reward estimation and policy optimization in multi-domain task-oriented dialog. The proposed approach estimates the reward signal and infers the user goal in the dialog sessions. The reward estimator evaluates the state-action pairs so that it can guide the dialog policy at each dialog turn. Extensive experiments on a multi-domain dialog dataset show that the dialog policy guided by the learned reward function achieves remarkably higher task success than state-of-the-art baselines.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1010.pdf",
        "title": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog"
    },
    {
        "paper_id": "D19-1020",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Medical relation extraction discovers relations between entity mentions in text, such as research articles. For this task, dependency syntax has been recognized as a crucial source of features. Yet in the medical domain, 1-best parse trees suffer from relatively low accuracies, diminishing their usefulness. We investigate a method to alleviate this problem by utilizing dependency forests. Forests contain more than one possible decisions and therefore have higher recall but more noise compared with 1-best outputs. A graph neural network is used to represent the forests, automatically distinguishing the useful syntactic information from parsing noise. Results on two benchmarks show that our method outperforms the standard tree-based methods, giving the state-of-the-art results in the literature.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1020.pdf",
        "title": "Leveraging Dependency Forest for Neural Medical Relation Extraction"
    },
    {
        "paper_id": "D19-1021",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Open relation extraction (OpenRE) aims to extract relational facts from the open-domain corpus. To this end, it discovers relation patterns between named entities and then clusters those semantically equivalent patterns into a united relation cluster. Most OpenRE methods typically confine themselves to unsupervised paradigms, without taking advantage of existing relational facts in knowledge bases (KBs) and their high-quality labeled instances. To address this issue, we propose Relational Siamese Networks (RSNs) to learn similarity metrics of relations from labeled data of pre-defined relations, and then transfer the relational knowledge to identify novel relations in unlabeled data. Experiment results on two real-world datasets show that our framework can achieve significant improvements as compared with other state-of-the-art methods. Our code is available at https://github.com/thunlp/RSN.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1021.pdf",
        "title": "Open Relation Extraction: Relational Knowledge Transfer from Supervised Data to Unsupervised Data"
    },
    {
        "paper_id": "D19-1022",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "While attention mechanisms have been proven to be effective in many NLP tasks, majority of them are data-driven. We propose a novel knowledge-attention encoder which incorporates prior knowledge from external lexical resources into deep neural networks for relation extraction task. Furthermore, we present three effective ways of integrating knowledge-attention with self-attention to maximize the utilization of both knowledge and data. The proposed relation extraction system is end-to-end and fully attention-based. Experiment results show that the proposed knowledge-attention mechanism has complementary strengths with self-attention, and our integrated models outperform existing CNN, RNN, and self-attention based models. State-of-the-art performance is achieved on TACRED, a complex and large-scale relation extraction dataset.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1022.pdf",
        "title": "Improving Relation Extraction with Knowledge-attention"
    },
    {
        "paper_id": "D19-1025",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Name tagging in low-resource languages or domains suffers from inadequate training data. Existing work heavily relies on additional information, while leaving those noisy annotations unexplored that extensively exist on the web. In this paper, we propose a novel neural model for name tagging solely based on weakly labeled (WL) data, so that it can be applied in any low-resource settings. To take the best advantage of all WL sentences, we split them into high-quality and noisy portions for two modules, respectively: (1) a classification module focusing on the large portion of noisy data can efficiently and robustly pretrain the tag classifier by capturing textual context semantics; and (2) a costly sequence labeling module focusing on high-quality data utilizes Partial-CRFs with non-entity sampling to achieve global optimum. Two modules are combined via shared parameters. Extensive experiments involving five low-resource languages and fine-grained food domain demonstrate our superior performance (6% and 7.8% F1 gains on average) as well as efficiency.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1025.pdf",
        "title": "Low-Resource Name Tagging Learned with Weakly Labeled Data"
    },
    {
        "paper_id": "D19-1026",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Despite of the recent success of collective entity linking (EL) methods, these \u201cglobal\u201d inference methods may yield sub-optimal results when the \u201call-mention coherence\u201d assumption breaks, and often suffer from high computational cost at the inference stage, due to the complex search space. In this paper, we propose a simple yet effective solution, called Dynamic Context Augmentation (DCA), for collective EL, which requires only one pass through the mentions in a document. DCA sequentially accumulates context information to make efficient, collective inference, and can cope with different local EL models as a plug-and-enhance module. We explore both supervised and reinforcement learning strategies for learning the DCA model. Extensive experiments show the effectiveness of our model with different learning settings, base models, decision orders and attention mechanisms.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1026.pdf",
        "title": "Learning Dynamic Context Augmentation for Global Entity Linking"
    },
    {
        "paper_id": "D19-1028",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Bootstrapping for Entity Set Expansion (ESE) aims at iteratively acquiring new instances of a specific target category. Traditional bootstrapping methods often suffer from two problems: 1) delayed feedback, i.e., the pattern evaluation relies on both its direct extraction quality and extraction quality in later iterations. 2) sparse supervision, i.e., only few seed entities are used as the supervision. To address the above two problems, we propose a novel bootstrapping method combining the Monte Carlo Tree Search (MCTS) algorithm with a deep similarity network, which can efficiently estimate delayed feedback for pattern evaluation and adaptively score entities given sparse supervision signals. Experimental results confirm the effectiveness of the proposed method.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1028.pdf",
        "title": "Learning to Bootstrap for Entity Set Expansion"
    },
    {
        "paper_id": "D19-1030",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "The identification of complex semantic structures such as events and entity relations, already a challenging Information Extraction task, is doubly difficult from sources written in under-resourced and under-annotated languages. We investigate the suitability of cross-lingual structure transfer techniques for these tasks. We exploit relation- and event-relevant language-universal features, leveraging both symbolic (including part-of-speech and dependency path) and distributional (including type representation and contextualized representation) information. By representing all entity mentions, event triggers, and contexts into this complex and structured multilingual common space, using graph convolutional networks, we can train a relation or event extractor from source language annotations and apply it to the target language. Extensive experiments on cross-lingual relation and event transfer among English, Chinese, and Arabic demonstrate that our approach achieves performance comparable to state-of-the-art supervised models trained on up to 3,000 manually annotated mentions: up to 62.6% F-score for Relation Extraction, and 63.1% F-score for Event Argument Role Labeling. The event argument role labeling model transferred from English to Chinese achieves similar performance as the model trained from Chinese. We thus find that language-universal symbolic and distributional representations are complementary for cross-lingual structure transfer.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1030.pdf",
        "title": "Cross-lingual Structure Transfer for Relation and Event Extraction"
    },
    {
        "paper_id": "D19-1034",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "In natural language processing, it is common that many entities contain other entities inside them. Most existing works on named entity recognition (NER) only deal with flat entities but ignore nested ones. We propose a boundary-aware neural model for nested NER which leverages entity boundaries to predict entity categorical labels. Our model can locate entities precisely by detecting boundaries using sequence labeling models. Based on the detected boundaries, our model utilizes the boundary-relevant regions to predict entity categorical labels, which can decrease computation cost and relieve error propagation problem in layered sequence labeling model. We introduce multitask learning to capture the dependencies of entity boundaries and their categorical labels, which helps to improve the performance of identifying entities. We conduct our experiments on GENIA dataset and the experimental results demonstrate that our model outperforms other state-of-the-art methods.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1034.pdf",
        "title": "A Boundary-aware Neural Model for Nested Named Entity Recognition"
    },
    {
        "paper_id": "D19-1037",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Distance supervision is widely used in relation extraction tasks, particularly when large-scale manual annotations are virtually impossible to conduct. Although Distantly Supervised Relation Extraction (DSRE) benefits from automatic labelling, it suffers from serious mislabelling issues, i.e. some or all of the instances for an entity pair (head and tail entities) do not express the labelled relation. In this paper, we propose a novel model that employs a collaborative curriculum learning framework to reduce the effects of mislabelled data. Specifically, we firstly propose an internal self-attention mechanism between the convolution operations in convolutional neural networks (CNNs) to learn a better sentence representation from the noisy inputs. Then we define two sentence selection models as two relation extractors in order to collaboratively learn and regularise each other under a curriculum scheme to alleviate noisy effects, where the curriculum could be constructed by conflicts or small loss. Finally, experiments are conducted on a widely-used public dataset and the results indicate that the proposed model significantly outperforms baselines including the state-of-the-art in terms of P@N and PR curve metrics, thus evidencing its capability of reducing noisy effects for DSRE.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1037.pdf",
        "title": "Self-Attention Enhanced CNNs and Collaborative Curriculum Learning for Distantly Supervised Relation Extraction"
    },
    {
        "paper_id": "D19-1040",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Rich entity representations are useful for a wide class of problems involving entities. Despite their importance, there is no standardized benchmark that evaluates the overall quality of entity representations. In this work, we propose EntEval: a test suite of diverse tasks that require nontrivial understanding of entities including entity typing, entity similarity, entity relation prediction, and entity disambiguation. In addition, we develop training techniques for learning better entity representations by using natural hyperlink annotations in Wikipedia. We identify effective objectives for incorporating the contextual information in hyperlinks into state-of-the-art pretrained language models (Peters et al., 2018) and show that they improve strong baselines on multiple EntEval tasks.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1040.pdf",
        "title": "EntEval: A Holistic Evaluation Benchmark for Entity Representations"
    },
    {
        "paper_id": "D19-1041",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "We propose a joint event and temporal relation extraction model with shared representation learning and structured prediction. The proposed method has two advantages over existing work. First, it improves event representation by allowing the event and relation modules to share the same contextualized embeddings and neural representation learner. Second, it avoids error propagation in the conventional pipeline systems by leveraging structured inference and learning methods to assign both the event labels and the temporal relation labels jointly. Experiments show that the proposed method can improve both event extraction and temporal relation extraction over state-of-the-art systems, with the end-to-end F1 improved by 10% and 6.8% on two benchmark datasets respectively.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1041.pdf",
        "title": "Joint Event and Temporal Relation Extraction with Shared Representations and Structured Prediction"
    },
    {
        "paper_id": "D19-1043",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "As an essential component of natural language processing, text classification relies on deep learning in recent years. Various neural networks are designed for text classification on the basis of word embedding. However, polysemy is a fundamental feature of the natural language, which brings challenges to text classification. One polysemic word contains more than one sense, while the word embedding procedure conflates different senses of a polysemic word into a single vector. Extracting the distinct representation for the specific sense could thus lead to fine-grained models with strong generalization ability. It has been demonstrated that multiple senses of a word actually reside in linear superposition within the word embedding so that specific senses can be extracted from the original word embedding. Therefore, we propose to use capsule networks to construct the vectorized representation of semantics and utilize hyperplanes to decompose each capsule to acquire the specific senses. A novel dynamic routing mechanism named \u2018routing-on-hyperplane\u2019 will select the proper sense for the downstream classification task. Our model is evaluated on 6 different datasets, and the experimental results show that our model is capable of extracting more discriminative semantic features and yields a significant performance gain compared to other baseline methods.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1043.pdf",
        "title": "Investigating Capsule Network and Semantic Feature on Hyperplanes for Text Classification"
    },
    {
        "paper_id": "D19-1056",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "We propose a Cross-lingual Encoder-Decoder model that simultaneously translates and generates sentences with Semantic Role Labeling annotations in a resource-poor target language. Unlike annotation projection techniques, our model does not need parallel data during inference time. Our approach can be applied in monolingual, multilingual and cross-lingual settings and is able to produce dependency-based and span-based SRL annotations. We benchmark the labeling performance of our model in different monolingual and multilingual settings using well-known SRL datasets. We then train our model in a cross-lingual setting to generate new SRL labeled data. Finally, we measure the effectiveness of our method by using the generated data to augment the training basis for resource-poor languages and perform manual evaluation to show that it produces high-quality sentences and assigns accurate semantic role annotations. Our proposed architecture offers a flexible method for leveraging SRL data in multiple languages.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1056.pdf",
        "title": "Translate and Label! An Encoder-Decoder Approach for Cross-lingual Semantic Role Labeling"
    },
    {
        "paper_id": "D19-1057",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "As a fundamental NLP task, semantic role labeling (SRL) aims to discover the semantic roles for each predicate within one sentence. This paper investigates how to incorporate syntactic knowledge into the SRL task effectively. We present different approaches of en- coding the syntactic information derived from dependency trees of different quality and representations; we propose a syntax-enhanced self-attention model and compare it with other two strong baseline methods; and we con- duct experiments with newly published deep contextualized word representations as well. The experiment results demonstrate that with proper incorporation of the high quality syntactic information, our model achieves a new state-of-the-art performance for the Chinese SRL task on the CoNLL-2009 dataset.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1057.pdf",
        "title": "Syntax-Enhanced Self-Attention-Based Semantic Role Labeling"
    },
    {
        "paper_id": "D19-1061",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "This paper describes a new dataset and experiments to determine whether authors of tweets possess the objects they tweet about. We work with 5,000 tweets and show that both humans and neural networks benefit from images in addition to text. We also introduce a simple yet effective strategy to incorporate visual information into any neural network beyond weights from pretrained networks. Specifically, we consider the tags identified in an image as an additional textual input, and leverage pretrained word embeddings as usually done with regular text. Experimental results show this novel strategy is beneficial.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1061.pdf",
        "title": "Extracting Possessions from Social Media: Images Complement Language"
    },
    {
        "paper_id": "D19-1063",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Mobile agents that can leverage help from humans can potentially accomplish more complex tasks than they could entirely on their own. We develop \u201cHelp, Anna!\u201d (HANNA), an interactive photo-realistic simulator in which an agent fulfills object-finding tasks by requesting and interpreting natural language-and-vision assistance. An agent solving tasks in a HANNA environment can leverage simulated human assistants, called ANNA (Automatic Natural Navigation Assistants), which, upon request, provide natural language and visual instructions to direct the agent towards the goals. To address the HANNA problem, we develop a memory-augmented neural agent that hierarchically models multiple levels of decision-making, and an imitation learning algorithm that teaches the agent to avoid repeating past mistakes while simultaneously predicting its own chances of making future progress. Empirically, our approach is able to ask for help more effectively than competitive baselines and, thus, attains higher task success rate on both previously seen and previously unseen environments.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1063.pdf",
        "title": "Help, Anna! Visual Navigation with Natural Multimodal Assistance via Retrospective Curiosity-Encouraging Imitation Learning"
    },
    {
        "paper_id": "D19-1068",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "The scarcity in annotated data poses a great challenge for event detection (ED). Cross-lingual ED aims to tackle this challenge by transferring knowledge between different languages to boost performance. However, previous cross-lingual methods for ED demonstrated a heavy dependency on parallel resources, which might limit their applicability. In this paper, we propose a new method for cross-lingual ED, demonstrating a minimal dependency on parallel resources. Specifically, to construct a lexical mapping between different languages, we devise a context-dependent translation method; to treat the word order difference problem, we propose a shared syntactic order event detector for multilingual co-training. The efficiency of our method is studied through extensive experiments on two standard datasets. Empirical results indicate that our method is effective in 1) performing cross-lingual transfer concerning different directions and 2) tackling the extremely annotation-poor scenario.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1068.pdf",
        "title": "Neural Cross-Lingual Event Detection with Minimal Parallel Resources"
    },
    {
        "paper_id": "D19-1070",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Tracking entities in procedural language requires understanding the transformations arising from actions on entities as well as those entities\u2019 interactions. While self-attention-based pre-trained language encoders like GPT and BERT have been successfully applied across a range of natural language understanding tasks, their ability to handle the nuances of procedural texts is still unknown. In this paper, we explore the use of pre-trained transformer networks for entity tracking tasks in procedural text. First, we test standard lightweight approaches for prediction with pre-trained transformers, and find that these approaches underperforms even simple baselines. We show that much stronger results can be attained by restructuring the input to guide the model to focus on a particular entity. Second, we assess the degree to which the transformer networks capture the process dynamics, investigating such factors as merged entities and oblique entity references. On two different tasks, ingredient detection in recipes and QA over scientific processes, we achieve state-of-the-art results, but our models still largely attend to shallow context clues and do not form complex representations of intermediate process state.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1070.pdf",
        "title": "Effective Use of Transformer Networks for Entity Tracking"
    },
    {
        "paper_id": "D19-1076",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Enabling cross-lingual NLP tasks by leveraging multilingual word embedding has recently attracted much attention. An important motivation is to support lower resourced languages, however, most efforts focus on demonstrating the effectiveness of the techniques using embeddings derived from similar languages to English with large parallel content. In this study, we first describe the general requirements for the success of these techniques and then present a noise tolerant piecewise linear technique to learn a non-linear mapping between two monolingual word embedding vector spaces. We evaluate our approach on inferring bilingual dictionaries. We show that our technique outperforms the state-of-the-art in lower resourced settings with an average of 3.7% improvement of precision @10 across 14 mostly low resourced languages.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1076.pdf",
        "title": "Supervised and Nonlinear Alignment of Two Embedding Spaces for Dictionary Induction in Low Resourced Languages"
    },
    {
        "paper_id": "D19-1081",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Modern sentence-level NMT systems often produce plausible translations of isolated sentences. However, when put in context, these translations may end up being inconsistent with each other. We propose a monolingual DocRepair model to correct inconsistencies between sentence-level translations. DocRepair performs automatic post-editing on a sequence of sentence-level translations, refining translations of sentences in context of each other. For training, the DocRepair model requires only monolingual document-level data in the target language. It is trained as a monolingual sequence-to-sequence model that maps inconsistent groups of sentences into consistent ones. The consistent groups come from the original training data; the inconsistent groups are obtained by sampling round-trip translations for each isolated sentence. We show that this approach successfully imitates inconsistencies we aim to fix: using contrastive evaluation, we show large improvements in the translation of several contextual phenomena in an English-Russian translation task, as well as improvements in the BLEU score. We also conduct a human evaluation and show a strong preference of the annotators to corrected translations over the baseline ones. Moreover, we analyze which discourse phenomena are hard to capture using monolingual data only.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1081.pdf",
        "title": "Context-Aware Monolingual Repair for Neural Machine Translation"
    },
    {
        "paper_id": "D19-1083",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "The general trend in NLP is towards increasing model capacity and performance via deeper neural networks. However, simply stacking more layers of the popular Transformer architecture for machine translation results in poor convergence and high computational overhead. Our empirical analysis suggests that convergence is poor due to gradient vanishing caused by the interaction between residual connection and layer normalization. We propose depth-scaled initialization (DS-Init), which decreases parameter variance at the initialization stage, and reduces output variance of residual connections so as to ease gradient back-propagation through normalization layers. To address computational cost, we propose a merged attention sublayer (MAtt) which combines a simplified average-based self-attention sublayer and the encoder-decoder attention sublayer on the decoder side. Results on WMT and IWSLT translation tasks with five translation directions show that deep Transformers with DS-Init and MAtt can substantially outperform their base counterpart in terms of BLEU (+1.1 BLEU on average for 12-layer models), while matching the decoding speed of the baseline model thanks to the efficiency improvements of MAtt. Source code for reproduction will be released soon.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1083.pdf",
        "title": "Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention"
    },
    {
        "paper_id": "D19-1084",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "We introduce a novel discriminative word alignment model, which we integrate into a Transformer-based machine translation model. In experiments based on a small number of labeled examples (\u223c1.7K\u20135K sentences) we evaluate its performance intrinsically on both English-Chinese and English-Arabic alignment, where we achieve major improvements over unsupervised baselines (11\u201327 F1). We evaluate the model extrinsically on data projection for Chinese NER, showing that our alignments lead to higher performance when used to project NER tags from English to Chinese. Finally, we perform an ablation analysis and an annotation experiment that jointly support the utility and feasibility of future manual alignment elicitation.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1084.pdf",
        "title": "A Discriminative Neural Model for Cross-Lingual Word Alignment"
    },
    {
        "paper_id": "D19-1085",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Zero pronouns (ZPs) are frequently omitted in pro-drop languages, but should be recalled in non-pro-drop languages. This discourse phenomenon poses a significant challenge for machine translation (MT) when translating texts from pro-drop to non-pro-drop languages. In this paper, we propose a unified and discourse-aware ZP translation approach for neural MT models. Specifically, we jointly learn to predict and translate ZPs in an end-to-end manner, allowing both components to interact with each other. In addition, we employ hierarchical neural networks to exploit discourse-level context, which is beneficial for ZP prediction and thus translation. Experimental results on both Chinese-English and Japanese-English data show that our approach significantly and accumulatively improves both translation performance and ZP prediction accuracy over not only baseline but also previous works using external ZP prediction models. Extensive analyses confirm that the performance improvement comes from the alleviation of different kinds of errors especially caused by subjective ZPs.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1085.pdf",
        "title": "One Model to Learn Both: Zero Pronoun Prediction and Translation"
    },
    {
        "paper_id": "D19-1092",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Treebank translation is a promising method for cross-lingual transfer of syntactic dependency knowledge. The basic idea is to map dependency arcs from a source treebank to its target translation according to word alignments. This method, however, can suffer from imperfect alignment between source and target words. To address this problem, we investigate syntactic transfer by code mixing, translating only confident words in a source treebank. Cross-lingual word embeddings are leveraged for transferring syntactic knowledge to the target from the resulting code-mixed treebank. Experiments on University Dependency Treebanks show that code-mixed treebanks are more effective than translated treebanks, giving highly competitive performances among cross-lingual parsing methods.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1092.pdf",
        "title": "Cross-Lingual Dependency Parsing Using Code-Mixed TreeBank"
    },
    {
        "paper_id": "D19-1093",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Transition-based top-down parsing with pointer networks has achieved state-of-the-art results in multiple parsing tasks, while having a linear time complexity. However, the decoder of these parsers has a sequential structure, which does not yield the most appropriate inductive bias for deriving tree structures. In this paper, we propose hierarchical pointer network parsers, and apply them to dependency and sentence-level discourse parsing tasks. Our results on standard benchmark datasets demonstrate the effectiveness of our approach, outperforming existing methods and setting a new state-of-the-art.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1093.pdf",
        "title": "Hierarchical Pointer Net Parsing"
    },
    {
        "paper_id": "D19-1094",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "The successful application of neural networks to a variety of NLP tasks has provided strong impetus to develop end-to-end models for semantic role labeling which forego the need for extensive feature engineering. Recent approaches rely on high-quality annotations which are costly to obtain, and mostly unavailable in low resource scenarios (e.g., rare languages or domains). Our work aims to reduce the annotation effort involved via semi-supervised learning. We propose an end-to-end SRL model and demonstrate it can effectively leverage unlabeled data under the cross-view training modeling paradigm. Our LSTM-based semantic role labeler is jointly trained with a sentence learner, which performs POS tagging, dependency parsing, and predicate identification which we argue are critical to learning directly from unlabeled data without recourse to external pre-processing tools. Experimental results on the CoNLL-2009 benchmark dataset show that our model outperforms the state of the art in English, and consistently improves performance in other languages, including Chinese, German, and Spanish.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1094.pdf",
        "title": "Semi-Supervised Semantic Role Labeling with Cross-View Training"
    },
    {
        "paper_id": "D19-1102",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Parsers are available for only a handful of the world\u2019s languages, since they require lots of training data. How far can we get with just a small amount of training data? We systematically compare a set of simple strategies for improving low-resource parsers: data augmentation, which has not been tested before; cross-lingual training; and transliteration. Experimenting on three typologically diverse low-resource languages\u2014North S\u00e1mi, Galician, and Kazah\u2014We find that (1) when only the low-resource treebank is available, data augmentation is very helpful; (2) when a related high-resource treebank is available, cross-lingual training is helpful and complements data augmentation; and (3) when the high-resource treebank uses a different writing system, transliteration into a shared orthographic spaces is also very helpful.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1102.pdf",
        "title": "A systematic comparison of methods for low-resource dependency parsing on genuinely low-resource languages"
    },
    {
        "paper_id": "D19-1109",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Inferring commonsense knowledge is a key challenge in machine learning. Due to the sparsity of training data, previous work has shown that supervised methods for commonsense knowledge mining underperform when evaluated on novel data. In this work, we develop a method for generating commonsense knowledge using a large, pre-trained bidirectional language model. By transforming relational triples into masked sentences, we can use this model to rank a triple\u2019s validity by the estimated pointwise mutual information between the two entities. Since we do not update the weights of the bidirectional model, our approach is not biased by the coverage of any one commonsense knowledge base. Though we do worse on a held-out test set than models explicitly trained on a corresponding training set, our approach outperforms these methods when mining commonsense knowledge from new sources, suggesting that our unsupervised technique generalizes better than current supervised approaches.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1109.pdf",
        "title": "Commonsense Knowledge Mining from Pretrained Models"
    },
    {
        "paper_id": "D19-1112",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Learning general representations of text is a fundamental problem for many natural language understanding (NLU) tasks. Previously, researchers have proposed to use language model pre-training and multi-task learning to learn robust representations. However, these methods can achieve sub-optimal performance in low-resource scenarios. Inspired by the recent success of optimization-based meta-learning algorithms, in this paper, we explore the model-agnostic meta-learning algorithm (MAML) and its variants for low-resource NLU tasks. We validate our methods on the GLUE benchmark and show that our proposed models can outperform several strong baselines. We further empirically demonstrate that the learned representations can be adapted to new tasks efficiently and effectively.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1112.pdf",
        "title": "Investigating Meta-Learning Algorithms for Low-Resource Natural Language Understanding Tasks"
    },
    {
        "paper_id": "D19-1114",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Semantic similarity modeling is central to many NLP problems such as natural language inference and question answering. Syntactic structures interact closely with semantics in learning compositional representations and alleviating long-range dependency issues. How-ever, such structure priors have not been well exploited in previous work for semantic mod-eling. To examine their effectiveness, we start with the Pairwise Word Interaction Model, one of the best models according to a recent reproducibility study, then introduce components for modeling context and structure using multi-layer BiLSTMs and TreeLSTMs. In addition, we introduce residual connections to the deep convolutional neural network component of the model. Extensive evaluations on eight benchmark datasets show that incorporating structural information contributes to consistent improvements over strong baselines.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1114.pdf",
        "title": "Incorporating Contextual and Syntactic Structures Improves Semantic Similarity Modeling"
    },
    {
        "paper_id": "D19-1122",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "We introduce a new dataset consisting of natural language interactions annotated with medical family histories, obtained during interactions with a genetic counselor and through crowdsourcing, following a questionnaire created by experts in the domain. We describe the data collection process and the annotations performed by medical professionals, including illness and personal attributes (name, age, gender, family relationships) for the patient and their family members. An initial system that performs argument identification and relation extraction shows promising results \u2013 average F-score of 0.87 on complex sentences on the targeted relations.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1122.pdf",
        "title": "Towards Extracting Medical Family History from Natural Language Interactions: A New Dataset and Baselines"
    },
    {
        "paper_id": "D19-1126",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Semantic slot filling is one of the major tasks in spoken language understanding (SLU). After a slot filling model is trained on precollected data, it is crucial to continually improve the model after deployment to learn users\u2019 new expressions. As the data amount grows, it becomes infeasible to either store such huge data and repeatedly retrain the model on all data or fine tune the model only on new data without forgetting old expressions. In this paper, we introduce a novel progressive slot filling model, ProgModel. ProgModel consists of a novel context gate that transfers previously learned knowledge to a small size expanded component; and meanwhile enables this new component to be fast trained to learn from new data. As such, ProgModel learns the new knowledge by only using new data at each time and meanwhile preserves the previously learned expressions. Our experiments show that ProgModel needs much less training time and smaller model size to outperform various model fine tuning competitors by up to 4.24% and 3.03% on two benchmark datasets.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1126.pdf",
        "title": "A Progressive Model to Enable Continual Learning for Semantic Slot Filling"
    },
    {
        "paper_id": "D19-1131",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Task-oriented dialog systems need to know when a query falls outside their range of supported intents, but current text classification corpora only define label sets that cover every example. We introduce a new dataset that includes queries that are out-of-scope\u2014i.e., queries that do not fall into any of the system\u2019s supported intents. This poses a new challenge because models cannot assume that every query at inference time belongs to a system-supported intent class. Our dataset also covers 150 intent classes over 10 domains, capturing the breadth that a production task-oriented agent must handle. We evaluate a range of benchmark classifiers on our dataset along with several different out-of-scope identification schemes. We find that while the classifiers perform well on in-scope intent classification, they struggle to identify out-of-scope queries. Our dataset and evaluation fill an important gap in the field, offering a way of more rigorously and realistically benchmarking text classification in task-driven dialog systems.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1131.pdf",
        "title": "An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction"
    },
    {
        "paper_id": "D19-1132",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Automatic data augmentation (AutoAugment) (Cubuk et al., 2019) searches for optimal perturbation policies via a controller trained using performance rewards of a sampled policy on the target task, hence reducing data-level model bias. While being a powerful algorithm, their work has focused on computer vision tasks, where it is comparatively easy to apply imperceptible perturbations without changing an image\u2019s semantic meaning. In our work, we adapt AutoAugment to automatically discover effective perturbation policies for natural language processing (NLP) tasks such as dialogue generation. We start with a pool of atomic operations that apply subtle semantic-preserving perturbations to the source inputs of a dialogue task (e.g., different POS-tag types of stopword dropout, grammatical errors, and paraphrasing). Next, we allow the controller to learn more complex augmentation policies by searching over the space of the various combinations of these atomic operations. Moreover, we also explore conditioning the controller on the source inputs of the target task, since certain strategies may not apply to inputs that do not contain that strategy\u2019s required linguistic features. Empirically, we demonstrate that both our input-agnostic and input-aware controllers discover useful data augmentation policies, and achieve significant improvements over the previous state-of-the-art, including trained on manually-designed policies.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1132.pdf",
        "title": "Automatically Learning Data Augmentation Policies for Dialogue Tasks"
    },
    {
        "paper_id": "D19-1140",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "We propose a neural machine translation (NMT) approach that, instead of pursuing adequacy and fluency (\u201chuman-oriented\u201d quality criteria), aims to generate translations that are best suited as input to a natural language processing component designed for a specific downstream task (a \u201cmachine-oriented\u201d criterion). Towards this objective, we present a reinforcement learning technique based on a new candidate sampling strategy, which exploits the results obtained on the downstream task as weak feedback. Experiments in sentiment classification of Twitter data in German and Italian show that feeding an English classifier with \u201cmachine-oriented\u201d translations significantly improves its performance. Classification results outperform those obtained with translations produced by general-purpose NMT models as well as by an approach based on reinforcement learning. Moreover, our results on both languages approximate the classification accuracy computed on gold standard English tweets.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1140.pdf",
        "title": "Machine Translation for Machines: the Sentiment Classification Use Case"
    },
    {
        "paper_id": "D19-1151",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Diacritic restoration has gained importance with the growing need for machines to understand written texts. The task is typically modeled as a sequence labeling problem and currently Bidirectional Long Short Term Memory (BiLSTM) models provide state-of-the-art results. Recently, Bai et al. (2018) show the advantages of Temporal Convolutional Neural Networks (TCN) over Recurrent Neural Networks (RNN) for sequence modeling in terms of performance and computational resources. As diacritic restoration benefits from both previous as well as subsequent timesteps, we further apply and evaluate a variant of TCN, Acausal TCN (A-TCN), which incorporates context from both directions (previous and future) rather than strictly incorporating previous context as in the case of TCN. A-TCN yields significant improvement over TCN for diacritization in three different languages: Arabic, Yoruba, and Vietnamese. Furthermore, A-TCN and BiLSTM have comparable performance, making A-TCN an efficient alternative over BiLSTM since convolutions can be trained in parallel. A-TCN is significantly faster than BiLSTM at inference time (270% 334% improvement in the amount of text diacritized per minute).",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1151.pdf",
        "title": "Efficient Convolutional Neural Networks for Diacritic Restoration"
    },
    {
        "paper_id": "D19-1168",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Document-level machine translation (MT) remains challenging due to the difficulty in efficiently using document context for translation. In this paper, we propose a hierarchical model to learn the global context for document-level neural machine translation (NMT). This is done through a sentence encoder to capture intra-sentence dependencies and a document encoder to model document-level inter-sentence consistency and coherence. With this hierarchical architecture, we feedback the extracted global document context to each word in a top-down fashion to distinguish different translations of a word according to its specific surrounding context. In addition, since large-scale in-domain document-level parallel corpora are usually unavailable, we use a two-step training strategy to take advantage of a large-scale corpus with out-of-domain parallel sentence pairs and a small-scale corpus with in-domain parallel document pairs to achieve the domain adaptability. Experimental results on several benchmark corpora show that our proposed model can significantly improve document-level translation performance over several strong NMT baselines.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1168.pdf",
        "title": "Hierarchical Modeling of Global Context for Document-Level Neural Machine Translation"
    },
    {
        "paper_id": "D19-1170",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Rapid progress has been made in the field of reading comprehension and question answering, where several systems have achieved human parity in some simplified settings. However, the performance of these models degrades significantly when they are applied to more realistic scenarios, such as answers involve various types, multiple text strings are correct answers, or discrete reasoning abilities are required. In this paper, we introduce the Multi-Type Multi-Span Network (MTMSN), a neural reading comprehension model that combines a multi-type answer predictor designed to support various answer types (e.g., span, count, negation, and arithmetic expression) with a multi-span extraction method for dynamically producing one or multiple text spans. In addition, an arithmetic expression reranking mechanism is proposed to rank expression candidates for further confirming the prediction. Experiments show that our model achieves 79.9 F1 on the DROP hidden test set, creating new state-of-the-art results. Source code (https://github.com/huminghao16/MTMSN) is released to facilitate future work.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1170.pdf",
        "title": "A Multi-Type Multi-Span Network for Reading Comprehension that Requires Discrete Reasoning"
    },
    {
        "paper_id": "D19-1171",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Supervised training of neural models to duplicate question detection in community Question Answering (CQA) requires large amounts of labeled question pairs, which can be costly to obtain. To minimize this cost, recent works thus often used alternative methods, e.g., adversarial domain adaptation. In this work, we propose two novel methods\u2014weak supervision using the title and body of a question, and the automatic generation of duplicate questions\u2014and show that both can achieve improved performances even though they do not require any labeled data. We provide a comparison of popular training strategies and show that our proposed approaches are more effective in many cases because they can utilize larger amounts of data from the CQA forums. Finally, we show that weak supervision with question title and body information is also an effective method to train CQA answer selection models without direct answer supervision.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1171.pdf",
        "title": "Neural Duplicate Question Detection without Labeled Training Data"
    },
    {
        "paper_id": "D19-1172",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "The ability to ask clarification questions is essential for knowledge-based question answering (KBQA) systems, especially for handling ambiguous phenomena. Despite its importance, clarification has not been well explored in current KBQA systems. Further progress requires supervised resources for training and evaluation, and powerful models for clarification-related text understanding and generation. In this paper, we construct a new clarification dataset, CLAQUA, with nearly 40K open-domain examples. The dataset supports three serial tasks: given a question, identify whether clarification is needed; if yes, generate a clarification question; then predict answers base on external user feedback. We provide representative baselines for these tasks and further introduce a coarse-to-fine model for clarification question generation. Experiments show that the proposed model achieves better performance than strong baselines. The further analysis demonstrates that our dataset brings new challenges and there still remain several unsolved problems, like reasonable automatic evaluation metrics for clarification question generation and powerful models for handling entity sparsity.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1172.pdf",
        "title": "Asking Clarification Questions in Knowledge-Based Question Answering"
    },
    {
        "paper_id": "D19-1184",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Neural models of dialog rely on generalized latent representations of language. This paper introduces a novel training procedure which explicitly learns multiple representations of language at several levels of granularity. The multi-granularity training algorithm modifies the mechanism by which negative candidate responses are sampled in order to control the granularity of learned latent representations. Strong performance gains are observed on the next utterance retrieval task using both the MultiWOZ dataset and the Ubuntu dialog corpus. Analysis significantly demonstrates that multiple granularities of representation are being learned, and that multi-granularity training facilitates better transfer to downstream tasks.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1184.pdf",
        "title": "Multi-Granularity Representations of Dialog"
    },
    {
        "paper_id": "D19-1188",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Neural conversation systems generate responses based on the sequence-to-sequence (SEQ2SEQ) paradigm. Typically, the model is equipped with a single set of learned parameters to generate responses for given input contexts. When confronting diverse conversations, its adaptability is rather limited and the model is hence prone to generate generic responses. In this work, we propose an Adaptive Neural Dialogue generation model, AdaND, which manages various conversations with conversation-specific parameterization. For each conversation, the model generates parameters of the encoder-decoder by referring to the input context. In particular, we propose two adaptive parameterization mechanisms: a context-aware and a topic-aware parameterization mechanism. The context-aware parameterization directly generates the parameters by capturing local semantics of the given context. The topic-aware parameterization enables parameter sharing among conversations with similar topics by first inferring the latent topics of the given context and then generating the parameters with respect to the distributional topics. Extensive experiments conducted on a large-scale real-world conversational dataset show that our model achieves superior performance in terms of both quantitative metrics and human evaluations.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1188.pdf",
        "title": "Adaptive Parameterization for Neural Dialogue Generation"
    },
    {
        "paper_id": "D19-1192",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Context modeling has a pivotal role in open domain conversation. Existing works either use heuristic methods or jointly learn context modeling and response generation with an encoder-decoder framework. This paper proposes an explicit context rewriting method, which rewrites the last utterance by considering context history. We leverage pseudo-parallel data and elaborate a context rewriting network, which is built upon the CopyNet with the reinforcement learning method. The rewritten utterance is beneficial to candidate retrieval, explainable context modeling, as well as enabling to employ a single-turn framework to the multi-turn scenario. The empirical results show that our model outperforms baselines in terms of the rewriting quality, the multi-turn response generation, and the end-to-end retrieval-based chatbots.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1192.pdf",
        "title": "Unsupervised Context Rewriting for Open Domain Conversation"
    },
    {
        "paper_id": "D19-1193",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "This paper proposes a dually interactive matching network (DIM) for presenting the personalities of dialogue agents in retrieval-based chatbots. This model develops from the interactive matching network (IMN) which models the matching degree between a context composed of multiple utterances and a response candidate. Compared with previous persona fusion approach which enhances the representation of a context by calculating its similarity with a given persona, the DIM model adopts a dual matching architecture, which performs interactive matching between responses and contexts and between responses and personas respectively for ranking response candidates. Experimental results on PERSONA-CHAT dataset show that the DIM model outperforms its baseline model, i.e., IMN with persona fusion, by a margin of 14.5% and outperforms the present state-of-the-art model by a margin of 27.7% in terms of top-1 accuracy hits@1.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1193.pdf",
        "title": "Dually Interactive Matching Network for Personalized Response Selection in Retrieval-Based Chatbots"
    },
    {
        "paper_id": "D19-1194",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Data-driven, knowledge-grounded neural conversation models are capable of generating more informative responses. However, these models have not yet demonstrated that they can zero-shot adapt to updated, unseen knowledge graphs. This paper proposes a new task about how to apply dynamic knowledge graphs in neural conversation model and presents a novel TV series conversation corpus (DyKgChat) for the task. Our new task and corpus aids in understanding the influence of dynamic knowledge graphs on responses generation. Also, we propose a preliminary model that selects an output from two networks at each time step: a sequence-to-sequence model (Seq2Seq) and a multi-hop reasoning model, in order to support dynamic knowledge graphs. To benchmark this new task and evaluate the capability of adaptation, we introduce several evaluation metrics and the experiments show that our proposed approach outperforms previous knowledge-grounded conversation models. The proposed corpus and model can motivate the future research directions.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1194.pdf",
        "title": "DyKgChat: Benchmarking Dialogue Generation Grounding on Dynamic Knowledge Graphs"
    },
    {
        "paper_id": "D19-1197",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "We study open domain response generation with limited message-response pairs. The problem exists in real-world applications but is less explored by the existing work. Since the paired data now is no longer enough to train a neural generation model, we consider leveraging the large scale of unpaired data that are much easier to obtain, and propose response generation with both paired and unpaired data. The generation model is defined by an encoder-decoder architecture with templates as prior, where the templates are estimated from the unpaired data as a neural hidden semi-markov model. By this means, response generation learned from the small paired data can be aided by the semantic and syntactic knowledge in the large unpaired data. To balance the effect of the prior and the input message to response generation, we propose learning the whole generation model with an adversarial approach. Empirical studies on question response generation and sentiment response generation indicate that when only a few pairs are available, our model can significantly outperform several state-of-the-art response generation models in terms of both automatic and human evaluation.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1197.pdf",
        "title": "Low-Resource Response Generation with Template Prior"
    },
    {
        "paper_id": "D19-1199",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Previous research on dialogue systems generally focuses on the conversation between two participants, yet multi-party conversations which involve more than two participants within one session bring up a more complicated but realistic scenario. In real multi- party conversations, we can observe who is speaking, but the addressee information is not always explicit. In this paper, we aim to tackle the challenge of identifying all the miss- ing addressees in a conversation session. To this end, we introduce a novel who-to-whom (W2W) model which models users and utterances in the session jointly in an interactive way. We conduct experiments on the benchmark Ubuntu Multi-Party Conversation Corpus and the experimental results demonstrate that our model outperforms baselines with consistent improvements.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1199.pdf",
        "title": "Who Is Speaking to Whom? Learning to Identify Utterance Addressee in Multi-Party Conversations"
    },
    {
        "paper_id": "D19-1201",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Variational autoencoders (VAEs) and Wasserstein autoencoders (WAEs) have achieved noticeable progress in open-domain response generation. Through introducing latent variables in continuous space, these models are capable of capturing utterance-level semantics, e.g., topic, syntactic properties, and thus can generate informative and diversified responses. In this work, we improve the WAE for response generation. In addition to the utterance-level information, we also model user-level information in latent continue space. Specifically, we embed user-level and utterance-level information into two multimodal distributions, and combine these two multimodal distributions into a mixed distribution. This mixed distribution will be used as the prior distribution of WAE in our proposed model, named as PersonaWAE. Experimental results on a large-scale real-world dataset confirm the superiority of our model for generating informative and personalized responses, where both automatic and human evaluations outperform state-of-the-art models.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1201.pdf",
        "title": "Modeling Personalization in Continuous Space for Response Generation via Augmented Wasserstein Autoencoders"
    },
    {
        "paper_id": "D19-1205",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Dialogue Acts play an important role in conversation modeling. Research has shown the utility of dialogue acts for the response selection task, however, the underlying assumption is that the dialogue acts are readily available, which is impractical, as dialogue acts are rarely available for new conversations. This paper proposes an end-to-end multi-task model for conversation modeling, which is optimized for two tasks, dialogue act prediction and response selection, with the latter being the task of interest. It proposes a novel way of combining the predicted dialogue acts of context and response with the context (previous utterances) and response (follow-up utterance) in a crossway fashion, such that, it achieves at par performance for the response selection task compared to the model that uses actual dialogue acts. Through experiments on two well known datasets, we demonstrate that the multi-task model not only improves the accuracy of the dialogue act prediction task but also improves the MRR for the response selection task. Also, the cross-stitching of dialogue acts of context and response with the context and response is better than using either one of them individually.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1205.pdf",
        "title": "A Practical Dialogue-Act-Driven Conversation Model for Multi-Turn Response Selection"
    },
    {
        "paper_id": "D19-1208",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Constructing an organized dataset comprised of a large number of images and several captions for each image is a laborious task, which requires vast human effort. On the other hand, collecting a large number of images and sentences separately may be immensely easier. In this paper, we develop a novel data-efficient semi-supervised framework for training an image captioning model. We leverage massive unpaired image and caption data by learning to associate them. To this end, our proposed semi-supervised learning method assigns pseudo-labels to unpaired samples via Generative Adversarial Networks to learn the joint distribution of image and caption. To evaluate, we construct scarcely-paired COCO dataset, a modified version of MS COCO caption dataset. The empirical results show the effectiveness of our method compared to several strong baselines, especially when the amount of the paired samples are scarce.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1208.pdf",
        "title": "Image Captioning with Very Scarce Supervised Data: Adversarial Semi-Supervised Learning Approach"
    },
    {
        "paper_id": "D19-1211",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Humor is a unique and creative communicative behavior often displayed during social interactions. It is produced in a multimodal manner, through the usage of words (text), gestures (visual) and prosodic cues (acoustic). Understanding humor from these three modalities falls within boundaries of multimodal language; a recent research trend in natural language processing that models natural language as it happens in face-to-face communication. Although humor detection is an established research area in NLP, in a multimodal context it has been understudied. This paper presents a diverse multimodal dataset, called UR-FUNNY, to open the door to understanding multimodal language used in expressing humor. The dataset and accompanying studies, present a framework in multimodal humor detection for the natural language processing community. UR-FUNNY is publicly available for research.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1211.pdf",
        "title": "UR-FUNNY: A Multimodal Language Dataset for Understanding Humor"
    },
    {
        "paper_id": "D19-1213",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "In the current video captioning models, the video frames are collected in one network and the semantics are mixed into one feature, which not only increase the difficulty of the caption decoding, but also decrease the interpretability of the captioning models. To address these problems, we propose an Adaptive Semantic Guidance Network (ASGN), which instantiates the whole video semantics to different POS-aware semantics with the supervision of part of speech (POS) tag. In the encoding process, the POS tag activates the related neurons and parses the whole semantic information into corresponding encoded video representations. Furthermore, the potential of the model is stimulated by the POS-aware video features. In the decoding process, the related video features of noun and verb are used as the supervision to construct a new adaptive attention model which can decide whether to attend to the video feature or not. With the explicit improving of the interpretability of the network, the learning process is more transparent and the results are more predictable. Extensive experiments demonstrate the effectiveness of our model when compared with state-of-the-art models.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1213.pdf",
        "title": "Guiding the Flowing of Semantics: Interpretable Video Captioning via POS Tag"
    },
    {
        "paper_id": "D19-1214",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Intent detection and slot filling are two main tasks for building a spoken language understanding (SLU) system. The two tasks are closely tied and the slots often highly depend on the intent. In this paper, we propose a novel framework for SLU to better incorporate the intent information, which further guiding the slot filling. In our framework, we adopt a joint model with Stack-Propagation which can directly use the intent information as input for slot filling, thus to capture the intent semantic knowledge. In addition, to further alleviate the error propagation, we perform the token-level intent detection for the Stack-Propagation framework. Experiments on two publicly datasets show that our model achieves the state-of-the-art performance and outperforms other previous methods by a large margin. Finally, we use the Bidirectional Encoder Representation from Transformer (BERT) model in our framework, which further boost our performance in SLU task.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1214.pdf",
        "title": "A Stack-Propagation Framework with Token-Level Intent Detection for Spoken Language Understanding"
    },
    {
        "paper_id": "D19-1217",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Video dialog is a new and challenging task, which requires the agent to answer questions combining video information with dialog history. And different from single-turn video question answering, the additional dialog history is important for video dialog, which often includes contextual information for the question. Existing visual dialog methods mainly use RNN to encode the dialog history as a single vector representation, which might be rough and straightforward. Some more advanced methods utilize hierarchical structure, attention and memory mechanisms, which still lack an explicit reasoning process. In this paper, we introduce a novel progressive inference mechanism for video dialog, which progressively updates query information based on dialog history and video content until the agent think the information is sufficient and unambiguous. In order to tackle the multi-modal fusion problem, we propose a cross-transformer module, which could learn more fine-grained and comprehensive interactions both inside and between the modalities. And besides answer generation, we also consider question generation, which is more challenging but significant for a complete video dialog system. We evaluate our method on two large-scale datasets, and the extensive experiments show the effectiveness of our method.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1217.pdf",
        "title": "Video Dialog via Progressive Inference and Cross-Transformer"
    },
    {
        "paper_id": "D19-1230",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Negation is a universal but complicated linguistic phenomenon, which has received considerable attention from the NLP community over the last decade, since a negated statement often carries both an explicit negative focus and implicit positive meanings. For the sake of understanding a negated statement, it is critical to precisely detect the negative focus in context. However, how to capture contextual information for negative focus detection is still an open challenge. To well address this, we come up with an attention-based neural network to model contextual information. In particular, we introduce a framework which consists of a Bidirectional Long Short-Term Memory (BiLSTM) neural network and a Conditional Random Fields (CRF) layer to effectively encode the order information and the long-range context dependency in a sentence. Moreover, we design two types of attention mechanisms, word-level contextual attention and topic-level contextual attention, to take advantage of contextual information across sentences from both the word perspective and the topic perspective, respectively. Experimental results on the SEM\u201912 shared task corpus show that our approach achieves the best performance on negative focus detection, yielding an absolute improvement of 2.11% over the state-of-the-art. This demonstrates the great effectiveness of the two types of contextual attention mechanisms.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1230.pdf",
        "title": "Negative Focus Detection via Contextual Attention Mechanism"
    },
    {
        "paper_id": "D19-1231",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Recently, neural approaches to coherence modeling have achieved state-of-the-art results in several evaluation tasks. However, we show that most of these models often fail on harder tasks with more realistic application scenarios. In particular, the existing models underperform on tasks that require the model to be sensitive to local contexts such as candidate ranking in conversational dialogue and in machine translation. In this paper, we propose a unified coherence model that incorporates sentence grammar, inter-sentence coherence relations, and global coherence patterns into a common neural framework. With extensive experiments on local and global discrimination tasks, we demonstrate that our proposed model outperforms existing models by a good margin, and establish a new state-of-the-art.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1231.pdf",
        "title": "A Unified Neural Coherence Model"
    },
    {
        "paper_id": "D19-1233",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Rhetorical structure trees have been shown to be useful for several document-level tasks including summarization and document classification. Previous approaches to RST parsing have used discriminative models; however, these are less sample efficient than generative models, and RST parsing datasets are typically small. In this paper, we present the first generative model for RST parsing. Our model is a document-level RNN grammar (RNNG) with a bottom-up traversal order. We show that, for our parser\u2019s traversal order, previous beam search algorithms for RNNGs have a left-branching bias which is ill-suited for RST parsing.We develop a novel beam search algorithm that keeps track of both structure-and word-generating actions without exhibit-ing this branching bias and results in absolute improvements of 6.8 and 2.9 on unlabelled and labelled F1 over previous algorithms. Overall, our generative model outperforms a discriminative model with the same features by 2.6 F1points and achieves performance comparable to the state-of-the-art, outperforming all published parsers from a recent replication study that do not use additional training data",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1233.pdf",
        "title": "Neural Generative Rhetorical Structure Parsing"
    },
    {
        "paper_id": "D19-1234",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "This paper provides a detailed comparison of a data programming approach with (i) off-the-shelf, state-of-the-art deep learning architectures that optimize their representations (BERT) and (ii) handcrafted-feature approaches previously used in the discourse analysis literature. We compare these approaches on the task of learning discourse structure for multi-party dialogue. The data programming paradigm offered by the Snorkel framework allows a user to label training data using expert-composed heuristics, which are then transformed via the \u201cgenerative step\u201d into probability distributions of the class labels given the data. We show that on our task the generative model outperforms both deep learning architectures as well as more traditional ML approaches when learning discourse structure\u2014it even outperforms the combination of deep learning methods and hand-crafted features. We also implement several strategies for \u201cdecoding\u201d our generative model output in order to improve our results. We conclude that weak supervision methods hold great promise as a means for creating and improving data sets for discourse structure.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1234.pdf",
        "title": "Weak Supervision for Learning Discourse Structure"
    },
    {
        "paper_id": "D19-1239",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Social media provides a timely yet challenging data source for adverse drug reaction (ADR) detection. Existing dictionary-based, semi-supervised learning approaches are intrinsically limited by the coverage and maintainability of laymen health vocabularies. In this paper, we introduce a data augmentation approach that leverages variational autoencoders to learn high-quality data distributions from a large unlabeled dataset, and subsequently, to automatically generate a large labeled training set from a small set of labeled samples. This allows for efficient social-media ADR detection with low training and re-training costs to adapt to the changes and emergence of informal medical laymen terms. An extensive evaluation performed on Twitter and Reddit data shows that our approach matches the performance of fully-supervised approaches while requiring only 25% of training data.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1239.pdf",
        "title": "Training Data Augmentation for Detecting Adverse Drug Reactions in User-Generated Content"
    },
    {
        "paper_id": "D19-1247",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "We tackle the task of question generation over knowledge bases. Conventional methods for this task neglect two crucial research issues: 1) the given predicate needs to be expressed; 2) the answer to the generated question needs to be definitive. In this paper, we strive toward the above two issues via incorporating diversified contexts and answer-aware loss. Specifically, we propose a neural encoder-decoder model with multi-level copy mechanisms to generate such questions. Furthermore, the answer aware loss is introduced to make generated questions corresponding to more definitive answers. Experiments demonstrate that our model achieves state-of-the-art performance. Meanwhile, such generated question is able to express the given predicate and correspond to a definitive answer.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1247.pdf",
        "title": "Generating Questions for Knowledge Bases via Incorporating Diversified Contexts and Answer-Aware Loss"
    },
    {
        "paper_id": "D19-1252",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "We present Unicoder, a universal language encoder that is insensitive to different languages. Given an arbitrary NLP task, a model can be trained with Unicoder using training data in one language and directly applied to inputs of the same task in other languages. Comparing to similar efforts such as Multilingual BERT and XLM , three new cross-lingual pre-training tasks are proposed, including cross-lingual word recovery, cross-lingual paraphrase classification and cross-lingual masked language model. These tasks help Unicoder learn the mappings among different languages from more perspectives. We also find that doing fine-tuning on multiple languages together can bring further improvement. Experiments are performed on two tasks: cross-lingual natural language inference (XNLI) and cross-lingual question answering (XQA), where XLM is our baseline. On XNLI, 1.8% averaged accuracy improvement (on 15 languages) is obtained. On XQA, which is a new cross-lingual dataset built by us, 5.5% averaged accuracy improvement (on French and German) is obtained.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1252.pdf",
        "title": "Unicoder: A Universal Language Encoder by Pre-training with Multiple Cross-lingual Tasks"
    },
    {
        "paper_id": "D19-1254",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "In this paper, we focus on unsupervised domain adaptation for Machine Reading Comprehension (MRC), where the source domain has a large amount of labeled data, while only unlabeled passages are available in the target domain. To this end, we propose an Adversarial Domain Adaptation framework (AdaMRC), where (i) pseudo questions are first generated for unlabeled passages in the target domain, and then (ii) a domain classifier is incorporated into an MRC model to predict which domain a given passage-question pair comes from. The classifier and the passage-question encoder are jointly trained using adversarial learning to enforce domain-invariant representation learning. Comprehensive evaluations demonstrate that our approach (i) is generalizable to different MRC models and datasets, (ii) can be combined with pre-trained large-scale language models (such as ELMo and BERT), and (iii) can be extended to semi-supervised learning.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1254.pdf",
        "title": "Adversarial Domain Adaptation for Machine Reading Comprehension"
    },
    {
        "paper_id": "D19-1255",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Commonsense and background knowledge is required for a QA model to answer many nontrivial questions. Different from existing work on knowledge-aware QA, we focus on a more challenging task of leveraging external knowledge to generate answers in natural language for a given question with context. In this paper, we propose a new neural model, Knowledge-Enriched Answer Generator (KEAG), which is able to compose a natural answer by exploiting and aggregating evidence from all four information sources available: question, passage, vocabulary and knowledge. During the process of answer generation, KEAG adaptively determines when to utilize symbolic knowledge and which fact from the knowledge is useful. This allows the model to exploit external knowledge that is not explicitly stated in the given text, but that is relevant for generating an answer. The empirical study on public benchmark of answer generation demonstrates that KEAG improves answer quality over models without knowledge and existing knowledge-aware models, confirming its effectiveness in leveraging knowledge.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1255.pdf",
        "title": "Incorporating External Knowledge into Machine Reading for Generative Question Answering"
    },
    {
        "paper_id": "D19-1256",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Answering multiple-choice questions in a setting in which no supporting documents are explicitly provided continues to stand as a core problem in natural language processing. The contribution of this article is two-fold. First, it describes a method which can be used to semantically rank documents extracted from Wikipedia or similar natural language corpora. Second, we propose a model employing the semantic ranking that holds the first place in two of the most popular leaderboards for answering multiple-choice questions: ARC Easy and Challenge. To achieve this, we introduce a self-attention based neural network that latently learns to rank documents by their importance related to a given question, whilst optimizing the objective of predicting the correct answer. These documents are considered relevant contexts for the underlying question. We have published the ranked documents so that they can be used off-the-shelf to improve downstream decision models.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1256.pdf",
        "title": "Answering questions by learning to rank - Learning to rank by answering questions"
    },
    {
        "paper_id": "D19-1259",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "We introduce PubMedQA, a novel biomedical question answering (QA) dataset collected from PubMed abstracts. The task of PubMedQA is to answer research questions with yes/no/maybe (e.g.: Do preoperative statins reduce atrial fibrillation after coronary artery bypass grafting?) using the corresponding abstracts. PubMedQA has 1k expert-annotated, 61.2k unlabeled and 211.3k artificially generated QA instances. Each PubMedQA instance is composed of (1) a question which is either an existing research article title or derived from one, (2) a context which is the corresponding abstract without its conclusion, (3) a long answer, which is the conclusion of the abstract and, presumably, answers the research question, and (4) a yes/no/maybe answer which summarizes the conclusion. PubMedQA is the first QA dataset where reasoning over biomedical research texts, especially their quantitative contents, is required to answer the questions. Our best performing model, multi-phase fine-tuning of BioBERT with long answer bag-of-word statistics as additional supervision, achieves 68.1% accuracy, compared to single human performance of 78.0% accuracy and majority-baseline of 55.2% accuracy, leaving much room for improvement. PubMedQA is publicly available at https://pubmedqa.github.io.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1259.pdf",
        "title": "PubMedQA: A Dataset for Biomedical Research Question Answering"
    },
    {
        "paper_id": "D19-1268",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Incompleteness is a common problem for existing knowledge graphs (KGs), and the completion of KG which aims to predict links between entities is challenging. Most existing KG completion methods only consider the direct relation between nodes and ignore the relation paths which contain useful information for link prediction. Recently, a few methods take relation paths into consideration but pay less attention to the order of relations in paths which is important for reasoning. In addition, these path-based models always ignore nonlinear contributions of path features for link prediction. To solve these problems, we propose a novel KG completion method named OPTransE. Instead of embedding both entities of a relation into the same latent space as in previous methods, we project the head entity and the tail entity of each relation into different spaces to guarantee the order of relations in the path. Meanwhile, we adopt a pooling strategy to extract nonlinear and complex features of different paths to further improve the performance of link prediction. Experimental results on two benchmark datasets show that the proposed model OPTransE performs better than state-of-the-art methods.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1268.pdf",
        "title": "Representation Learning with Ordered Relation Paths for Knowledge Graph Completion"
    },
    {
        "paper_id": "D19-1272",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "In this paper, we present a novel method for measurably adjusting the semantics of text while preserving its sentiment and fluency, a task we call semantic text exchange. This is useful for text data augmentation and the semantic correction of text generated by chatbots and virtual assistants. We introduce a pipeline called SMERTI that combines entity replacement, similarity masking, and text infilling. We measure our pipeline\u2019s success by its Semantic Text Exchange Score (STES): the ability to preserve the original text\u2019s sentiment and fluency while adjusting semantic content. We propose to use masking (replacement) rate threshold as an adjustable parameter to control the amount of semantic change in the text. Our experiments demonstrate that SMERTI can outperform baseline models on Yelp reviews, Amazon reviews, and news headlines.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1272.pdf",
        "title": "Keep Calm and Switch On! Preserving Sentiment and Fluency in Semantic Text Exchange"
    },
    {
        "paper_id": "D19-1278",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Semantic parses are directed acyclic graphs (DAGs), so semantic parsing should be modeled as graph prediction. But predicting graphs presents difficult technical challenges, so it is simpler and more common to predict the *linearized* graphs found in semantic parsing datasets using well-understood sequence models. The cost of this simplicity is that the predicted strings may not be well-formed graphs. We present recurrent neural network DAG grammars, a graph-aware sequence model that generates only well-formed graphs while sidestepping many difficulties in graph prediction. We test our model on the Parallel Meaning Bank\u2014a multilingual semantic graphbank. Our approach yields competitive results in English and establishes the first results for German, Italian and Dutch.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1278.pdf",
        "title": "Semantic graph parsing with recurrent neural network DAG grammars"
    },
    {
        "paper_id": "D19-1282",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Commonsense reasoning aims to empower machines with the human ability to make presumptions about ordinary situations in our daily life. In this paper, we propose a textual inference framework for answering commonsense questions, which effectively utilizes external, structured commonsense knowledge graphs to perform explainable inferences. The framework first grounds a question-answer pair from the semantic space to the knowledge-based symbolic space as a schema graph, a related sub-graph of external knowledge graphs. It represents schema graphs with a novel knowledge-aware graph network module named KagNet, and finally scores answers with graph representations. Our model is based on graph convolutional networks and LSTMs, with a hierarchical path-based attention mechanism. The intermediate attention scores make it transparent and interpretable, which thus produce trustworthy inferences. Using ConceptNet as the only external resource for Bert-based models, we achieved state-of-the-art performance on the CommonsenseQA, a large-scale dataset for commonsense reasoning.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1282.pdf",
        "title": "KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning"
    },
    {
        "paper_id": "D19-1284",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Many question answering (QA) tasks only provide weak supervision for how the answer should be computed. For example, TriviaQA answers are entities that can be mentioned multiple times in supporting documents, while DROP answers can be computed by deriving many different equations from numbers in the reference text. In this paper, we show it is possible to convert such tasks into discrete latent variable learning problems with a precomputed, task-specific set of possible solutions (e.g. different mentions or equations) that contains one correct option. We then develop a hard EM learning scheme that computes gradients relative to the most likely solution at each update. Despite its simplicity, we show that this approach significantly outperforms previous methods on six QA tasks, including absolute gains of 2\u201310%, and achieves the state-of-the-art on five of them. Using hard updates instead of maximizing marginal likelihood is key to these results as it encourages the model to find the one correct answer, which we show through detailed qualitative analysis.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1284.pdf",
        "title": "A Discrete Hard EM Approach for Weakly Supervised Question Answering"
    },
    {
        "paper_id": "D19-1291",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Argumentation is a type of discourse where speakers try to persuade their audience about the reasonableness of a claim by presenting supportive arguments. Most work in argument mining has focused on modeling arguments in monologues. We propose a computational model for argument mining in online persuasive discussion forums that brings together the micro-level (argument as product) and macro-level (argument as process) models of argumentation. Fundamentally, this approach relies on identifying relations between components of arguments in a discussion thread. Our approach for relation prediction uses contextual information in terms of fine-tuning a pre-trained language model and leveraging discourse relations based on Rhetorical Structure Theory. We additionally propose a candidate selection method to automatically predict what parts of one\u2019s argument will be targeted by other participants in the discussion. Our models obtain significant improvements compared to recent state-of-the-art approaches using pointer networks and a pre-trained language model.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1291.pdf",
        "title": "AMPERSAND: Argument Mining for PERSuAsive oNline Discussions"
    },
    {
        "paper_id": "D19-1294",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "The ongoing neural revolution in machine translation has made it easier to model larger contexts beyond the sentence-level, which can potentially help resolve some discourse-level ambiguities such as pronominal anaphora, thus enabling better translations. Unfortunately, even when the resulting improvements are seen as substantial by humans, they remain virtually unnoticed by traditional automatic evaluation measures like BLEU, as only a few words end up being affected. Thus, specialized evaluation measures are needed. With this aim in mind, we contribute an extensive, targeted dataset that can be used as a test suite for pronoun translation, covering multiple source languages and different pronoun errors drawn from real system translations, for English. We further propose an evaluation measure to differentiate good and bad pronoun translations. We also conduct a user study to report correlations with human judgments.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1294.pdf",
        "title": "Evaluating Pronominal Anaphora in Machine Translation: An Evaluation Measure and a Test Suite"
    },
    {
        "paper_id": "D19-1296",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "We present a method for extracting causality knowledge from Wikipedia, such as Protectionism -> Trade war, where the cause and effect entities correspond to Wikipedia articles. Such causality knowledge is easy to verify by reading corresponding Wikipedia articles, to translate to multiple languages through Wikidata, and to connect to knowledge bases derived from Wikipedia. Our method exploits Wikipedia article sections that describe causality and the redundancy stemming from the multilinguality of Wikipedia. Experiments showed that our method achieved precision and recall above 98% and 64%, respectively. In particular, it could extract causalities whose cause and effect were written distantly in a Wikipedia article. We have released the code and data for further research.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1296.pdf",
        "title": "Weakly Supervised Multilingual Causality Extraction from Wikipedia"
    },
    {
        "paper_id": "D19-1298",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "In this paper, we propose a novel neural single-document extractive summarization model for long documents, incorporating both the global context of the whole document and the local context within the current topic. We evaluate the model on two datasets of scientific papers , Pubmed and arXiv, where it outperforms previous work, both extractive and abstractive models, on ROUGE-1, ROUGE-2 and METEOR scores. We also show that, consistently with our goal, the benefits of our method become stronger as we apply it to longer documents. Rather surprisingly, an ablation study indicates that the benefits of our model seem to come exclusively from modeling the local context, even for the longest documents.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1298.pdf",
        "title": "Extractive Summarization of Long Documents by Combining Global and Local Context"
    },
    {
        "paper_id": "D19-1300",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "In this work, we re-examine the problem of extractive text summarization for long documents. We observe that the process of extracting summarization of human can be divided into two stages: 1) a rough reading stage to look for sketched information, and 2) a subsequent careful reading stage to select key sentences to form the summary. By simulating such a two-stage process, we propose a novel approach for extractive summarization. We formulate the problem as a contextual-bandit problem and solve it with policy gradient. We adopt a convolutional neural network to encode gist of paragraphs for rough reading, and a decision making policy with an adapted termination mechanism for careful reading. Experiments on the CNN and DailyMail datasets show that our proposed method can provide high-quality summaries with varied length, and significantly outperform the state-of-the-art extractive methods in terms of ROUGE metrics.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1300.pdf",
        "title": "Reading Like HER: Human Reading Inspired Extractive Summarization"
    },
    {
        "paper_id": "D19-1301",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "We propose a contrastive attention mechanism to extend the sequence-to-sequence framework for abstractive sentence summarization task, which aims to generate a brief summary of a given source sentence. The proposed contrastive attention mechanism accommodates two categories of attention: one is the conventional attention that attends to relevant parts of the source sentence, the other is the opponent attention that attends to irrelevant or less relevant parts of the source sentence. Both attentions are trained in an opposite way so that the contribution from the conventional attention is encouraged and the contribution from the opponent attention is discouraged through a novel softmax and softmin functionality. Experiments on benchmark datasets show that, the proposed contrastive attention mechanism is more focused on the relevant parts for the summary than the conventional attention mechanism, and greatly advances the state-of-the-art performance on the abstractive sentence summarization task. We release the code at https://github.com/travel-go/ Abstractive-Text-Summarization.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1301.pdf",
        "title": "Contrastive Attention Mechanism for Abstractive Sentence Summarization"
    },
    {
        "paper_id": "D19-1307",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Reinforcement Learning (RL)based document summarisation systems yield state-of-the-art performance in terms of ROUGE scores, because they directly use ROUGE as the rewards during training. However, summaries with high ROUGE scores often receive low human judgement. To find a better reward function that can guide RL to generate human-appealing summaries, we learn a reward function from human ratings on 2,500 summaries. Our reward function only takes the document and system summary as input. Hence, once trained, it can be used to train RL based summarisation systems without using any reference summaries. We show that our learned rewards have significantly higher correlation with human ratings than previous approaches. Human evaluation experiments show that, compared to the state-of-the-art supervised-learning systems and ROUGE-as-rewards RL summarisation systems, the RL systems using our learned rewards during training generate summaries with higher human ratings. The learned reward function and our source code are available at https://github.com/yg211/summary-reward-no-reference.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1307.pdf",
        "title": "Better Rewards Yield Better Summaries: Learning to Summarise Without References"
    },
    {
        "paper_id": "D19-1308",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Generating diverse sequences is important in many NLP applications such as question generation or summarization that exhibit semantically one-to-many relationships between source and the target sequences. We present a method to explicitly separate diversification from generation using a general plug-and-play module (called SELECTOR) that wraps around and guides an existing encoder-decoder model. The diversification stage uses a mixture of experts to sample different binary masks on the source sequence for diverse content selection. The generation stage uses a standard encoder-decoder model given each selected content from the source sequence. Due to the non-differentiable nature of discrete sampling and the lack of ground truth labels for binary mask, we leverage a proxy for ground truth mask and adopt stochastic hard-EM for training. In question generation (SQuAD) and abstractive summarization (CNN-DM), our method demonstrates significant improvements in accuracy, diversity and training efficiency, including state-of-the-art top-1 accuracy in both datasets, 6% gain in top-5 accuracy, and 3.7 times faster training over a state-of-the-art model. Our code is publicly available at https://github.com/clovaai/FocusSeq2Seq.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1308.pdf",
        "title": "Mixture Content Selection for Diverse Sequence Generation"
    },
    {
        "paper_id": "D19-1312",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Referring Expression Generation (REG) is the task of generating contextually appropriate references to entities. A limitation of existing REG systems is that they rely on entity-specific supervised training, which means that they cannot handle entities not seen during training. In this study, we address this in two ways. First, we propose task setups in which we specifically test a REG system\u2019s ability to generalize to entities not seen during training. Second, we propose a profile-based deep neural network model, ProfileREG, which encodes both the local context and an external profile of the entity to generate reference realizations. Our model generates tokens by learning to choose between generating pronouns, generating from a fixed vocabulary, or copying a word from the profile. We evaluate our model on three different splits of the WebNLG dataset, and show that it outperforms competitive baselines in all settings according to automatic and human evaluations.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1312.pdf",
        "title": "Referring Expression Generation Using Entity Profiles"
    },
    {
        "paper_id": "D19-1313",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Paraphrasing plays an important role in various natural language processing (NLP) tasks, such as question answering, information retrieval and sentence simplification. Recently, neural generative models have shown promising results in paraphrase generation. However, prior work mainly focused on single paraphrase generation, while ignoring the fact that diversity is essential for enhancing generalization capability and robustness of downstream applications. Few works have been done to solve diverse paraphrase generation. In this paper, we propose a novel approach with two discriminators and multiple generators to generate a variety of different paraphrases. A reinforcement learning algorithm is applied to train our model. Our experiments on two real-world datasets demonstrate that our model not only gains a significant increase in diversity but also improves generation quality over several state-of-the-art baselines.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1313.pdf",
        "title": "Exploring Diverse Expressions for Paraphrase Generation"
    },
    {
        "paper_id": "D19-1315",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "The automated generation of information indicating the characteristics of articles such as headlines, key phrases, summaries and categories helps writers to alleviate their workload. Previous research has tackled these tasks using neural abstractive summarization and classification methods. However, the outputs may be inconsistent if they are generated individually. The purpose of our study is to generate multiple outputs consistently. We introduce a multi-task learning model with a shared encoder and multiple decoders for each task. We propose a novel loss function called hierarchical consistency loss to maintain consistency among the attention weights of the decoders. To evaluate the consistency, we employ a human evaluation. The results show that our model generates more consistent headlines, key phrases and categories. In addition, our model outperforms the baseline model on the ROUGE scores, and generates more adequate and fluent headlines.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1315.pdf",
        "title": "Keeping Consistency of Sentence Generation and Document Classification with Multi-Task Learning"
    },
    {
        "paper_id": "D19-1316",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "In this paper, we introduce a novel task called feedback comment generation \u2014 a task of automatically generating feedback comments such as a hint or an explanatory note for writing learning for non-native learners of English. There has been almost no work on this task nor corpus annotated with feedback comments. We have taken the first step by creating learner corpora consisting of approximately 1,900 essays where all preposition errors are manually annotated with feedback comments. We have tested three baseline methods on the dataset, showing that a simple neural retrieval-based method sets a baseline performance with an F-measure of 0.34 to 0.41. Finally, we have looked into the results to explore what modifications we need to make to achieve better performance. We also have explored problems unaddressed in this work",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1316.pdf",
        "title": "Toward a Task of Feedback Comment Generation for Writing Learning"
    },
    {
        "paper_id": "D19-1317",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Question generation (QG) is the task of generating a question from a reference sentence and a specified answer within the sentence. A major challenge in QG is to identify answer-relevant context words to finish the declarative-to-interrogative sentence transformation. Existing sequence-to-sequence neural models achieve this goal by proximity-based answer position encoding under the intuition that neighboring words of answers are of high possibility to be answer-relevant. However, such intuition may not apply to all cases especially for sentences with complex answer-relevant relations. Consequently, the performance of these models drops sharply when the relative distance between the answer fragment and other non-stop sentence words that also appear in the ground truth question increases. To address this issue, we propose a method to jointly model the unstructured sentence and the structured answer-relevant relation (extracted from the sentence in advance) for question generation. Specifically, the structured answer-relevant relation acts as the to the point context and it thus naturally helps keep the generated question to the point, while the unstructured sentence provides the full information. Extensive experiments show that to the point context helps our question generation model achieve significant improvements on several automatic evaluation metrics. Furthermore, our model is capable of generating diverse questions for a sentence which conveys multiple relations of its answer fragment.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1317.pdf",
        "title": "Improving Question Generation With to the Point Context"
    },
    {
        "paper_id": "D19-1321",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Existing neural methods for data-to-text generation are still struggling to produce long and diverse texts: they are insufficient to model input data dynamically during generation, to capture inter-sentence coherence, or to generate diversified expressions. To address these issues, we propose a Planning-based Hierarchical Variational Model (PHVM). Our model first plans a sequence of groups (each group is a subset of input items to be covered by a sentence) and then realizes each sentence conditioned on the planning result and the previously generated context, thereby decomposing long text generation into dependent sentence generation sub-tasks. To capture expression diversity, we devise a hierarchical latent structure where a global planning latent variable models the diversity of reasonable planning and a sequence of local latent variables controls sentence realization. Experiments show that our model outperforms state-of-the-art baselines in long and diverse text generation.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1321.pdf",
        "title": "Long and Diverse Text Generation with Planning-based Hierarchical Variational Model"
    },
    {
        "paper_id": "D19-1324",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Recent neural network approaches to summarization are largely either selection-based extraction or generation-based abstraction. In this work, we present a neural model for single-document summarization based on joint extraction and syntactic compression. Our model chooses sentences from the document, identifies possible compressions based on constituency parses, and scores those compressions with a neural model to produce the final summary. For learning, we construct oracle extractive-compressive summaries, then learn both of our components jointly with this supervision. Experimental results on the CNN/Daily Mail and New York Times datasets show that our model achieves strong performance (comparable to state-of-the-art systems) as evaluated by ROUGE. Moreover, our approach outperforms an off-the-shelf compression module, and human and manual evaluation shows that our model\u2019s output generally remains grammatical.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1324.pdf",
        "title": "Neural Extractive Text Summarization with Syntactic Compression"
    },
    {
        "paper_id": "D19-1330",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "In this paper, we introduce a novel interactive approach to translate a source language into two different languages simultaneously and interactively. Specifically, the generation of one language relies on not only previously generated outputs by itself, but also the outputs predicted in the other language. Experimental results on IWSLT and WMT datasets demonstrate that our method can obtain significant improvements over both conventional Neural Machine Translation (NMT) model and multilingual NMT model.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1330.pdf",
        "title": "Synchronously Generating Two Languages with Interactive Decoding"
    },
    {
        "paper_id": "D19-1334",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Multi-hop knowledge graph (KG) reasoning is an effective and explainable method for predicting the target entity via reasoning paths in query answering (QA) task. Most previous methods assume that every relation in KGs has enough triples for training, regardless of those few-shot relations which cannot provide sufficient triples for training robust reasoning models. In fact, the performance of existing multi-hop reasoning methods drops significantly on few-shot relations. In this paper, we propose a meta-based multi-hop reasoning method (Meta-KGR), which adopts meta-learning to learn effective meta parameters from high-frequency relations that could quickly adapt to few-shot relations. We evaluate Meta-KGR on two public datasets sampled from Freebase and NELL, and the experimental results show that Meta-KGR outperforms state-of-the-art methods in few-shot scenarios. In the future, our codes and datasets will also be available to provide more details.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1334.pdf",
        "title": "Adapting Meta Knowledge Graph Information for Multi-Hop Reasoning over Few-Shot Relations"
    },
    {
        "paper_id": "D19-1336",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "In this paper, we focus on the task of generating a pun sentence given a pair of word senses. A major challenge for pun generation is the lack of large-scale pun corpus to guide supervised learning. To remedy this, we propose an adversarial generative network for pun generation (Pun-GAN). It consists of a generator to produce pun sentences, and a discriminator to distinguish between the generated pun sentences and the real sentences with specific word senses. The output of the discriminator is then used as a reward to train the generator via reinforcement learning, encouraging it to produce pun sentences which can support two word senses simultaneously. Experiments show that the proposed Pun-GAN can generate sentences that are more ambiguous and diverse in both automatic and human evaluation.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1336.pdf",
        "title": "Pun-GAN: Generative Adversarial Network for Pun Generation"
    },
    {
        "paper_id": "D19-1342",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Aspect-level sentiment classification, which is a fine-grained sentiment analysis task, has received lots of attention these years. There is a phenomenon that people express both positive and negative sentiments towards an aspect at the same time. Such opinions with conflicting sentiments, however, are ignored by existing studies, which design models based on the absence of them. We argue that the exclusion of conflict opinions is problematic, for the reason that it represents an important style of human thinking \u2013 dialectic thinking. If a real-world sentiment classification system ignores the existence of conflict opinions when it is designed, it will incorrectly mixed conflict opinions into other sentiment polarity categories in action. Existing models have problems when recognizing conflicting opinions, such as data sparsity. In this paper, we propose a multi-label classification model with dual attention mechanism to address these problems.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1342.pdf",
        "title": "Recognizing Conflict Opinions in Aspect-level Sentiment Classification with Dual Attention Networks"
    },
    {
        "paper_id": "D19-1345",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Recently, researches have explored the graph neural network (GNN) techniques on text classification, since GNN does well in handling complex structures and preserving global information. However, previous methods based on GNN are mainly faced with the practical problems of fixed corpus level graph structure which don\u2019t support online testing and high memory consumption. To tackle the problems, we propose a new GNN based model that builds graphs for each input text with global parameters sharing instead of a single graph for the whole corpus. This method removes the burden of dependence between an individual text and entire corpus which support online testing, but still preserve global information. Besides, we build graphs by much smaller windows in the text, which not only extract more local features but also significantly reduce the edge numbers as well as memory consumption. Experiments show that our model outperforms existing models on several text classification datasets even with consuming less memory.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1345.pdf",
        "title": "Text Level Graph Neural Network for Text Classification"
    },
    {
        "paper_id": "D19-1350",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "In recent years, advances in neural variational inference have achieved many successes in text processing. Examples include neural topic models which are typically built upon variational autoencoder (VAE) with an objective of minimising the error of reconstructing original documents based on the learned latent topic vectors. However, minimising reconstruction errors does not necessarily lead to high quality topics. In this paper, we borrow the idea of reinforcement learning and incorporate topic coherence measures as reward signals to guide the learning of a VAE-based topic model. Furthermore, our proposed model is able to automatically separating background words dynamically from topic words, thus eliminating the pre-processing step of filtering infrequent and/or top frequent words, typically required for learning traditional topic models. Experimental results on the 20 Newsgroups and the NIPS datasets show superior performance both on perplexity and topic coherence measure compared to state-of-the-art neural topic models.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1350.pdf",
        "title": "Neural Topic Model with Reinforcement Learning"
    },
    {
        "paper_id": "D19-1359",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Current research in knowledge-based Word Sense Disambiguation (WSD) indicates that performances depend heavily on the Lexical Knowledge Base (LKB) employed. This paper introduces SyntagNet, a novel resource consisting of manually disambiguated lexical-semantic combinations. By capturing sense distinctions evoked by syntagmatic relations, SyntagNet enables knowledge-based WSD systems to establish a new state of the art which challenges the hitherto unrivaled performances attained by supervised approaches. To the best of our knowledge, SyntagNet is the first large-scale manually-curated resource of this kind made available to the community (at http://syntagnet.org).",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1359.pdf",
        "title": "SyntagNet: Challenging Supervised Word Sense Disambiguation with Lexical-Semantic Combinations"
    },
    {
        "paper_id": "D19-1368",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Bilinear models such as DistMult and ComplEx are effective methods for knowledge graph (KG) completion. However, they require large batch sizes, which becomes a performance bottleneck when training on large scale datasets due to memory constraints. In this paper we use occurrences of entity-relation pairs in the dataset to construct a joint learning model and to increase the quality of sampled negatives during training. We show on three standard datasets that when these two techniques are combined, they give a significant improvement in performance, especially when the batch size and the number of generated negative examples are low relative to the size of the dataset. We then apply our techniques to a dataset containing 2 million entities and demonstrate that our model outperforms the baseline by 2.8% absolute on hits@1.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1368.pdf",
        "title": "Using Pairwise Occurrence Information to Improve Knowledge Graph Completion on Large-Scale Datasets"
    },
    {
        "paper_id": "D19-1370",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "When trained effectively, the Variational Autoencoder (VAE) is both a powerful language model and an effective representation learning framework. In practice, however, VAEs are trained with the evidence lower bound (ELBO) as a surrogate objective to the intractable marginal data likelihood. This approach to training yields unstable results, frequently leading to a disastrous local optimum known as posterior collapse. In this paper, we investigate a simple fix for posterior collapse which yields surprisingly effective results. The combination of two known heuristics, previously considered only in isolation, substantially improves held-out likelihood, reconstruction, and latent representation learning when compared with previous state-of-the-art methods. More interestingly, while our experiments demonstrate superiority on these principle evaluations, our method obtains a worse ELBO. We use these results to argue that the typical surrogate objective for VAEs may not be sufficient or necessarily appropriate for balancing the goals of representation learning and data distribution modeling.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1370.pdf",
        "title": "A Surprisingly Effective Fix for Deep Latent Variable Modeling of Text"
    },
    {
        "paper_id": "D19-1372",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Much previous work has been done in attempting to identify humor in text. In this paper we extend that capability by proposing a new task: assessing whether or not a joke is humorous. We present a novel way of approaching this problem by building a model that learns to identify humorous jokes based on ratings gleaned from Reddit pages, consisting of almost 16,000 labeled instances. Using these ratings to determine the level of humor, we then employ a Transformer architecture for its advantages in learning from sentence context. We demonstrate the effectiveness of this approach and show results that are comparable to human performance. We further demonstrate our model\u2019s increased capabilities on humor identification problems, such as the previously created datasets for short jokes and puns. These experiments show that this method outperforms all previous work done on these tasks, with an F-measure of 93.1% for the Puns dataset and 98.6% on the Short Jokes dataset.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1372.pdf",
        "title": "Humor Detection: A Transformer Gets the Last Laugh"
    },
    {
        "paper_id": "D19-1374",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "We propose a practical scheme to train a single multilingual sequence labeling model that yields state of the art results and is small and fast enough to run on a single CPU. Starting from a public multilingual BERT checkpoint, our final model is 6x smaller and 27x faster, and has higher accuracy than a state-of-the-art multilingual baseline. We show that our model especially outperforms on low-resource languages, and works on codemixed input text without being explicitly trained on codemixed examples. We showcase the effectiveness of our method by reporting on part-of-speech tagging and morphological prediction on 70 treebanks and 48 languages.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1374.pdf",
        "title": "Small and Practical BERT Models for Sequence Labeling"
    },
    {
        "paper_id": "D19-1376",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "We present PaLM, a hybrid parser and neural language model. Building on an RNN language model, PaLM adds an attention layer over text spans in the left context. An unsupervised constituency parser can be derived from its attention weights, using a greedy decoding algorithm. We evaluate PaLM on language modeling, and empirically show that it outperforms strong baselines. If syntactic annotations are available, the attention component can be trained in a supervised manner, providing syntactically-informed representations of the context, and further improving language modeling performance.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1376.pdf",
        "title": "PaLM: A Hybrid Parser and Language Model"
    },
    {
        "paper_id": "D19-1379",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "In transductive learning, an unlabeled test set is used for model training. Although this setting deviates from the common assumption of a completely unseen test set, it is applicable in many real-world scenarios, wherein the texts to be processed are known in advance. However, despite its practical advantages, transductive learning is underexplored in natural language processing. Here we conduct an empirical study of transductive learning for neural models and demonstrate its utility in syntactic and semantic tasks. Specifically, we fine-tune language models (LMs) on an unlabeled test set to obtain test-set-specific word representations. Through extensive experiments, we demonstrate that despite its simplicity, transductive LM fine-tuning consistently improves state-of-the-art neural models in in-domain and out-of-domain settings.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1379.pdf",
        "title": "Transductive Learning of Neural Language Models for Syntactic and Semantic Analysis"
    },
    {
        "paper_id": "D19-1380",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Vector averaging remains one of the most popular sentence embedding methods in spite of its obvious disregard for syntactic structure. While more complex sequential or convolutional networks potentially yield superior classification performance, the improvements in classification accuracy are typically mediocre compared to the simple vector averaging. As an efficient alternative, we propose the use of discrete cosine transform (DCT) to compress word sequences in an order-preserving manner. The lower order DCT coefficients represent the overall feature patterns in sentences, which results in suitable embeddings for tasks that could benefit from syntactic features. Our results in semantic probing tasks demonstrate that DCT embeddings indeed preserve more syntactic information compared with vector averaging. With practically equivalent complexity, the model yields better overall performance in downstream classification tasks that correlate with syntactic features, which illustrates the capacity of DCT to preserve word order information.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1380.pdf",
        "title": "Efficient Sentence Embedding using Discrete Cosine Transform"
    },
    {
        "paper_id": "D19-1381",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "We tackle the nested and overlapping event detection task and propose a novel search-based neural network (SBNN) structured prediction model that treats the task as a search problem on a relation graph of trigger-argument structures. Unlike existing structured prediction tasks such as dependency parsing, the task targets to detect DAG structures, which constitute events, from the relation graph. We define actions to construct events and use all the beams in a beam search to detect all event structures that may be overlapping and nested. The search process constructs events in a bottom-up manner while modelling the global properties for nested and overlapping structures simultaneously using neural networks. We show that the model achieves performance comparable to the state-of-the-art model Turku Event Extraction System (TEES) on the BioNLP Cancer Genetics (CG) Shared Task 2013 without the use of any syntactic and hand-engineered features. Further analyses on the development set show that our model is more computationally efficient while yielding higher F1-score performance.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1381.pdf",
        "title": "A Search-based Neural Model for Biomedical Nested and Overlapping Event Detection"
    },
    {
        "paper_id": "D19-1383",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "As a step toward better document-level understanding, we explore classification of a sequence of sentences into their corresponding categories, a task that requires understanding sentences in context of the document. Recent successful models for this task have used hierarchical models to contextualize sentence representations, and Conditional Random Fields (CRFs) to incorporate dependencies between subsequent labels. In this work, we show that pretrained language models, BERT (Devlin et al., 2018) in particular, can be used for this task to capture contextual dependencies without the need for hierarchical encoding nor a CRF. Specifically, we construct a joint sentence representation that allows BERT Transformer layers to directly utilize contextual information from all words in all sentences. Our approach achieves state-of-the-art results on four datasets, including a new dataset of structured scientific abstracts.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1383.pdf",
        "title": "Pretrained Language Models for Sequential Sentence Classification"
    },
    {
        "paper_id": "D19-1387",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Bidirectional Encoder Representations from Transformers (BERT) represents the latest incarnation of pretrained language models which have recently advanced a wide range of natural language processing tasks. In this paper, we showcase how BERT can be usefully applied in text summarization and propose a general framework for both extractive and abstractive models. We introduce a novel document-level encoder based on BERT which is able to express the semantics of a document and obtain representations for its sentences. Our extractive model is built on top of this encoder by stacking several inter-sentence Transformer layers. For abstractive summarization, we propose a new fine-tuning schedule which adopts different optimizers for the encoder and the decoder as a means of alleviating the mismatch between the two (the former is pretrained while the latter is not). We also demonstrate that a two-staged fine-tuning approach can further boost the quality of the generated summaries. Experiments on three datasets show that our model achieves state-of-the-art results across the board in both extractive and abstractive settings.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1387.pdf",
        "title": "Text Summarization with Pretrained Encoders"
    },
    {
        "paper_id": "D19-1388",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Under special circumstances, summaries should conform to a particular style with patterns, such as court judgments and abstracts in academic papers. To this end, the prototype document-summary pairs can be utilized to generate better summaries. There are two main challenges in this task: (1) the model needs to incorporate learned patterns from the prototype, but (2) should avoid copying contents other than the patternized words\u2014such as irrelevant facts\u2014into the generated summaries. To tackle these challenges, we design a model named Prototype Editing based Summary Generator (PESG). PESG first learns summary patterns and prototype facts by analyzing the correlation between a prototype document and its summary. Prototype facts are then utilized to help extract facts from the input document. Next, an editing generator generates new summary based on the summary pattern or extracted facts. Finally, to address the second challenge, a fact checker is used to estimate mutual information between the input document and generated summary, providing an additional signal for the generator. Extensive experiments conducted on a large-scale real-world text summarization dataset show that PESG achieves the state-of-the-art performance in terms of both automatic metrics and human evaluations.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1388.pdf",
        "title": "How to Write Summaries with Patterns? Learning towards Abstractive Summarization through Prototype Editing"
    },
    {
        "paper_id": "D19-1399",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Dependency tree structures capture long-distance and syntactic relationships between words in a sentence. The syntactic relations (e.g., nominal subject, object) can potentially infer the existence of certain named entities. In addition, the performance of a named entity recognizer could benefit from the long-distance dependencies between the words in dependency trees. In this work, we propose a simple yet effective dependency-guided LSTM-CRF model to encode the complete dependency trees and capture the above properties for the task of named entity recognition (NER). The data statistics show strong correlations between the entity types and dependency relations. We conduct extensive experiments on several standard datasets and demonstrate the effectiveness of the proposed model in improving NER and achieving state-of-the-art performance. Our analysis reveals that the significant improvements mainly result from the dependency relations and long-distance interactions provided by dependency trees.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1399.pdf",
        "title": "Dependency-Guided LSTM-CRF for Named Entity Recognition"
    },
    {
        "paper_id": "D19-1400",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Large training datasets are required to achieve competitive performance in most natural language tasks. The acquisition process for these datasets is labor intensive, expensive, and time consuming. This process is also prone to human errors. In this work, we show that cross-cultural differences can be harnessed for natural language text classification. We present a transfer-learning framework that leverages widely-available unaligned bilingual corpora for classification tasks, using no task-specific data. Our empirical evaluation on two tasks \u2013 formality classification and sarcasm detection \u2013 shows that the cross-cultural difference between German and American English, as manifested in product review text, can be applied to achieve good performance for formality classification, while the difference between Japanese and American English can be applied to achieve good performance for sarcasm detection \u2013 both without any task-specific labeled data.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1400.pdf",
        "title": "Cross-Cultural Transfer Learning for Text Classification"
    },
    {
        "paper_id": "D19-1402",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "We propose a novel on-device sequence model for text classification using recurrent projections. Our model ProSeqo uses dynamic recurrent projections without the need to store or look up any pre-trained embeddings. This results in fast and compact neural networks that can perform on-device inference for complex short and long text classification tasks. We conducted exhaustive evaluation on multiple text classification tasks. Results show that ProSeqo outperformed state-of-the-art neural and on-device approaches for short text classification tasks such as dialog act and intent prediction. To the best of our knowledge, ProSeqo is the first on-device long text classification neural model. It achieved comparable results to previous neural approaches for news article, answers and product categorization, while preserving small memory footprint and maintaining high accuracy.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1402.pdf",
        "title": "ProSeqo: Projection Sequence Networks for On-Device Text Classification"
    },
    {
        "paper_id": "D19-1418",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "State-of-the-art models often make use of superficial patterns in the data that do not generalize well to out-of-domain or adversarial settings. For example, textual entailment models often learn that particular key words imply entailment, irrespective of context, and visual question answering models learn to predict prototypical answers, without considering evidence in the image. In this paper, we show that if we have prior knowledge of such biases, we can train a model to be more robust to domain shift. Our method has two stages: we (1) train a naive model that makes predictions exclusively based on dataset biases, and (2) train a robust model as part of an ensemble with the naive one in order to encourage it to focus on other patterns in the data that are more likely to generalize. Experiments on five datasets with out-of-domain test sets show significantly improved robustness in all settings, including a 12 point gain on a changing priors visual question answering dataset and a 9 point gain on an adversarial question answering test set.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1418.pdf",
        "title": "Don\u2019t Take the Easy Way Out: Ensemble Based Methods for Avoiding Known Dataset Biases"
    },
    {
        "paper_id": "D19-1420",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Selective rationalization has become a common mechanism to ensure that predictive models reveal how they use any available features. The selection may be soft or hard, and identifies a subset of input features relevant for prediction. The setup can be viewed as a co-operate game between the selector (aka rationale generator) and the predictor making use of only the selected features. The co-operative setting may, however, be compromised for two reasons. First, the generator typically has no direct access to the outcome it aims to justify, resulting in poor performance. Second, there\u2019s typically no control exerted on the information left outside the selection. We revise the overall co-operative framework to address these challenges. We introduce an introspective model which explicitly predicts and incorporates the outcome into the selection process. Moreover, we explicitly control the rationale complement via an adversary so as not to leave any useful information out of the selection. We show that the two complementary mechanisms maintain both high predictive accuracy and lead to comprehensive rationales.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1420.pdf",
        "title": "Rethinking Cooperative Rationalization: Introspective Extraction and Complement Control"
    },
    {
        "paper_id": "D19-1422",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "CRF has been used as a powerful model for statistical sequence labeling. For neural sequence labeling, however, BiLSTM-CRF does not always lead to better results compared with BiLSTM-softmax local classification. This can be because the simple Markov label transition model of CRF does not give much information gain over strong neural encoding. For better representing label sequences, we investigate a hierarchically-refined label attention network, which explicitly leverages label embeddings and captures potential long-term label dependency by giving each word incrementally refined label distributions with hierarchical attention. Results on POS tagging, NER and CCG supertagging show that the proposed model not only improves the overall tagging accuracy with similar number of parameters, but also significantly speeds up the training and testing compared to BiLSTM-CRF.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1422.pdf",
        "title": "Hierarchically-Refined Label Attention Network for Sequence Labeling"
    },
    {
        "paper_id": "D19-1426",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Natural language has recently been explored as a new medium of supervision for training machine learning models. Here, we explore learning classification tasks using language in a conversational setting \u2013 where the automated learner does not simply receive language input from a teacher, but can proactively engage the teacher by asking questions. We present a reinforcement learning framework, where the learner\u2019s actions correspond to question types and the reward for asking a question is based on how the teacher\u2019s response changes performance of the resulting machine learning model on the learning task. In this framework, learning good question-asking strategies corresponds to asking sequences of questions that maximize the cumulative (discounted) reward, and hence quickly lead to effective classifiers. Empirical analysis across three domains shows that learned question-asking strategies expedite classifier training by asking appropriate questions at different points in the learning process. The approach allows learning classifiers from a blend of strategies, including learning from observations, explanations and clarifications.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1426.pdf",
        "title": "Learning to Ask for Conversational Machine Learning"
    },
    {
        "paper_id": "D19-1429",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "In sequence labeling, previous domain adaptation methods focus on the adaptation from the source domain to the entire target domain without considering the diversity of individual target domain samples, which may lead to negative transfer results for certain samples. Besides, an important characteristic of sequence labeling tasks is that different elements within a given sample may also have diverse domain relevance, which requires further consideration. To take the multi-level domain relevance discrepancy into account, in this paper, we propose a fine-grained knowledge fusion model with the domain relevance modeling scheme to control the balance between learning from the target domain data and learning from the source domain model. Experiments on three sequence labeling tasks show that our fine-grained knowledge fusion model outperforms strong baselines and other state-of-the-art sequence labeling domain adaptation methods.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1429.pdf",
        "title": "Fine-grained Knowledge Fusion for Sequence Labeling Domain Adaptation"
    },
    {
        "paper_id": "D19-1431",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Link prediction is an important way to complete knowledge graphs (KGs), while embedding-based methods, effective for link prediction in KGs, perform poorly on relations that only have a few associative triples. In this work, we propose a Meta Relational Learning (MetaR) framework to do the common but challenging few-shot link prediction in KGs, namely predicting new triples about a relation by only observing a few associative triples. We solve few-shot link prediction by focusing on transferring relation-specific meta information to make model learn the most important knowledge and learn faster, corresponding to relation meta and gradient meta respectively in MetaR. Empirically, our model achieves state-of-the-art results on few-shot link prediction KG benchmarks.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1431.pdf",
        "title": "Meta Relational Learning for Few-Shot Link Prediction in Knowledge Graphs"
    },
    {
        "paper_id": "D19-1437",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Most sequence-to-sequence (seq2seq) models are autoregressive; they generate each token by conditioning on previously generated tokens. In contrast, non-autoregressive seq2seq models generate all tokens in one pass, which leads to increased efficiency through parallel processing on hardware such as GPUs. However, directly modeling the joint distribution of all tokens simultaneously is challenging, and even with increasingly complex model structures accuracy lags significantly behind autoregressive models. In this paper, we propose a simple, efficient, and effective model for non-autoregressive sequence generation using latent variable models. Specifically, we turn to generative flow, an elegant technique to model complex distributions using neural networks, and design several layers of flow tailored for modeling the conditional density of sequential latent variables. We evaluate this model on three neural machine translation (NMT) benchmark datasets, achieving comparable performance with state-of-the-art non-autoregressive NMT models and almost constant decoding time w.r.t the sequence length.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1437.pdf",
        "title": "FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow"
    },
    {
        "paper_id": "D19-1457",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Our goal is to better comprehend procedural text, e.g., a paragraph about photosynthesis, by not only predicting what happens, but *why* some actions need to happen before others. Our approach builds on a prior process comprehension framework for predicting actions\u2019 effects, to also identify subsequent steps that those effects enable. We present our new model (XPAD) that biases effect predictions towards those that (1) explain more of the actions in the paragraph and (2) are more plausible with respect to background knowledge. We also extend an existing benchmark dataset for procedural text comprehension, ProPara, by adding the new task of explaining actions by predicting their dependencies. We find that XPAD significantly outperforms prior systems on this task, while maintaining the performance on the original task in ProPara. The dataset is available at http://data.allenai.org/propara",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1457.pdf",
        "title": "Everything Happens for a Reason: Discovering the Purpose of Actions in Procedural Text"
    },
    {
        "paper_id": "D19-1461",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "The detection of offensive language in the context of a dialogue has become an increasingly important application of natural language processing. The detection of trolls in public forums (Gal\u00e1n-Garc\u00eda et al., 2016), and the deployment of chatbots in the public domain (Wolf et al., 2017) are two examples that show the necessity of guarding against adversarially offensive behavior on the part of humans. In this work, we develop a training scheme for a model to become robust to such human attacks by an iterative build it, break it, fix it scheme with humans and models in the loop. In detailed experiments we show this approach is considerably more robust than previous systems. Further, we show that offensive language used within a conversation critically depends on the dialogue context, and cannot be viewed as a single sentence offensive detection task as in most previous work. Our newly collected tasks and methods are all made open source and publicly available.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1461.pdf",
        "title": "Build it Break it Fix it for Dialogue Safety: Robustness from Adversarial Human Attack"
    },
    {
        "paper_id": "D19-1463",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "How to incorporate external knowledge into a neural dialogue model is critically important for dialogue systems to behave like real humans. To handle this problem, memory networks are usually a great choice and a promising way. However, existing memory networks do not perform well when leveraging heterogeneous information from different sources. In this paper, we propose a novel and versatile external memory networks called Heterogeneous Memory Networks (HMNs), to simultaneously utilize user utterances, dialogue history and background knowledge tuples. In our method, historical sequential dialogues are encoded and stored into the context-aware memory enhanced by gating mechanism while grounding knowledge tuples are encoded and stored into the context-free memory. During decoding, the decoder augmented with HMNs recurrently selects each word in one response utterance from these two memories and a general vocabulary. Experimental results on multiple real-world datasets show that HMNs significantly outperform the state-of-the-art data-driven task-oriented dialogue models in most domains.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1463.pdf",
        "title": "Task-Oriented Conversation Generation Using Heterogeneous Memory Networks"
    },
    {
        "paper_id": "D19-1467",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Aspect level sentiment classification is a fine-grained sentiment analysis task. To detect the sentiment towards a particular aspect in a sentence, previous studies have developed various attention-based methods for generating aspect-specific sentence representations. However, the attention may inherently introduce noise and downgrade the performance. In this paper, we propose constrained attention networks (CAN), a simple yet effective solution, to regularize the attention for multi-aspect sentiment analysis, which alleviates the drawback of the attention mechanism. Specifically, we introduce orthogonal regularization on multiple aspects and sparse regularization on each single aspect. Experimental results on two public datasets demonstrate the effectiveness of our approach. We further extend our approach to multi-task settings and outperform the state-of-the-art methods.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1467.pdf",
        "title": "CAN: Constrained Attention Networks for Multi-Aspect Sentiment Analysis"
    },
    {
        "paper_id": "D19-1470",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "The prevalent use of social media leads to a vast amount of online conversations being produced on a daily basis. It presents a concrete challenge for individuals to better discover and engage in social media discussions. In this paper, we present a novel framework to automatically recommend conversations to users based on their prior conversation behaviors. Built on neural collaborative filtering, our model explores deep semantic features that measure how a user\u2019s preferences match an ongoing conversation\u2019s context. Furthermore, to identify salient characteristics from interleaving user interactions, our model incorporates graph-structured networks, where both replying relations and temporal features are encoded as conversation context. Experimental results on two large-scale datasets collected from Twitter and Reddit show that our model yields better performance than previous state-of-the-art models, which only utilize lexical features and ignore past user interactions in the conversations.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1470.pdf",
        "title": "Neural Conversation Recommendation with Online Interaction Modeling"
    },
    {
        "paper_id": "D19-1485",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Automatically verifying rumorous information has become an important and challenging task in natural language processing and social media analytics. Previous studies reveal that people\u2019s stances towards rumorous messages can provide indicative clues for identifying the veracity of rumors, and thus determining the stances of public reactions is a crucial preceding step for rumor veracity prediction. In this paper, we propose a hierarchical multi-task learning framework for jointly predicting rumor stance and veracity on Twitter, which consists of two components. The bottom component of our framework classifies the stances of tweets in a conversation discussing a rumor via modeling the structural property based on a novel graph convolutional network. The top component predicts the rumor veracity by exploiting the temporal dynamics of stance evolution. Experimental results on two benchmark datasets show that our method outperforms previous methods in both rumor stance classification and veracity prediction.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1485.pdf",
        "title": "Modeling Conversation Structure and Temporal Dynamics for Jointly Predicting Rumor Stance and Veracity"
    },
    {
        "paper_id": "D19-1488",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Short text classification has found rich and critical applications in news and tweet tagging to help users find relevant information. Due to lack of labeled training data in many practical use cases, there is a pressing need for studying semi-supervised short text classification. Most existing studies focus on long texts and achieve unsatisfactory performance on short texts due to the sparsity and limited labeled data. In this paper, we propose a novel heterogeneous graph neural network based method for semi-supervised short text classification, leveraging full advantage of few labeled data and large unlabeled data through information propagation along the graph. In particular, we first present a flexible HIN (heterogeneous information network) framework for modeling the short texts, which can integrate any type of additional information as well as capture their relations to address the semantic sparsity. Then, we propose Heterogeneous Graph ATtention networks (HGAT) to embed the HIN for short text classification based on a dual-level attention mechanism, including node-level and type-level attentions. The attention mechanism can learn the importance of different neighboring nodes as well as the importance of different node (information) types to a current node. Extensive experimental results have demonstrated that our proposed model outperforms state-of-the-art methods across six benchmark datasets significantly.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1488.pdf",
        "title": "Heterogeneous Graph Attention Networks for Semi-supervised Short Text Classification"
    },
    {
        "paper_id": "D19-1491",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "This paper presents a novel architecture for recursive context-aware lexical simplification, REC-LS, that is capable of (1) making use of the wider context when detecting the words in need of simplification and suggesting alternatives, and (2) taking previous simplification steps into account. We show that our system outputs lexical simplifications that are grammatically correct and semantically appropriate, and outperforms the current state-of-the-art systems in lexical simplification.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1491.pdf",
        "title": "Recursive Context-Aware Lexical Simplification"
    },
    {
        "paper_id": "D19-1496",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Adversarial attacks against machine learning models have threatened various real-world applications such as spam filtering and sentiment analysis. In this paper, we propose a novel framework, learning to discriminate perturbations (DISP), to identify and adjust malicious perturbations, thereby blocking adversarial attacks for text classification models. To identify adversarial attacks, a perturbation discriminator validates how likely a token in the text is perturbed and provides a set of potential perturbations. For each potential perturbation, an embedding estimator learns to restore the embedding of the original word based on the context and a replacement token is chosen based on approximate kNN search. DISP can block adversarial attacks for any NLP model without modifying the model structure or training procedure. Extensive experiments on two benchmark datasets demonstrate that DISP significantly outperforms baseline methods in blocking adversarial attacks for text classification. In addition, in-depth analysis shows the robustness of DISP across different situations.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1496.pdf",
        "title": "Learning to Discriminate Perturbations for Blocking Adversarial Attacks in Text Classification"
    },
    {
        "paper_id": "D19-1498",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Document-level relation extraction is a complex human process that requires logical inference to extract relationships between named entities in text. Existing approaches use graph-based neural models with words as nodes and edges as relations between them, to encode relations across sentences. These models are node-based, i.e., they form pair representations based solely on the two target node representations. However, entity relations can be better expressed through unique edge representations formed as paths between nodes. We thus propose an edge-oriented graph neural model for document-level relation extraction. The model utilises different types of nodes and edges to create a document-level graph. An inference mechanism on the graph edges enables to learn intra- and inter-sentence relations using multi-instance learning internally. Experiments on two document-level biomedical datasets for chemical-disease and gene-disease associations show the usefulness of the proposed edge-oriented approach.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1498.pdf",
        "title": "Connecting the Dots: Document-level Neural Relation Extraction with Edge-oriented Graphs"
    },
    {
        "paper_id": "D19-1499",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Text style transfer task requires the model to transfer a sentence of one style to another style while retaining its original content meaning, which is a challenging problem that has long suffered from the shortage of parallel data. In this paper, we first propose a semi-supervised text style transfer model that combines the small-scale parallel data with the large-scale nonparallel data. With these two types of training data, we introduce a projection function between the latent space of different styles and design two constraints to train it. We also introduce two other simple but effective semi-supervised methods to compare with. To evaluate the performance of the proposed methods, we build and release a novel style transfer dataset that alters sentences between the style of ancient Chinese poem and the modern Chinese.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1499.pdf",
        "title": "Semi-supervised Text Style Transfer: Cross Projection in Latent Space"
    },
    {
        "paper_id": "D19-1505",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Management of collaborative documents can be difficult, given the profusion of edits and comments that multiple authors make during a document\u2019s evolution. Reliably modeling the relationship between edits and comments is a crucial step towards helping the user keep track of a document in flux. A number of authoring tasks, such as categorizing and summarizing edits, detecting completed to-dos, and visually rearranging comments could benefit from such a contribution. Thus, in this paper we explore the relationship between comments and edits by defining two novel, related tasks: Comment Ranking and Edit Anchoring. We begin by collecting a dataset with more than half a million comment-edit pairs based on Wikipedia revision histories. We then propose a hierarchical multi-layer deep neural-network to model the relationship between edits and comments. Our architecture tackles both Comment Ranking and Edit Anchoring tasks by encoding specific edit actions such as additions and deletions, while also accounting for document context. In a number of evaluation settings, our experimental results show that our approach outperforms several strong baselines significantly. We are able to achieve a precision@1 of 71.0% and a precision@3 of 94.4% for Comment Ranking, while we achieve 74.4% accuracy on Edit Anchoring.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1505.pdf",
        "title": "Modeling the Relationship between User Comments and Edits in Document Revision"
    },
    {
        "paper_id": "D19-1506",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Recently, there has been a great interest in the development of small and accurate neural networks that run entirely on devices such as mobile phones, smart watches and IoT. This enables user privacy, consistent user experience and low latency. Although a wide range of applications have been targeted from wake word detection to short text classification, yet there are no on-device networks for long text classification. We propose a novel projection attention neural network PRADO that combines trainable projections with attention and convolutions. We evaluate our approach on multiple large document text classification tasks. Our results show the effectiveness of the trainable projection model in finding semantically similar phrases and reaching high performance while maintaining compact size. Using this approach, we train tiny neural networks just 200 Kilobytes in size that improve over prior CNN and LSTM models and achieve near state of the art performance on multiple long document classification tasks. We also apply our model for transfer learning, show its robustness and ability to further improve the performance in limited data scenarios.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1506.pdf",
        "title": "PRADO: Projection Attention Networks for Document Classification On-Device"
    },
    {
        "paper_id": "D19-1510",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "We propose LaserTagger - a sequence tagging approach that casts text generation as a text editing task. Target texts are reconstructed from the inputs using three main edit operations: keeping a token, deleting it, and adding a phrase before the token. To predict the edit operations, we propose a novel model, which combines a BERT encoder with an autoregressive Transformer decoder. This approach is evaluated on English text on four tasks: sentence fusion, sentence splitting, abstractive summarization, and grammar correction. LaserTagger achieves new state-of-the-art results on three of these tasks, performs comparably to a set of strong seq2seq baselines with a large number of training examples, and outperforms them when the number of examples is limited. Furthermore, we show that at inference time tagging can be more than two orders of magnitude faster than comparable seq2seq models, making it more attractive for running in a live environment.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1510.pdf",
        "title": "Encode, Tag, Realize: High-Precision Text Editing"
    },
    {
        "paper_id": "D19-1512",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Automatic news comment generation is beneficial for real applications but has not attracted enough attention from the research community. In this paper, we propose a \u201cread-attend-comment\u201d procedure for news comment generation and formalize the procedure with a reading network and a generation network. The reading network comprehends a news article and distills some important points from it, then the generation network creates a comment by attending to the extracted discrete points and the news title. We optimize the model in an end-to-end manner by maximizing a variational lower bound of the true objective using the back-propagation algorithm. Experimental results on two public datasets indicate that our model can significantly outperform existing methods in terms of both automatic evaluation and human judgment.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1512.pdf",
        "title": "Read, Attend and Comment: A Deep Architecture for Automatic News Comment Generation"
    },
    {
        "paper_id": "D19-1515",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "The phrase grounding task aims to ground each entity mention in a given caption of an image to a corresponding region in that image. Although there are clear dependencies between how different mentions of the same caption should be grounded, previous structured prediction methods that aim to capture such dependencies need to resort to approximate inference or non-differentiable losses. In this paper, we formulate phrase grounding as a sequence labeling task where we treat candidate regions as potential labels, and use neural chain Conditional Random Fields (CRFs) to model dependencies among regions for adjacent mentions. In contrast to standard sequence labeling tasks, the phrase grounding task is defined such that there may be multiple correct candidate regions. To address this multiplicity of gold labels, we define so-called Soft-Label Chain CRFs, and present an algorithm that enables convenient end-to-end training. Our method establishes a new state-of-the-art on phrase grounding on the Flickr30k Entities dataset. Analysis shows that our model benefits both from the entity dependencies captured by the CRF and from the soft-label training regime. Our code is available at github.com/liujch1998/SoftLabelCCRF",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1515.pdf",
        "title": "Phrase Grounding by Soft-Label Chain Conditional Random Field"
    },
    {
        "paper_id": "D19-1521",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "This paper studies keyphrase extraction in real-world scenarios where documents are from diverse domains and have variant content quality. We curate and release OpenKP, a large scale open domain keyphrase extraction dataset with near one hundred thousand web documents and expert keyphrase annotations. To handle the variations of domain and content quality, we develop BLING-KPE, a neural keyphrase extraction model that goes beyond language understanding using visual presentations of documents and weak supervision from search queries. Experimental results on OpenKP confirm the effectiveness of BLING-KPE and the contributions of its neural architecture, visual features, and search log weak supervision. Zero-shot evaluations on DUC-2001 demonstrate the improved generalization ability of learning from the open domain data compared to a specific domain.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1521.pdf",
        "title": "Open Domain Web Keyphrase Extraction Beyond Language Modeling"
    },
    {
        "paper_id": "D19-1524",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "We introduce a new task of modeling the role and function for on-line resource citations in scientific literature. By categorizing the on-line resources and analyzing the purpose of resource citations in scientific texts, it can greatly help resource search and recommendation systems to better understand and manage the scientific resources. For this novel task, we are the first to create an annotation scheme, which models the different granularity of information from a hierarchical perspective. And we construct a dataset SciRes, which includes 3,088 manually annotated resource contexts. In this paper, we propose a possible solution by using a multi-task framework to build the scientific resource classifier (SciResCLF) for jointly recognizing the role and function types. Then we use the classification results to help a scientific resource recommendation (SciResREC) task. Experiments show that our model achieves the best results on both the classification task and the recommendation task. The SciRes dataset is released for future research.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1524.pdf",
        "title": "A Context-based Framework for Modeling the Role and Function of On-line Resource Citations in Scientific Literature"
    },
    {
        "paper_id": "D19-1526",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Hashing is promising for large-scale information retrieval tasks thanks to the efficiency of distance evaluation between binary codes. Generative hashing is often used to generate hashing codes in an unsupervised way. However, existing generative hashing methods only considered the use of simple priors, like Gaussian and Bernoulli priors, which limits these methods to further improve their performance. In this paper, two mixture-prior generative models are proposed, under the objective to produce high-quality hashing codes for documents. Specifically, a Gaussian mixture prior is first imposed onto the variational auto-encoder (VAE), followed by a separate step to cast the continuous latent representation of VAE into binary code. To avoid the performance loss caused by the separate casting, a model using a Bernoulli mixture prior is further developed, in which an end-to-end training is admitted by resorting to the straight-through (ST) discrete gradient estimator. Experimental results on several benchmark datasets demonstrate that the proposed methods, especially the one using Bernoulli mixture priors, consistently outperform existing ones by a substantial margin.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1526.pdf",
        "title": "Document Hashing with Mixture-Prior Generative Models"
    },
    {
        "paper_id": "D19-1530",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "This paper treats gender bias latent in word embeddings. Previous mitigation attempts rely on the operationalisation of gender bias as a projection over a linear subspace. An alternative approach is Counterfactual Data Augmentation (CDA), in which a corpus is duplicated and augmented to remove bias, e.g. by swapping all inherently-gendered words in the copy. We perform an empirical comparison of these approaches on the English Gigaword and Wikipedia, and find that whilst both successfully reduce direct bias and perform well in tasks which quantify embedding quality, CDA variants outperform projection-based methods at the task of drawing non-biased gender analogies by an average of 19% across both corpora. We propose two improvements to CDA: Counterfactual Data Substitution (CDS), a variant of CDA in which potentially biased text is randomly substituted to avoid duplication, and the Names Intervention, a novel name-pairing technique that vastly increases the number of words being treated. CDA/S with the Names Intervention is the only approach which is able to mitigate indirect gender bias: following debiasing, previously biased words are significantly less clustered according to gender (cluster purity is reduced by 49%), thus improving on the state-of-the-art for bias mitigation.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1530.pdf",
        "title": "It\u2019s All in the Name: Mitigating Gender Bias with Name-Based Counterfactual Data Substitution"
    },
    {
        "paper_id": "D19-1533",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Contextualized word representations are able to give different representations for the same word in different contexts, and they have been shown to be effective in downstream natural language processing tasks, such as question answering, named entity recognition, and sentiment analysis. However, evaluation on word sense disambiguation (WSD) in prior work shows that using contextualized word representations does not outperform the state-of-the-art approach that makes use of non-contextualized word embeddings. In this paper, we explore different strategies of integrating pre-trained contextualized word representations and our best strategy achieves accuracies exceeding the best prior published accuracies by significant margins on multiple benchmark WSD datasets.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1533.pdf",
        "title": "Improved Word Sense Disambiguation Using Pre-Trained Contextualized Word Representations"
    },
    {
        "paper_id": "D19-1535",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Context-dependent semantic parsing has proven to be an important yet challenging task. To leverage the advances in context-independent semantic parsing, we propose to perform follow-up query analysis, aiming to restate context-dependent natural language queries with contextual information. To accomplish the task, we propose STAR, a novel approach with a well-designed two-phase process. It is parser-independent and able to handle multifarious follow-up scenarios in different domains. Experiments on the FollowUp dataset show that STAR outperforms the state-of-the-art baseline by a large margin of nearly 8%. The superiority on parsing results verifies the feasibility of follow-up query analysis. We also explore the extensibility of STAR on the SQA dataset, which is very promising.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1535.pdf",
        "title": "A Split-and-Recombine Approach for Follow-up Query Analysis"
    },
    {
        "paper_id": "D19-1538",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Recently, semantic role labeling (SRL) has earned a series of success with even higher performance improvements, which can be mainly attributed to syntactic integration and enhanced word representation. However, most of these efforts focus on English, while SRL on multiple languages more than English has received relatively little attention so that is kept underdevelopment. Thus this paper intends to fill the gap on multilingual SRL with special focus on the impact of syntax and contextualized word representation. Unlike existing work, we propose a novel method guided by syntactic rule to prune arguments, which enables us to integrate syntax into multilingual SRL model simply and effectively. We present a unified SRL model designed for multiple languages together with the proposed uniform syntax enhancement. Our model achieves new state-of-the-art results on the CoNLL-2009 benchmarks of all seven languages. Besides, we pose a discussion on the syntactic role among different languages and verify the effectiveness of deep enhanced representation for multilingual SRL.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1538.pdf",
        "title": "Syntax-aware Multilingual Semantic Role Labeling"
    },
    {
        "paper_id": "D19-1539",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "We present a new approach for pretraining a bi-directional transformer model that provides significant performance gains across a variety of language understanding problems. Our model solves a cloze-style word reconstruction task, where each word is ablated and must be predicted given the rest of the text. Experiments demonstrate large performance gains on GLUE and new state of the art results on NER as well as constituency parsing benchmarks, consistent with BERT. We also present a detailed analysis of a number of factors that contribute to effective pretraining, including data domain and size, model capacity, and variations on the cloze objective.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1539.pdf",
        "title": "Cloze-driven Pretraining of Self-attention Networks"
    },
    {
        "paper_id": "D19-1540",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "A core problem of information retrieval (IR) is relevance matching, which is to rank documents by relevance to a user\u2019s query. On the other hand, many NLP problems, such as question answering and paraphrase identification, can be considered variants of semantic matching, which is to measure the semantic distance between two pieces of short texts. While at a high level both relevance and semantic matching require modeling textual similarity, many existing techniques for one cannot be easily adapted to the other. To bridge this gap, we propose a novel model, HCAN (Hybrid Co-Attention Network), that comprises (1) a hybrid encoder module that includes ConvNet-based and LSTM-based encoders, (2) a relevance matching module that measures soft term matches with importance weighting at multiple granularities, and (3) a semantic matching module with co-attention mechanisms that capture context-aware semantic relatedness. Evaluations on multiple IR and NLP benchmarks demonstrate state-of-the-art effectiveness compared to approaches that do not exploit pretraining on external data. Extensive ablation studies suggest that relevance and semantic matching signals are complementary across many problem settings, regardless of the choice of underlying encoders.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1540.pdf",
        "title": "Bridging the Gap between Relevance Matching and Semantic Matching for Short Text Similarity Modeling"
    },
    {
        "paper_id": "D19-1541",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Semantic role labeling (SRL) aims to identify the predicate-argument structure of a sentence. Inspired by the strong correlation between syntax and semantics, previous works pay much attention to improve SRL performance on exploiting syntactic knowledge, achieving significant results. Pipeline methods based on automatic syntactic trees and multi-task learning (MTL) approaches using standard syntactic trees are two common research orientations. In this paper, we adopt a simple unified span-based model for both span-based and word-based Chinese SRL as a strong baseline. Besides, we present a MTL framework that includes the basic SRL module and a dependency parser module. Different from the commonly used hard parameter sharing strategy in MTL, the main idea is to extract implicit syntactic representations from the dependency parser as external inputs for the basic SRL model. Experiments on the benchmarks of Chinese Proposition Bank 1.0 and CoNLL-2009 Chinese datasets show that our proposed framework can effectively improve the performance over the strong baselines. With the external BERT representations, our framework achieves new state-of-the-art 87.54 and 88.5 F1 scores on the two test data of the two benchmarks, respectively. In-depth analysis are conducted to gain more insights on the proposed framework and the effectiveness of syntax.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1541.pdf",
        "title": "A Syntax-aware Multi-task Learning Framework for Chinese Semantic Role Labeling"
    },
    {
        "paper_id": "D19-1542",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "A semantic equivalence assessment is defined as a task that assesses semantic equivalence in a sentence pair by binary judgment (i.e., paraphrase identification) or grading (i.e., semantic textual similarity measurement). It constitutes a set of tasks crucial for research on natural language understanding. Recently, BERT realized a breakthrough in sentence representation learning (Devlin et al., 2019), which is broadly transferable to various NLP tasks. While BERT\u2019s performance improves by increasing its model size, the required computational power is an obstacle preventing practical applications from adopting the technology. Herein, we propose to inject phrasal paraphrase relations into BERT in order to generate suitable representations for semantic equivalence assessment instead of increasing the model size. Experiments on standard natural language understanding tasks confirm that our method effectively improves a smaller BERT model while maintaining the model size. The generated model exhibits superior performance compared to a larger BERT model on semantic equivalence assessment tasks. Furthermore, it achieves larger performance gains on tasks with limited training datasets for fine-tuning, which is a property desirable for transfer learning.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1542.pdf",
        "title": "Transfer Fine-Tuning: A BERT Case Study"
    },
    {
        "paper_id": "D19-1543",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "On text-to-SQL generation, the input utterance usually contains lots of tokens that are related to column names or cells in the table, called table-related tokens. These table-related tokens are troublesome for the downstream neural semantic parser because it brings complex semantics and hinders the sharing across the training examples. However, existing approaches either ignore handling these tokens before the semantic parser or simply use deterministic approaches based on string-match or word embedding similarity. In this work, we propose a more efficient approach to handle table-related tokens before the semantic parser. First, we formulate it as a sequential tagging problem and propose a two-stage anonymization model to learn the semantic relationship between tables and input utterances. Then, we leverage the implicit supervision from SQL queries by policy gradient to guide the training. Experiments demonstrate that our approach consistently improves performances of different neural semantic parsers and significantly outperforms deterministic approaches.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1543.pdf",
        "title": "Data-Anonymous Encoding for Text-to-SQL Generation"
    },
    {
        "paper_id": "D19-1544",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Semantic role labeling (SRL) involves extracting propositions (i.e. predicates and their typed arguments) from natural language sentences. State-of-the-art SRL models rely on powerful encoders (e.g., LSTMs) and do not model non-local interaction between arguments. We propose a new approach to modeling these interactions while maintaining efficient inference. Specifically, we use Capsule Networks (Sabour et al., 2017): each proposition is encoded as a tuple of capsules, one capsule per argument type (i.e. role). These tuples serve as embeddings of entire propositions. In every network layer, the capsules interact with each other and with representations of words in the sentence. Each iteration results in updated proposition embeddings and updated predictions about the SRL structure. Our model substantially outperforms the non-refinement baseline model on all 7 CoNLL-2019 languages and achieves state-of-the-art results on 5 languages (including English) for dependency SRL. We analyze the types of mistakes corrected by the refinement procedure. For example, each role is typically (but not always) filled with at most one argument. Whereas enforcing this approximate constraint is not useful with the modern SRL system, iterative procedure corrects the mistakes by capturing this intuition in a flexible and context-sensitive way.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1544.pdf",
        "title": "Capturing Argument Interaction in Semantic Role Labeling with Capsule Networks"
    },
    {
        "paper_id": "D19-1545",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Programmers typically organize executable source code using high-level coding patterns or idiomatic structures such as nested loops, exception handlers and recursive blocks, rather than as individual code tokens. In contrast, state of the art (SOTA) semantic parsers still map natural language instructions to source code by building the code syntax tree one node at a time. In this paper, we introduce an iterative method to extract code idioms from large source code corpora by repeatedly collapsing most-frequent depth-2 subtrees of their syntax trees, and train semantic parsers to apply these idioms during decoding. Applying idiom-based decoding on a recent context-dependent semantic parsing task improves the SOTA by 2.2% BLEU score while reducing training time by more than 50%. This improved speed enables us to scale up the model by training on an extended training set that is 5\u00d7 larger, to further move up the SOTA by an additional 2.3% BLEU and 0.9% exact match. Finally, idioms also significantly improve accuracy of semantic parsing to SQL on the ATIS-SQL dataset, when training data is limited.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1545.pdf",
        "title": "Learning Programmatic Idioms for Scalable Semantic Parsing"
    },
    {
        "paper_id": "D19-1547",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "As a promising paradigm, interactive semantic parsing has shown to improve both semantic parsing accuracy and user confidence in the results. In this paper, we propose a new, unified formulation of the interactive semantic parsing problem, where the goal is to design a model-based intelligent agent. The agent maintains its own state as the current predicted semantic parse, decides whether and where human intervention is needed, and generates a clarification question in natural language. A key part of the agent is a world model: it takes a percept (either an initial question or subsequent feedback from the user) and transitions to a new state. We then propose a simple yet remarkably effective instantiation of our framework, demonstrated on two text-to-SQL datasets (WikiSQL and Spider) with different state-of-the-art base semantic parsers. Compared to an existing interactive semantic parsing approach that treats the base parser as a black box, our approach solicits less user feedback but yields higher run-time accuracy.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1547.pdf",
        "title": "Model-based Interactive Semantic Parsing: A Unified Framework and A Text-to-SQL Case Study"
    },
    {
        "paper_id": "D19-1548",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Recent studies on AMR-to-text generation often formalize the task as a sequence-to-sequence (seq2seq) learning problem by converting an Abstract Meaning Representation (AMR) graph into a word sequences. Graph structures are further modeled into the seq2seq framework in order to utilize the structural information in the AMR graphs. However, previous approaches only consider the relations between directly connected concepts while ignoring the rich structure in AMR graphs. In this paper we eliminate such a strong limitation and propose a novel structure-aware self-attention approach to better model the relations between indirectly connected concepts in the state-of-the-art seq2seq model, i.e. the Transformer. In particular, a few different methods are explored to learn structural representations between two concepts. Experimental results on English AMR benchmark datasets show that our approach significantly outperforms the state-of-the-art with 29.66 and 31.82 BLEU scores on LDC2015E86 and LDC2017T10, respectively. To the best of our knowledge, these are the best results achieved so far by supervised models on the benchmarks.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1548.pdf",
        "title": "Modeling Graph Structure in Transformer for Better AMR-to-Text Generation"
    },
    {
        "paper_id": "D19-1554",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Recent work has shown that current text classification models are fragile and sensitive to simple perturbations. In this work, we propose a novel adversarial training approach, LexicalAT, to improve the robustness of current classification models. The proposed approach consists of a generator and a classifier. The generator learns to generate examples to attack the classifier while the classifier learns to defend these attacks. Considering the diversity of attacks, the generator uses a large-scale lexical knowledge base, WordNet, to generate attacking examples by replacing some words in training examples with their synonyms (e.g., sad and unhappy), neighbor words (e.g., fox and wolf), or super-superior words (e.g., chair and armchair). Due to the discrete generation step in the generator, we use policy gradient, a reinforcement learning approach, to train the two modules. Experiments show LexicalAT outperforms strong baselines and reduces test errors on various neural networks, including CNN, RNN, and BERT.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1554.pdf",
        "title": "LexicalAT: Lexical-Based Adversarial Reinforcement Training for Robust Sentiment Classification"
    },
    {
        "paper_id": "D19-1555",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Opinionated text often involves attributes such as authorship and location that influence the sentiments expressed for different aspects. We posit that structural and semantic correspondence is both prevalent in opinionated text, especially when associated with attributes, and crucial in accurately revealing its latent aspect and sentiment structure. However, it is not recognized by existing approaches. We propose Trait, an unsupervised probabilistic model that discovers aspects and sentiments from text and associates them with different attributes. To this end, Trait infers and leverages structural and semantic correspondence using a Markov Random Field. We show empirically that by incorporating attributes explicitly Trait significantly outperforms state-of-the-art baselines both by generating attribute profiles that accord with our intuitions, as shown via visualization, and yielding topics of greater semantic cohesion.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1555.pdf",
        "title": "Leveraging Structural and Semantic Correspondence for Attribute-Oriented Aspect Sentiment Discovery"
    },
    {
        "paper_id": "D19-1557",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "This paper proposes a way to improve the performance of existing algorithms for text classification in domains with strong language semantics. A proposed domain adaptation layer learns weights to combine a generic and a domain specific (DS) word embedding into a domain adapted (DA) embedding. The DA word embeddings are then used as inputs to a generic encoder + classifier framework to perform a downstream task such as classification. This adaptation layer is particularly suited to data sets that are modest in size, and which are, therefore, not ideal candidates for (re)training a deep neural network architecture. Results on binary and multi-class classification tasks using popular encoder architectures, including current state-of-the-art methods (with and without the shallow adaptation layer) show the effectiveness of the proposed approach.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1557.pdf",
        "title": "Shallow Domain Adaptive Embeddings for Sentiment Analysis"
    },
    {
        "paper_id": "D19-1563",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Emotion cause analysis, which aims to identify the reasons behind emotions, is a key topic in sentiment analysis. A variety of neural network models have been proposed recently, however, these previous models mostly focus on the learning architecture with local textual information, ignoring the discourse and prior knowledge, which play crucial roles in human text comprehension. In this paper, we propose a new method to extract emotion cause with a hierarchical neural model and knowledge-based regularizations, which aims to incorporate discourse context information and restrain the parameters by sentiment lexicon and common knowledge. The experimental results demonstrate that our proposed method achieves the state-of-the-art performance on two public datasets in different languages (Chinese and English), outperforming a number of competitive baselines by at least 2.08% in F-measure.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1563.pdf",
        "title": "A Knowledge Regularized Hierarchical Approach for Emotion Cause Analysis"
    },
    {
        "paper_id": "D19-1565",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Propaganda aims at influencing people\u2019s mindset with the purpose of advancing a specific agenda. Previous work has addressed propaganda detection at document level, typically labelling all articles from a propagandistic news outlet as propaganda. Such noisy gold labels inevitably affect the quality of any learning system trained on them. A further issue with most existing systems is the lack of explainability. To overcome these limitations, we propose a novel task: performing fine-grained analysis of texts by detecting all fragments that contain propaganda techniques as well as their type. In particular, we create a corpus of news articles manually annotated at fragment level with eighteen propaganda techniques and propose a suitable evaluation measure. We further design a novel multi-granularity neural network, and we show that it outperforms several strong BERT-based baselines.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1565.pdf",
        "title": "Fine-Grained Analysis of Propaganda in News Article"
    },
    {
        "paper_id": "D19-1569",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "We propose a method based on neural networks to identify the sentiment polarity of opinion words expressed on a specific aspect of a sentence. Although a large majority of works typically focus on leveraging the expressive power of neural networks in handling this task, we explore the possibility of integrating dependency trees with neural networks for representation learning. To this end, we present a convolution over a dependency tree (CDT) model which exploits a Bi-directional Long Short Term Memory (Bi-LSTM) to learn representations for features of a sentence, and further enhance the embeddings with a graph convolutional network (GCN) which operates directly on the dependency tree of the sentence. Our approach propagates both contextual and dependency information from opinion words to aspect words, offering discriminative properties for supervision. Experimental results ranks our approach as the new state-of-the-art in aspect-based sentiment classification.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1569.pdf",
        "title": "Aspect-Level Sentiment Analysis Via Convolution over Dependency Tree"
    },
    {
        "paper_id": "D19-1570",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Many Data Augmentation (DA) methods have been proposed for neural machine translation. Existing works measure the superiority of DA methods in terms of their performance on a specific test set, but we find that some DA methods do not exhibit consistent improvements across translation tasks. Based on the observation, this paper makes an initial attempt to answer a fundamental question: what benefits, which are consistent across different methods and tasks, does DA in general obtain? Inspired by recent theoretic advances in deep learning, the paper understands DA from two perspectives towards the generalization ability of a model: input sensitivity and prediction margin, which are defined independent of specific test set thereby may lead to findings with relatively low variance. Extensive experiments show that relatively consistent benefits across five DA methods and four translation tasks are achieved regarding both perspectives.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1570.pdf",
        "title": "Understanding Data Augmentation in Neural Machine Translation: Two Perspectives towards Generalization"
    },
    {
        "paper_id": "D19-1571",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Previous work on neural noisy channel modeling relied on latent variable models that incrementally process the source and target sentence. This makes decoding decisions based on partial source prefixes even though the full source is available. We pursue an alternative approach based on standard sequence to sequence models which utilize the entire source. These models perform remarkably well as channel models, even though they have neither been trained on, nor designed to factor over incomplete target sentences. Experiments with neural language models trained on billions of words show that noisy channel models can outperform a direct model by up to 3.2 BLEU on WMT\u201917 German-English translation. We evaluate on four language-pairs and our channel models consistently outperform strong alternatives such right-to-left reranking models and ensembles of direct models.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1571.pdf",
        "title": "Simple and Effective Noisy Channel Modeling for Neural Machine Translation"
    },
    {
        "paper_id": "D19-1576",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "The key to multilingual grammar induction is to couple grammar parameters of different languages together by exploiting the similarity between languages. Previous work relies on linguistic phylogenetic knowledge to specify similarity between languages. In this work, we propose a novel universal grammar induction approach that represents language identities with continuous vectors and employs a neural network to predict grammar parameters based on the representation. Without any prior linguistic phylogenetic knowledge, we automatically capture similarity between languages with the vector representations and softly tie the grammar parameters of different languages. In our experiments, we apply our approach to 15 languages across 8 language families and subfamilies in the Universal Dependency Treebank dataset, and we observe substantial performance gain on average over monolingual and multilingual baselines.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1576.pdf",
        "title": "Multilingual Grammar Induction with Continuous Language Identification"
    },
    {
        "paper_id": "D19-1581",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Recognizing affective events that trigger positive or negative sentiment has a wide range of natural language processing applications but remains a challenging problem mainly because the polarity of an event is not necessarily predictable from its constituent words. In this paper, we propose to propagate affective polarity using discourse relations. Our method is simple and only requires a very small seed lexicon and a large raw corpus. Our experiments using Japanese data show that our method learns affective events effectively without manually labeled data. It also improves supervised learning results when labeled data are small.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1581.pdf",
        "title": "Minimally Supervised Learning of Affective Events Using Discourse Relations"
    },
    {
        "paper_id": "D19-1582",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Syntactic relations are broadly used in many NLP tasks. For event detection, syntactic relation representations based on dependency tree can better capture the interrelations between candidate trigger words and related entities than sentence representations. But, existing studies only use first-order syntactic relations (i.e., the arcs) in dependency trees to identify trigger words. For this reason, this paper proposes a new method for event detection, which uses a dependency tree based graph convolution network with aggregative attention to explicitly model and aggregate multi-order syntactic representations in sentences. Experimental comparison with state-of-the-art baselines shows the superiority of the proposed method.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1582.pdf",
        "title": "Event Detection with Multi-Order Graph Convolution and Aggregated Attention"
    },
    {
        "paper_id": "D19-1585",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "We examine the capabilities of a unified, multi-task framework for three information extraction tasks: named entity recognition, relation extraction, and event extraction. Our framework (called DyGIE++) accomplishes all tasks by enumerating, refining, and scoring text spans designed to capture local (within-sentence) and global (cross-sentence) context. Our framework achieves state-of-the-art results across all tasks, on four datasets from a variety of domains. We perform experiments comparing different techniques to construct span representations. Contextualized embeddings like BERT perform well at capturing relationships among entities in the same or adjacent sentences, while dynamic span graph updates model long-range cross-sentence relationships. For instance, propagating span representations via predicted coreference links can enable the model to disambiguate challenging entity mentions. Our code is publicly available at https://github.com/dwadden/dygiepp and can be easily adapted for new tasks or datasets.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1585.pdf",
        "title": "Entity, Relation, and Event Extraction with Contextualized Span Representations"
    },
    {
        "paper_id": "D19-1588",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "We apply BERT to coreference resolution, achieving a new state of the art on the GAP (+11.5 F1) and OntoNotes (+3.9 F1) benchmarks. A qualitative analysis of model predictions indicates that, compared to ELMo and BERT-base, BERT-large is particularly better at distinguishing between related but distinct entities (e.g., President and CEO), but that there is still room for improvement in modeling document-level context, conversations, and mention paraphrasing. We will release all code and trained models upon publication.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1588.pdf",
        "title": "BERT for Coreference Resolution: Baselines and Analysis"
    },
    {
        "paper_id": "D19-1589",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Generating a long, coherent text such as a paragraph requires a high-level control of different levels of relations between sentences (e.g., tense, coreference). We call such a logical connection between sentences as a (paragraph) flow. In order to produce a coherent flow of text, we explore two forms of intersentential relations in a paragraph: one is a human-created linguistical relation that forms a structure (e.g., discourse tree) and the other is a relation from latent representation learned from the sentences themselves. Our two proposed models incorporate each form of relations into document-level language models: the former is a supervised model that jointly learns a language model as well as discourse relation prediction, and the latter is an unsupervised model that is hierarchically conditioned by a recurrent neural network (RNN) over the latent information. Our proposed models with both forms of relations outperform the baselines in partially conditioned paragraph generation task. Our codes and data are publicly available.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1589.pdf",
        "title": "Linguistic Versus Latent Relations for Modeling Coherent Flow in Paragraphs"
    },
    {
        "paper_id": "D19-1590",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "We propose new BERT-based methods for recognizing event causality such as \u201csmoke cigarettes\u201d \u2013> \u201cdie of lung cancer\u201d written in web texts. In our methods, we grasp each annotator\u2019s policy by training multiple classifiers, each of which predicts the labels given by a single annotator, and combine the resulting classifiers\u2019 outputs to predict the final labels determined by majority vote. Furthermore, we investigate the effect of supplying background knowledge to our classifiers. Since BERT models are pre-trained with a large corpus, some sort of background knowledge for event causality may be learned during pre-training. Our experiments with a Japanese dataset suggest that this is actually the case: Performance improved when we pre-trained the BERT models with web texts containing a large number of event causalities instead of Wikipedia articles or randomly sampled web texts. However, this effect was limited. Therefore, we further improved performance by simply adding texts related to an input causality candidate as background knowledge to the input of the BERT models. We believe these findings indicate a promising future research direction.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1590.pdf",
        "title": "Event Causality Recognition Exploiting Multiple Annotators\u2019 Judgments and Background Knowledge"
    },
    {
        "paper_id": "D19-1596",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "While models for Visual Question Answering (VQA) have steadily improved over the years, interacting with one quickly reveals that these models lack consistency. For instance, if a model answers \u201cred\u201d to \u201cWhat color is the balloon?\u201d, it might answer \u201cno\u201d if asked, \u201cIs the balloon red?\u201d. These responses violate simple notions of entailment and raise questions about how effectively VQA models ground language. In this work, we introduce a dataset, ConVQA, and metrics that enable quantitative evaluation of consistency in VQA. For a given observable fact in an image (e.g. the balloon\u2019s color), we generate a set of logically consistent question-answer (QA) pairs (e.g. Is the balloon red?) and also collect a human-annotated set of common-sense based consistent QA pairs (e.g. Is the balloon the same color as tomato sauce?). Further, we propose a consistency-improving data augmentation module, a Consistency Teacher Module (CTM). CTM automatically generates entailed (or similar-intent) questions for a source QA pair and fine-tunes the VQA model if the VQA\u2019s answer to the entailed question is consistent with the source QA pair. We demonstrate that our CTM-based training improves the consistency of VQA models on the Con-VQA datasets and is a strong baseline for further research.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1596.pdf",
        "title": "Sunny and Dark Outside?! Improving Answer Consistency in VQA through Entailed Question Generation"
    },
    {
        "paper_id": "D19-1599",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "BERT model has been successfully applied to open-domain QA tasks. However, previous work trains BERT by viewing passages corresponding to the same question as independent training instances, which may cause incomparable scores for answers from different passages. To tackle this issue, we propose a multi-passage BERT model to globally normalize answer scores across all passages of the same question, and this change enables our QA model find better answers by utilizing more passages. In addition, we find that splitting articles into passages with the length of 100 words by sliding window improves performance by 4%. By leveraging a passage ranker to select high-quality passages, multi-passage BERT gains additional 2%. Experiments on four standard benchmarks showed that our multi-passage BERT outperforms all state-of-the-art models on all benchmarks. In particular, on the OpenSQuAD dataset, our model gains 21.4% EM and 21.5% F1 over all non-BERT models, and 5.8% EM and 6.5% F1 over BERT-based models.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1599.pdf",
        "title": "Multi-passage BERT: A Globally Normalized BERT Model for Open-domain Question Answering"
    },
    {
        "paper_id": "D19-1616",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Text summarization is considered as a challenging task in the NLP community. The availability of datasets for the task of multilingual text summarization is rare, and such datasets are difficult to construct. In this work, we build an abstract text summarizer for the German language text using the state-of-the-art \u201cTransformer\u201d model. We propose an iterative data augmentation approach which uses synthetic data along with the real summarization data for the German language. To generate synthetic data, the Common Crawl (German) dataset is exploited, which covers different domains. The synthetic data is effective for the low resource condition and is particularly helpful for our multilingual scenario where availability of summarizing data is still a challenging issue. The data are also useful in deep learning scenarios where the neural models require a large amount of training data for utilization of its capacity. The obtained summarization performance is measured in terms of ROUGE and BLEU score. We achieve an absolute improvement of +1.5 and +16.0 in ROUGE1 F1 (R1_F1) on the development and test sets, respectively, compared to the system which does not rely on data augmentation.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1616.pdf",
        "title": "Abstract Text Summarization: A Low Resource Challenge"
    },
    {
        "paper_id": "D19-1627",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Contextualized word embeddings have boosted many NLP tasks compared with traditional static word embeddings. However, the word with a specific sense may have different contextualized embeddings due to its various contexts. To further investigate what contextualized word embeddings capture, this paper analyzes whether they can indicate the corresponding sense definitions and proposes a general framework that is capable of explaining word meanings given contextualized word embeddings for better interpretation. The experiments show that both ELMo and BERT embeddings can be well interpreted via a readable textual form, and the findings may benefit the research community for a better understanding of what the embeddings capture.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1627.pdf",
        "title": "What Does This Word Mean? Explaining Contextualized Embeddings with Natural Language Definition"
    },
    {
        "paper_id": "D19-1634",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Automatic post-editing (APE), which aims to correct errors in the output of machine translation systems in a post-processing step, is an important task in natural language processing. While recent work has achieved considerable performance gains by using neural networks, how to model the copying mechanism for APE remains a challenge. In this work, we propose a new method for modeling copying for APE. To better identify translation errors, our method learns the representations of source sentences and system outputs in an interactive way. These representations are used to explicitly indicate which words in the system outputs should be copied. Finally, CopyNet (Gu et.al., 2016) can be combined with our method to place the copied words in correct positions in post-edited translations. Experiments on the datasets of the WMT 2016-2017 APE shared tasks show that our approach outperforms all best published results.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1634.pdf",
        "title": "Learning to Copy for Automatic Post-Editing"
    },
    {
        "paper_id": "D19-1638",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "We present set to ordered text, a natural language generation task applied to automatically generating discharge instructions from admission ICD (International Classification of Diseases) codes. This task differs from other natural language generation tasks in the following ways: (1) The input is a set of identifiable entities (ICD codes) where the relations between individual entity are not explicitly specified. (2) The output text is not a narrative description (e.g. news articles) composed from the input. Rather, inferences are made from the input (symptoms specified in ICD codes) to generate the output (instructions). (3) There is an optimal order in which each sentence (instruction) should appear in the output. Unlike most other tasks, neither the input (ICD codes) nor their corresponding symptoms appear in the output, so the ordering of the output instructions needs to be learned in an unsupervised fashion. Based on clinical intuition, we hypothesize that each instruction in the output is mapped to a subset of ICD codes specified in the input. We propose a neural architecture that jointly models (a) subset selection: choosing relevant subsets from a set of input entities; (b) content ordering: learning the order of instructions; and (c) text generation: representing the instructions corresponding to the selected subsets in natural language. In addition, we penalize redundancy during beam search to improve tractability for long text generation. Our model outperforms baseline models in BLEU scores and human evaluation. We plan to extend this work to other tasks such as recipe generation from ingredients.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1638.pdf",
        "title": "Set to Ordered Text: Generating Discharge Instructions from Medical Billing Codes"
    },
    {
        "paper_id": "D19-1647",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Attributing a particular property to a person by naming another person, who is typically wellknown for the respective property, is called a Vossian Antonomasia (VA). This subtpye of metonymy, which overlaps with metaphor, has a specific syntax and is especially frequent in journalistic texts. While identifying Vossian Antonomasia is of particular interest in the study of stylistics, it is also a source of errors in relation and fact extraction as an explicitly mentioned entity occurs only metaphorically and should not be associated with respective contexts. Despite rather simple syntactic variations, the automatic extraction of VA was never addressed as yet since it requires a deeper semantic understanding of mentioned entities and underlying relations. In this paper, we propose a first method for the extraction of VAs that works completely automatically. Our approaches use named entity recognition, distant supervision based on Wikidata, and a bi-directional LSTM for postprocessing. The evaluation on 1.8 million articles of the New York Times corpus shows that our approach significantly outperforms the only existing semi-automatic approach for VA identification by more than 30 percentage points in precision.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1647.pdf",
        "title": "\u201cA Buster Keaton of Linguistics\u201d: First Automated Approaches for the Extraction of Vossian Antonomasia"
    },
    {
        "paper_id": "D19-1648",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "We propose a method to improve named entity recognition (NER) for chemical compounds using multi-task learning by jointly training a chemical NER model and a chemical com- pound paraphrase model. Our method en- ables the long short-term memory (LSTM) of the NER model to capture chemical com- pound paraphrases by sharing the parameters of the LSTM and character embeddings be- tween the two models. The experimental re- sults on the BioCreative IV\u2019s CHEMDNER task show that our method improves chemi- cal NER and achieves state-of-the-art perfor- mance.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1648.pdf",
        "title": "Multi-Task Learning for Chemical Named Entity Recognition with Chemical Compound Paraphrasing"
    },
    {
        "paper_id": "D19-1653",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "In online arguments, identifying how users construct their arguments to persuade others is important in order to understand a persuasive strategy directly. However, existing research lacks empirical investigations on highly semantic aspects of elementary units (EUs), such as propositions for a persuasive online argument. Therefore, this paper focuses on a pilot study, revealing a persuasion strategy using EUs. Our contributions are as follows: (1) annotating five types of EUs in a persuasive forum, the so-called ChangeMyView, (2) revealing both intuitive and non-intuitive strategic insights for the persuasion by analyzing 4612 annotated EUs, and (3) proposing baseline neural models that identify the EU boundary and type. Our observations imply that EUs definitively characterize online persuasion strategies.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1653.pdf",
        "title": "Revealing and Predicting Online Persuasion Strategy with Elementary Units"
    },
    {
        "paper_id": "D19-1659",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Flipping sentiment while preserving sentence meaning is challenging because parallel sentences with the same content but different sentiment polarities are not always available for model learning. We introduce a method for acquiring imperfectly aligned sentences from non-parallel corpora and propose a model that learns to minimize the sentiment and content losses in a fully end-to-end manner. Our model is simple and offers well-balanced results across two domains: Yelp restaurant and Amazon product reviews.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1659.pdf",
        "title": "Learning to Flip the Sentiment of Reviews from Non-Parallel Corpora"
    },
    {
        "paper_id": "D19-1665",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Stance detection in social media is a well-studied task in a variety of domains. Nevertheless, previous work has mostly focused on multiclass versions of the problem, where the labels are mutually exclusive, and typically positive, negative or neutral. In this paper, we address versions of the task in which an utterance can have multiple labels, thus corresponding to multilabel classification. We propose a method that explicitly incorporates label dependencies in the training objective and compare it against a variety of baselines, as well as a reduction of multilabel to multiclass learning. In experiments with three datasets, we find that our proposed method improves upon all baselines on two out of three datasets. We also show that the reduction of multilabel to multiclass classification can be very competitive, especially in cases where the output consists of a small number of labels and one can enumerate over all label combinations.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1665.pdf",
        "title": "Incorporating Label Dependencies in Multilabel Stance Detection"
    },
    {
        "paper_id": "D19-1667",
        "conference": "emnlp",
        "year": "2019",
        "abstract": "Judgment prediction for legal cases has attracted much research efforts for its practice use, of which the ultimate goal is prison term prediction. While existing work merely predicts the total prison term, in reality a defendant is often charged with multiple crimes. In this paper, we argue that charge-based prison term prediction (CPTP) not only better fits realistic needs, but also makes the total prison term prediction more accurate and interpretable. We collect the first large-scale structured data for CPTP and evaluate several competitive baselines. Based on the observation that fine-grained feature selection is the key to achieving good performance, we propose the Deep Gating Network (DGN) for charge-specific feature selection and aggregation. Experiments show that DGN achieves the state-of-the-art performance.",
        "url_pdf": "https://www.aclweb.org/anthology/D19-1667.pdf",
        "title": "Charge-Based Prison Term Prediction with Deep Gating Network"
    },
    {
        "paper_id": "P16-1111",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1111.pdf",
        "title": "Predicting the Rise and Fall of Scientific Topics from Trends in their Rhetorical Framing"
    },
    {
        "paper_id": "P16-1112",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1112.pdf",
        "title": "Compositional Sequence Labeling Models for Error Detection in Learner Writing"
    },
    {
        "paper_id": "P16-1113",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1113.pdf",
        "title": "Neural Semantic Role Labeling with Dependency Path Embeddings"
    },
    {
        "paper_id": "P16-1116",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1116.pdf",
        "title": "RBPB: Regularization-Based Pattern Balancing Method for Event Extraction"
    },
    {
        "paper_id": "P16-1118",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1118.pdf",
        "title": "Addressing Limited Data for Textual Entailment Across Domains"
    },
    {
        "paper_id": "P16-1120",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1120.pdf",
        "title": "Bilingual Segmented Topic Model"
    },
    {
        "paper_id": "P16-1123",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1123.pdf",
        "title": "Relation Classification via Multi-Level Attention CNNs"
    },
    {
        "paper_id": "P16-1126",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1126.pdf",
        "title": "The Creation and Analysis of a Website Privacy Policy Corpus"
    },
    {
        "paper_id": "P16-1127",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1127.pdf",
        "title": "Sequence-based Structured Prediction for Semantic Parsing"
    },
    {
        "paper_id": "P16-1129",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1129.pdf",
        "title": "Towards Constructing Sports News from Live Text Commentary"
    },
    {
        "paper_id": "P16-1130",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1130.pdf",
        "title": "A Continuous Space Rule Selection Model for Syntax-based Statistical Machine Translation"
    },
    {
        "paper_id": "P16-1131",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1131.pdf",
        "title": "Probabilistic Graph-based Dependency Parsing with Convolutional Neural Network"
    },
    {
        "paper_id": "P16-1134",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1134.pdf",
        "title": "Segment-Level Sequence Modeling using Gated Recursive Semi-Markov Conditional Random Fields"
    },
    {
        "paper_id": "P16-1137",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1137.pdf",
        "title": "Commonsense Knowledge Base Completion"
    },
    {
        "paper_id": "P16-1140",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1140.pdf",
        "title": "Investigating Language Universal and Specific Properties in Word Embeddings"
    },
    {
        "paper_id": "P16-1148",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1148.pdf",
        "title": "Inferring Perceived Demographics from User Emotional Tone and User-Environment Emotional Contrast"
    },
    {
        "paper_id": "P16-1150",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1150.pdf",
        "title": "Which argument is more convincing? Analyzing and predicting convincingness of Web arguments using bidirectional LSTM"
    },
    {
        "paper_id": "P16-1154",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1154.pdf",
        "title": "Incorporating Copying Mechanism in Sequence-to-Sequence Learning"
    },
    {
        "paper_id": "P16-1159",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1159.pdf",
        "title": "Minimum Risk Training for Neural Machine Translation"
    },
    {
        "paper_id": "P16-1161",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1161.pdf",
        "title": "Target-Side Context for Discriminative Models in Statistical Machine Translation"
    },
    {
        "paper_id": "P16-1168",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1168.pdf",
        "title": "Cross-Lingual Image Caption Generation"
    },
    {
        "paper_id": "P16-1170",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1170.pdf",
        "title": "Generating Natural Questions About an Image"
    },
    {
        "paper_id": "P16-1173",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1173.pdf",
        "title": "Phrase Structure Annotation and Parsing for Learner English"
    },
    {
        "paper_id": "P16-1178",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1178.pdf",
        "title": "Linguistic Benchmarks of Online News Article Quality"
    },
    {
        "paper_id": "P16-1181",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1181.pdf",
        "title": "How to Train Dependency Parsers with Inexact Search for Joint Sentence Boundary Detection and Parsing of Entire Documents"
    },
    {
        "paper_id": "P16-1188",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1188.pdf",
        "title": "Learning-Based Single-Document Summarization with Compression and Anaphoricity Constraints"
    },
    {
        "paper_id": "P16-1191",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1191.pdf",
        "title": "Supersense Embeddings: A Unified Model for Supersense Interpretation, Prediction, and Utilization"
    },
    {
        "paper_id": "P16-1195",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1195.pdf",
        "title": "Summarizing Source Code using a Neural Attention Model"
    },
    {
        "paper_id": "P16-1201",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1201.pdf",
        "title": "Leveraging FrameNet to Improve Automatic Event Detection"
    },
    {
        "paper_id": "P16-1206",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1206.pdf",
        "title": "A New Psychometric-inspired Evaluation Metric for Chinese Word Segmentation"
    },
    {
        "paper_id": "P16-1209",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1209.pdf",
        "title": "Recurrent neural network models for disease name recognition using domain invariant features"
    },
    {
        "paper_id": "P16-1218",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1218.pdf",
        "title": "Graph-based Dependency Parsing with Bidirectional LSTM"
    },
    {
        "paper_id": "P16-1220",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1220.pdf",
        "title": "Question Answering on Freebase via Relation Extraction and Textual Evidence"
    },
    {
        "paper_id": "P16-1222",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1222.pdf",
        "title": "Chinese Couplet Generation with Neural Network Structures"
    },
    {
        "paper_id": "P16-1223",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1223.pdf",
        "title": "A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task"
    },
    {
        "paper_id": "P16-1226",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1226.pdf",
        "title": "Improving Hypernymy Detection with an Integrated Path-based and Distributional Method"
    },
    {
        "paper_id": "P16-1228",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1228.pdf",
        "title": "Harnessing Deep Neural Networks with Logic Rules"
    },
    {
        "paper_id": "P16-1230",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1230.pdf",
        "title": "On-line Active Reward Learning for Policy Optimisation in Spoken Dialogue Systems"
    },
    {
        "paper_id": "P16-1231",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-1231.pdf",
        "title": "Globally Normalized Transition-Based Neural Networks"
    },
    {
        "paper_id": "P16-2002",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-2002.pdf",
        "title": "Scalable Semi-Supervised Query Classification Using Matrix Sketching"
    },
    {
        "paper_id": "P16-2006",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-2006.pdf",
        "title": "Incremental Parsing with Minimal Features Using Bi-Directional LSTM"
    },
    {
        "paper_id": "P16-2011",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-2011.pdf",
        "title": "A Language-Independent Neural Network for Event Detection"
    },
    {
        "paper_id": "P16-2018",
        "conference": "acl",
        "year": "2016",
        "abstract": "",
        "url_pdf": "https://www.aclweb.org/anthology/P16-2018.pdf",
        "title": "Recognizing Salient Entities in Shopping Queries"
    },
    {
        "paper_id": "P17-1003",
        "conference": "acl",
        "year": "2017",
        "abstract": "Harnessing the statistical power of neural networks to perform language understanding and symbolic reasoning is difficult, when it requires executing efficient discrete operations against a large knowledge-base. In this work, we introduce a Neural Symbolic Machine, which contains (a) a neural \u201cprogrammer\u201d, i.e., a sequence-to-sequence model that maps language utterances to programs and utilizes a key-variable memory to handle compositionality (b) a symbolic \u201ccomputer\u201d, i.e., a Lisp interpreter that performs program execution, and helps find good programs by pruning the search space. We apply REINFORCE to directly optimize the task reward of this structured prediction problem. To train with weak supervision and improve the stability of REINFORCE, we augment it with an iterative maximum-likelihood training process. NSM outperforms the state-of-the-art on the WebQuestionsSP dataset when trained from question-answer pairs only, without requiring any feature engineering or domain-specific knowledge.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-1003.pdf",
        "title": "Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision"
    },
    {
        "paper_id": "P17-1005",
        "conference": "acl",
        "year": "2017",
        "abstract": "We introduce a neural semantic parser which is interpretable and scalable. Our model converts natural language utterances to intermediate, domain-general natural language representations in the form of predicate-argument structures, which are induced with a transition system and subsequently mapped to target domains. The semantic parser is trained end-to-end using annotated logical forms or their denotations. We achieve the state of the art on SPADES and GRAPHQUESTIONS and obtain competitive results on GEOQUERY and WEBQUESTIONS. The induced predicate-argument structures shed light on the types of representations useful for semantic parsing and how these are different from linguistically motivated ones.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-1005.pdf",
        "title": "Learning Structured Natural Language Representations for Semantic Parsing"
    },
    {
        "paper_id": "P17-1009",
        "conference": "acl",
        "year": "2017",
        "abstract": "While joint models have been developed for many NLP tasks, the vast majority of event coreference resolvers, including the top-performing resolvers competing in the recent TAC KBP 2016 Event Nugget Detection and Coreference task, are pipeline-based, where the propagation of errors from the trigger detection component to the event coreference component is a major performance limiting factor. To address this problem, we propose a model for jointly learning event coreference, trigger detection, and event anaphoricity. Our joint model is novel in its choice of tasks and its features for capturing cross-task interactions. To our knowledge, this is the first attempt to train a mention-ranking model and employ event anaphoricity for event coreference. Our model achieves the best results to date on the KBP 2016 English and Chinese datasets.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-1009.pdf",
        "title": "Joint Learning for Event Coreference Resolution"
    },
    {
        "paper_id": "P17-1011",
        "conference": "acl",
        "year": "2017",
        "abstract": "Discourse modes play an important role in writing composition and evaluation. This paper presents a study on the manual and automatic identification of narration,exposition, description, argument and emotion expressing sentences in narrative essays. We annotate a corpus to study the characteristics of discourse modes and describe a neural sequence labeling model for identification. Evaluation results show that discourse modes can be identified automatically with an average F1-score of 0.7. We further demonstrate that discourse modes can be used as features that improve automatic essay scoring (AES). The impacts of discourse modes for AES are also discussed.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-1011.pdf",
        "title": "Discourse Mode Identification in Essays"
    },
    {
        "paper_id": "P17-1012",
        "conference": "acl",
        "year": "2017",
        "abstract": "The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence. We present a faster and simpler architecture based on a succession of convolutional layers. This allows to encode the source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies. On WMT\u201916 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and on WMT\u201915 English-German we outperform several recently published results. Our models obtain almost the same accuracy as a very deep LSTM setup on WMT\u201914 English-French translation. We speed up CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-1012.pdf",
        "title": "A Convolutional Encoder Model for Neural Machine Translation"
    },
    {
        "paper_id": "P17-1014",
        "conference": "acl",
        "year": "2017",
        "abstract": "Sequence-to-sequence models have shown strong performance across a broad range of applications. However, their application to parsing and generating text using Abstract Meaning Representation (AMR) has been limited, due to the relatively limited amount of labeled data and the non-sequential nature of the AMR graphs. We present a novel training procedure that can lift this limitation using millions of unlabeled sentences and careful preprocessing of the AMR graphs. For AMR parsing, our model achieves competitive results of 62.1 SMATCH, the current best score reported without significant use of external semantic resources. For AMR generation, our model establishes a new state-of-the-art performance of BLEU 33.8. We present extensive ablative and qualitative analysis including strong evidence that sequence-based AMR models are robust against ordering variations of graph-to-sequence conversions.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-1014.pdf",
        "title": "Neural AMR: Sequence-to-Sequence Models for Parsing and Generation"
    },
    {
        "paper_id": "P17-1051",
        "conference": "acl",
        "year": "2017",
        "abstract": "We present in this paper a novel framework for morpheme segmentation which uses the morpho-syntactic regularities preserved by word representations, in addition to orthographic features, to segment words into morphemes. This framework is the first to consider vocabulary-wide syntactico-semantic information for this task. We also analyze the deficiencies of available benchmarking datasets and introduce our own dataset that was created on the basis of compositionality. We validate our algorithm across datasets and present state-of-the-art results.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-1051.pdf",
        "title": "MORSE: Semantic-ally Drive-n MORpheme SEgment-er"
    },
    {
        "paper_id": "P17-1052",
        "conference": "acl",
        "year": "2017",
        "abstract": "This paper proposes a low-complexity word-level deep convolutional neural network (CNN) architecture for text categorization that can efficiently represent long-range associations in text. In the literature, several deep and complex neural networks have been proposed for this task, assuming availability of relatively large amounts of training data. However, the associated computational complexity increases as the networks go deeper, which poses serious challenges in practical applications. Moreover, it was shown recently that shallow word-level CNNs are more accurate and much faster than the state-of-the-art very deep nets such as character-level CNNs even in the setting of large training data. Motivated by these findings, we carefully studied deepening of word-level CNNs to capture global representations of text, and found a simple network architecture with which the best accuracy can be obtained by increasing the network depth without increasing computational cost by much. We call it deep pyramid CNN. The proposed model with 15 weight layers outperforms the previous best models on six benchmark datasets for sentiment classification and topic categorization.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-1052.pdf",
        "title": "Deep Pyramid Convolutional Neural Networks for Text Categorization"
    },
    {
        "paper_id": "P17-1053",
        "conference": "acl",
        "year": "2017",
        "abstract": "Relation detection is a core component of many NLP applications including Knowledge Base Question Answering (KBQA). In this paper, we propose a hierarchical recurrent neural network enhanced by residual learning which detects KB relations given an input question. Our method uses deep residual bidirectional LSTMs to compare questions and relation names via different levels of abstraction. Additionally, we propose a simple KBQA system that integrates entity linking and our proposed relation detector to make the two components enhance each other. Our experimental results show that our approach not only achieves outstanding relation detection performance, but more importantly, it helps our KBQA system achieve state-of-the-art accuracy for both single-relation (SimpleQuestions) and multi-relation (WebQSP) QA benchmarks.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-1053.pdf",
        "title": "Improved Neural Relation Detection for Knowledge Base Question Answering"
    },
    {
        "paper_id": "P17-1085",
        "conference": "acl",
        "year": "2017",
        "abstract": "We present a novel attention-based recurrent neural network for joint extraction of entity mentions and relations. We show that attention along with long short term memory (LSTM) network can extract semantic relations between entity mentions without having access to dependency trees. Experiments on Automatic Content Extraction (ACE) corpora show that our model significantly outperforms feature-based joint model by Li and Ji (2014). We also compare our model with an end-to-end tree-based LSTM model (SPTree) by Miwa and Bansal (2016) and show that our model performs within 1% on entity mentions and 2% on relations. Our fine-grained analysis also shows that our model performs significantly better on Agent-Artifact relations, while SPTree performs better on Physical and Part-Whole relations.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-1085.pdf",
        "title": "Going out on a limb: Joint Extraction of Entity Mentions and Relations without Dependency Trees"
    },
    {
        "paper_id": "P17-1171",
        "conference": "acl",
        "year": "2017",
        "abstract": "This paper proposes to tackle open-domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-1171.pdf",
        "title": "Reading Wikipedia to Answer Open-Domain Questions"
    },
    {
        "paper_id": "P17-1176",
        "conference": "acl",
        "year": "2017",
        "abstract": "While end-to-end neural machine translation (NMT) has made remarkable progress recently, it still suffers from the data scarcity problem for low-resource language pairs and domains. In this paper, we propose a method for zero-resource NMT by assuming that parallel sentences have close probabilities of generating a sentence in a third language. Based on the assumption, our method is able to train a source-to-target NMT model (\u201cstudent\u201d) without parallel corpora available guided by an existing pivot-to-target NMT model (\u201cteacher\u201d) on a source-pivot parallel corpus. Experimental results show that the proposed method significantly improves over a baseline pivot-based model by +3.0 BLEU points across various language pairs.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-1176.pdf",
        "title": "A Teacher-Student Framework for Zero-Resource Neural Machine Translation"
    },
    {
        "paper_id": "P17-1185",
        "conference": "acl",
        "year": "2017",
        "abstract": "Skip-Gram Negative Sampling (SGNS) word embedding model, well known by its implementation in \u201cword2vec\u201d software, is usually optimized by stochastic gradient descent. However, the optimization of SGNS objective can be viewed as a problem of searching for a good matrix with the low-rank constraint. The most standard way to solve this type of problems is to apply Riemannian optimization framework to optimize the SGNS objective over the manifold of required low-rank matrices. In this paper, we propose an algorithm that optimizes SGNS objective using Riemannian optimization and demonstrates its superiority over popular competitors, such as the original method to train SGNS and SVD over SPPMI matrix.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-1185.pdf",
        "title": "Riemannian Optimization for Skip-Gram Negative Sampling"
    },
    {
        "paper_id": "P17-1187",
        "conference": "acl",
        "year": "2017",
        "abstract": "Sememes are minimum semantic units of word meanings, and the meaning of each word sense is typically composed by several sememes. Since sememes are not explicit for each word, people manually annotate word sememes and form linguistic common-sense knowledge bases. In this paper, we present that, word sememe information can improve word representation learning (WRL), which maps words into a low-dimensional semantic space and serves as a fundamental step for many NLP tasks. The key idea is to utilize word sememes to capture exact meanings of a word within specific contexts accurately. More specifically, we follow the framework of Skip-gram and present three sememe-encoded models to learn representations of sememes, senses and words, where we apply the attention scheme to detect word senses in various contexts. We conduct experiments on two tasks including word similarity and word analogy, and our models significantly outperform baselines. The results indicate that WRL can benefit from sememes via the attention scheme, and also confirm our models being capable of correctly modeling sememe information.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-1187.pdf",
        "title": "Improved Word Representation Learning with Sememes"
    },
    {
        "paper_id": "P17-1189",
        "conference": "acl",
        "year": "2017",
        "abstract": "Previous studies on Chinese semantic role labeling (SRL) have concentrated on a single semantically annotated corpus. But the training data of single corpus is often limited. Whereas the other existing semantically annotated corpora for Chinese SRL are scattered across different annotation frameworks. But still, Data sparsity remains a bottleneck. This situation calls for larger training datasets, or effective approaches which can take advantage of highly heterogeneous data. In this paper, we focus mainly on the latter, that is, to improve Chinese SRL by using heterogeneous corpora together. We propose a novel progressive learning model which augments the Progressive Neural Network with Gated Recurrent Adapters. The model can accommodate heterogeneous inputs and effectively transfer knowledge between them. We also release a new corpus, Chinese SemBank, for Chinese SRL. Experiments on CPB 1.0 show that our model outperforms state-of-the-art methods.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-1189.pdf",
        "title": "A Progressive Learning Approach to Chinese SRL Using Heterogeneous Data"
    },
    {
        "paper_id": "P17-1190",
        "conference": "acl",
        "year": "2017",
        "abstract": "We consider the problem of learning general-purpose, paraphrastic sentence embeddings, revisiting the setting of Wieting et al. (2016b). While they found LSTM recurrent networks to underperform word averaging, we present several developments that together produce the opposite conclusion. These include training on sentence pairs rather than phrase pairs, averaging states to represent sequences, and regularizing aggressively. These improve LSTMs in both transfer learning and supervised settings. We also introduce a new recurrent architecture, the Gated Recurrent Averaging Network, that is inspired by averaging and LSTMs while outperforming them both. We analyze our learned models, finding evidence of preferences for particular parts of speech and dependency relations.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-1190.pdf",
        "title": "Revisiting Recurrent Networks for Paraphrastic Sentence Embeddings"
    },
    {
        "paper_id": "P17-1191",
        "conference": "acl",
        "year": "2017",
        "abstract": "Type-level word embeddings use the same set of parameters to represent all instances of a word regardless of its context, ignoring the inherent lexical ambiguity in language. Instead, we embed semantic concepts (or synsets) as defined in WordNet and represent a word token in a particular context by estimating a distribution over relevant semantic concepts. We use the new, context-sensitive embeddings in a model for predicting prepositional phrase (PP) attachments and jointly learn the concept embeddings and model parameters. We show that using context-sensitive embeddings improves the accuracy of the PP attachment model by 5.4% absolute points, which amounts to a 34.4% relative reduction in errors.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-1191.pdf",
        "title": "Ontology-Aware Token Embeddings for Prepositional Phrase Attachment"
    },
    {
        "paper_id": "P17-1194",
        "conference": "acl",
        "year": "2017",
        "abstract": "We propose a sequence labeling framework with a secondary training objective, learning to predict surrounding words for every word in the dataset. This language modeling objective incentivises the system to learn general-purpose patterns of semantic and syntactic composition, which are also useful for improving accuracy on different sequence labeling tasks. The architecture was evaluated on a range of datasets, covering the tasks of error detection in learner texts, named entity recognition, chunking and POS-tagging. The novel language modeling objective provided consistent performance improvements on every benchmark, without requiring any additional annotated or unannotated data.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-1194.pdf",
        "title": "Semi-supervised Multitask Learning for Sequence Labeling"
    },
    {
        "paper_id": "P17-1195",
        "conference": "acl",
        "year": "2017",
        "abstract": "We have been developing an end-to-end math problem solving system that accepts natural language input. The current paper focuses on how we analyze the problem sentences to produce logical forms. We chose a hybrid approach combining a shallow syntactic analyzer and a manually-developed lexicalized grammar. A feature of the grammar is that it is extensively typed on the basis of a formal ontology for pre-university math. These types are helpful in semantic disambiguation inside and across sentences. Experimental results show that the hybrid system produces a well-formed logical form with 88% precision and 56% recall.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-1195.pdf",
        "title": "Semantic Parsing of Pre-university Math Problems"
    },
    {
        "paper_id": "P17-2001",
        "conference": "acl",
        "year": "2017",
        "abstract": "Temporal relation classification is becoming an active research field. Lots of methods have been proposed, while most of them focus on extracting features from external resources. Less attention has been paid to a significant advance in a closely related task: relation extraction. In this work, we borrow a state-of-the-art method in relation extraction by adopting bidirectional long short-term memory (Bi-LSTM) along dependency paths (DP). We make a \u201ccommon root\u201d assumption to extend DP representations of cross-sentence links. In the final comparison to two state-of-the-art systems on TimeBank-Dense, our model achieves comparable performance, without using external knowledge, as well as manually annotated attributes of entities (class, tense, polarity, etc.).",
        "url_pdf": "https://www.aclweb.org/anthology/P17-2001.pdf",
        "title": "Classifying Temporal Relations by Bidirectional LSTM over Dependency Paths"
    },
    {
        "paper_id": "P17-2007",
        "conference": "acl",
        "year": "2017",
        "abstract": "In this paper, we address semantic parsing in a multilingual context. We train one multilingual model that is capable of parsing natural language sentences from multiple different languages into their corresponding formal semantic representations. We extend an existing sequence-to-tree model to a multi-task learning framework which shares the decoder for generating semantic representations. We report evaluation results on the multilingual GeoQuery corpus and introduce a new multilingual version of the ATIS corpus.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-2007.pdf",
        "title": "Neural Architectures for Multilingual Semantic Parsing"
    },
    {
        "paper_id": "P17-2010",
        "conference": "acl",
        "year": "2017",
        "abstract": "Traditionally, compound splitters are evaluated intrinsically on gold-standard data or extrinsically on the task of statistical machine translation. We explore a novel way for the extrinsic evaluation of compound splitters, namely recognizing textual entailment. Compound splitting has great potential for this novel task that is both transparent and well-defined. Moreover, we show that it addresses certain aspects that are either ignored in intrinsic evaluations or compensated for by taskinternal mechanisms in statistical machine translation. We show significant improvements using different compound splitting methods on a German textual entailment dataset.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-2010.pdf",
        "title": "Evaluating Compound Splitters Extrinsically with Textual Entailment"
    },
    {
        "paper_id": "P17-2021",
        "conference": "acl",
        "year": "2017",
        "abstract": "We present a simple method to incorporate syntactic information about the target language in a neural machine translation system by translating into linearized, lexicalized constituency trees. An experiment on the WMT16 German-English news translation task resulted in an improved BLEU score when compared to a syntax-agnostic NMT baseline trained on the same dataset. An analysis of the translations from the syntax-aware system shows that it performs more reordering during translation in comparison to the baseline. A small-scale human evaluation also showed an advantage to the syntax-aware system.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-2021.pdf",
        "title": "Towards String-To-Tree Neural Machine Translation"
    },
    {
        "paper_id": "P17-2022",
        "conference": "acl",
        "year": "2017",
        "abstract": "Informal first-person narratives are a unique resource for computational models of everyday events and people\u2019s affective reactions to them. People blogging about their day tend not to explicitly say I am happy. Instead they describe situations from which other humans can readily infer their affective reactions. However current sentiment dictionaries are missing much of the information needed to make similar inferences. We build on recent work that models affect in terms of lexical predicate functions and affect on the predicate\u2019s arguments. We present a method to learn proxies for these functions from first-person narratives. We construct a novel fine-grained test set, and show that the patterns we learn improve our ability to predict first-person affective reactions to everyday events, from a Stanford sentiment baseline of .67F to .75F.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-2022.pdf",
        "title": "Learning Lexico-Functional Patterns for First-Person Affect"
    },
    {
        "paper_id": "P17-2034",
        "conference": "acl",
        "year": "2017",
        "abstract": "We present a new visual reasoning language dataset, containing 92,244 pairs of examples of natural statements grounded in synthetic images with 3,962 unique sentences. We describe a method of crowdsourcing linguistically-diverse data, and present an analysis of our data. The data demonstrates a broad set of linguistic phenomena, requiring visual and set-theoretic reasoning. We experiment with various models, and show the data presents a strong challenge for future research.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-2034.pdf",
        "title": "A Corpus of Natural Language for Visual Reasoning"
    },
    {
        "paper_id": "P17-2043",
        "conference": "acl",
        "year": "2017",
        "abstract": "This paper derives an Integer Linear Programming (ILP) formulation to obtain an oracle summary of the compressive summarization paradigm in terms of ROUGE. The oracle summary is essential to reveal the upper bound performance of the paradigm. Experimental results on the DUC dataset showed that ROUGE scores of compressive oracles are significantly higher than those of extractive oracles and state-of-the-art summarization systems. These results reveal that compressive summarization is a promising paradigm and encourage us to continue with the research to produce informative summaries.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-2043.pdf",
        "title": "Oracle Summaries of Compressive Summarization"
    },
    {
        "paper_id": "P17-2045",
        "conference": "acl",
        "year": "2017",
        "abstract": "We propose a model to automatically describe changes introduced in the source code of a program using natural language. Our method receives as input a set of code commits, which contains both the modifications and message introduced by an user. These two modalities are used to train an encoder-decoder architecture. We evaluated our approach on twelve real world open source projects from four different programming languages. Quantitative and qualitative results showed that the proposed approach can generate feasible and semantically sound descriptions not only in standard in-project settings, but also in a cross-project setting.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-2045.pdf",
        "title": "A Neural Architecture for Generating Natural Language Descriptions from Source Code Changes"
    },
    {
        "paper_id": "P17-2046",
        "conference": "acl",
        "year": "2017",
        "abstract": "We propose novel radical features from automatic translation for event extraction. Event detection is a complex language processing task for which it is expensive to collect training data, making generalisation challenging. We derive meaningful subword features from automatic translations into target language. Results suggest this method is particularly useful when using languages with writing systems that facilitate easy decomposition into subword features, e.g., logograms and Cangjie. The best result combines logogram features from Chinese and Japanese with syllable features from Korean, providing an additional 3.0 points f-score when added to state-of-the-art generalisation features on the TAC KBP 2015 Event Nugget task.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-2046.pdf",
        "title": "English Event Detection With Translated Language Features"
    },
    {
        "paper_id": "P17-2047",
        "conference": "acl",
        "year": "2017",
        "abstract": "A critical task for question answering is the final answer selection stage, which has to combine multiple signals available about each answer candidate. This paper proposes EviNets: a novel neural network architecture for factoid question answering. EviNets scores candidate answer entities by combining the available supporting evidence, e.g., structured knowledge bases and unstructured text documents. EviNets represents each piece of evidence with a dense embeddings vector, scores their relevance to the question, and aggregates the support for each candidate to predict their final scores. Each of the components is generic and allows plugging in a variety of models for semantic similarity scoring and information aggregation. We demonstrate the effectiveness of EviNets in experiments on the existing TREC QA and WikiMovies benchmarks, and on the new Yahoo! Answers dataset introduced in this paper. EviNets can be extended to other information types and could facilitate future work on combining evidence signals for joint reasoning in question answering.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-2047.pdf",
        "title": "EviNets: Neural Networks for Combining Evidence Signals for Factoid Question Answering"
    },
    {
        "paper_id": "P17-2052",
        "conference": "acl",
        "year": "2017",
        "abstract": "As entity type systems become richer and more fine-grained, we expect the number of types assigned to a given entity to increase. However, most fine-grained typing work has focused on datasets that exhibit a low degree of type multiplicity. In this paper, we consider the high-multiplicity regime inherent in data sources such as Wikipedia that have semi-open type systems. We introduce a set-prediction approach to this problem and show that our model outperforms unstructured baselines on a new Wikipedia-based fine-grained typing corpus.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-2052.pdf",
        "title": "Fine-Grained Entity Typing with High-Multiplicity Assignments"
    },
    {
        "paper_id": "P17-2055",
        "conference": "acl",
        "year": "2017",
        "abstract": "Information extraction (IE) from text has largely focused on relations between individual entities, such as who has won which award. However, some facts are never fully mentioned, and no IE method has perfect recall. Thus, it is beneficial to also tap contents about the cardinalities of these relations, for example, how many awards someone has won. We introduce this novel problem of extracting cardinalities and discusses the specific challenges that set it apart from standard IE. We present a distant supervision method using conditional random fields. A preliminary evaluation results in precision between 3% and 55%, depending on the difficulty of relations.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-2055.pdf",
        "title": "Cardinal Virtues: Extracting Relation Cardinalities from Text"
    },
    {
        "paper_id": "P17-2059",
        "conference": "acl",
        "year": "2017",
        "abstract": "While natural languages are compositional, how state-of-the-art neural models achieve compositionality is still unclear. We propose a deep network, which not only achieves competitive accuracy for text classification, but also exhibits compositional behavior. That is, while creating hierarchical representations of a piece of text, such as a sentence, the lower layers of the network distribute their layer-specific attention weights to individual words. In contrast, the higher layers compose meaningful phrases and clauses, whose lengths increase as the networks get deeper until fully composing the sentence.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-2059.pdf",
        "title": "A Deep Network with Visual Text Composition Behavior"
    },
    {
        "paper_id": "P17-2060",
        "conference": "acl",
        "year": "2017",
        "abstract": "Neural machine translation (NMT) becomes a new approach to machine translation and generates much more fluent results compared to statistical machine translation (SMT). However, SMT is usually better than NMT in translation adequacy. It is therefore a promising direction to combine the advantages of both NMT and SMT. In this paper, we propose a neural system combination framework leveraging multi-source NMT, which takes as input the outputs of NMT and SMT systems and produces the final translation. Extensive experiments on the Chinese-to-English translation task show that our model archives significant improvement by 5.3 BLEU points over the best single system output and 3.4 BLEU points over the state-of-the-art traditional system combination methods.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-2060.pdf",
        "title": "Neural System Combination for Machine Translation"
    },
    {
        "paper_id": "P17-2066",
        "conference": "acl",
        "year": "2017",
        "abstract": "In recent years, automatic generation of image descriptions (captions), that is, image captioning, has attracted a great deal of attention. In this paper, we particularly consider generating Japanese captions for images. Since most available caption datasets have been constructed for English language, there are few datasets for Japanese. To tackle this problem, we construct a large-scale Japanese image caption dataset based on images from MS-COCO, which is called STAIR Captions. STAIR Captions consists of 820,310 Japanese captions for 164,062 images. In the experiment, we show that a neural network trained using STAIR Captions can generate more natural and better Japanese captions, compared to those generated using English-Japanese machine translation after generating English captions.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-2066.pdf",
        "title": "STAIR Captions: Constructing a Large-Scale Japanese Image Caption Dataset"
    },
    {
        "paper_id": "P17-2069",
        "conference": "acl",
        "year": "2017",
        "abstract": "Count-based distributional semantic models suffer from sparsity due to unobserved but plausible co-occurrences in any text collection. This problem is amplified for models like Anchored Packed Trees (APTs), that take the grammatical type of a co-occurrence into account. We therefore introduce a novel form of distributional inference that exploits the rich type structure in APTs and infers missing data by the same mechanism that is used for semantic composition.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-2069.pdf",
        "title": "Improving Semantic Composition with Offset Inference"
    },
    {
        "paper_id": "P17-2070",
        "conference": "acl",
        "year": "2017",
        "abstract": "Distributed word representations are widely used for modeling words in NLP tasks. Most of the existing models generate one representation per word and do not consider different meanings of a word. We present two approaches to learn multiple topic-sensitive representations per word by using Hierarchical Dirichlet Process. We observe that by modeling topics and integrating topic distributions for each document we obtain representations that are able to distinguish between different meanings of a given word. Our models yield statistically significant improvements for the lexical substitution task indicating that commonly used single word representations, even when combined with contextual information, are insufficient for this task.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-2070.pdf",
        "title": "Learning Topic-Sensitive Word Representations"
    },
    {
        "paper_id": "P17-2080",
        "conference": "acl",
        "year": "2017",
        "abstract": "Deep latent variable models have been shown to facilitate the response generation for open-domain dialog systems. However, these latent variables are highly randomized, leading to uncontrollable generated responses. In this paper, we propose a framework allowing conditional response generation based on specific attributes. These attributes can be either manually assigned or automatically detected. Moreover, the dialog states for both speakers are modeled separately in order to reflect personal features. We validate this framework on two different scenarios, where the attribute refers to genericness and sentiment states respectively. The experiment result testified the potential of our model, where meaningful responses can be generated in accordance with the specified attributes.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-2080.pdf",
        "title": "A Conditional Variational Framework for Dialog Generation"
    },
    {
        "paper_id": "P17-2081",
        "conference": "acl",
        "year": "2017",
        "abstract": "We show that the task of question answering (QA) can significantly benefit from the transfer learning of models trained on a different large, fine-grained QA dataset. We achieve the state of the art in two well-studied QA datasets, WikiQA and SemEval-2016 (Task 3A), through a basic transfer learning technique from SQuAD. For WikiQA, our model outperforms the previous best model by more than 8%. We demonstrate that finer supervision provides better guidance for learning lexical and syntactic information than coarser supervision, through quantitative results and visual analysis. We also show that a similar transfer learning procedure achieves the state of the art on an entailment task.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-2081.pdf",
        "title": "Question Answering through Transfer Learning from Large Fine-grained Supervision Data"
    },
    {
        "paper_id": "P17-2083",
        "conference": "acl",
        "year": "2017",
        "abstract": "We propose a novel generative neural network architecture for Dialogue Act classification. Building upon the Recurrent Neural Network framework, our model incorporates a novel attentional technique and a label to label connection for sequence learning, akin to Hidden Markov Models. The experiments show that both of these innovations lead our model to outperform strong baselines for dialogue act classification on MapTask and Switchboard corpora. We further empirically analyse the effectiveness of each of the new innovations.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-2083.pdf",
        "title": "A Generative Attentional Neural Network Model for Dialogue Act Classification"
    },
    {
        "paper_id": "P17-2085",
        "conference": "acl",
        "year": "2017",
        "abstract": "Traditional Entity Linking (EL) technologies rely on rich structures and properties in the target knowledge base (KB). However, in many applications, the KB may be as simple and sparse as lists of names of the same type (e.g., lists of products). We call it as List-only Entity Linking problem. Fortunately, some mentions may have more cues for linking, which can be used as seed mentions to bridge other mentions and the uninformative entities. In this work, we select most linkable mentions as seed mentions and disambiguate other mentions by comparing them with the seed mentions rather than directly with the entities. Our experiments on linking mentions to seven automatically mined lists show promising results and demonstrate the effectiveness of our approach.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-2085.pdf",
        "title": "List-only Entity Linking"
    },
    {
        "paper_id": "P17-2095",
        "conference": "acl",
        "year": "2017",
        "abstract": "Word segmentation plays a pivotal role in improving any Arabic NLP application. Therefore, a lot of research has been spent in improving its accuracy. Off-the-shelf tools, however, are: i) complicated to use and ii) domain/dialect dependent. We explore three language-independent alternatives to morphological segmentation using: i) data-driven sub-word units, ii) characters as a unit of learning, and iii) word embeddings learned using a character CNN (Convolution Neural Network). On the tasks of Machine Translation and POS tagging, we found these methods to achieve close to, and occasionally surpass state-of-the-art performance. In our analysis, we show that a neural machine translation system is sensitive to the ratio of source and target tokens, and a ratio close to 1 or greater, gives optimal performance.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-2095.pdf",
        "title": "Challenging Language-Dependent Segmentation for Arabic: An Application to Machine Translation and Part-of-Speech Tagging"
    },
    {
        "paper_id": "P17-2096",
        "conference": "acl",
        "year": "2017",
        "abstract": "Neural models with minimal feature engineering have achieved competitive performance against traditional methods for the task of Chinese word segmentation. However, both training and working procedures of the current neural models are computationally inefficient. In this paper, we propose a greedy neural word segmenter with balanced word and character embedding inputs to alleviate the existing drawbacks. Our segmenter is truly end-to-end, capable of performing segmentation much faster and even more accurate than state-of-the-art neural models on Chinese benchmark datasets.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-2096.pdf",
        "title": "Fast and Accurate Neural Word Segmentation for Chinese"
    },
    {
        "paper_id": "P17-2097",
        "conference": "acl",
        "year": "2017",
        "abstract": "We consider the ROC story cloze task (Mostafazadeh et al., 2016) and present several findings. We develop a model that uses hierarchical recurrent networks with attention to encode the sentences in the story and score candidate endings. By discarding the large training set and only training on the validation set, we achieve an accuracy of 74.7%. Even when we discard the story plots (sentences before the ending) and only train to choose the better of two endings, we can still reach 72.5%. We then analyze this \u201cending-only\u201d task setting. We estimate human accuracy to be 78% and find several types of clues that lead to this high accuracy, including those related to sentiment, negation, and general ending likelihood regardless of the story context.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-2097.pdf",
        "title": "Pay Attention to the Ending:Strong Neural Baselines for the ROC Story Cloze Task"
    },
    {
        "paper_id": "P17-2100",
        "conference": "acl",
        "year": "2017",
        "abstract": "Current Chinese social media text summarization models are based on an encoder-decoder framework. Although its generated summaries are similar to source texts literally, they have low semantic relevance. In this work, our goal is to improve semantic relevance between source texts and summaries for Chinese social media summarization. We introduce a Semantic Relevance Based neural model to encourage high semantic similarity between texts and summaries. In our model, the source text is represented by a gated attention encoder, while the summary representation is produced by a decoder. Besides, the similarity score between the representations is maximized during training. Our experiments show that the proposed model outperforms baseline systems on a social media corpus.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-2100.pdf",
        "title": "Improving Semantic Relevance for Sequence-to-Sequence Learning of Chinese Social Media Text Summarization"
    },
    {
        "paper_id": "P17-2102",
        "conference": "acl",
        "year": "2017",
        "abstract": "Pew research polls report 62 percent of U.S. adults get news on social media (Gottfried and Shearer, 2016). In a December poll, 64 percent of U.S. adults said that \u201cmade-up news\u201d has caused a \u201cgreat deal of confusion\u201d about the facts of current events (Barthel et al., 2016). Fabricated stories in social media, ranging from deliberate propaganda to hoaxes and satire, contributes to this confusion in addition to having serious effects on global stability. In this work we build predictive models to classify 130 thousand news posts as suspicious or verified, and predict four sub-types of suspicious news \u2013 satire, hoaxes, clickbait and propaganda. We show that neural network models trained on tweet content and social network interactions outperform lexical models. Unlike previous work on deception detection, we find that adding syntax and grammar features to our models does not improve performance. Incorporating linguistic features improves classification results, however, social interaction features are most informative for finer-grained separation between four types of suspicious news posts.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-2102.pdf",
        "title": "Separating Facts from Fiction: Linguistic Models to Classify Suspicious and Trusted News Posts on Twitter"
    },
    {
        "paper_id": "P17-2103",
        "conference": "acl",
        "year": "2017",
        "abstract": "Counterfactual statements, describing events that did not occur and their consequents, have been studied in areas including problem-solving, affect management, and behavior regulation. People with more counterfactual thinking tend to perceive life events as more personally meaningful. Nevertheless, counterfactuals have not been studied in computational linguistics. We create a counterfactual tweet dataset and explore approaches for detecting counterfactuals using rule-based and supervised statistical approaches. A combined rule-based and statistical approach yielded the best results (F1 = 0.77) outperforming either approach used alone.",
        "url_pdf": "https://www.aclweb.org/anthology/P17-2103.pdf",
        "title": "Recognizing Counterfactual Thinking in Social Media Texts"
    },
    {
        "paper_id": "P18-1001",
        "conference": "acl",
        "year": "2018",
        "abstract": "We introduce Probabilistic FastText, a new model for word embeddings that can capture multiple word senses, sub-word structure, and uncertainty information. In particular, we represent each word with a Gaussian mixture density, where the mean of a mixture component is given by the sum of n-grams. This representation allows the model to share the \u201cstrength\u201d across sub-word structures (e.g. Latin roots), producing accurate representations of rare, misspelt, or even unseen words. Moreover, each component of the mixture can capture a different word sense. Probabilistic FastText outperforms both FastText, which has no probabilistic model, and dictionary-level probabilistic embeddings, which do not incorporate subword structures, on several word-similarity benchmarks, including English RareWord and foreign language datasets. We also achieve state-of-art performance on benchmarks that measure ability to discern different meanings. Thus, our model is the first to achieve best of both the worlds: multi-sense representations while having enriched semantics on rare words.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1001.pdf",
        "title": "Probabilistic FastText for Multi-Sense Word Embeddings"
    },
    {
        "paper_id": "P18-1002",
        "conference": "acl",
        "year": "2018",
        "abstract": "Motivations like domain adaptation, transfer learning, and feature learning have fueled interest in inducing embeddings for rare or unseen words, n-grams, synsets, and other textual features. This paper introduces a la carte embedding, a simple and general alternative to the usual word2vec-based approaches for building such representations that is based upon recent theoretical results for GloVe-like embeddings. Our method relies mainly on a linear transformation that is efficiently learnable using pretrained word vectors and linear regression. This transform is applicable on the fly in the future when a new text feature or rare word is encountered, even if only a single usage example is available. We introduce a new dataset showing how the a la carte method requires fewer examples of words in context to learn high-quality embeddings and we obtain state-of-the-art results on a nonce task and some unsupervised document classification tasks.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1002.pdf",
        "title": "A La Carte Embedding: Cheap but Effective Induction of Semantic Feature Vectors"
    },
    {
        "paper_id": "P18-1003",
        "conference": "acl",
        "year": "2018",
        "abstract": "Word embedding models such as GloVe rely on co-occurrence statistics to learn vector representations of word meaning. While we may similarly expect that co-occurrence statistics can be used to capture rich information about the relationships between different words, existing approaches for modeling such relationships are based on manipulating pre-trained word vectors. In this paper, we introduce a novel method which directly learns relation vectors from co-occurrence statistics. To this end, we first introduce a variant of GloVe, in which there is an explicit connection between word vectors and PMI weighted co-occurrence vectors. We then show how relation vectors can be naturally embedded into the resulting vector space.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1003.pdf",
        "title": "Unsupervised Learning of Distributional Relation Vectors"
    },
    {
        "paper_id": "P18-1004",
        "conference": "acl",
        "year": "2018",
        "abstract": "Semantic specialization of distributional word vectors, referred to as retrofitting, is a process of fine-tuning word vectors using external lexical knowledge in order to better embed some semantic relation. Existing retrofitting models integrate linguistic constraints directly into learning objectives and, consequently, specialize only the vectors of words from the constraints. In this work, in contrast, we transform external lexico-semantic relations into training examples which we use to learn an explicit retrofitting model (ER). The ER model allows us to learn a global specialization function and specialize the vectors of words unobserved in the training data as well. We report large gains over original distributional vector spaces in (1) intrinsic word similarity evaluation and on (2) two downstream tasks \u2212 lexical simplification and dialog state tracking. Finally, we also successfully specialize vector spaces of new languages (i.e., unseen in the training data) by coupling ER with shared multilingual distributional vector spaces.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1004.pdf",
        "title": "Explicit Retrofitting of Distributional Word Vectors"
    },
    {
        "paper_id": "P18-1005",
        "conference": "acl",
        "year": "2018",
        "abstract": "Unsupervised neural machine translation (NMT) is a recently proposed approach for machine translation which aims to train the model without using any labeled data. The models proposed for unsupervised NMT often use only one shared encoder to map the pairs of sentences from different languages to a shared-latent space, which is weak in keeping the unique and internal characteristics of each language, such as the style, terminology, and sentence structure. To address this issue, we introduce an extension by utilizing two independent encoders but sharing some partial weights which are responsible for extracting high-level representations of the input sentences. Besides, two different generative adversarial networks (GANs), namely the local GAN and global GAN, are proposed to enhance the cross-language translation. With this new approach, we achieve significant improvements on English-German, English-French and Chinese-to-English translation tasks.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1005.pdf",
        "title": "Unsupervised Neural Machine Translation with Weight Sharing"
    },
    {
        "paper_id": "P18-1007",
        "conference": "acl",
        "year": "2018",
        "abstract": "Subword units are an effective way to alleviate the open vocabulary problems in neural machine translation (NMT). While sentences are usually converted into unique subword sequences, subword segmentation is potentially ambiguous and multiple segmentations are possible even with the same vocabulary. The question addressed in this paper is whether it is possible to harness the segmentation ambiguity as a noise to improve the robustness of NMT. We present a simple regularization method, subword regularization, which trains the model with multiple subword segmentations probabilistically sampled during training. In addition, for better subword sampling, we propose a new subword segmentation algorithm based on a unigram language model. We experiment with multiple corpora and report consistent improvements especially on low resource and out-of-domain settings.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1007.pdf",
        "title": "Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"
    },
    {
        "paper_id": "P18-1008",
        "conference": "acl",
        "year": "2018",
        "abstract": "The past year has witnessed rapid advances in sequence-to-sequence (seq2seq) modeling for Machine Translation (MT). The classic RNN-based approaches to MT were first out-performed by the convolutional seq2seq model, which was then out-performed by the more recent Transformer model. Each of these new approaches consists of a fundamental architecture accompanied by a set of modeling and training techniques that are in principle applicable to other seq2seq architectures. In this paper, we tease apart the new architectures and their accompanying techniques in two ways. First, we identify several key modeling and training techniques, and apply them to the RNN architecture, yielding a new RNMT+ model that outperforms all of the three fundamental architectures on the benchmark WMT\u201914 English to French and English to German tasks. Second, we analyze the properties of each fundamental seq2seq architecture and devise new hybrid architectures intended to combine their strengths. Our hybrid models obtain further improvements, outperforming the RNMT+ model on both benchmark datasets.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1008.pdf",
        "title": "The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation"
    },
    {
        "paper_id": "P18-1009",
        "conference": "acl",
        "year": "2018",
        "abstract": "We introduce a new entity typing task: given a sentence with an entity mention, the goal is to predict a set of free-form phrases (e.g. skyscraper, songwriter, or criminal) that describe appropriate types for the target entity. This formulation allows us to use a new type of distant supervision at large scale: head words, which indicate the type of the noun phrases they appear in. We show that these ultra-fine types can be crowd-sourced, and introduce new evaluation sets that are much more diverse and fine-grained than existing benchmarks. We present a model that can predict ultra-fine types, and is trained using a multitask objective that pools our new head-word supervision with prior supervision from entity linking. Experimental results demonstrate that our model is effective in predicting entity types at varying granularity; it achieves state of the art performance on an existing fine-grained entity typing benchmark, and sets baselines for our newly-introduced datasets.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1009.pdf",
        "title": "Ultra-Fine Entity Typing"
    },
    {
        "paper_id": "P18-1010",
        "conference": "acl",
        "year": "2018",
        "abstract": "Extraction from raw text to a knowledge base of entities and fine-grained types is often cast as prediction into a flat set of entity and type labels, neglecting the rich hierarchies over types and entities contained in curated ontologies. Previous attempts to incorporate hierarchical structure have yielded little benefit and are restricted to shallow ontologies. This paper presents new methods using real and complex bilinear mappings for integrating hierarchical information, yielding substantial improvement over flat predictions in entity linking and fine-grained entity typing, and achieving new state-of-the-art results for end-to-end models on the benchmark FIGER dataset. We also present two new human-annotated datasets containing wide and deep hierarchies which we will release to the community to encourage further research in this direction: MedMentions, a collection of PubMed abstracts in which 246k mentions have been mapped to the massive UMLS ontology; and TypeNet, which aligns Freebase types with the WordNet hierarchy to obtain nearly 2k entity types. In experiments on all three datasets we show substantial gains from hierarchy-aware training.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1010.pdf",
        "title": "Hierarchical Losses and New Resources for Fine-grained Entity Typing and Linking"
    },
    {
        "paper_id": "P18-1018",
        "conference": "acl",
        "year": "2018",
        "abstract": "Semantic relations are often signaled with prepositional or possessive marking\u2014but extreme polysemy bedevils their analysis and automatic interpretation. We introduce a new annotation scheme, corpus, and task for the disambiguation of prepositions and possessives in English. Unlike previous approaches, our annotations are comprehensive with respect to types and tokens of these markers; use broadly applicable supersense classes rather than fine-grained dictionary definitions; unite prepositions and possessives under the same class inventory; and distinguish between a marker\u2019s lexical contribution and the role it marks in the context of a predicate or scene. Strong interannotator agreement rates, as well as encouraging disambiguation results with established supervised methods, speak to the viability of the scheme and task.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1018.pdf",
        "title": "Comprehensive Supersense Disambiguation of English Prepositions and Possessives"
    },
    {
        "paper_id": "P18-1022",
        "conference": "acl",
        "year": "2018",
        "abstract": "We report on a comparative style analysis of hyperpartisan (extremely one-sided) news and fake news. A corpus of 1,627 articles from 9 political publishers, three each from the mainstream, the hyperpartisan left, and the hyperpartisan right, have been fact-checked by professional journalists at BuzzFeed: 97% of the 299 fake news articles identified are also hyperpartisan. We show how a style analysis can distinguish hyperpartisan news from the mainstream (F1 = 0.78), and satire from both (F1 = 0.81). But stylometry is no silver bullet as style-based fake news detection does not work (F1 = 0.46). We further reveal that left-wing and right-wing news share significantly more stylistic similarities than either does with the mainstream. This result is robust: it has been confirmed by three different modeling approaches, one of which employs Unmasking in a novel way. Applications of our results include partisanship detection and pre-screening for semi-automatic fake news detection.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1022.pdf",
        "title": "A Stylometric Inquiry into Hyperpartisan and Fake News"
    },
    {
        "paper_id": "P18-1026",
        "conference": "acl",
        "year": "2018",
        "abstract": "Many NLP applications can be framed as a graph-to-sequence learning problem. Previous work proposing neural architectures on graph-to-sequence obtained promising results compared to grammar-based approaches but still rely on linearisation heuristics and/or standard recurrent networks to achieve the best performance. In this work propose a new model that encodes the full structural information contained in the graph. Our architecture couples the recently proposed Gated Graph Neural Networks with an input transformation that allows nodes and edges to have their own hidden representations, while tackling the parameter explosion problem present in previous work. Experimental results shows that our model outperforms strong baselines in generation from AMR graphs and syntax-based neural machine translation.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1026.pdf",
        "title": "Graph-to-Sequence Learning using Gated Graph Neural Networks"
    },
    {
        "paper_id": "P18-1028",
        "conference": "acl",
        "year": "2018",
        "abstract": "Recurrent and convolutional neural networks comprise two distinct families of models that have proven to be useful for encoding natural language utterances. In this paper we present SoPa, a new model that aims to bridge these two approaches. SoPa combines neural representation learning with weighted finite-state automata (WFSAs) to learn a soft version of traditional surface patterns. We show that SoPa is an extension of a one-layer CNN, and that such CNNs are equivalent to a restricted version of SoPa, and accordingly, to a restricted form of WFSA. Empirically, on three text classification tasks, SoPa is comparable or better than both a BiLSTM (RNN) baseline and a CNN baseline, and is particularly useful in small data settings.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1028.pdf",
        "title": "Bridging CNNs, RNNs, and Weighted Finite-State Machines"
    },
    {
        "paper_id": "P18-1030",
        "conference": "acl",
        "year": "2018",
        "abstract": "Bi-directional LSTMs are a powerful tool for text representation. On the other hand, they have been shown to suffer various limitations due to their sequential nature. We investigate an alternative LSTM structure for encoding text, which consists of a parallel state for each word. Recurrent steps are used to perform local and global information exchange between words simultaneously, rather than incremental reading of a sequence of words. Results on various classification and sequence labelling benchmarks show that the proposed model has strong representation power, giving highly competitive performances compared to stacked BiLSTM models with similar parameter numbers.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1030.pdf",
        "title": "Sentence-State LSTM for Text Representation"
    },
    {
        "paper_id": "P18-1034",
        "conference": "acl",
        "year": "2018",
        "abstract": "We present a generative model to map natural language questions into SQL queries. Existing neural network based approaches typically generate a SQL query word-by-word, however, a large portion of the generated results is incorrect or not executable due to the mismatch between question words and table contents. Our approach addresses this problem by considering the structure of table and the syntax of SQL language. The quality of the generated SQL query is significantly improved through (1) learning to replicate content from column names, cells or SQL keywords; and (2) improving the generation of WHERE clause by leveraging the column-cell relation. Experiments are conducted on WikiSQL, a recently released dataset with the largest question- SQL pairs. Our approach significantly improves the state-of-the-art execution accuracy from 69.0% to 74.4%.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1034.pdf",
        "title": "Semantic Parsing with Syntax- and Table-Aware SQL Generation"
    },
    {
        "paper_id": "P18-1039",
        "conference": "acl",
        "year": "2018",
        "abstract": "To solve math word problems, previous statistical approaches attempt at learning a direct mapping from a problem description to its corresponding equation system. However, such mappings do not include the information of a few higher-order operations that cannot be explicitly represented in equations but are required to solve the problem. The gap between natural language and equations makes it difficult for a learned model to generalize from limited data. In this work we present an intermediate meaning representation scheme that tries to reduce this gap. We use a sequence-to-sequence model with a novel attention regularization term to generate the intermediate forms, then execute them to obtain the final answers. Since the intermediate forms are latent, we propose an iterative labeling framework for learning by leveraging supervision signals from both equations and answers. Our experiments show using intermediate forms outperforms directly predicting equations.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1039.pdf",
        "title": "Using Intermediate Representations to Solve Math Word Problems"
    },
    {
        "paper_id": "P18-1044",
        "conference": "acl",
        "year": "2018",
        "abstract": "Japanese predicate-argument structure (PAS) analysis involves zero anaphora resolution, which is notoriously difficult. To improve the performance of Japanese PAS analysis, it is straightforward to increase the size of corpora annotated with PAS. However, since it is prohibitively expensive, it is promising to take advantage of a large amount of raw corpora. In this paper, we propose a novel Japanese PAS analysis model based on semi-supervised adversarial training with a raw corpus. In our experiments, our model outperforms existing state-of-the-art models for Japanese PAS analysis.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1044.pdf",
        "title": "Neural Adversarial Training for Semi-supervised Japanese Predicate-argument Structure Analysis"
    },
    {
        "paper_id": "P18-1047",
        "conference": "acl",
        "year": "2018",
        "abstract": "The relational facts in sentences are often complicated. Different relational triplets may have overlaps in a sentence. We divided the sentences into three types according to triplet overlap degree, including Normal, EntityPairOverlap and SingleEntiyOverlap. Existing methods mainly focus on Normal class and fail to extract relational triplets precisely. In this paper, we propose an end-to-end model based on sequence-to-sequence learning with copy mechanism, which can jointly extract relational facts from sentences of any of these classes. We adopt two different strategies in decoding process: employing only one united decoder or applying multiple separated decoders. We test our models in two public datasets and our model outperform the baseline method significantly.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1047.pdf",
        "title": "Extracting Relational Facts by an End-to-End Neural Model with Copy Mechanism"
    },
    {
        "paper_id": "P18-1048",
        "conference": "acl",
        "year": "2018",
        "abstract": "Due to the ability of encoding and mapping semantic information into a high-dimensional latent feature space, neural networks have been successfully used for detecting events to a certain extent. However, such a feature space can be easily contaminated by spurious features inherent in event detection. In this paper, we propose a self-regulated learning approach by utilizing a generative adversarial network to generate spurious features. On the basis, we employ a recurrent network to eliminate the fakes. Detailed experiments on the ACE 2005 and TAC-KBP 2015 corpora show that our proposed method is highly effective and adaptable.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1048.pdf",
        "title": "Self-regulation: Employing a Generative Adversarial Network to Improve Event Detection"
    },
    {
        "paper_id": "P18-1049",
        "conference": "acl",
        "year": "2018",
        "abstract": "We propose a context-aware neural network model for temporal information extraction. This model has a uniform architecture for event-event, event-timex and timex-timex pairs. A Global Context Layer (GCL), inspired by Neural Turing Machine (NTM), stores processed temporal relations in narrative order, and retrieves them for use when relevant entities come in. Relations are then classified in context. The GCL model has long-term memory and attention mechanisms to resolve irregular long-distance dependencies that regular RNNs such as LSTM cannot recognize. It does not require any new input features, while outperforming the existing models in literature. To our knowledge it is also the first model to use NTM-like architecture to process the information from global context in discourse-scale natural text processing. We are going to release the source code in the future.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1049.pdf",
        "title": "Context-Aware Neural Model for Temporal Information Extraction"
    },
    {
        "paper_id": "P18-1050",
        "conference": "acl",
        "year": "2018",
        "abstract": "Inspired by the double temporality characteristic of narrative texts, we propose a novel approach for acquiring rich temporal \u201cbefore/after\u201d event knowledge across sentences in narrative stories. The double temporality states that a narrative story often describes a sequence of events following the chronological order and therefore, the temporal order of events matches with their textual order. We explored narratology principles and built a weakly supervised approach that identifies 287k narrative paragraphs from three large corpora. We then extracted rich temporal event knowledge from these narrative paragraphs. Such event knowledge is shown useful to improve temporal relation classification and outperforms several recent neural network models on the narrative cloze task.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1050.pdf",
        "title": "Temporal Event Knowledge Acquisition via Identifying Narratives"
    },
    {
        "paper_id": "P18-1053",
        "conference": "acl",
        "year": "2018",
        "abstract": "Recent neural network models for Chinese zero pronoun resolution gain great performance by capturing semantic information for zero pronouns and candidate antecedents, but tend to be short-sighted, operating solely by making local decisions. They typically predict coreference links between the zero pronoun and one single candidate antecedent at a time while ignoring their influence on future decisions. Ideally, modeling useful information of preceding potential antecedents is crucial for classifying later zero pronoun-candidate antecedent pairs, a need which leads traditional models of zero pronoun resolution to draw on reinforcement learning. In this paper, we show how to integrate these goals, applying deep reinforcement learning to deal with the task. With the help of the reinforcement learning agent, our system learns the policy of selecting antecedents in a sequential manner, where useful information provided by earlier predicted antecedents could be utilized for making later coreference decisions. Experimental results on OntoNotes 5.0 show that our approach substantially outperforms the state-of-the-art methods under three experimental settings.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1053.pdf",
        "title": "Deep Reinforcement Learning for Chinese Zero Pronoun Resolution"
    },
    {
        "paper_id": "P18-1061",
        "conference": "acl",
        "year": "2018",
        "abstract": "Sentence scoring and sentence selection are two main steps in extractive document summarization systems. However, previous works treat them as two separated subtasks. In this paper, we present a novel end-to-end neural network framework for extractive document summarization by jointly learning to score and select sentences. It first reads the document sentences with a hierarchical encoder to obtain the representation of sentences. Then it builds the output summary by extracting sentences one by one. Different from previous methods, our approach integrates the selection strategy into the scoring model, which directly predicts the relative importance given previously selected sentences. Experiments on the CNN/Daily Mail dataset show that the proposed framework significantly outperforms the state-of-the-art extractive summarization models.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1061.pdf",
        "title": "Neural Document Summarization by Jointly Learning to Score and Select Sentences"
    },
    {
        "paper_id": "P18-1063",
        "conference": "acl",
        "year": "2018",
        "abstract": "Inspired by how humans summarize long documents, we propose an accurate and fast summarization model that first selects salient sentences and then rewrites them abstractively (i.e., compresses and paraphrases) to generate a concise overall summary. We use a novel sentence-level policy gradient method to bridge the non-differentiable computation between these two neural networks in a hierarchical way, while maintaining language fluency. Empirically, we achieve the new state-of-the-art on all metrics (including human evaluation) on the CNN/Daily Mail dataset, as well as significantly higher abstractiveness scores. Moreover, by first operating at the sentence-level and then the word-level, we enable parallel decoding of our neural generative model that results in substantially faster (10-20x) inference speed as well as 4x faster training convergence than previous long-paragraph encoder-decoder models. We also demonstrate the generalization of our model on the test-only DUC-2002 dataset, where we achieve higher scores than a state-of-the-art model.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1063.pdf",
        "title": "Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting"
    },
    {
        "paper_id": "P18-1064",
        "conference": "acl",
        "year": "2018",
        "abstract": "An accurate abstractive summary of a document should contain all its salient information and should be logically entailed by the input document. We improve these important aspects of abstractive summarization via multi-task learning with the auxiliary tasks of question generation and entailment generation, where the former teaches the summarization model how to look for salient questioning-worthy details, and the latter teaches the model how to rewrite a summary which is a directed-logical subset of the input document. We also propose novel multi-task architectures with high-level (semantic) layer-specific sharing across multiple encoder and decoder layers of the three tasks, as well as soft-sharing mechanisms (and show performance ablations and analysis examples of each contribution). Overall, we achieve statistically significant improvements over the state-of-the-art on both the CNN/DailyMail and Gigaword datasets, as well as on the DUC-2002 transfer setup. We also present several quantitative and qualitative analysis studies of our model\u2019s learned saliency and entailment skills.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1064.pdf",
        "title": "Soft Layer-Specific Multi-Task Summarization with Entailment and Question Generation"
    },
    {
        "paper_id": "P18-1067",
        "conference": "acl",
        "year": "2018",
        "abstract": "Previous works in computer science, as well as political and social science, have shown correlation in text between political ideologies and the moral foundations expressed within that text. Additional work has shown that policy frames, which are used by politicians to bias the public towards their stance on an issue, are also correlated with political ideology. Based on these associations, this work takes a first step towards modeling both the language and how politicians frame issues on Twitter, in order to predict the moral foundations that are used by politicians to express their stances on issues. The contributions of this work includes a dataset annotated for the moral foundations, annotation guidelines, and probabilistic graphical models which show the usefulness of jointly modeling abstract political slogans, as opposed to the unigrams of previous works, with policy frames for the prediction of the morality underlying political tweets.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1067.pdf",
        "title": "Classification of Moral Foundations in Microblog Political Discourse"
    },
    {
        "paper_id": "P18-1068",
        "conference": "acl",
        "year": "2018",
        "abstract": "Semantic parsing aims at mapping natural language utterances into structured meaning representations. In this work, we propose a structure-aware neural architecture which decomposes the semantic parsing process into two stages. Given an input utterance, we first generate a rough sketch of its meaning, where low-level information (such as variable names and arguments) is glossed over. Then, we fill in missing details by taking into account the natural language input and the sketch itself. Experimental results on four datasets characteristic of different domains and meaning representations show that our approach consistently improves performance, achieving competitive results despite the use of relatively simple decoders.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1068.pdf",
        "title": "Coarse-to-Fine Decoding for Neural Semantic Parsing"
    },
    {
        "paper_id": "P18-1069",
        "conference": "acl",
        "year": "2018",
        "abstract": "In this work we focus on confidence modeling for neural semantic parsers which are built upon sequence-to-sequence models. We outline three major causes of uncertainty, and design various metrics to quantify these factors. These metrics are then used to estimate confidence scores that indicate whether model predictions are likely to be correct. Beyond confidence estimation, we identify which parts of the input contribute to uncertain predictions allowing users to interpret their model, and verify or refine its input. Experimental results show that our confidence model significantly outperforms a widely used method that relies on posterior probability, and improves the quality of interpretation compared to simply relying on attention scores.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1069.pdf",
        "title": "Confidence Modeling for Neural Semantic Parsing"
    },
    {
        "paper_id": "P18-1073",
        "conference": "acl",
        "year": "2018",
        "abstract": "Recent work has managed to learn cross-lingual word embeddings without parallel data by mapping monolingual embeddings to a shared space through adversarial training. However, their evaluation has focused on favorable conditions, using comparable corpora or closely-related languages, and we show that they often fail in more realistic scenarios. This work proposes an alternative approach based on a fully unsupervised initialization that explicitly exploits the structural similarity of the embeddings, and a robust self-learning algorithm that iteratively improves this solution. Our method succeeds in all tested scenarios and obtains the best published results in standard datasets, even surpassing previous supervised systems. Our implementation is released as an open source project at https://github.com/artetxem/vecmap.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1073.pdf",
        "title": "A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings"
    },
    {
        "paper_id": "P18-1074",
        "conference": "acl",
        "year": "2018",
        "abstract": "We propose a multi-lingual multi-task architecture to develop supervised models with a minimal amount of labeled data for sequence labeling. In this new architecture, we combine various transfer models using two layers of parameter sharing. On the first layer, we construct the basis of the architecture to provide universal word representation and feature extraction capability for all models. On the second level, we adopt different parameter sharing strategies for different transfer schemes. This architecture proves to be particularly effective for low-resource settings, when there are less than 200 training sentences for the target task. Using Name Tagging as a target task, our approach achieved 4.3%-50.5% absolute F-score gains compared to the mono-lingual single-task baseline model.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1074.pdf",
        "title": "A Multi-lingual Multi-task Architecture for Low-resource Sequence Labeling"
    },
    {
        "paper_id": "P18-1075",
        "conference": "acl",
        "year": "2018",
        "abstract": "Bilingual tasks, such as bilingual lexicon induction and cross-lingual classification, are crucial for overcoming data sparsity in the target language. Resources required for such tasks are often out-of-domain, thus domain adaptation is an important problem here. We make two contributions. First, we test a delightfully simple method for domain adaptation of bilingual word embeddings. We evaluate these embeddings on two bilingual tasks involving different domains: cross-lingual twitter sentiment classification and medical bilingual lexicon induction. Second, we tailor a broadly applicable semi-supervised classification method from computer vision to these tasks. We show that this method also helps in low-resource setups. Using both methods together we achieve large improvements over our baselines, by using only additional unlabeled data.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1075.pdf",
        "title": "Two Methods for Domain Adaptation of Bilingual Tasks: Delightfully Simple and Broadly Applicable"
    },
    {
        "paper_id": "P18-1076",
        "conference": "acl",
        "year": "2018",
        "abstract": "We introduce a neural reading comprehension model that integrates external commonsense knowledge, encoded as a key-value memory, in a cloze-style setting. Instead of relying only on document-to-question interaction or discrete features as in prior work, our model attends to relevant external knowledge and combines this knowledge with the context representation before inferring the answer. This allows the model to attract and imply knowledge from an external knowledge source that is not explicitly stated in the text, but that is relevant for inferring the answer. Our model improves results over a very strong baseline on a hard Common Nouns dataset, making it a strong competitor of much more complex models. By including knowledge explicitly, our model can also provide evidence about the background knowledge used in the RC process.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1076.pdf",
        "title": "Knowledgeable Reader: Enhancing Cloze-Style Reading Comprehension with External Commonsense Knowledge"
    },
    {
        "paper_id": "P18-1084",
        "conference": "acl",
        "year": "2018",
        "abstract": "We present a deep neural network that leverages images to improve bilingual text embeddings. Relying on bilingual image tags and descriptions, our approach conditions text embedding induction on the shared visual information for both languages, producing highly correlated bilingual embeddings. In particular, we propose a novel model based on Partial Canonical Correlation Analysis (PCCA). While the original PCCA finds linear projections of two views in order to maximize their canonical correlation conditioned on a shared third variable, we introduce a non-linear Deep PCCA (DPCCA) model, and develop a new stochastic iterative algorithm for its optimization. We evaluate PCCA and DPCCA on multilingual word similarity and cross-lingual image description retrieval. Our models outperform a large variety of previous methods, despite not having access to any visual signal during test time inference.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1084.pdf",
        "title": "Bridging Languages through Images with Deep Partial Canonical Correlation Analysis"
    },
    {
        "paper_id": "P18-1085",
        "conference": "acl",
        "year": "2018",
        "abstract": "We introduce Picturebook, a large-scale lookup operation to ground language via \u2018snapshots\u2019 of our physical world accessed through image search. For each word in a vocabulary, we extract the top-k images from Google image search and feed the images through a convolutional network to extract a word embedding. We introduce a multimodal gating function to fuse our Picturebook embeddings with other word representations. We also introduce Inverse Picturebook, a mechanism to map a Picturebook embedding back into words. We experiment and report results across a wide range of tasks: word similarity, natural language inference, semantic relatedness, sentiment/topic classification, image-sentence ranking and machine translation. We also show that gate activations corresponding to Picturebook embeddings are highly correlated to human judgments of concreteness ratings.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1085.pdf",
        "title": "Illustrative Language Understanding: Large-Scale Visual Grounding with Image Search"
    },
    {
        "paper_id": "P18-1087",
        "conference": "acl",
        "year": "2018",
        "abstract": "Target-oriented sentiment classification aims at classifying sentiment polarities over individual opinion targets in a sentence. RNN with attention seems a good fit for the characteristics of this task, and indeed it achieves the state-of-the-art performance. After re-examining the drawbacks of attention mechanism and the obstacles that block CNN to perform well in this classification task, we propose a new model that achieves new state-of-the-art results on a few benchmarks. Instead of attention, our model employs a CNN layer to extract salient features from the transformed word representations originated from a bi-directional RNN layer. Between the two layers, we propose a component which first generates target-specific representations of words in the sentence, and then incorporates a mechanism for preserving the original contextual information from the RNN layer.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1087.pdf",
        "title": "Transformation Networks for Target-Oriented Sentiment Classification"
    },
    {
        "paper_id": "P18-1090",
        "conference": "acl",
        "year": "2018",
        "abstract": "The goal of sentiment-to-sentiment \u201ctranslation\u201d is to change the underlying sentiment of a sentence while keeping its content. The main challenge is the lack of parallel data. To solve this problem, we propose a cycled reinforcement learning method that enables training on unpaired data by collaboration between a neutralization module and an emotionalization module. We evaluate our approach on two review datasets, Yelp and Amazon. Experimental results show that our approach significantly outperforms the state-of-the-art systems. Especially, the proposed method substantially improves the content preservation performance. The BLEU score is improved from 1.64 to 22.46 and from 0.56 to 14.06 on the two datasets, respectively.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1090.pdf",
        "title": "Unpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach"
    },
    {
        "paper_id": "P18-1093",
        "conference": "acl",
        "year": "2018",
        "abstract": "Sarcasm is a sophisticated speech act which commonly manifests on social communities such as Twitter and Reddit. The prevalence of sarcasm on the social web is highly disruptive to opinion mining systems due to not only its tendency of polarity flipping but also usage of figurative language. Sarcasm commonly manifests with a contrastive theme either between positive-negative sentiments or between literal-figurative scenarios. In this paper, we revisit the notion of modeling contrast in order to reason with sarcasm. More specifically, we propose an attention-based neural model that looks in-between instead of across, enabling it to explicitly model contrast and incongruity. We conduct extensive experiments on six benchmark datasets from Twitter, Reddit and the Internet Argument Corpus. Our proposed model not only achieves state-of-the-art performance on all datasets but also enjoys improved interpretability.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1093.pdf",
        "title": "Reasoning with Sarcasm by Reading In-Between"
    },
    {
        "paper_id": "P18-1097",
        "conference": "acl",
        "year": "2018",
        "abstract": "Most of the neural sequence-to-sequence (seq2seq) models for grammatical error correction (GEC) have two limitations: (1) a seq2seq model may not be well generalized with only limited error-corrected data; (2) a seq2seq model may fail to completely correct a sentence with multiple errors through normal seq2seq inference. We attempt to address these limitations by proposing a fluency boost learning and inference mechanism. Fluency boosting learning generates fluency-boost sentence pairs during training, enabling the error correction model to learn how to improve a sentence\u2019s fluency from more instances, while fluency boosting inference allows the model to correct a sentence incrementally with multiple inference steps until the sentence\u2019s fluency stops increasing. Experiments show our approaches improve the performance of seq2seq models for GEC, achieving state-of-the-art results on both CoNLL-2014 and JFLEG benchmark datasets.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1097.pdf",
        "title": "Fluency Boost Learning and Inference for Neural Grammatical Error Correction"
    },
    {
        "paper_id": "P18-1103",
        "conference": "acl",
        "year": "2018",
        "abstract": "Human generates responses relying on semantic and functional dependencies, including coreference relation, among dialogue elements and their context. In this paper, we investigate matching a response with its multi-turn context using dependency information based entirely on attention. Our solution is inspired by the recently proposed Transformer in machine translation (Vaswani et al., 2017) and we extend the attention mechanism in two ways. First, we construct representations of text segments at different granularities solely with stacked self-attention. Second, we try to extract the truly matched segment pairs with attention across the context and response. We jointly introduce those two kinds of attention in one uniform neural network. Experiments on two large-scale multi-turn response selection tasks show that our proposed model significantly outperforms the state-of-the-art models.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1103.pdf",
        "title": "Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network"
    },
    {
        "paper_id": "P18-1108",
        "conference": "acl",
        "year": "2018",
        "abstract": "In this work, we propose a novel constituency parsing scheme. The model first predicts a real-valued scalar, named syntactic distance, for each split position in the sentence. The topology of grammar tree is then determined by the values of syntactic distances. Compared to traditional shift-reduce parsing schemes, our approach is free from the potentially disastrous compounding error. It is also easier to parallelize and much faster. Our model achieves the state-of-the-art single model F1 score of 92.1 on PTB and 86.4 on CTB dataset, which surpasses the previous single model results by a large margin.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1108.pdf",
        "title": "Straight to the Tree: Constituency Parsing with Neural Syntactic Distance"
    },
    {
        "paper_id": "P18-1110",
        "conference": "acl",
        "year": "2018",
        "abstract": "We revisit domain adaptation for parsers in the neural era. First we show that recent advances in word representations greatly diminish the need for domain adaptation when the target domain is syntactically similar to the source domain. As evidence, we train a parser on the Wall Street Journal alone that achieves over 90% F1 on the Brown corpus. For more syntactically distant domains, we provide a simple way to adapt a parser using only dozens of partial annotations. For instance, we increase the percentage of error-free geometry-domain parses in a held-out set from 45% to 73% using approximately five dozen training examples. In the process, we demonstrate a new state-of-the-art single model result on the Wall Street Journal test set of 94.3%. This is an absolute increase of 1.7% over the previous state-of-the-art of 92.6%.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1110.pdf",
        "title": "Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples"
    },
    {
        "paper_id": "P18-1111",
        "conference": "acl",
        "year": "2018",
        "abstract": "Revealing the implicit semantic relation between the constituents of a noun-compound is important for many NLP applications. It has been addressed in the literature either as a classification task to a set of pre-defined relations or by producing free text paraphrases explicating the relations. Most existing paraphrasing methods lack the ability to generalize, and have a hard time interpreting infrequent or new noun-compounds. We propose a neural model that generalizes better by representing paraphrases in a continuous space, generalizing for both unseen noun-compounds and rare paraphrases. Our model helps improving performance on both the noun-compound paraphrasing and classification tasks.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1111.pdf",
        "title": "Paraphrase to Explicate: Revealing Implicit Noun-Compound Relations"
    },
    {
        "paper_id": "P18-1112",
        "conference": "acl",
        "year": "2018",
        "abstract": "We explore the notion of subjectivity, and hypothesize that word embeddings learnt from input corpora of varying levels of subjectivity behave differently on natural language processing tasks such as classifying a sentence by sentiment, subjectivity, or topic. Through systematic comparative analyses, we establish this to be the case indeed. Moreover, based on the discovery of the outsized role that sentiment words play on subjectivity-sensitive tasks such as sentiment classification, we develop a novel word embedding SentiVec which is infused with sentiment information from a lexical resource, and is shown to outperform baselines on such tasks.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1112.pdf",
        "title": "Searching for the X-Factor: Exploring Corpus Subjectivity for Word Embeddings"
    },
    {
        "paper_id": "P18-1113",
        "conference": "acl",
        "year": "2018",
        "abstract": "Metaphoric expressions are widespread in natural language, posing a significant challenge for various natural language processing tasks such as Machine Translation. Current word embedding based metaphor identification models cannot identify the exact metaphorical words within a sentence. In this paper, we propose an unsupervised learning method that identifies and interprets metaphors at word-level without any preprocessing, outperforming strong baselines in the metaphor identification task. Our model extends to interpret the identified metaphors, paraphrasing them into their literal counterparts, so that they can be better translated by machines. We evaluated this with two popular translation systems for English to Chinese, showing that our model improved the systems significantly.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1113.pdf",
        "title": "Word Embedding and WordNet Based Metaphor Identification and Interpretation"
    },
    {
        "paper_id": "P18-1114",
        "conference": "acl",
        "year": "2018",
        "abstract": "Traditional word embedding approaches learn semantic information at word level while ignoring the meaningful internal structures of words like morphemes. Furthermore, existing morphology-based models directly incorporate morphemes to train word embeddings, but still neglect the latent meanings of morphemes. In this paper, we explore to employ the latent meanings of morphological compositions of words to train and enhance word embeddings. Based on this purpose, we propose three Latent Meaning Models (LMMs), named LMM-A, LMM-S and LMM-M respectively, which adopt different strategies to incorporate the latent meanings of morphemes during the training process. Experiments on word similarity, syntactic analogy and text classification are conducted to validate the feasibility of our models. The results demonstrate that our models outperform the baselines on five word similarity datasets. On Wordsim-353 and RG-65 datasets, our models nearly achieve 5% and 7% gains over the classic CBOW model, respectively. For the syntactic analogy and text classification tasks, our models also surpass all the baselines including a morphology-based model.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1114.pdf",
        "title": "Incorporating Latent Meanings of Morphological Compositions to Enhance Word Embeddings"
    },
    {
        "paper_id": "P18-1118",
        "conference": "acl",
        "year": "2018",
        "abstract": "We present a document-level neural machine translation model which takes both source and target document context into account using memory networks. We model the problem as a structured prediction problem with interdependencies among the observed and hidden variables, i.e., the source sentences and their unobserved target translations in the document. The resulting structured prediction problem is tackled with a neural translation model equipped with two memory components, one each for the source and target side, to capture the documental interdependencies. We train the model end-to-end, and propose an iterative decoding algorithm based on block coordinate descent. Experimental results of English translations from French, German, and Estonian documents show that our model is effective in exploiting both source and target document context, and statistically significantly outperforms the previous work in terms of BLEU and METEOR.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1118.pdf",
        "title": "Document Context Neural Machine Translation with Memory Networks"
    },
    {
        "paper_id": "P18-1119",
        "conference": "acl",
        "year": "2018",
        "abstract": "The purpose of text geolocation is to associate geographic information contained in a document with a set (or sets) of coordinates, either implicitly by using linguistic features and/or explicitly by using geographic metadata combined with heuristics. We introduce a geocoder (location mention disambiguator) that achieves state-of-the-art (SOTA) results on three diverse datasets by exploiting the implicit lexical clues. Moreover, we propose a new method for systematic encoding of geographic metadata to generate two distinct views of the same text. To that end, we introduce the Map Vector (MapVec), a sparse representation obtained by plotting prior geographic probabilities, derived from population figures, on a World Map. We then integrate the implicit (language) and explicit (map) features to significantly improve a range of metrics. We also introduce an open-source dataset for geoparsing of news events covering global disease outbreaks and epidemics to help future evaluation in geoparsing.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1119.pdf",
        "title": "Which Melbourne? Augmenting Geocoding with Maps"
    },
    {
        "paper_id": "P18-1129",
        "conference": "acl",
        "year": "2018",
        "abstract": "Many natural language processing tasks can be modeled into structured prediction and solved as a search problem. In this paper, we distill an ensemble of multiple models trained with different initialization into a single model. In addition to learning to match the ensemble\u2019s probability output on the reference states, we also use the ensemble to explore the search space and learn from the encountered states in the exploration. Experimental results on two typical search-based structured prediction tasks \u2013 transition-based dependency parsing and neural machine translation show that distillation can effectively improve the single model\u2019s performance and the final model achieves improvements of 1.32 in LAS and 2.65 in BLEU score on these two tasks respectively over strong baselines and it outperforms the greedy structured prediction models in previous literatures.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1129.pdf",
        "title": "Distilling Knowledge for Search-based Structured Prediction"
    },
    {
        "paper_id": "P18-1130",
        "conference": "acl",
        "year": "2018",
        "abstract": "We introduce a novel architecture for dependency parsing: stack-pointer networks (StackPtr). Combining pointer networks (Vinyals et al., 2015) with an internal stack, the proposed model first reads and encodes the whole sentence, then builds the dependency tree top-down (from root-to-leaf) in a depth-first fashion. The stack tracks the status of the depth-first search and the pointer networks select one child for the word at the top of the stack at each step. The StackPtr parser benefits from the information of whole sentence and all previously derived subtree structures, and removes the left-to-right restriction in classical transition-based parsers. Yet the number of steps for building any (non-projective) parse tree is linear in the length of the sentence just as other transition-based parsers, yielding an efficient decoding algorithm with O(n2) time complexity. We evaluate our model on 29 treebanks spanning 20 languages and different dependency annotation schemas, and achieve state-of-the-art performances on 21 of them",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1130.pdf",
        "title": "Stack-Pointer Networks for Dependency Parsing"
    },
    {
        "paper_id": "P18-1132",
        "conference": "acl",
        "year": "2018",
        "abstract": "Language exhibits hierarchical structure, but recent work using a subject-verb agreement diagnostic argued that state-of-the-art language models, LSTMs, fail to learn long-range syntax sensitive dependencies. Using the same diagnostic, we show that, in fact, LSTMs do succeed in learning such dependencies\u2014provided they have enough capacity. We then explore whether models that have access to explicit syntactic information learn agreement more effectively, and how the way in which this structural information is incorporated into the model impacts performance. We find that the mere presence of syntactic information does not improve accuracy, but when model architecture is determined by syntax, number agreement is improved. Further, we find that the choice of how syntactic structure is built affects how well number agreement is learned: top-down construction outperforms left-corner and bottom-up variants in capturing non-local structural dependencies.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1132.pdf",
        "title": "LSTMs Can Learn Syntax-Sensitive Dependencies Well, But Modeling Structure Makes Them Better"
    },
    {
        "paper_id": "P18-1136",
        "conference": "acl",
        "year": "2018",
        "abstract": "End-to-end task-oriented dialog systems usually suffer from the challenge of incorporating knowledge bases. In this paper, we propose a novel yet simple end-to-end differentiable model called memory-to-sequence (Mem2Seq) to address this issue. Mem2Seq is the first neural generative model that combines the multi-hop attention over memories with the idea of pointer network. We empirically show how Mem2Seq controls each generation step, and how its multi-hop attention mechanism helps in learning correlations between memories. In addition, our model is quite general without complicated task-specific designs. As a result, we show that Mem2Seq can be trained faster and attain the state-of-the-art performance on three different task-oriented dialog datasets.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1136.pdf",
        "title": "Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems"
    },
    {
        "paper_id": "P18-1138",
        "conference": "acl",
        "year": "2018",
        "abstract": "End-to-end neural dialogue generation has shown promising results recently, but it does not employ knowledge to guide the generation and hence tends to generate short, general, and meaningless responses. In this paper, we propose a neural knowledge diffusion (NKD) model to introduce knowledge into dialogue generation. This method can not only match the relevant facts for the input utterance but diffuse them to similar entities. With the help of facts matching and entity diffusion, the neural dialogue generation is augmented with the ability of convergent and divergent thinking over the knowledge base. Our empirical study on a real-world dataset prove that our model is capable of generating meaningful, diverse and natural responses for both factoid-questions and knowledge grounded chi-chats. The experiment results also show that our model outperforms competitive baseline models significantly.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1138.pdf",
        "title": "Knowledge Diffusion for Neural Dialogue Generation"
    },
    {
        "paper_id": "P18-1141",
        "conference": "acl",
        "year": "2018",
        "abstract": "We present a new method for estimating vector space representations of words: embedding learning by concept induction. We test this method on a highly parallel corpus and learn semantic representations of words in 1259 different languages in a single common space. An extensive experimental evaluation on crosslingual word similarity and sentiment analysis indicates that concept-based multilingual embedding learning performs better than previous approaches.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1141.pdf",
        "title": "Embedding Learning Through Multilingual Concept Induction"
    },
    {
        "paper_id": "P18-1144",
        "conference": "acl",
        "year": "2018",
        "abstract": "We investigate a lattice-structured LSTM model for Chinese NER, which encodes a sequence of input characters as well as all potential words that match a lexicon. Compared with character-based methods, our model explicitly leverages word and word sequence information. Compared with word-based methods, lattice LSTM does not suffer from segmentation errors. Gated recurrent cells allow our model to choose the most relevant characters and words from a sentence for better NER results. Experiments on various datasets show that lattice LSTM outperforms both word-based and character-based LSTM baselines, achieving the best results.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1144.pdf",
        "title": "Chinese NER Using Lattice LSTM"
    },
    {
        "paper_id": "P18-1145",
        "conference": "acl",
        "year": "2018",
        "abstract": "Neural network based models commonly regard event detection as a word-wise classification task, which suffer from the mismatch problem between words and event triggers, especially in languages without natural word delimiters such as Chinese. In this paper, we propose Nugget Proposal Networks (NPNs), which can solve the word-trigger mismatch problem by directly proposing entire trigger nuggets centered at each character regardless of word boundaries. Specifically, NPNs perform event detection in a character-wise paradigm, where a hybrid representation for each character is first learned to capture both structural and semantic information from both characters and words. Then based on learned representations, trigger nuggets are proposed and categorized by exploiting character compositional structures of Chinese event triggers. Experiments on both ACE2005 and TAC KBP 2017 datasets show that NPNs significantly outperform the state-of-the-art methods.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1145.pdf",
        "title": "Nugget Proposal Networks for Chinese Event Detection"
    },
    {
        "paper_id": "P18-1150",
        "conference": "acl",
        "year": "2018",
        "abstract": "The problem of AMR-to-text generation is to recover a text representing the same meaning as an input AMR graph. The current state-of-the-art method uses a sequence-to-sequence model, leveraging LSTM for encoding a linearized AMR structure. Although being able to model non-local semantic information, a sequence LSTM can lose information from the AMR graph structure, and thus facing challenges with large-graphs, which result in long sequences. We introduce a neural graph-to-sequence model, using a novel LSTM structure for directly encoding graph-level semantics. On a standard benchmark, our model shows superior results to existing methods in the literature.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1150.pdf",
        "title": "A Graph-to-Sequence Model for AMR-to-Text Generation"
    },
    {
        "paper_id": "P18-1151",
        "conference": "acl",
        "year": "2018",
        "abstract": "A knowledge base is a large repository of facts that are mainly represented as RDF triples, each of which consists of a subject, a predicate (relationship), and an object. The RDF triple representation offers a simple interface for applications to access the facts. However, this representation is not in a natural language form, which is difficult for humans to understand. We address this problem by proposing a system to translate a set of RDF triples into natural sentences based on an encoder-decoder framework. To preserve as much information from RDF triples as possible, we propose a novel graph-based triple encoder. The proposed encoder encodes not only the elements of the triples but also the relationships both within a triple and between the triples. Experimental results show that the proposed encoder achieves a consistent improvement over the baseline models by up to 17.6%, 6.0%, and 16.4% in three common metrics BLEU, METEOR, and TER, respectively.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1151.pdf",
        "title": "GTR-LSTM: A Triple Encoder for Sentence Generation from RDF Data"
    },
    {
        "paper_id": "P18-1152",
        "conference": "acl",
        "year": "2018",
        "abstract": "Despite their local fluency, long-form text generated from RNNs is often generic, repetitive, and even self-contradictory. We propose a unified learning framework that collectively addresses all the above issues by composing a committee of discriminators that can guide a base RNN generator towards more globally coherent generations. More concretely, discriminators each specialize in a different principle of communication, such as Grice\u2019s maxims, and are collectively combined with the base RNN generator through a composite decoding objective. Human evaluation demonstrates that text generated by our model is preferred over that of baselines by a large margin, significantly enhancing the overall coherence, style, and information of the generations.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1152.pdf",
        "title": "Learning to Write with Cooperative Discriminators"
    },
    {
        "paper_id": "P18-1154",
        "conference": "acl",
        "year": "2018",
        "abstract": "This paper examines the problem of generating natural language descriptions of chess games. We introduce a new large-scale chess commentary dataset and propose methods to generate commentary for individual moves in a chess game. The introduced dataset consists of more than 298K chess move-commentary pairs across 11K chess games. We highlight how this task poses unique research challenges in natural language generation: the data contain a large variety of styles of commentary and frequently depend on pragmatic context. We benchmark various baselines and propose an end-to-end trainable neural model which takes into account multiple pragmatic aspects of the game state that may be commented upon to describe a given chess move. Through a human study on predictions for a subset of the data which deals with direct move descriptions, we observe that outputs from our models are rated similar to ground truth commentary texts in terms of correctness and fluency.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1154.pdf",
        "title": "Learning to Generate Move-by-Move Commentary for Chess Games from Large-Scale Social Forum Data"
    },
    {
        "paper_id": "P18-1156",
        "conference": "acl",
        "year": "2018",
        "abstract": "We propose DuoRC, a novel dataset for Reading Comprehension (RC) that motivates several new challenges for neural approaches in language understanding beyond those offered by existing RC datasets. DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie - one from Wikipedia and the other from IMDb - written by two different authors. We asked crowdsourced workers to create questions from one version of the plot and a different set of workers to extract or synthesize answers from the other version. This unique characteristic of DuoRC where questions and answers are created from different versions of a document narrating the same underlying story, ensures by design, that there is very little lexical overlap between the questions created from one version and the segments containing the answer in the other version. Further, since the two versions have different levels of plot detail, narration style, vocabulary, etc., answering questions from the second version requires deeper language understanding and incorporating external background knowledge. Additionally, the narrative style of passages arising from movie plots (as opposed to typical descriptive passages in existing datasets) exhibits the need to perform complex reasoning over events across multiple sentences. Indeed, we observe that state-of-the-art neural RC models which have achieved near human performance on the SQuAD dataset, even when coupled with traditional NLP techniques to address the challenges presented in DuoRC exhibit very poor performance (F1 score of 37.42% on DuoRC v/s 86% on SQuAD dataset). This opens up several interesting research avenues wherein DuoRC could complement other RC datasets to explore novel neural approaches for studying language understanding.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1156.pdf",
        "title": "DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension"
    },
    {
        "paper_id": "P18-1157",
        "conference": "acl",
        "year": "2018",
        "abstract": "We propose a simple yet robust stochastic answer network (SAN) that simulates multi-step reasoning in machine reading comprehension. Compared to previous work such as ReasoNet which used reinforcement learning to determine the number of steps, the unique feature is the use of a kind of stochastic prediction dropout on the answer module (final layer) of the neural network during the training. We show that this simple trick improves robustness and achieves results competitive to the state-of-the-art on the Stanford Question Answering Dataset (SQuAD), the Adversarial SQuAD, and the Microsoft MAchine Reading COmprehension Dataset (MS MARCO).",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1157.pdf",
        "title": "Stochastic Answer Networks for Machine Reading Comprehension"
    },
    {
        "paper_id": "P18-1160",
        "conference": "acl",
        "year": "2018",
        "abstract": "Neural models for question answering (QA) over documents have achieved significant performance improvements. Although effective, these models do not scale to large corpora due to their complex modeling of interactions between the document and the question. Moreover, recent work has shown that such models are sensitive to adversarial inputs. In this paper, we study the minimal context required to answer the question, and find that most questions in existing datasets can be answered with a small set of sentences. Inspired by this observation, we propose a simple sentence selector to select the minimal set of sentences to feed into the QA model. Our overall system achieves significant reductions in training (up to 15 times) and inference times (up to 13 times), with accuracy comparable to or better than the state-of-the-art on SQuAD, NewsQA, TriviaQA and SQuAD-Open. Furthermore, our experimental results and analyses show that our approach is more robust to adversarial inputs.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1160.pdf",
        "title": "Efficient and Robust Question Answering from Minimal Context over Documents"
    },
    {
        "paper_id": "P18-1165",
        "conference": "acl",
        "year": "2018",
        "abstract": "We present a study on reinforcement learning (RL) from human bandit feedback for sequence-to-sequence learning, exemplified by the task of bandit neural machine translation (NMT). We investigate the reliability of human bandit feedback, and analyze the influence of reliability on the learnability of a reward estimator, and the effect of the quality of reward estimates on the overall RL task. Our analysis of cardinal (5-point ratings) and ordinal (pairwise preferences) feedback shows that their intra- and inter-annotator \u03b1-agreement is comparable. Best reliability is obtained for standardized cardinal feedback, and cardinal feedback is also easiest to learn and generalize from. Finally, improvements of over 1 BLEU can be obtained by integrating a regression-based reward estimator trained on cardinal feedback for 800 translations into RL for NMT. This shows that RL is possible even from small amounts of fairly reliable human feedback, pointing to a great potential for applications at larger scale.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1165.pdf",
        "title": "Reliability and Learnability of Human Bandit Feedback for Sequence-to-Sequence Reinforcement Learning"
    },
    {
        "paper_id": "P18-1166",
        "conference": "acl",
        "year": "2018",
        "abstract": "With parallelizable attention networks, the neural Transformer is very fast to train. However, due to the auto-regressive architecture and self-attention in the decoder, the decoding procedure becomes slow. To alleviate this issue, we propose an average attention network as an alternative to the self-attention network in the decoder of the neural Transformer. The average attention network consists of two layers, with an average layer that models dependencies on previous positions and a gating layer that is stacked over the average layer to enhance the expressiveness of the proposed attention network. We apply this network on the decoder part of the neural Transformer to replace the original target-side self-attention model. With masking tricks and dynamic programming, our model enables the neural Transformer to decode sentences over four times faster than its original version with almost no loss in training time and translation performance. We conduct a series of experiments on WMT17 translation tasks, where on 6 different language pairs, we obtain robust and consistent speed-ups in decoding.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1166.pdf",
        "title": "Accelerating Neural Transformer via an Average Attention Network"
    },
    {
        "paper_id": "P18-1171",
        "conference": "acl",
        "year": "2018",
        "abstract": "In this paper, we present a sequence-to-sequence based approach for mapping natural language sentences to AMR semantic graphs. We transform the sequence to graph mapping problem to a word sequence to transition action sequence problem using a special transition system called a cache transition system. To address the sparsity issue of neural AMR parsing, we feed feature embeddings from the transition state to provide relevant local information for each decoder state. We present a monotonic hard attention model for the transition framework to handle the strictly left-to-right alignment between each transition state and the current buffer input focus. We evaluate our neural transition model on the AMR parsing task, and our parser outperforms other sequence-to-sequence approaches and achieves competitive results in comparison with the best-performing models.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1171.pdf",
        "title": "Sequence-to-sequence Models for Cache Transition Systems"
    },
    {
        "paper_id": "P18-1173",
        "conference": "acl",
        "year": "2018",
        "abstract": "We introduce structured projection of intermediate gradients (SPIGOT), a new method for backpropagating through neural networks that include hard-decision structured predictions (e.g., parsing) in intermediate layers. SPIGOT requires no marginal inference, unlike structured attention networks and reinforcement learning-inspired solutions. Like so-called straight-through estimators, SPIGOT defines gradient-like quantities associated with intermediate nondifferentiable operations, allowing backpropagation before and after them; SPIGOT\u2019s proxy aims to ensure that, after a parameter update, the intermediate structure will remain well-formed. We experiment on two structured NLP pipelines: syntactic-then-semantic dependency parsing, and semantic parsing followed by sentiment classification. We show that training with SPIGOT leads to a larger improvement on the downstream task than a modularly-trained pipeline, the straight-through estimator, and structured attention, reaching a new state of the art on semantic dependency parsing.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1173.pdf",
        "title": "Backpropagating through Structured Argmax using a SPIGOT"
    },
    {
        "paper_id": "P18-1177",
        "conference": "acl",
        "year": "2018",
        "abstract": "We study the task of generating from Wikipedia articles question-answer pairs that cover content beyond a single sentence. We propose a neural network approach that incorporates coreference knowledge via a novel gating mechanism. As compared to models that only take into account sentence-level information (Heilman and Smith, 2010; Du et al., 2017; Zhou et al., 2017), we find that the linguistic knowledge introduced by the coreference representation aids question generation significantly, producing models that outperform the current state-of-the-art. We apply our system (composed of an answer span extraction system and the passage-level QG system) to the 10,000 top ranking Wikipedia articles and create a corpus of over one million question-answer pairs. We provide qualitative analysis for the this large-scale generated corpus from Wikipedia.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1177.pdf",
        "title": "Harvesting Paragraph-level Question-Answer Pairs from Wikipedia"
    },
    {
        "paper_id": "P18-1178",
        "conference": "acl",
        "year": "2018",
        "abstract": "Machine reading comprehension (MRC) on real web data usually requires the machine to answer a question by analyzing multiple passages retrieved by search engine. Compared with MRC on a single passage, multi-passage MRC is more challenging, since we are likely to get multiple confusing answer candidates from different passages. To address this problem, we propose an end-to-end neural model that enables those answer candidates from different passages to verify each other based on their content representations. Specifically, we jointly train three modules that can predict the final answer based on three factors: the answer boundary, the answer content and the cross-passage answer verification. The experimental results show that our method outperforms the baseline by a large margin and achieves the state-of-the-art performance on the English MS-MARCO dataset and the Chinese DuReader dataset, both of which are designed for MRC in real-world settings.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1178.pdf",
        "title": "Multi-Passage Machine Reading Comprehension with Cross-Passage Answer Verification"
    },
    {
        "paper_id": "P18-1181",
        "conference": "acl",
        "year": "2018",
        "abstract": "In this paper, we propose a joint architecture that captures language, rhyme and meter for sonnet modelling. We assess the quality of generated poems using crowd and expert judgements. The stress and rhyme models perform very well, as generated poems are largely indistinguishable from human-written poems. Expert evaluation, however, reveals that a vanilla language model captures meter implicitly, and that machine-generated poems still underperform in terms of readability and emotion. Our research shows the importance expert evaluation for poetry generation, and that future research should look beyond rhyme/meter and focus on poetic language.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1181.pdf",
        "title": "Deep-speare: A joint neural model of poetic language, meter and rhyme"
    },
    {
        "paper_id": "P18-1182",
        "conference": "acl",
        "year": "2018",
        "abstract": "Traditionally, Referring Expression Generation (REG) models first decide on the form and then on the content of references to discourse entities in text, typically relying on features such as salience and grammatical function. In this paper, we present a new approach (NeuralREG), relying on deep neural networks, which makes decisions about form and content in one go without explicit feature extraction. Using a delexicalized version of the WebNLG corpus, we show that the neural model substantially improves over two strong baselines.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1182.pdf",
        "title": "NeuralREG: An end-to-end approach to referring expression generation"
    },
    {
        "paper_id": "P18-1186",
        "conference": "acl",
        "year": "2018",
        "abstract": "We introduce the new Multimodal Named Entity Disambiguation (MNED) task for multimodal social media posts such as Snapchat or Instagram captions, which are composed of short captions with accompanying images. Social media posts bring significant challenges for disambiguation tasks because 1) ambiguity not only comes from polysemous entities, but also from inconsistent or incomplete notations, 2) very limited context is provided with surrounding words, and 3) there are many emerging entities often unseen during training. To this end, we build a new dataset called SnapCaptionsKB, a collection of Snapchat image captions submitted to public and crowd-sourced stories, with named entity mentions fully annotated and linked to entities in an external knowledge base. We then build a deep zeroshot multimodal network for MNED that 1) extracts contexts from both text and image, and 2) predicts correct entity in the knowledge graph embeddings space, allowing for zeroshot disambiguation of entities unseen in training set as well. The proposed model significantly outperforms the state-of-the-art text-only NED models, showing efficacy and potentials of the MNED task.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1186.pdf",
        "title": "Multimodal Named Entity Disambiguation for Noisy Social Media Posts"
    },
    {
        "paper_id": "P18-1188",
        "conference": "acl",
        "year": "2018",
        "abstract": "Document modeling is essential to a variety of natural language understanding tasks. We propose to use external information to improve document modeling for problems that can be framed as sentence extraction. We develop a framework composed of a hierarchical document encoder and an attention-based extractor with attention over external information. We evaluate our model on extractive document summarization (where the external information is image captions and the title of the document) and answer selection (where the external information is a question). We show that our model consistently outperforms strong baselines, in terms of both informativeness and fluency (for CNN document summarization) and achieves state-of-the-art results for answer selection on WikiQA and NewsQA.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1188.pdf",
        "title": "Document Modeling with External Attention for Sentence Extraction"
    },
    {
        "paper_id": "P18-1189",
        "conference": "acl",
        "year": "2018",
        "abstract": "Most real-world document collections involve various types of metadata, such as author, source, and date, and yet the most commonly-used approaches to modeling text corpora ignore this information. While specialized models have been developed for particular applications, few are widely used in practice, as customization typically requires derivation of a custom inference algorithm. In this paper, we build on recent advances in variational inference methods and propose a general neural framework, based on topic models, to enable flexible incorporation of metadata and allow for rapid exploration of alternative models. Our approach achieves strong performance, with a manageable tradeoff between perplexity, coherence, and sparsity. Finally, we demonstrate the potential of our framework through an exploration of a corpus of articles about US immigration.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1189.pdf",
        "title": "Neural Models for Documents with Metadata"
    },
    {
        "paper_id": "P18-1191",
        "conference": "acl",
        "year": "2018",
        "abstract": "We present a new large-scale corpus of Question-Answer driven Semantic Role Labeling (QA-SRL) annotations, and the first high-quality QA-SRL parser. Our corpus, QA-SRL Bank 2.0, consists of over 250,000 question-answer pairs for over 64,000 sentences across 3 domains and was gathered with a new crowd-sourcing scheme that we show has high precision and good recall at modest cost. We also present neural models for two QA-SRL subtasks: detecting argument spans for a predicate and generating questions to label the semantic relationship. The best models achieve question accuracy of 82.6% and span-level accuracy of 77.6% (under human evaluation) on the full pipelined QA-SRL prediction task. They can also, as we show, be used to gather additional annotations at low cost.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1191.pdf",
        "title": "Large-Scale QA-SRL Parsing"
    },
    {
        "paper_id": "P18-1192",
        "conference": "acl",
        "year": "2018",
        "abstract": "Semantic role labeling (SRL) is dedicated to recognizing the predicate-argument structure of a sentence. Previous studies have shown syntactic information has a remarkable contribution to SRL performance. However, such perception was challenged by a few recent neural SRL models which give impressive performance without a syntactic backbone. This paper intends to quantify the importance of syntactic information to dependency SRL in deep learning framework. We propose an enhanced argument labeling model companying with an extended korder argument pruning algorithm for effectively exploiting syntactic information. Our model achieves state-of-the-art results on the CoNLL-2008, 2009 benchmarks for both English and Chinese, showing the quantitative significance of syntax to neural SRL together with a thorough empirical survey over existing models.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1192.pdf",
        "title": "Syntax for Semantic Role Labeling, To Be, Or Not To Be"
    },
    {
        "paper_id": "P18-1195",
        "conference": "acl",
        "year": "2018",
        "abstract": "Despite the effectiveness of recurrent neural network language models, their maximum likelihood estimation suffers from two limitations. It treats all sentences that do not match the ground truth as equally poor, ignoring the structure of the output space. Second, it suffers from \u2019exposure bias\u2019: during training tokens are predicted given ground-truth sequences, while at test time prediction is conditioned on generated output sequences. To overcome these limitations we build upon the recent reward augmented maximum likelihood approach that encourages the model to predict sentences that are close to the ground truth according to a given performance metric. We extend this approach to token-level loss smoothing, and propose improvements to the sequence-level smoothing approach. Our experiments on two different tasks, image captioning and machine translation, show that token-level and sequence-level loss smoothing are complementary, and significantly improve results.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1195.pdf",
        "title": "Token-level and sequence-level loss smoothing for RNN language models"
    },
    {
        "paper_id": "P18-1196",
        "conference": "acl",
        "year": "2018",
        "abstract": "Numeracy is the ability to understand and work with numbers. It is a necessary skill for composing and understanding documents in clinical, scientific, and other technical domains. In this paper, we explore different strategies for modelling numerals with language models, such as memorisation and digit-by-digit composition, and propose a novel neural architecture that uses a continuous probability density function to model numerals from an open vocabulary. Our evaluation on clinical and scientific datasets shows that using hierarchical models to distinguish numerals from words improves a perplexity metric on the subset of numerals by 2 and 4 orders of magnitude, respectively, over non-hierarchical models. A combination of strategies can further improve perplexity. Our continuous probability density function model reduces mean absolute percentage errors by 18% and 54% in comparison to the second best strategy for each dataset, respectively.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1196.pdf",
        "title": "Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers"
    },
    {
        "paper_id": "P18-1197",
        "conference": "acl",
        "year": "2018",
        "abstract": "With the recent success of Recurrent Neural Networks (RNNs) in Machine Translation (MT), attention mechanisms have become increasingly popular. The purpose of this paper is two-fold; firstly, we propose a novel attention model on Tree Long Short-Term Memory Networks (Tree-LSTMs), a tree-structured generalization of standard LSTM. Secondly, we study the interaction between attention and syntactic structures, by experimenting with three LSTM variants: bidirectional-LSTMs, Constituency Tree-LSTMs, and Dependency Tree-LSTMs. Our models are evaluated on two semantic relatedness tasks: semantic relatedness scoring for sentence pairs (SemEval 2012, Task 6 and SemEval 2014, Task 1) and paraphrase detection for question pairs (Quora, 2017).",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1197.pdf",
        "title": "To Attend or not to Attend: A Case Study on Syntactic Structures for Semantic Relatedness"
    },
    {
        "paper_id": "P18-1201",
        "conference": "acl",
        "year": "2018",
        "abstract": "Most previous supervised event extraction methods have relied on features derived from manual annotations, and thus cannot be applied to new event types without extra annotation effort. We take a fresh look at event extraction and model it as a generic grounding problem: mapping each event mention to a specific type in a target event ontology. We design a transferable architecture of structural and compositional neural networks to jointly represent and map event mentions and types into a shared semantic space. Based on this new framework, we can select, for each event mention, the event type which is semantically closest in this space as its type. By leveraging manual annotations available for a small set of existing event types, our framework can be applied to new unseen event types without additional manual annotations. When tested on 23 unseen event types, our zero-shot framework, without manual annotations, achieved performance comparable to a supervised model trained from 3,000 sentences annotated with 500 event mentions.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1201.pdf",
        "title": "Zero-Shot Transfer Learning for Event Extraction"
    },
    {
        "paper_id": "P18-1202",
        "conference": "acl",
        "year": "2018",
        "abstract": "Fine-grained opinion analysis aims to extract aspect and opinion terms from each sentence for opinion summarization. Supervised learning methods have proven to be effective for this task. However, in many domains, the lack of labeled data hinders the learning of a precise extraction model. In this case, unsupervised domain adaptation methods are desired to transfer knowledge from the source domain to any unlabeled target domain. In this paper, we develop a novel recursive neural network that could reduce domain shift effectively in word level through syntactic relations. We treat these relations as invariant \u201cpivot information\u201d across domains to build structural correspondences and generate an auxiliary task to predict the relation between any two adjacent words in the dependency tree. In the end, we demonstrate state-of-the-art results on three benchmark datasets.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1202.pdf",
        "title": "Recursive Neural Structural Correspondence Network for Cross-domain Aspect and Opinion Co-Extraction"
    },
    {
        "paper_id": "P18-1205",
        "conference": "acl",
        "year": "2018",
        "abstract": "Chit-chat models are known to have several problems: they lack specificity, do not display a consistent personality and are often not very captivating. In this work we present the task of making chit-chat more engaging by conditioning on profile information. We collect data and train models to (i)condition on their given profile information; and (ii) information about the person they are talking to, resulting in improved dialogues, as measured by next utterance prediction. Since (ii) is initially unknown our model is trained to engage its partner with personal topics, and we show the resulting dialogue can be used to predict profile information about the interlocutors.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1205.pdf",
        "title": "Personalizing Dialogue Agents: I have a dog, do you have pets too?"
    },
    {
        "paper_id": "P18-1209",
        "conference": "acl",
        "year": "2018",
        "abstract": "Multimodal research is an emerging field of artificial intelligence, and one of the main research problems in this field is multimodal fusion. The fusion of multimodal data is the process of integrating multiple unimodal representations into one compact multimodal representation. Previous research in this field has exploited the expressiveness of tensors for multimodal representation. However, these methods often suffer from exponential increase in dimensions and in computational complexity introduced by transformation of input into tensor. In this paper, we propose the Low-rank Multimodal Fusion method, which performs multimodal fusion using low-rank tensors to improve efficiency. We evaluate our model on three different tasks: multimodal sentiment analysis, speaker trait analysis, and emotion recognition. Our model achieves competitive results on all these tasks while drastically reducing computational complexity. Additional experiments also show that our model can perform robustly for a wide range of low-rank settings, and is indeed much more efficient in both training and inference compared to other methods that utilize tensor representations.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1209.pdf",
        "title": "Efficient Low-rank Multimodal Fusion With Modality-Specific Factors"
    },
    {
        "paper_id": "P18-1211",
        "conference": "acl",
        "year": "2018",
        "abstract": "We present a generative probabilistic model of documents as sequences of sentences, and show that inference in it can lead to extraction of long-range latent discourse structure from a collection of documents. The approach is based on embedding sequences of sentences from longer texts into a 2- or 3-D spatial grids, in which one or two coordinates model smooth topic transitions, while the third captures the sequential nature of the modeled text. A significant advantage of our approach is that the learned models are naturally visualizable and interpretable, as semantic similarity and sequential structure are modeled along orthogonal directions in the grid. We show that the method is effective in capturing discourse structures in narrative text across multiple genres, including biographies, stories, and newswire reports. In particular, our method outperforms or is competitive with state-of-the-art generative approaches on tasks such as predicting the outcome of a story, and sentence ordering.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1211.pdf",
        "title": "A Spatial Model for Extracting and Visualizing Latent Discourse Structure in Text"
    },
    {
        "paper_id": "P18-1220",
        "conference": "acl",
        "year": "2018",
        "abstract": "We propose a novel approach to OCR post-correction that exploits repeated texts in large corpora both as a source of noisy target outputs for unsupervised training and as a source of evidence when decoding. A sequence-to-sequence model with attention is applied for single-input correction, and a new decoder with multi-input attention averaging is developed to search for consensus among multiple sequences. We design two ways of training the correction model without human annotation, either training to match noisily observed textual variants or bootstrapping from a uniform error model. On two corpora of historical newspapers and books, we show that these unsupervised techniques cut the character and word error rates nearly in half on single inputs and, with the addition of multi-input decoding, can rival supervised methods.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1220.pdf",
        "title": "Multi-Input Attention for Unsupervised OCR Correction"
    },
    {
        "paper_id": "P18-1221",
        "conference": "acl",
        "year": "2018",
        "abstract": "Text in many domains involves a significant amount of named entities. Predicting the entity names is often challenging for a language model as they appear less frequent on the training corpus. In this paper, we propose a novel and effective approach to building a language model which can learn the entity names by leveraging their entity type information. We also introduce two benchmark datasets based on recipes and Java programming codes, on which we evaluate the proposed model. Experimental results show that our model achieves 52.2% better perplexity in recipe generation and 22.06% on code generation than state-of-the-art language models.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1221.pdf",
        "title": "Building Language Models for Text with Named Entities"
    },
    {
        "paper_id": "P18-1222",
        "conference": "acl",
        "year": "2018",
        "abstract": "Hypertext documents, such as web pages and academic papers, are of great importance in delivering information in our daily life. Although being effective on plain documents, conventional text embedding methods suffer from information loss if directly adapted to hyper-documents. In this paper, we propose a general embedding approach for hyper-documents, namely, hyperdoc2vec, along with four criteria characterizing necessary information that hyper-document embedding models should preserve. Systematic comparisons are conducted between hyperdoc2vec and several competitors on two tasks, i.e., paper classification and citation recommendation, in the academic paper domain. Analyses and experiments both validate the superiority of hyperdoc2vec to other models w.r.t. the four criteria.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1222.pdf",
        "title": "hyperdoc2vec: Distributed Representations of Hypertext Documents"
    },
    {
        "paper_id": "P18-1228",
        "conference": "acl",
        "year": "2018",
        "abstract": "Because word semantics can substantially change across communities and contexts, capturing domain-specific word semantics is an important challenge. Here, we propose SemAxis, a simple yet powerful framework to characterize word semantics using many semantic axes in word-vector spaces beyond sentiment. We demonstrate that SemAxis can capture nuanced semantic representations in multiple online communities. We also show that, when the sentiment axis is examined, SemAxis outperforms the state-of-the-art approaches in building domain-specific sentiment lexicons.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1228.pdf",
        "title": "SemAxis: A Lightweight Framework to Characterize Domain-Specific Word Semantics Beyond Sentiment"
    },
    {
        "paper_id": "P18-1229",
        "conference": "acl",
        "year": "2018",
        "abstract": "We present a novel end-to-end reinforcement learning approach to automatic taxonomy induction from a set of terms. While prior methods treat the problem as a two-phase task (i.e.,, detecting hypernymy pairs followed by organizing these pairs into a tree-structured hierarchy), we argue that such two-phase methods may suffer from error propagation, and cannot effectively optimize metrics that capture the holistic structure of a taxonomy. In our approach, the representations of term pairs are learned using multiple sources of information and used to determine which term to select and where to place it on the taxonomy via a policy network. All components are trained in an end-to-end manner with cumulative rewards, measured by a holistic tree metric over the training taxonomies. Experiments on two public datasets of different domains show that our approach outperforms prior state-of-the-art taxonomy induction methods up to 19.6% on ancestor F1.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1229.pdf",
        "title": "End-to-End Reinforcement Learning for Automatic Taxonomy Induction"
    },
    {
        "paper_id": "P18-1232",
        "conference": "acl",
        "year": "2018",
        "abstract": "Word embeddings have been widely used in sentiment classification because of their efficacy for semantic representations of words. Given reviews from different domains, some existing methods for word embeddings exploit sentiment information, but they cannot produce domain-sensitive embeddings. On the other hand, some other existing methods can generate domain-sensitive word embeddings, but they cannot distinguish words with similar contexts but opposite sentiment polarity. We propose a new method for learning domain-sensitive and sentiment-aware embeddings that simultaneously capture the information of sentiment semantics and domain sensitivity of individual words. Our method can automatically determine and produce domain-common embeddings and domain-specific embeddings. The differentiation of domain-common and domain-specific words enables the advantage of data augmentation of common semantics from multiple domains and capture the varied semantics of specific words from different domains at the same time. Experimental results show that our model provides an effective way to learn domain-sensitive and sentiment-aware word embeddings which benefit sentiment classification at both sentence level and lexicon term level.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1232.pdf",
        "title": "Learning Domain-Sensitive and Sentiment-Aware Word Embeddings"
    },
    {
        "paper_id": "P18-1239",
        "conference": "acl",
        "year": "2018",
        "abstract": "We conduct the most comprehensive study to date into translating words via images. To facilitate research on the task, we introduce a large-scale multilingual corpus of images, each labeled with the word it represents. Past datasets have been limited to only a few high-resource languages and unrealistically easy translation settings. In contrast, we have collected by far the largest available dataset for this task, with images for approximately 10,000 words in each of 100 languages. We run experiments on a dozen high resource languages and 20 low resources languages, demonstrating the effect of word concreteness and part-of-speech on translation quality. %We find that while image features work best for concrete nouns, they are sometimes effective on other parts of speech. To improve image-based translation, we introduce a novel method of predicting word concreteness from images, which improves on a previous state-of-the-art unsupervised technique. This allows us to predict when image-based translation may be effective, enabling consistent improvements to a state-of-the-art text-based word translation system. Our code and the Massively Multilingual Image Dataset (MMID) are available at http://multilingual-images.org/.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1239.pdf",
        "title": "Learning Translations via Images with a Massively Multilingual Image Dataset"
    },
    {
        "paper_id": "P18-1246",
        "conference": "acl",
        "year": "2018",
        "abstract": "The rise of neural networks, and particularly recurrent neural networks, has produced significant advances in part-of-speech tagging accuracy. One characteristic common among these models is the presence of rich initial word encodings. These encodings typically are composed of a recurrent character-based representation with dynamically and pre-trained word embeddings. However, these encodings do not consider a context wider than a single word and it is only through subsequent recurrent layers that word or sub-word information interacts. In this paper, we investigate models that use recurrent neural networks with sentence-level context for initial character and word-based representations. In particular we show that optimal results are obtained by integrating these context sensitive representations through synchronized training with a meta-model that learns to combine their states.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1246.pdf",
        "title": "Morphosyntactic Tagging with a Meta-BiLSTM Model over Context Sensitive Token Encodings"
    },
    {
        "paper_id": "P18-1248",
        "conference": "acl",
        "year": "2018",
        "abstract": "Shi, Huang, and Lee (2017a) obtained state-of-the-art results for English and Chinese dependency parsing by combining dynamic-programming implementations of transition-based dependency parsers with a minimal set of bidirectional LSTM features. However, their results were limited to projective parsing. In this paper, we extend their approach to support non-projectivity by providing the first practical implementation of the MH\u2084 algorithm, an O(n4) mildly nonprojective dynamic-programming parser with very high coverage on non-projective treebanks. To make MH\u2084 compatible with minimal transition-based feature sets, we introduce a transition-based interpretation of it in which parser items are mapped to sequences of transitions. We thus obtain the first implementation of global decoding for non-projective transition-based parsing, and demonstrate empirically that it is effective than its projective counterpart in parsing a number of highly non-projective languages.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1248.pdf",
        "title": "Global Transition-based Non-projective Dependency Parsing"
    },
    {
        "paper_id": "P18-1250",
        "conference": "acl",
        "year": "2018",
        "abstract": "Motivated by the positive impact of empty category on syntactic parsing, we study neural models for pre- and in-parsing detection of empty category, which has not previously been investigated. We find several non-obvious facts: (a) BiLSTM can capture non-local contextual information which is essential for detecting empty categories, (b) even with a BiLSTM, syntactic information is still able to enhance the detection, and (c) automatic detection of empty categories improves parsing quality for overt words. Our neural ECD models outperform the prior state-of-the-art by significant margins.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1250.pdf",
        "title": "Pre- and In-Parsing Models for Neural Empty Category Detection"
    },
    {
        "paper_id": "P18-1252",
        "conference": "acl",
        "year": "2018",
        "abstract": "Treebank conversion is a straightforward and effective way to exploit various heterogeneous treebanks for boosting parsing performance. However, previous work mainly focuses on unsupervised treebank conversion and has made little progress due to the lack of manually labeled data where each sentence has two syntactic trees complying with two different guidelines at the same time, referred as bi-tree aligned data. In this work, we for the first time propose the task of supervised treebank conversion. First, we manually construct a bi-tree aligned dataset containing over ten thousand sentences. Then, we propose two simple yet effective conversion approaches (pattern embedding and treeLSTM) based on the state-of-the-art deep biaffine parser. Experimental results show that 1) the two conversion approaches achieve comparable conversion accuracy, and 2) treebank conversion is superior to the widely used multi-task learning framework in multi-treebank exploitation and leads to significantly higher parsing accuracy.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1252.pdf",
        "title": "Supervised Treebank Conversion: Data and Approaches"
    },
    {
        "paper_id": "P18-1255",
        "conference": "acl",
        "year": "2018",
        "abstract": "Inquiry is fundamental to communication, and machines cannot effectively collaborate with humans unless they can ask questions. In this work, we build a neural network model for the task of ranking clarification questions. Our model is inspired by the idea of expected value of perfect information: a good question is one whose expected answer will be useful. We study this problem using data from StackExchange, a plentiful online resource in which people routinely ask clarifying questions to posts so that they can better offer assistance to the original poster. We create a dataset of clarification questions consisting of 77K posts paired with a clarification question (and answer) from three domains of StackExchange: askubuntu, unix and superuser. We evaluate our model on 500 samples of this dataset against expert human judgments and demonstrate significant improvements over controlled baselines.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-1255.pdf",
        "title": "Learning to Ask Good Questions: Ranking Clarification Questions using Neural Expected Value of Perfect Information"
    },
    {
        "paper_id": "P18-2002",
        "conference": "acl",
        "year": "2018",
        "abstract": "Increasing the capacity of recurrent neural networks (RNN) usually involves augmenting the size of the hidden layer, with significant increase of computational cost. Recurrent neural tensor networks (RNTN) increase capacity using distinct hidden layer weights for each word, but with greater costs in memory usage. In this paper, we introduce restricted recurrent neural tensor networks (r-RNTN) which reserve distinct hidden layer weights for frequent vocabulary words while sharing a single set of weights for infrequent words. Perplexity evaluations show that for fixed hidden layer sizes, r-RNTNs improve language model performance over RNNs using only a small fraction of the parameters of unrestricted RNTNs. These results hold for r-RNTNs using Gated Recurrent Units and Long Short-Term Memory.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-2002.pdf",
        "title": "Restricted Recurrent Neural Tensor Networks: Exploiting Word Frequency and Compositionality"
    },
    {
        "paper_id": "P18-2005",
        "conference": "acl",
        "year": "2018",
        "abstract": "Written text often provides sufficient clues to identify the author, their gender, age, and other important attributes. Consequently, the authorship of training and evaluation corpora can have unforeseen impacts, including differing model performance for different user groups, as well as privacy implications. In this paper, we propose an approach to explicitly obscure important author characteristics at training time, such that representations learned are invariant to these attributes. Evaluating on two tasks, we show that this leads to increased privacy in the learned representations, as well as more robust models to varying evaluation conditions, including out-of-domain corpora.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-2005.pdf",
        "title": "Towards Robust and Privacy-preserving Text Representations"
    },
    {
        "paper_id": "P18-2010",
        "conference": "acl",
        "year": "2018",
        "abstract": "We use dependency triples automatically extracted from a Web-scale corpus to perform unsupervised semantic frame induction. We cast the frame induction problem as a triclustering problem that is a generalization of clustering for triadic data. Our replicable benchmarks demonstrate that the proposed graph-based approach, Triframes, shows state-of-the art results on this task on a FrameNet-derived dataset and performing on par with competitive methods on a verb class clustering task.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-2010.pdf",
        "title": "Unsupervised Semantic Frame Induction using Triclustering"
    },
    {
        "paper_id": "P18-2012",
        "conference": "acl",
        "year": "2018",
        "abstract": "We present a new architecture for named entity recognition. Our model employs multiple independent bidirectional LSTM units across the same input and promotes diversity among them by employing an inter-model regularization term. By distributing computation across multiple smaller LSTMs we find a significant reduction in the total number of parameters. We find our architecture achieves state-of-the-art performance on the CoNLL 2003 NER dataset.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-2012.pdf",
        "title": "Named Entity Recognition With Parallel Recurrent Neural Networks"
    },
    {
        "paper_id": "P18-2013",
        "conference": "acl",
        "year": "2018",
        "abstract": "State-of-the-art knowledge base completion (KBC) models predict a score for every known or unknown fact via a latent factorization over entity and relation embeddings. We observe that when they fail, they often make entity predictions that are incompatible with the type required by the relation. In response, we enhance each base factorization with two type-compatibility terms between entity-relation pairs, and combine the signals in a novel manner. Without explicit supervision from a type catalog, our proposed modification obtains up to 7% MRR gains over base models, and new state-of-the-art results on several datasets. Further analysis reveals that our models better represent the latent types of entities and their embeddings also predict supervised types better than the embeddings fitted by baseline models.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-2013.pdf",
        "title": "Type-Sensitive Knowledge Base Inference Without Explicit Type Supervision"
    },
    {
        "paper_id": "P18-2014",
        "conference": "acl",
        "year": "2018",
        "abstract": "We present a novel graph-based neural network model for relation extraction. Our model treats multiple pairs in a sentence simultaneously and considers interactions among them. All the entities in a sentence are placed as nodes in a fully-connected graph structure. The edges are represented with position-aware contexts around the entity pairs. In order to consider different relation paths between two entities, we construct up to l-length walks between each pair. The resulting walks are merged and iteratively used to update the edge representations into longer walks representations. We show that the model achieves performance comparable to the state-of-the-art systems on the ACE 2005 dataset without using any external tools.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-2014.pdf",
        "title": "A Walk-based Model on Entity Graphs for Relation Extraction"
    },
    {
        "paper_id": "P18-2015",
        "conference": "acl",
        "year": "2018",
        "abstract": "This paper addresses the tasks of automatic seed selection for bootstrapping relation extraction, and noise reduction for distantly supervised relation extraction. We first point out that these tasks are related. Then, inspired by ranking relation instances and patterns computed by the HITS algorithm, and selecting cluster centroids using the K-means, LSA, or NMF method, we propose methods for selecting the initial seeds from an existing resource, or reducing the level of noise in the distantly labeled data. Experiments show that our proposed methods achieve a better performance than the baseline systems in both tasks.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-2015.pdf",
        "title": "Ranking-Based Automatic Seed Selection and Noise Reduction for Weakly Supervised Relation Extraction"
    },
    {
        "paper_id": "P18-2016",
        "conference": "acl",
        "year": "2018",
        "abstract": "LocatedNear relation is a kind of commonsense knowledge describing two physical objects that are typically found near each other in real life. In this paper, we study how to automatically extract such relationship through a sentence-level relation classifier and aggregating the scores of entity pairs from a large corpus. Also, we release two benchmark datasets for evaluation and future research.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-2016.pdf",
        "title": "Automatic Extraction of Commonsense LocatedNear Knowledge"
    },
    {
        "paper_id": "P18-2020",
        "conference": "acl",
        "year": "2018",
        "abstract": "We ask how to practically build a model for German named entity recognition (NER) that performs at the state of the art for both contemporary and historical texts, i.e., a big-data and a small-data scenario. The two best-performing model families are pitted against each other (linear-chain CRFs and BiLSTM) to observe the trade-off between expressiveness and data requirements. BiLSTM outperforms the CRF when large datasets are available and performs inferior for the smallest dataset. BiLSTMs profit substantially from transfer learning, which enables them to be trained on multiple corpora, resulting in a new state-of-the-art model for German NER on two contemporary German corpora (CoNLL 2003 and GermEval 2014) and two historic corpora.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-2020.pdf",
        "title": "A Named Entity Recognition Shootout for German"
    },
    {
        "paper_id": "P18-2021",
        "conference": "acl",
        "year": "2018",
        "abstract": "Software developers and testers have long struggled with how to elicit proactive responses from their coworkers when reviewing code for security vulnerabilities and errors. For a code review to be successful, it must not only identify potential problems but also elicit an active response from the colleague responsible for modifying the code. To understand the factors that contribute to this outcome, we analyze a novel dataset of more than one million code reviews for the Google Chromium project, from which we extract linguistic features of feedback that elicited responsive actions from coworkers. Using a manually-labeled subset of reviewer comments, we trained a highly accurate classifier to identify acted-upon comments (AUC = 0.85). Our results demonstrate the utility of our dataset, the feasibility of using NLP for this new task, and the potential of NLP to improve our understanding of how communications between colleagues can be authored to elicit positive, proactive responses.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-2021.pdf",
        "title": "A dataset for identifying actionable feedback in collaborative software development"
    },
    {
        "paper_id": "P18-2023",
        "conference": "acl",
        "year": "2018",
        "abstract": "Analogical reasoning is effective in capturing linguistic regularities. This paper proposes an analogical reasoning task on Chinese. After delving into Chinese lexical knowledge, we sketch 68 implicit morphological relations and 28 explicit semantic relations. A big and balanced dataset CA8 is then built for this task, including 17813 questions. Furthermore, we systematically explore the influences of vector representations, context features, and corpora on analogical reasoning. With the experiments, CA8 is proved to be a reliable benchmark for evaluating Chinese word embeddings.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-2023.pdf",
        "title": "Analogical Reasoning on Chinese Morphological and Semantic Relations"
    },
    {
        "paper_id": "P18-2051",
        "conference": "acl",
        "year": "2018",
        "abstract": "We explore strategies for incorporating target syntax into Neural Machine Translation. We specifically focus on syntax in ensembles containing multiple sentence representations. We formulate beam search over such ensembles using WFSTs, and describe a delayed SGD update training procedure that is especially effective for long representations like linearized syntax. Our approach gives state-of-the-art performance on a difficult Japanese-English task.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-2051.pdf",
        "title": "Multi-representation ensembles and delayed SGD updates improve syntax-based NMT"
    },
    {
        "paper_id": "P18-2058",
        "conference": "acl",
        "year": "2018",
        "abstract": "Recent BIO-tagging-based neural semantic role labeling models are very high performing, but assume gold predicates as part of the input and cannot incorporate span-level features. We propose an end-to-end approach for jointly predicting all predicates, arguments spans, and the relations between them. The model makes independent decisions about what relationship, if any, holds between every possible word-span pair, and learns contextualized span representations that provide rich, shared input features for each decision. Experiments demonstrate that this approach sets a new state of the art on PropBank SRL without gold predicates.",
        "url_pdf": "https://www.aclweb.org/anthology/P18-2058.pdf",
        "title": "Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling"
    },
    {
        "paper_id": "P19-1001",
        "conference": "acl",
        "year": "2019",
        "abstract": "Currently, researchers have paid great attention to retrieval-based dialogues in open-domain. In particular, people study the problem by investigating context-response matching for multi-turn response selection based on publicly recognized benchmark data sets. State-of-the-art methods require a response to interact with each utterance in a context from the beginning, but the interaction is performed in a shallow way. In this work, we let utterance-response interaction go deep by proposing an interaction-over-interaction network (IoI). The model performs matching by stacking multiple interaction blocks in which residual information from one time of interaction initiates the interaction process again. Thus, matching information within an utterance-response pair is extracted from the interaction of the pair in an iterative fashion, and the information flows along the chain of the blocks via representations. Evaluation results on three benchmark data sets indicate that IoI can significantly outperform state-of-the-art methods in terms of various matching metrics. Through further analysis, we also unveil how the depth of interaction affects the performance of IoI.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1001.pdf",
        "title": "One Time of Interaction May Not Be Enough: Go Deep with an Interaction-over-Interaction Network for Response Selection in Dialogues"
    },
    {
        "paper_id": "P19-1006",
        "conference": "acl",
        "year": "2019",
        "abstract": "Response selection plays an important role in fully automated dialogue systems. Given the dialogue context, the goal of response selection is to identify the best-matched next utterance (i.e., response) from multiple candidates. Despite the efforts of many previous useful models, this task remains challenging due to the huge semantic gap and also the large size of candidate set. To address these issues, we propose a Spatio-Temporal Matching network (STM) for response selection. In detail, soft alignment is first used to obtain the local relevance between the context and the response. And then, we construct spatio-temporal features by aggregating attention images in time dimension and make use of 3D convolution and pooling operations to extract matching information. Evaluation on two large-scale multi-turn response selection tasks has demonstrated that our proposed model significantly outperforms the state-of-the-art model. Particularly, visualization analysis shows that the spatio-temporal features enables matching information in segment pairs and time sequences, and have good interpretability for multi-turn text matching.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1006.pdf",
        "title": "Constructing Interpretive Spatio-Temporal Features for Multi-Turn Responses Selection"
    },
    {
        "paper_id": "P19-1013",
        "conference": "acl",
        "year": "2019",
        "abstract": "We propose a new domain adaptation method for Combinatory Categorial Grammar (CCG) parsing, based on the idea of automatic generation of CCG corpora exploiting cheaper resources of dependency trees. Our solution is conceptually simple, and not relying on a specific parser architecture, making it applicable to the current best-performing parsers. We conduct extensive parsing experiments with detailed discussion; on top of existing benchmark datasets on (1) biomedical texts and (2) question sentences, we create experimental datasets of (3) speech conversation and (4) math problems. When applied to the proposed method, an off-the-shelf CCG parser shows significant performance gains, improving from 90.7% to 96.6% on speech conversation, and from 88.5% to 96.8% on math problems.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1013.pdf",
        "title": "Automatic Generation of High Quality CCGbanks for Parser Domain Adaptation"
    },
    {
        "paper_id": "P19-1019",
        "conference": "acl",
        "year": "2019",
        "abstract": "While machine translation has traditionally relied on large amounts of parallel corpora, a recent research line has managed to train both Neural Machine Translation (NMT) and Statistical Machine Translation (SMT) systems using monolingual corpora only. In this paper, we identify and address several deficiencies of existing unsupervised SMT approaches by exploiting subword information, developing a theoretically well founded unsupervised tuning method, and incorporating a joint refinement procedure. Moreover, we use our improved SMT system to initialize a dual NMT model, which is further fine-tuned through on-the-fly back-translation. Together, we obtain large improvements over the previous state-of-the-art in unsupervised machine translation. For instance, we get 22.5 BLEU points in English-to-German WMT 2014, 5.5 points more than the previous best unsupervised system, and 0.5 points more than the (supervised) shared task winner back in 2014.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1019.pdf",
        "title": "An Effective Approach to Unsupervised Machine Translation"
    },
    {
        "paper_id": "P19-1021",
        "conference": "acl",
        "year": "2019",
        "abstract": "It has been shown that the performance of neural machine translation (NMT) drops starkly in low-resource conditions, underperforming phrase-based statistical machine translation (PBSMT) and requiring large amounts of auxiliary data to achieve competitive results. In this paper, we re-assess the validity of these results, arguing that they are the result of lack of system adaptation to low-resource settings. We discuss some pitfalls to be aware of when training low-resource NMT systems, and recent techniques that have shown to be especially helpful in low-resource settings, resulting in a set of best practices for low-resource NMT. In our experiments on German\u2013English with different amounts of IWSLT14 training data, we show that, without the use of any auxiliary monolingual or multilingual data, an optimized NMT system can outperform PBSMT with far less data than previously claimed. We also apply these techniques to a low-resource Korean\u2013English dataset, surpassing previously reported results by 4 BLEU.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1021.pdf",
        "title": "Revisiting Low-Resource Neural Machine Translation: A Case Study"
    },
    {
        "paper_id": "P19-1023",
        "conference": "acl",
        "year": "2019",
        "abstract": "We study relation extraction for knowledge base (KB) enrichment. Specifically, we aim to extract entities and their relationships from sentences in the form of triples and map the elements of the extracted triples to an existing KB in an end-to-end manner. Previous studies focus on the extraction itself and rely on Named Entity Disambiguation (NED) to map triples into the KB space. This way, NED errors may cause extraction errors that affect the overall precision and recall.To address this problem, we propose an end-to-end relation extraction model for KB enrichment based on a neural encoder-decoder model. We collect high-quality training data by distant supervision with co-reference resolution and paraphrase detection. We propose an n-gram based attention model that captures multi-word entity names in a sentence. Our model employs jointly learned word and entity embeddings to support named entity disambiguation. Finally, our model uses a modified beam search and a triple classifier to help generate high-quality triples. Our model outperforms state-of-the-art baselines by 15.51% and 8.38% in terms of F1 score on two real-world datasets.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1023.pdf",
        "title": "Neural Relation Extraction for Knowledge Base Enrichment"
    },
    {
        "paper_id": "P19-1029",
        "conference": "acl",
        "year": "2019",
        "abstract": "Not all types of supervision signals are created equal: Different types of feedback have different costs and effects on learning. We show how self-regulation strategies that decide when to ask for which kind of feedback from a teacher (or from oneself) can be cast as a learning-to-learn problem leading to improved cost-aware sequence-to-sequence learning. In experiments on interactive neural machine translation, we find that the self-regulator discovers an \ud835\udf16-greedy strategy for the optimal cost-quality trade-off by mixing different feedback types including corrections, error markups, and self-supervision. Furthermore, we demonstrate its robustness under domain shift and identify it as a promising alternative to active learning.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1029.pdf",
        "title": "Self-Regulated Interactive Sequence-to-Sequence Learning"
    },
    {
        "paper_id": "P19-1035",
        "conference": "acl",
        "year": "2019",
        "abstract": "We propose two novel manipulation strategies for increasing and decreasing the difficulty of C-tests automatically. This is a crucial step towards generating learner-adaptive exercises for self-directed language learning and preparing language assessment tests. To reach the desired difficulty level, we manipulate the size and the distribution of gaps based on absolute and relative gap difficulty predictions. We evaluate our approach in corpus-based experiments and in a user study with 60 participants. We find that both strategies are able to generate C-tests with the desired difficulty level.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1035.pdf",
        "title": "Manipulating the Difficulty of C-Tests"
    },
    {
        "paper_id": "P19-1036",
        "conference": "acl",
        "year": "2019",
        "abstract": "Text classification aims at mapping documents into a set of predefined categories. Supervised machine learning models have shown great success in this area but they require a large number of labeled documents to reach adequate accuracy. This is particularly true when the number of target categories is in the tens or the hundreds. In this work, we explore an unsupervised approach to classify documents into categories simply described by a label. The proposed method is inspired by the way a human proceeds in this situation: It draws on textual similarity between the most relevant words in each document and a dictionary of keywords for each category reflecting its semantics and lexical field. The novelty of our method hinges on the enrichment of the category labels through a combination of human expertise and language models, both generic and domain specific. Our experiments on 5 standard corpora show that the proposed method increases F1-score over relying solely on human expertise and can also be on par with simple supervised approaches. It thus provides a practical alternative to situations where low cost text categorization is needed, as we illustrate with our application to operational risk incidents classification.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1036.pdf",
        "title": "Towards Unsupervised Text Classification Leveraging Experts and Word Embeddings"
    },
    {
        "paper_id": "P19-1041",
        "conference": "acl",
        "year": "2019",
        "abstract": "This paper tackles the problem of disentangling the latent representations of style and content in language models. We propose a simple yet effective approach, which incorporates auxiliary multi-task and adversarial objectives, for style prediction and bag-of-words prediction, respectively. We show, both qualitatively and quantitatively, that the style and content are indeed disentangled in the latent space. This disentangled latent representation learning can be applied to style transfer on non-parallel corpora. We achieve high performance in terms of transfer accuracy, content preservation, and language fluency, in comparison to various previous approaches.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1041.pdf",
        "title": "Disentangled Representation Learning for Non-Parallel Text Style Transfer"
    },
    {
        "paper_id": "P19-1042",
        "conference": "acl",
        "year": "2019",
        "abstract": "Automatic grammatical error correction (GEC) research has made remarkable progress in the past decade. However, all existing approaches to GEC correct errors by considering a single sentence alone and ignoring crucial cross-sentence context. Some errors can only be corrected reliably using cross-sentence context and models can also benefit from the additional contextual information in correcting other errors. In this paper, we address this serious limitation of existing approaches and improve strong neural encoder-decoder models by appropriately modeling wider contexts. We employ an auxiliary encoder that encodes previous sentences and incorporate the encoding in the decoder via attention and gating mechanisms. Our approach results in statistically significant improvements in overall GEC performance over strong baselines across multiple test sets. Analysis of our cross-sentence GEC model on a synthetic dataset shows high performance in verb tense corrections that require cross-sentence context.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1042.pdf",
        "title": "Cross-Sentence Grammatical Error Correction"
    },
    {
        "paper_id": "P19-1046",
        "conference": "acl",
        "year": "2019",
        "abstract": "We propose a general strategy named \u2018divide, conquer and combine\u2019 for multimodal fusion. Instead of directly fusing features at holistic level, we conduct fusion hierarchically so that both local and global interactions are considered for a comprehensive interpretation of multimodal embeddings. In the \u2018divide\u2019 and \u2018conquer\u2019 stages, we conduct local fusion by exploring the interaction of a portion of the aligned feature vectors across various modalities lying within a sliding window, which ensures that each part of multimodal embeddings are explored sufficiently. On its basis, global fusion is conducted in the \u2018combine\u2019 stage to explore the interconnection across local interactions, via an Attentive Bi-directional Skip-connected LSTM that directly connects distant local interactions and integrates two levels of attention mechanism. In this way, local interactions can exchange information sufficiently and thus obtain an overall view of multimodal information. Our method achieves state-of-the-art performance on multimodal affective computing with higher efficiency.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1046.pdf",
        "title": "Divide, Conquer and Combine: Hierarchical Feature Fusion Network with Local and Global Perspectives for Multimodal Affective Computing"
    },
    {
        "paper_id": "P19-1048",
        "conference": "acl",
        "year": "2019",
        "abstract": "Aspect-based sentiment analysis produces a list of aspect terms and their corresponding sentiments for a natural language sentence. This task is usually done in a pipeline manner, with aspect term extraction performed first, followed by sentiment predictions toward the extracted aspect terms. While easier to develop, such an approach does not fully exploit joint information from the two subtasks and does not use all available sources of training information that might be helpful, such as document-level labeled sentiment corpus. In this paper, we propose an interactive multi-task learning network (IMN) which is able to jointly learn multiple related tasks simultaneously at both the token level as well as the document level. Unlike conventional multi-task learning methods that rely on learning common features for the different tasks, IMN introduces a message passing architecture where information is iteratively passed to different tasks through a shared set of latent variables. Experimental results demonstrate superior performance of the proposed method against multiple baselines on three benchmark datasets.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1048.pdf",
        "title": "An Interactive Multi-Task Learning Network for End-to-End Aspect-Based Sentiment Analysis"
    },
    {
        "paper_id": "P19-1056",
        "conference": "acl",
        "year": "2019",
        "abstract": "This paper focuses on two related subtasks of aspect-based sentiment analysis, namely aspect term extraction and aspect sentiment classification, which we call aspect term-polarity co-extraction. The former task is to extract aspects of a product or service from an opinion document, and the latter is to identify the polarity expressed in the document about these extracted aspects. Most existing algorithms address them as two separate tasks and solve them one by one, or only perform one task, which can be complicated for real applications. In this paper, we treat these two tasks as two sequence labeling problems and propose a novel Dual crOss-sharEd RNN framework (DOER) to generate all aspect term-polarity pairs of the input sentence simultaneously. Specifically, DOER involves a dual recurrent neural network to extract the respective representation of each task, and a cross-shared unit to consider the relationship between them. Experimental results demonstrate that the proposed framework outperforms state-of-the-art baselines on three benchmark datasets.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1056.pdf",
        "title": "DOER: Dual Cross-Shared RNN for Aspect Term-Polarity Co-Extraction"
    },
    {
        "paper_id": "P19-1069",
        "conference": "acl",
        "year": "2019",
        "abstract": "The well-known problem of knowledge acquisition is one of the biggest issues in Word Sense Disambiguation (WSD), where annotated data are still scarce in English and almost absent in other languages. In this paper we formulate the assumption of One Sense per Wikipedia Category and present OneSeC, a language-independent method for the automatic extraction of hundreds of thousands of sentences in which a target word is tagged with its meaning. Our automatically-generated data consistently lead a supervised WSD model to state-of-the-art performance when compared with other automatic and semi-automatic methods. Moreover, our approach outperforms its competitors on multilingual and domain-specific settings, where it beats the existing state of the art on all languages and most domains. All the training data are available for research purposes at http://trainomatic.org/onesec.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1069.pdf",
        "title": "Just \u201cOneSeC\u201d for Producing Multilingual Sense-Annotated Data"
    },
    {
        "paper_id": "P19-1074",
        "conference": "acl",
        "year": "2019",
        "abstract": "Multiple entities in a document generally exhibit complex inter-sentence relations, and cannot be well handled by existing relation extraction (RE) methods that typically focus on extracting intra-sentence relations for single entity pairs. In order to accelerate the research on document-level RE, we introduce DocRED, a new dataset constructed from Wikipedia and Wikidata with three features: (1) DocRED annotates both named entities and relations, and is the largest human-annotated dataset for document-level RE from plain text; (2) DocRED requires reading multiple sentences in a document to extract entities and infer their relations by synthesizing all information of the document; (3) along with the human-annotated data, we also offer large-scale distantly supervised data, which enables DocRED to be adopted for both supervised and weakly supervised scenarios. In order to verify the challenges of document-level RE, we implement recent state-of-the-art methods for RE and conduct a thorough evaluation of these methods on DocRED. Empirical results show that DocRED is challenging for existing RE methods, which indicates that document-level RE remains an open problem and requires further efforts. Based on the detailed analysis on the experiments, we discuss multiple promising directions for future research. We make DocRED and the code for our baselines publicly available at https://github.com/thunlp/DocRED.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1074.pdf",
        "title": "DocRED: A Large-Scale Document-Level Relation Extraction Dataset"
    },
    {
        "paper_id": "P19-1079",
        "conference": "acl",
        "year": "2019",
        "abstract": "We present methods for multi-task learning that take advantage of natural groupings of related tasks. Task groups may be defined along known properties of the tasks, such as task domain or language. Such task groups represent supervised information at the inter-task level and can be encoded into the model. We investigate two variants of neural network architectures that accomplish this, learning different feature spaces at the levels of individual tasks, task groups, as well as the universe of all tasks: (1) parallel architectures encode each input simultaneously into feature spaces at different levels; (2) serial architectures encode each input successively into feature spaces at different levels in the task hierarchy. We demonstrate the methods on natural language understanding (NLU) tasks, where a grouping of tasks into different task domains leads to improved performance on ATIS, Snips, and a large in-house dataset.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1079.pdf",
        "title": "Multi-Task Networks with Universe, Group, and Task Feature Learning"
    },
    {
        "paper_id": "P19-1081",
        "conference": "acl",
        "year": "2019",
        "abstract": "We study a conversational reasoning model that strategically traverses through a large-scale common fact knowledge graph (KG) to introduce engaging and contextually diverse entities and attributes. For this study, we collect a new Open-ended Dialog <-> KG parallel corpus called OpenDialKG, where each utterance from 15K human-to-human role-playing dialogs is manually annotated with ground-truth reference to corresponding entities and paths from a large-scale KG with 1M+ facts. We then propose the DialKG Walker model that learns the symbolic transitions of dialog contexts as structured traversals over KG, and predicts natural entities to introduce given previous dialog contexts via a novel domain-agnostic, attention-based graph path decoder. Automatic and human evaluations show that our model can retrieve more natural and human-like responses than the state-of-the-art baselines or rule-based models, in both in-domain and cross-domain tasks. The proposed model also generates a KG walk path for each entity retrieved, providing a natural way to explain conversational reasoning.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1081.pdf",
        "title": "OpenDialKG: Explainable Conversational Reasoning with Attention-based Walks over Knowledge Graphs"
    },
    {
        "paper_id": "P19-1085",
        "conference": "acl",
        "year": "2019",
        "abstract": "Fact verification (FV) is a challenging task which requires to retrieve relevant evidence from plain text and use the evidence to verify given claims. Many claims require to simultaneously integrate and reason over several pieces of evidence for verification. However, previous work employs simple models to extract information from evidence without letting evidence communicate with each other, e.g., merely concatenate the evidence for processing. Therefore, these methods are unable to grasp sufficient relational and logical information among the evidence. To alleviate this issue, we propose a graph-based evidence aggregating and reasoning (GEAR) framework which enables information to transfer on a fully-connected evidence graph and then utilizes different aggregators to collect multi-evidence information. We further employ BERT, an effective pre-trained language representation model, to improve the performance. Experimental results on a large-scale benchmark dataset FEVER have demonstrated that GEAR could leverage multi-evidence information for FV and thus achieves the promising result with a test FEVER score of 67.10%. Our code is available at https://github.com/thunlp/GEAR.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1085.pdf",
        "title": "GEAR: Graph-based Evidence Aggregating and Reasoning for Fact Verification"
    },
    {
        "paper_id": "P19-1087",
        "conference": "acl",
        "year": "2019",
        "abstract": "This paper describes novel models tailored for a new application, that of extracting the symptoms mentioned in clinical conversations along with their status. Lack of any publicly available corpus in this privacy-sensitive domain led us to develop our own corpus, consisting of about 3K conversations annotated by professional medical scribes. We propose two novel deep learning approaches to infer the symptom names and their status: (1) a new hierarchical span-attribute tagging (SA-T) model, trained using curriculum learning, and (2) a variant of sequence-to-sequence model which decodes the symptoms and their status from a few speaker turns within a sliding window over the conversation. This task stems from a realistic application of assisting medical providers in capturing symptoms mentioned by patients from their clinical conversations. To reflect this application, we define multiple metrics. From inter-rater agreement, we find that the task is inherently difficult. We conduct comprehensive evaluations on several contrasting conditions and observe that the performance of the models range from an F-score of 0.5 to 0.8 depending on the condition. Our analysis not only reveals the inherent challenges of the task, but also provides useful directions to improve the models.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1087.pdf",
        "title": "Extracting Symptoms and their Status from Clinical Conversations"
    },
    {
        "paper_id": "P19-1088",
        "conference": "acl",
        "year": "2019",
        "abstract": "The quality of a counseling intervention relies highly on the active collaboration between clients and counselors. In this paper, we explore several linguistic aspects of the collaboration process occurring during counseling conversations. Specifically, we address the differences between high-quality and low-quality counseling. Our approach examines participants\u2019 turn-by-turn interaction, their linguistic alignment, the sentiment expressed by speakers during the conversation, as well as the different topics being discussed. Our results suggest important language differences in low- and high-quality counseling, which we further use to derive linguistic features able to capture the differences between the two groups. These features are then used to build automatic classifiers that can predict counseling quality with accuracies of up to 88%.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1088.pdf",
        "title": "What Makes a Good Counselor? Learning to Distinguish between High-quality and Low-quality Counseling Conversations"
    },
    {
        "paper_id": "P19-1109",
        "conference": "acl",
        "year": "2019",
        "abstract": "Complex Word Identification (CWI) is concerned with detection of words in need of simplification and is a crucial first step in a simplification pipeline. It has been shown that reliable CWI systems considerably improve text simplification. However, most CWI systems to date address the task on a word-by-word basis, not taking the context into account. In this paper, we present a novel approach to CWI based on sequence modelling. Our system is capable of performing CWI in context, does not require extensive feature engineering and outperforms state-of-the-art systems on this task.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1109.pdf",
        "title": "Complex Word Identification as a Sequence Labelling Task"
    },
    {
        "paper_id": "P19-1112",
        "conference": "acl",
        "year": "2019",
        "abstract": "In visual communication, text emphasis is used to increase the comprehension of written text to convey the author\u2019s intent. We study the problem of emphasis selection, i.e. choosing candidates for emphasis in short written text, to enable automated design assistance in authoring. Without knowing the author\u2019s intent and only considering the input text, multiple emphasis selections are valid. We propose a model that employs end-to-end label distribution learning (LDL) on crowd-sourced data and predicts a selection distribution, capturing the inter-subjectivity (common-sense) in the audience as well as the ambiguity of the input. We compare the model with several baselines in which the problem is transformed to single-label learning by mapping label distributions to absolute labels via majority voting.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1112.pdf",
        "title": "Learning Emphasis Selection for Written Text in Visual Media from Crowd-Sourced Label Distributions"
    },
    {
        "paper_id": "P19-1119",
        "conference": "acl",
        "year": "2019",
        "abstract": "Unsupervised bilingual word embedding (UBWE), together with other technologies such as back-translation and denoising, has helped unsupervised neural machine translation (UNMT) achieve remarkable results in several language pairs. In previous methods, UBWE is first trained using non-parallel monolingual corpora and then this pre-trained UBWE is used to initialize the word embedding in the encoder and decoder of UNMT. That is, the training of UBWE and UNMT are separate. In this paper, we first empirically investigate the relationship between UBWE and UNMT. The empirical findings show that the performance of UNMT is significantly affected by the performance of UBWE. Thus, we propose two methods that train UNMT with UBWE agreement. Empirical results on several language pairs show that the proposed methods significantly outperform conventional UNMT.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1119.pdf",
        "title": "Unsupervised Bilingual Word Embedding Agreement for Unsupervised Neural Machine Translation"
    },
    {
        "paper_id": "P19-1120",
        "conference": "acl",
        "year": "2019",
        "abstract": "Transfer learning or multilingual model is essential for low-resource neural machine translation (NMT), but the applicability is limited to cognate languages by sharing their vocabularies. This paper shows effective techniques to transfer a pretrained NMT model to a new, unrelated language without shared vocabularies. We relieve the vocabulary mismatch by using cross-lingual word embedding, train a more language-agnostic encoder by injecting artificial noises, and generate synthetic data easily from the pretraining data without back-translation. Our methods do not require restructuring the vocabulary or retraining the model. We improve plain NMT transfer by up to +5.1% BLEU in five low-resource translation tasks, outperforming multilingual joint training by a large margin. We also provide extensive ablation studies on pretrained embedding, synthetic data, vocabulary size, and parameter freezing for a better understanding of NMT transfer.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1120.pdf",
        "title": "Effective Cross-lingual Transfer of Neural Machine Translation Models without Shared Vocabularies"
    },
    {
        "paper_id": "P19-1127",
        "conference": "acl",
        "year": "2019",
        "abstract": "Pre-trained embeddings such as word embeddings and sentence embeddings are fundamental tools facilitating a wide range of downstream NLP tasks. In this work, we investigate how to learn a general-purpose embedding of textual relations, defined as the shortest dependency path between entities. Textual relation embedding provides a level of knowledge between word/phrase level and sentence level, and we show that it can facilitate downstream tasks requiring relational understanding of the text. To learn such an embedding, we create the largest distant supervision dataset by linking the entire English ClueWeb09 corpus to Freebase. We use global co-occurrence statistics between textual and knowledge base relations as the supervision signal to train the embedding. Evaluation on two relational understanding tasks demonstrates the usefulness of the learned textual relation embedding. The data and code can be found at https://github.com/czyssrs/GloREPlus",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1127.pdf",
        "title": "Global Textual Relation Embedding for Relational Understanding"
    },
    {
        "paper_id": "P19-1130",
        "conference": "acl",
        "year": "2019",
        "abstract": "In practical scenario, relation extraction needs to first identify entity pairs that have relation and then assign a correct relation class. However, the number of non-relation entity pairs in context (negative instances) usually far exceeds the others (positive instances), which negatively affects a model\u2019s performance. To mitigate this problem, we propose a multi-task architecture which jointly trains a model to perform relation identification with cross-entropy loss and relation classification with ranking loss. Meanwhile, we observe that a sentence may have multiple entities and relation mentions, and the patterns in which the entities appear in a sentence may contain useful semantic information that can be utilized to distinguish between positive and negative instances. Thus we further incorporate the embeddings of character-wise/word-wise BIO tag from the named entity recognition task into character/word embeddings to enrich the input representation. Experiment results show that our proposed approach can significantly improve the performance of a baseline model with more than 10% absolute increase in F1-score, and outperform the state-of-the-art models on ACE 2005 Chinese and English corpus. Moreover, BIO tag embeddings are particularly effective and can be used to improve other models as well.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1130.pdf",
        "title": "Exploiting Entity BIO Tag Embeddings and Multi-task Learning for Relation Extraction with Imbalanced Data"
    },
    {
        "paper_id": "P19-1136",
        "conference": "acl",
        "year": "2019",
        "abstract": "In this paper, we present GraphRel, an end-to-end relation extraction model which uses graph convolutional networks (GCNs) to jointly learn named entities and relations. In contrast to previous baselines, we consider the interaction between named entities and relations via a 2nd-phase relation-weighted GCN to better extract relations. Linear and dependency structures are both used to extract both sequential and regional features of the text, and a complete word graph is further utilized to extract implicit features among all word pairs of the text. With the graph-based approach, the prediction for overlapping relations is substantially improved over previous sequential approaches. We evaluate GraphRel on two public datasets: NYT and WebNLG. Results show that GraphRel maintains high precision while increasing recall substantially. Also, GraphRel outperforms previous work by 3.2% and 5.8% (F1 score), achieving a new state-of-the-art for relation extraction.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1136.pdf",
        "title": "GraphRel: Modeling Text as Relational Graphs for Joint Entity and Relation Extraction"
    },
    {
        "paper_id": "P19-1137",
        "conference": "acl",
        "year": "2019",
        "abstract": "Pattern-based labeling methods have achieved promising results in alleviating the inevitable labeling noises of distantly supervised neural relation extraction. However, these methods require significant expert labor to write relation-specific patterns, which makes them too sophisticated to generalize quickly. To ease the labor-intensive workload of pattern writing and enable the quick generalization to new relation types, we propose a neural pattern diagnosis framework, DIAG-NRE, that can automatically summarize and refine high-quality relational patterns from noise data with human experts in the loop. To demonstrate the effectiveness of DIAG-NRE, we apply it to two real-world datasets and present both significant and interpretable improvements over state-of-the-art methods.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1137.pdf",
        "title": "DIAG-NRE: A Neural Pattern Diagnosis Framework for Distantly Supervised Neural Relation Extraction"
    },
    {
        "paper_id": "P19-1140",
        "conference": "acl",
        "year": "2019",
        "abstract": "Entity alignment typically suffers from the issues of structural heterogeneity and limited seed alignments. In this paper, we propose a novel Multi-channel Graph Neural Network model (MuGNN) to learn alignment-oriented knowledge graph (KG) embeddings by robustly encoding two KGs via multiple channels. Each channel encodes KGs via different relation weighting schemes with respect to self-attention towards KG completion and cross-KG attention for pruning exclusive entities respectively, which are further combined via pooling techniques. Moreover, we also infer and transfer rule knowledge for completing two KGs consistently. MuGNN is expected to reconcile the structural differences of two KGs, and thus make better use of seed alignments. Extensive experiments on five publicly available datasets demonstrate our superior performance (5% Hits@1 up on average). Source code and data used in the experiments can be accessed at https://github.com/thunlp/MuGNN .",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1140.pdf",
        "title": "Multi-Channel Graph Neural Network for Entity Alignment"
    },
    {
        "paper_id": "P19-1147",
        "conference": "acl",
        "year": "2019",
        "abstract": "This work examines the robustness of self-attentive neural networks against adversarial input perturbations. Specifically, we investigate the attention and feature extraction mechanisms of state-of-the-art recurrent neural networks and self-attentive architectures for sentiment analysis, entailment and machine translation under adversarial attacks. We also propose a novel attack algorithm for generating more natural adversarial examples that could mislead neural models but not humans. Experimental results show that, compared to recurrent neural models, self-attentive models are more robust against adversarial perturbation. In addition, we provide theoretical explanations for their superior robustness to support our claims.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1147.pdf",
        "title": "On the Robustness of Self-Attentive Models"
    },
    {
        "paper_id": "P19-1153",
        "conference": "acl",
        "year": "2019",
        "abstract": "A lot of work has been done in the field of image compression via machine learning, but not much attention has been given to the compression of natural language. Compressing text into lossless representations while making features easily retrievable is not a trivial task, yet has huge benefits. Most methods designed to produce feature rich sentence embeddings focus solely on performing well on downstream tasks and are unable to properly reconstruct the original sequence from the learned embedding. In this work, we propose a near lossless method for encoding long sequences of texts as well as all of their sub-sequences into feature rich representations. We test our method on sentiment analysis and show good performance across all sub-sentence and sentence embeddings.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1153.pdf",
        "title": "Towards Lossless Encoding of Sentences"
    },
    {
        "paper_id": "P19-1173",
        "conference": "acl",
        "year": "2019",
        "abstract": "Morphological tagging is challenging for morphologically rich languages due to the large target space and the need for more training data to minimize model sparsity. Dialectal variants of morphologically rich languages suffer more as they tend to be more noisy and have less resources. In this paper we explore the use of multitask learning and adversarial training to address morphological richness and dialectal variations in the context of full morphological tagging. We use multitask learning for joint morphological modeling for the features within two dialects, and as a knowledge-transfer scheme for cross-dialectal modeling. We use adversarial training to learn dialect invariant features that can help the knowledge-transfer scheme from the high to low-resource variants. We work with two dialectal variants: Modern Standard Arabic (high-resource \u201cdialect\u2019\u201d) and Egyptian Arabic (low-resource dialect) as a case study. Our models achieve state-of-the-art results for both. Furthermore, adversarial training provides more significant improvement when using smaller training datasets in particular.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1173.pdf",
        "title": "Adversarial Multitask Learning for Joint Multi-Feature and Multi-Dialect Morphological Modeling"
    },
    {
        "paper_id": "P19-1175",
        "conference": "acl",
        "year": "2019",
        "abstract": "We present a simple yet powerful data augmentation method for boosting Neural Machine Translation (NMT) performance by leveraging information retrieved from a Translation Memory (TM). We propose and test two methods for augmenting NMT training data with fuzzy TM matches. Tests on the DGT-TM data set for two language pairs show consistent and substantial improvements over a range of baseline systems. The results suggest that this method is promising for any translation environment in which a sizeable TM is available and a certain amount of repetition across translations is to be expected, especially considering its ease of implementation.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1175.pdf",
        "title": "Neural Fuzzy Repair: Integrating Fuzzy Matches into Neural Machine Translation"
    },
    {
        "paper_id": "P19-1183",
        "conference": "acl",
        "year": "2019",
        "abstract": "In this paper, we address a novel task, namely weakly-supervised spatio-temporally grounding natural sentence in video. Specifically, given a natural sentence and a video, we localize a spatio-temporal tube in the video that semantically corresponds to the given sentence, with no reliance on any spatio-temporal annotations during training. First, a set of spatio-temporal tubes, referred to as instances, are extracted from the video. We then encode these instances and the sentence using our newly proposed attentive interactor which can exploit their fine-grained relationships to characterize their matching behaviors. Besides a ranking loss, a novel diversity loss is introduced to train our attentive interactor to strengthen the matching behaviors of reliable instance-sentence pairs and penalize the unreliable ones. We also contribute a dataset, called VID-sentence, based on the ImageNet video object detection dataset, to serve as a benchmark for our task. Results from extensive experiments demonstrate the superiority of our model over the baseline approaches.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1183.pdf",
        "title": "Weakly-Supervised Spatio-Temporally Grounding Natural Sentence in Video"
    },
    {
        "paper_id": "P19-1185",
        "conference": "acl",
        "year": "2019",
        "abstract": "Architecture search is the process of automatically learning the neural model or cell structure that best suits the given task. Recently, this approach has shown promising performance improvements (on language modeling and image classification) with reasonable training speed, using a weight sharing strategy called Efficient Neural Architecture Search (ENAS). In our work, we first introduce a novel continual architecture search (CAS) approach, so as to continually evolve the model parameters during the sequential training of several tasks, without losing performance on previously learned tasks (via block-sparsity and orthogonality constraints), thus enabling life-long learning. Next, we explore a multi-task architecture search (MAS) approach over ENAS for finding a unified, single cell structure that performs well across multiple tasks (via joint controller rewards), and hence allows more generalizable transfer of the cell structure knowledge to an unseen new task. We empirically show the effectiveness of our sequential continual learning and parallel multi-task learning based architecture search approaches on diverse sentence-pair classification tasks (GLUE) and multimodal-generation based video captioning tasks. Further, we present several ablations and analyses on the learned cell structures.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1185.pdf",
        "title": "Continual and Multi-Task Architecture Search"
    },
    {
        "paper_id": "P19-1186",
        "conference": "acl",
        "year": "2019",
        "abstract": "Supervised models of NLP rely on large collections of text which closely resemble the intended testing setting. Unfortunately matching text is often not available in sufficient quantity, and moreover, within any domain of text, data is often highly heterogenous. In this paper we propose a method to distill the important domain signal as part of a multi-domain learning system, using a latent variable model in which parts of a neural model are stochastically gated based on the inferred domain. We compare the use of discrete versus continuous latent variables, operating in a domain-supervised or a domain semi-supervised setting, where the domain is known only for a subset of training inputs. We show that our model leads to substantial performance improvements over competitive benchmark domain adaptation methods, including methods using adversarial learning.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1186.pdf",
        "title": "Semi-supervised Stochastic Multi-Domain Learning using Variational Inference"
    },
    {
        "paper_id": "P19-1193",
        "conference": "acl",
        "year": "2019",
        "abstract": "Automatic topic-to-essay generation is a challenging task since it requires generating novel, diverse, and topic-consistent paragraph-level text with a set of topics as input. Previous work tends to perform essay generation based solely on the given topics while ignoring massive commonsense knowledge. However, this commonsense knowledge provides additional background information, which can help to generate essays that are more novel and diverse. Towards filling this gap, we propose to integrate commonsense from the external knowledge base into the generator through dynamic memory mechanism. Besides, the adversarial training based on a multi-label discriminator is employed to further improve topic-consistency. We also develop a series of automatic evaluation metrics to comprehensively assess the quality of the generated essay. Experiments show that with external commonsense knowledge and adversarial training, the generated essays are more novel, diverse, and topic-consistent than existing methods in terms of both automatic and human evaluation.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1193.pdf",
        "title": "Enhancing Topic-to-Essay Generation with External Commonsense Knowledge"
    },
    {
        "paper_id": "P19-1195",
        "conference": "acl",
        "year": "2019",
        "abstract": "Recent approaches to data-to-text generation have shown great promise thanks to the use of large-scale datasets and the application of neural network architectures which are trained end-to-end. These models rely on representation learning to select content appropriately, structure it coherently, and verbalize it grammatically, treating entities as nothing more than vocabulary tokens. In this work we propose an entity-centric neural architecture for data-to-text generation. Our model creates entity-specific representations which are dynamically updated. Text is generated conditioned on the data input and entity memory representations using hierarchical attention at each time step. We present experiments on the RotoWire benchmark and a (five times larger) new dataset on the baseball domain which we create. Our results show that the proposed model outperforms competitive baselines in automatic and human evaluation.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1195.pdf",
        "title": "Data-to-text Generation with Entity Modeling"
    },
    {
        "paper_id": "P19-1197",
        "conference": "acl",
        "year": "2019",
        "abstract": "Table-to-text generation aims to translate the structured data into the unstructured text. Most existing methods adopt the encoder-decoder framework to learn the transformation, which requires large-scale training samples. However, the lack of large parallel data is a major practical problem for many domains. In this work, we consider the scenario of low resource table-to-text generation, where only limited parallel data is available. We propose a novel model to separate the generation into two stages: key fact prediction and surface realization. It first predicts the key facts from the tables, and then generates the text with the key facts. The training of key fact prediction needs much fewer annotated data, while surface realization can be trained with pseudo parallel corpus. We evaluate our model on a biography generation dataset. Our model can achieve 27.34 BLEU score with only 1,000 parallel data, while the baseline model only obtain the performance of 9.71 BLEU score.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1197.pdf",
        "title": "Key Fact as Pivot: A Two-Stage Model for Low Resource Table-to-Text Generation"
    },
    {
        "paper_id": "P19-1204",
        "conference": "acl",
        "year": "2019",
        "abstract": "Currently, no large-scale training data is available for the task of scientific paper summarization. In this paper, we propose a novel method that automatically generates summaries for scientific papers, by utilizing videos of talks at scientific conferences. We hypothesize that such talks constitute a coherent and concise description of the papers\u2019 content, and can form the basis for good summaries. We collected 1716 papers and their corresponding videos, and created a dataset of paper summaries. A model trained on this dataset achieves similar performance as models trained on a dataset of summaries created manually. In addition, we validated the quality of our summaries by human experts.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1204.pdf",
        "title": "TalkSumm: A Dataset and Scalable Annotation Method for Scientific Paper Summarization Based on Conference Talks"
    },
    {
        "paper_id": "P19-1206",
        "conference": "acl",
        "year": "2019",
        "abstract": "This paper focuses on the end-to-end abstractive summarization of a single product review without supervision. We assume that a review can be described as a discourse tree, in which the summary is the root, and the child sentences explain their parent in detail. By recursively estimating a parent from its children, our model learns the latent discourse tree without an external parser and generates a concise summary. We also introduce an architecture that ranks the importance of each sentence on the tree to support summary generation focusing on the main review point. The experimental results demonstrate that our model is competitive with or outperforms other unsupervised approaches. In particular, for relatively long reviews, it achieves a competitive or better performance than supervised models. The induced tree shows that the child sentences provide additional information about their parent, and the generated summary abstracts the entire review.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1206.pdf",
        "title": "Unsupervised Neural Single-Document Summarization of Reviews via Learning Latent Discourse Structure and its Ranking"
    },
    {
        "paper_id": "P19-1209",
        "conference": "acl",
        "year": "2019",
        "abstract": "When writing a summary, humans tend to choose content from one or two sentences and merge them into a single summary sentence. However, the mechanisms behind the selection of one or multiple source sentences remain poorly understood. Sentence fusion assumes multi-sentence input; yet sentence selection methods only work with single sentences and not combinations of them. There is thus a crucial gap between sentence selection and fusion to support summarizing by both compressing single sentences and fusing pairs. This paper attempts to bridge the gap by ranking sentence singletons and pairs together in a unified space. Our proposed framework attempts to model human methodology by selecting either a single sentence or a pair of sentences, then compressing or fusing the sentence(s) to produce a summary sentence. We conduct extensive experiments on both single- and multi-document summarization datasets and report findings on sentence selection and abstraction.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1209.pdf",
        "title": "Scoring Sentence Singletons and Pairs for Abstractive Summarization"
    },
    {
        "paper_id": "P19-1212",
        "conference": "acl",
        "year": "2019",
        "abstract": "Most existing text summarization datasets are compiled from the news domain, where summaries have a flattened discourse structure. In such datasets, summary-worthy content often appears in the beginning of input articles. Moreover, large segments from input articles are present verbatim in their respective summaries. These issues impede the learning and evaluation of systems that can understand an article\u2019s global content structure as well as produce abstractive summaries with high compression ratio. In this work, we present a novel dataset, BIGPATENT, consisting of 1.3 million records of U.S. patent documents along with human written abstractive summaries. Compared to existing summarization datasets, BIGPATENT has the following properties: i) summaries contain a richer discourse structure with more recurring entities, ii) salient content is evenly distributed in the input, and iii) lesser and shorter extractive fragments are present in the summaries. Finally, we train and evaluate baselines and popular learning models on BIGPATENT to shed light on new challenges and motivate future directions for summarization research.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1212.pdf",
        "title": "BIGPATENT: A Large-Scale Dataset for Abstractive and Coherent Summarization"
    },
    {
        "paper_id": "P19-1216",
        "conference": "acl",
        "year": "2019",
        "abstract": "Multi-sentence compression (MSC) aims to generate a grammatical but reduced compression from multiple input sentences while retaining their key information. Previous dominating approach for MSC is the extraction-based word graph approach. A few variants further leveraged lexical substitution to yield more abstractive compression. However, two limitations exist. First, the word graph approach that simply concatenates fragments from multiple sentences may yield non-fluent or ungrammatical compression. Second, lexical substitution is often inappropriate without the consideration of context information. To tackle the above-mentioned issues, we present a neural rewriter for multi-sentence compression that does not need any parallel corpus. Empirical studies have shown that our approach achieves comparable results upon automatic evaluation and improves the grammaticality of compression based on human evaluation. A parallel corpus with more than 140,000 (sentence group, compression) pairs is also constructed as a by-product for future research.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1216.pdf",
        "title": "Unsupervised Rewriter for Multi-Sentence Compression"
    },
    {
        "paper_id": "P19-1220",
        "conference": "acl",
        "year": "2019",
        "abstract": "This study tackles generative reading comprehension (RC), which consists of answering questions based on textual evidence and natural language generation (NLG). We propose a multi-style abstractive summarization model for question answering, called Masque. The proposed model has two key characteristics. First, unlike most studies on RC that have focused on extracting an answer span from the provided passages, our model instead focuses on generating a summary from the question and multiple passages. This serves to cover various answer styles required for real-world applications. Second, whereas previous studies built a specific model for each answer style because of the difficulty of acquiring one general model, our approach learns multi-style answers within a model to improve the NLG capability for all styles involved. This also enables our model to give an answer in the target style. Experiments show that our model achieves state-of-the-art performance on the Q&A task and the Q&A + NLG task of MS MARCO 2.1 and the summary task of NarrativeQA. We observe that the transfer of the style-independent NLG capability to the target style is the key to its success.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1220.pdf",
        "title": "Multi-style Generative Reading Comprehension"
    },
    {
        "paper_id": "P19-1221",
        "conference": "acl",
        "year": "2019",
        "abstract": "This paper considers the reading comprehension task in which multiple documents are given as input. Prior work has shown that a pipeline of retriever, reader, and reranker can improve the overall performance. However, the pipeline system is inefficient since the input is re-encoded within each module, and is unable to leverage upstream components to help downstream training. In this work, we present RE3QA, a unified question answering model that combines context retrieving, reading comprehension, and answer reranking to predict the final answer. Unlike previous pipelined approaches, RE3QA shares contextualized text representation across different components, and is carefully designed to use high-quality upstream outputs (e.g., retrieved context or candidate answers) for directly supervising downstream modules (e.g., the reader or the reranker). As a result, the whole network can be trained end-to-end to avoid the context inconsistency problem. Experiments show that our model outperforms the pipelined baseline and achieves state-of-the-art results on two versions of TriviaQA and two variants of SQuAD.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1221.pdf",
        "title": "Retrieve, Read, Rerank: Towards End-to-End Multi-Document Reading Comprehension"
    },
    {
        "paper_id": "P19-1225",
        "conference": "acl",
        "year": "2019",
        "abstract": "Question answering (QA) using textual sources for purposes such as reading comprehension (RC) has attracted much attention. This study focuses on the task of explainable multi-hop QA, which requires the system to return the answer with evidence sentences by reasoning and gathering disjoint pieces of the reference texts. It proposes the Query Focused Extractor (QFE) model for evidence extraction and uses multi-task learning with the QA model. QFE is inspired by extractive summarization models; compared with the existing method, which extracts each evidence sentence independently, it sequentially extracts evidence sentences by using an RNN with an attention mechanism on the question sentence. It enables QFE to consider the dependency among the evidence sentences and cover important information in the question sentence. Experimental results show that QFE with a simple RC baseline model achieves a state-of-the-art evidence extraction score on HotpotQA. Although designed for RC, it also achieves a state-of-the-art evidence extraction score on FEVER, which is a recognizing textual entailment task on a large textual database.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1225.pdf",
        "title": "Answering while Summarizing: Multi-task Learning for Multi-hop QA with Evidence Extraction"
    },
    {
        "paper_id": "P19-1227",
        "conference": "acl",
        "year": "2019",
        "abstract": "Open-domain question answering (OpenQA) aims to answer questions through text retrieval and reading comprehension. Recently, lots of neural network-based models have been proposed and achieved promising results in OpenQA. However, the success of these models relies on a massive volume of training data (usually in English), which is not available in many other languages, especially for those low-resource languages. Therefore, it is essential to investigate cross-lingual OpenQA. In this paper, we construct a novel dataset XQA for cross-lingual OpenQA research. It consists of a training set in English as well as development and test sets in eight other languages. Besides, we provide several baseline systems for cross-lingual OpenQA, including two machine translation-based methods and one zero-shot cross-lingual method (multilingual BERT). Experimental results show that the multilingual BERT model achieves the best results in almost all target languages, while the performance of cross-lingual OpenQA is still much lower than that of English. Our analysis indicates that the performance of cross-lingual OpenQA is related to not only how similar the target language and English are, but also how difficult the question set of the target language is. The XQA dataset is publicly available at http://github.com/thunlp/XQA.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1227.pdf",
        "title": "XQA: A Cross-lingual Open-domain Question Answering Dataset"
    },
    {
        "paper_id": "P19-1229",
        "conference": "acl",
        "year": "2019",
        "abstract": "During the past decades, due to the lack of sufficient labeled data, most studies on cross-domain parsing focus on unsupervised domain adaptation, assuming there is no target-domain training data. However, unsupervised approaches make limited progress so far due to the intrinsic difficulty of both domain adaptation and parsing. This paper tackles the semi-supervised domain adaptation problem for Chinese dependency parsing, based on two newly-annotated large-scale domain-aware datasets. We propose a simple domain embedding approach to merge the source- and target-domain training data, which is shown to be more effective than both direct corpus concatenation and multi-task learning. In order to utilize unlabeled target-domain data, we employ the recent contextualized word representations and show that a simple fine-tuning procedure can further boost cross-domain parsing accuracy by large margin.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1229.pdf",
        "title": "Semi-supervised Domain Adaptation for Dependency Parsing"
    },
    {
        "paper_id": "P19-1231",
        "conference": "acl",
        "year": "2019",
        "abstract": "In this work, we explore the way to perform named entity recognition (NER) using only unlabeled data and named entity dictionaries. To this end, we formulate the task as a positive-unlabeled (PU) learning problem and accordingly propose a novel PU learning algorithm to perform the task. We prove that the proposed algorithm can unbiasedly and consistently estimate the task loss as if there is fully labeled data. A key feature of the proposed method is that it does not require the dictionaries to label every entity within a sentence, and it even does not require the dictionaries to label all of the words constituting an entity. This greatly reduces the requirement on the quality of the dictionaries and makes our method generalize well with quite simple dictionaries. Empirical studies on four public NER datasets demonstrate the effectiveness of our proposed method. We have published the source code at https://github.com/v-mipeng/LexiconNER.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1231.pdf",
        "title": "Distantly Supervised Named Entity Recognition using Positive-Unlabeled Learning"
    },
    {
        "paper_id": "P19-1236",
        "conference": "acl",
        "year": "2019",
        "abstract": "Due to limitation of labeled resources, cross-domain named entity recognition (NER) has been a challenging task. Most existing work considers a supervised setting, making use of labeled data for both the source and target domains. A disadvantage of such methods is that they cannot train for domains without NER data. To address this issue, we consider using cross-domain LM as a bridge cross-domains for NER domain adaptation, performing cross-domain and cross-task knowledge transfer by designing a novel parameter generation network. Results show that our method can effectively extract domain differences from cross-domain LM contrast, allowing unsupervised domain adaptation while also giving state-of-the-art results among supervised domain adaptation methods.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1236.pdf",
        "title": "Cross-Domain NER using Cross-Domain Language Modeling"
    },
    {
        "paper_id": "P19-1240",
        "conference": "acl",
        "year": "2019",
        "abstract": "A huge volume of user-generated content is daily produced on social media. To facilitate automatic language understanding, we study keyphrase prediction, distilling salient information from massive posts. While most existing methods extract words from source posts to form keyphrases, we propose a sequence-to-sequence (seq2seq) based neural keyphrase generation framework, enabling absent keyphrases to be created. Moreover, our model, being topic-aware, allows joint modeling of corpus-level latent topic representations, which helps alleviate data sparsity widely exhibited in social media language. Experiments on three datasets collected from English and Chinese social media platforms show that our model significantly outperforms both extraction and generation models without exploiting latent topics. Further discussions show that our model learns meaningful topics, which interprets its superiority in social media keyphrase generation.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1240.pdf",
        "title": "Topic-Aware Neural Keyphrase Generation for Social Media Language"
    },
    {
        "paper_id": "P19-1241",
        "conference": "acl",
        "year": "2019",
        "abstract": "The availability of large-scale online social data, coupled with computational methods can help us answer fundamental questions relat- ing to our social lives, particularly our health and well-being. The #MeToo trend has led to people talking about personal experiences of harassment more openly. This work at- tempts to aggregate such experiences of sex- ual abuse to facilitate a better understanding of social media constructs and to bring about social change. It has been found that disclo- sure of abuse has positive psychological im- pacts. Hence, we contend that such informa- tion can leveraged to create better campaigns for social change by analyzing how users react to these stories and to obtain a better insight into the consequences of sexual abuse. We use a three part Twitter-Specific Social Media Lan- guage Model to segregate personal recollec- tions of sexual harassment from Twitter posts. An extensive comparison with state-of-the-art generic and specific models along with a de- tailed error analysis explores the merit of our proposed model.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1241.pdf",
        "title": "#YouToo? Detection of Personal Recollections of Sexual Harassment on Social Media"
    },
    {
        "paper_id": "P19-1244",
        "conference": "acl",
        "year": "2019",
        "abstract": "Claim verification is generally a task of verifying the veracity of a given claim, which is critical to many downstream applications. It is cumbersome and inefficient for human fact-checkers to find consistent pieces of evidence, from which solid verdict could be inferred against the claim. In this paper, we propose a novel end-to-end hierarchical attention network focusing on learning to represent coherent evidence as well as their semantic relatedness with the claim. Our model consists of three main components: 1) A coherence-based attention layer embeds coherent evidence considering the claim and sentences from relevant articles; 2) An entailment-based attention layer attends on sentences that can semantically infer the claim on top of the first attention; and 3) An output layer predicts the verdict based on the embedded evidence. Experimental results on three public benchmark datasets show that our proposed model outperforms a set of state-of-the-art baselines.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1244.pdf",
        "title": "Sentence-Level Evidence Embedding for Claim Verification with Hierarchical Attention Networks"
    },
    {
        "paper_id": "P19-1249",
        "conference": "acl",
        "year": "2019",
        "abstract": "Celebrities are among the most prolific users of social media, promoting their personas and rallying followers. This activity is closely tied to genuine writing samples, which makes them worthy research subjects in many respects, not least profiling. With this paper we introduce the Webis Celebrity Corpus 2019. For its construction the Twitter feeds of 71,706 verified accounts have been carefully linked with their respective Wikidata items, crawling both. After cleansing, the resulting profiles contain an average of 29,968 words per profile and up to 239 pieces of personal information. A cross-evaluation that checked the correct association of Twitter account and Wikidata item revealed an error rate of only 0.6%, rendering the profiles highly reliable. Our corpus comprises a wide cross-section of local and global celebrities, forming a unique combination of scale, profile comprehensiveness, and label reliability. We further establish the state of the art\u2019s profiling performance by evaluating the winning approaches submitted to the PAN gender prediction tasks in a transfer learning experiment. They are only outperformed by our own deep learning approach, which we also use to exemplify celebrity occupation prediction for the first time.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1249.pdf",
        "title": "Celebrity Profiling"
    },
    {
        "paper_id": "P19-1251",
        "conference": "acl",
        "year": "2019",
        "abstract": "Accompanied by modern industrial developments, air pollution has already become a major concern for human health. Hence, air quality measures, such as the concentration of PM2.5, have attracted increasing attention. Even some studies apply historical measurements into air quality forecast, the changes of air quality conditions are still hard to monitor. In this paper, we propose to exploit social media and natural language processing techniques to enhance air quality prediction. Social media users are treated as social sensors with their findings and locations. After filtering noisy tweets using word selection and topic modeling, a deep learning model based on convolutional neural networks and over-tweet-pooling is proposed to enhance air quality prediction. We conduct experiments on 7-month real-world Twitter datasets in the five most heavily polluted states in the USA. The results show that our approach significantly improves air quality prediction over the baseline that does not use social media by 6.9% to 17.7% in macro-F1 scores.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1251.pdf",
        "title": "Enhancing Air Quality Prediction with Social Media and Natural Language Processing"
    },
    {
        "paper_id": "P19-1254",
        "conference": "acl",
        "year": "2019",
        "abstract": "Writers often rely on plans or sketches to write long stories, but most current language models generate word by word from left to right. We explore coarse-to-fine models for creating narrative texts of several hundred words, and introduce new models which decompose stories by abstracting over actions and entities. The model first generates the predicate-argument structure of the text, where different mentions of the same entity are marked with placeholder tokens. It then generates a surface realization of the predicate-argument structure, and finally replaces the entity placeholders with context-sensitive names and references. Human judges prefer the stories from our models to a wide range of previous approaches to hierarchical text generation. Extensive analysis shows that our methods can help improve the diversity and coherence of events and entities in generated stories.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1254.pdf",
        "title": "Strategies for Structuring Story Generation"
    },
    {
        "paper_id": "P19-1256",
        "conference": "acl",
        "year": "2019",
        "abstract": "Recent neural language generation systems often hallucinate contents (i.e., producing irrelevant or contradicted facts), especially when trained on loosely corresponding pairs of the input structure and text. To mitigate this issue, we propose to integrate a language understanding module for data refinement with self-training iterations to effectively induce strong equivalence between the input data and the paired text. Experiments on the E2E challenge dataset show that our proposed framework can reduce more than 50% relative unaligned noise from the original data-text pairs. A vanilla sequence-to-sequence neural NLG model trained on the refined data has improved on content correctness compared with the current state-of-the-art ensemble generator.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1256.pdf",
        "title": "A Simple Recipe towards Reducing Hallucination in Neural Surface Realisation"
    },
    {
        "paper_id": "P19-1257",
        "conference": "acl",
        "year": "2019",
        "abstract": "Automatic commenting of online articles can provide additional opinions and facts to the reader, which improves user experience and engagement on social media platforms. Previous work focuses on automatic commenting based solely on textual content. However, in real-scenarios, online articles usually contain multiple modal contents. For instance, graphic news contains plenty of images in addition to text. Contents other than text are also vital because they are not only more attractive to the reader but also may provide critical information. To remedy this, we propose a new task: cross-model automatic commenting (CMAC), which aims to make comments by integrating multiple modal contents. We construct a large-scale dataset for this task and explore several representative methods. Going a step further, an effective co-attention model is presented to capture the dependency between textual and visual information. Evaluation results show that our proposed model can achieve better performance than competitive baselines.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1257.pdf",
        "title": "Cross-Modal Commentator: Automatic Machine Commenting Based on Cross-Modal Information"
    },
    {
        "paper_id": "P19-1266",
        "conference": "acl",
        "year": "2019",
        "abstract": "Comparing between Deep Neural Network (DNN) models based on their performance on unseen data is crucial for the progress of the NLP field. However, these models have a large number of hyper-parameters and, being non-convex, their convergence point depends on the random values chosen at initialization and during training. Proper DNN comparison hence requires a comparison between their empirical score distributions on unseen data, rather than between single evaluation scores as is standard for more simple, convex models. In this paper, we propose to adapt to this problem a recently proposed test for the Almost Stochastic Dominance relation between two distributions. We define the criteria for a high quality comparison method between DNNs, and show, both theoretically and through analysis of extensive experimental results with leading DNN models for sequence tagging tasks, that the proposed test meets all criteria while previously proposed methods fail to do so. We hope the test we propose here will set a new working practice in the NLP community.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1266.pdf",
        "title": "Deep Dominance - How to Properly Compare Deep Neural Models"
    },
    {
        "paper_id": "P19-1276",
        "conference": "acl",
        "year": "2019",
        "abstract": "We consider open domain event extraction, the task of extracting unconstraint types of events from news clusters. A novel latent variable neural model is constructed, which is scalable to very large corpus. A dataset is collected and manually annotated, with task-specific evaluation metrics being designed. Results show that the proposed unsupervised model gives better performance compared to the state-of-the-art method for event schema induction.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1276.pdf",
        "title": "Open Domain Event Extraction Using Neural Latent Variable Models"
    },
    {
        "paper_id": "P19-1284",
        "conference": "acl",
        "year": "2019",
        "abstract": "The success of neural networks comes hand in hand with a desire for more interpretability. We focus on text classifiers and make them more interpretable by having them provide a justification\u2013a rationale\u2013for their predictions. We approach this problem by jointly training two neural network models: a latent model that selects a rationale (i.e. a short and informative part of the input text), and a classifier that learns from the words in the rationale alone. Previous work proposed to assign binary latent masks to input positions and to promote short selections via sparsity-inducing penalties such as L0 regularisation. We propose a latent model that mixes discrete and continuous behaviour allowing at the same time for binary selections and gradient-based training without REINFORCE. In our formulation, we can tractably compute the expected value of penalties such as L0, which allows us to directly optimise the model towards a pre-specified text selection rate. We show that our approach is competitive with previous work on rationale extraction, and explore further uses in attention mechanisms.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1284.pdf",
        "title": "Interpretable Neural Predictions with Differentiable Binary Variables"
    },
    {
        "paper_id": "P19-1296",
        "conference": "acl",
        "year": "2019",
        "abstract": "The training objective of neural machine translation (NMT) is to minimize the loss between the words in the translated sentences and those in the references. In NMT, there is a natural correspondence between the source sentence and the target sentence. However, this relationship has only been represented using the entire neural network and the training objective is computed in word-level. In this paper, we propose a sentence-level agreement module to directly minimize the difference between the representation of source and target sentence. The proposed agreement module can be integrated into NMT as an additional training objective function and can also be used to enhance the representation of the source sentences. Empirical results on the NIST Chinese-to-English and WMT English-to-German tasks show the proposed agreement module can significantly improve the NMT performance.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1296.pdf",
        "title": "Sentence-Level Agreement for Neural Machine Translation"
    },
    {
        "paper_id": "P19-1305",
        "conference": "acl",
        "year": "2019",
        "abstract": "Abstractive Sentence Summarization (ASSUM) targets at grasping the core idea of the source sentence and presenting it as the summary. It is extensively studied using statistical models or neural models based on the large-scale monolingual source-summary parallel corpus. But there is no cross-lingual parallel corpus, whose source sentence language is different to the summary language, to directly train a cross-lingual ASSUM system. We propose to solve this zero-shot problem by using resource-rich monolingual ASSUM system to teach zero-shot cross-lingual ASSUM system on both summary word generation and attention. This teaching process is along with a back-translation process which simulates source-summary pairs. Experiments on cross-lingual ASSUM task show that our proposed method is significantly better than pipeline baselines and previous works, and greatly enhances the cross-lingual performances closer to the monolingual performances.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1305.pdf",
        "title": "Zero-Shot Cross-Lingual Abstractive Sentence Summarization through Teaching Generation and Attention"
    },
    {
        "paper_id": "P19-1306",
        "conference": "acl",
        "year": "2019",
        "abstract": "In this paper, we propose to boost low-resource cross-lingual document retrieval performance with deep bilingual query-document representations. We match queries and documents in both source and target languages with four components, each of which is implemented as a term interaction-based deep neural network with cross-lingual word embeddings as input. By including query likelihood scores as extra features, our model effectively learns to rerank the retrieved documents by using a small number of relevance labels for low-resource language pairs. Due to the shared cross-lingual word embedding space, the model can also be directly applied to another language pair without any training label. Experimental results on the Material dataset show that our model outperforms the competitive translation-based baselines on English-Swahili, English-Tagalog, and English-Somali cross-lingual information retrieval tasks.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1306.pdf",
        "title": "Improving Low-Resource Cross-lingual Document Retrieval by Reranking with Deep Bilingual Representations"
    },
    {
        "paper_id": "P19-1308",
        "conference": "acl",
        "year": "2019",
        "abstract": "The task of unsupervised bilingual lexicon induction (UBLI) aims to induce word translations from monolingual corpora in two languages. Previous work has shown that morphological variation is an intractable challenge for the UBLI task, where the induced translation in failure case is usually morphologically related to the correct translation. To tackle this challenge, we propose a morphology-aware alignment model for the UBLI task. The proposed model aims to alleviate the adverse effect of morphological variation by introducing grammatical information learned by the pre-trained denoising language model. Results show that our approach can substantially outperform several state-of-the-art unsupervised systems, and even achieves competitive performance compared to supervised methods.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1308.pdf",
        "title": "MAAM: A Morphology-Aware Alignment Model for Unsupervised Bilingual Lexicon Induction"
    },
    {
        "paper_id": "P19-1309",
        "conference": "acl",
        "year": "2019",
        "abstract": "Machine translation is highly sensitive to the size and quality of the training data, which has led to an increasing interest in collecting and filtering large parallel corpora. In this paper, we propose a new method for this task based on multilingual sentence embeddings. In contrast to previous approaches, which rely on nearest neighbor retrieval with a hard threshold over cosine similarity, our proposed method accounts for the scale inconsistencies of this measure, considering the margin between a given sentence pair and its closest candidates instead. Our experiments show large improvements over existing methods. We outperform the best published results on the BUCC mining task and the UN reconstruction task by more than 10 F1 and 30 precision points, respectively. Filtering the English-German ParaCrawl corpus with our approach, we obtain 31.2 BLEU points on newstest2014, an improvement of more than one point over the best official filtered version.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1309.pdf",
        "title": "Margin-based Parallel Corpus Mining with Multilingual Sentence Embeddings"
    },
    {
        "paper_id": "P19-1317",
        "conference": "acl",
        "year": "2019",
        "abstract": "Biomedical concepts are often mentioned in medical documents under different name variations (synonyms). This mismatch between surface forms is problematic, resulting in difficulties pertaining to learning effective representations. Consequently, this has tremendous implications such as rendering downstream applications inefficacious and/or potentially unreliable. This paper proposes a new framework for learning robust representations of biomedical names and terms. The idea behind our approach is to consider and encode contextual meaning, conceptual meaning, and the similarity between synonyms during the representation learning process. Via extensive experiments, we show that our proposed method outperforms other baselines on a battery of retrieval, similarity and relatedness benchmarks. Moreover, our proposed method is also able to compute meaningful representations for unseen names, resulting in high practical utility in real-world applications.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1317.pdf",
        "title": "Robust Representation Learning of Biomedical Names"
    },
    {
        "paper_id": "P19-1318",
        "conference": "acl",
        "year": "2019",
        "abstract": "While word embeddings have been shown to implicitly encode various forms of attributional knowledge, the extent to which they capture relational information is far more limited. In previous work, this limitation has been addressed by incorporating relational knowledge from external knowledge bases when learning the word embedding. Such strategies may not be optimal, however, as they are limited by the coverage of available resources and conflate similarity with other forms of relatedness. As an alternative, in this paper we propose to encode relational knowledge in a separate word embedding, which is aimed to be complementary to a given standard word embedding. This relational word embedding is still learned from co-occurrence statistics, and can thus be used even when no external knowledge base is available. Our analysis shows that relational word vectors do indeed capture information that is complementary to what is encoded in standard word embeddings.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1318.pdf",
        "title": "Relational Word Embeddings"
    },
    {
        "paper_id": "P19-1321",
        "conference": "acl",
        "year": "2019",
        "abstract": "Word embedding models typically learn two types of vectors: target word vectors and context word vectors. These vectors are normally learned such that they are predictive of some word co-occurrence statistic, but they are otherwise unconstrained. However, the words from a given language can be organized in various natural groupings, such as syntactic word classes (e.g. nouns, adjectives, verbs) and semantic themes (e.g. sports, politics, sentiment). Our hypothesis in this paper is that embedding models can be improved by explicitly imposing a cluster structure on the set of context word vectors. To this end, our model relies on the assumption that context word vectors are drawn from a mixture of von Mises-Fisher (vMF) distributions, where the parameters of this mixture distribution are jointly optimized with the word vectors. We show that this results in word vectors which are qualitatively different from those obtained with existing word embedding models. We furthermore show that our embedding model can also be used to learn high-quality document representations.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1321.pdf",
        "title": "Word and Document Embedding with vMF-Mixture Priors on Context Word Vectors"
    },
    {
        "paper_id": "P19-1328",
        "conference": "acl",
        "year": "2019",
        "abstract": "Previous studies on lexical substitution tend to obtain substitute candidates by finding the target word\u2019s synonyms from lexical resources (e.g., WordNet) and then rank the candidates based on its contexts. These approaches have two limitations: (1) They are likely to overlook good substitute candidates that are not the synonyms of the target words in the lexical resources; (2) They fail to take into account the substitution\u2019s influence on the global context of the sentence. To address these issues, we propose an end-to-end BERT-based lexical substitution approach which can propose and validate substitute candidates without using any annotated data or manually curated resources. Our approach first applies dropout to the target word\u2019s embedding for partially masking the word, allowing BERT to take balanced consideration of the target word\u2019s semantics and contexts for proposing substitute candidates, and then validates the candidates based on their substitution\u2019s influence on the global contextualized representation of the sentence. Experiments show our approach performs well in both proposing and ranking substitute candidates, achieving the state-of-the-art results in both LS07 and LS14 benchmarks.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1328.pdf",
        "title": "BERT-based Lexical Substitution"
    },
    {
        "paper_id": "P19-1335",
        "conference": "acl",
        "year": "2019",
        "abstract": "We present the zero-shot entity linking task, where mentions must be linked to unseen entities without in-domain labeled data. The goal is to enable robust transfer to highly specialized domains, and so no metadata or alias tables are assumed. In this setting, entities are only identified by text descriptions, and models must rely strictly on language understanding to resolve the new entities. First, we show that strong reading comprehension models pre-trained on large unlabeled data can be used to generalize to unseen entities. Second, we propose a simple and effective adaptive pre-training strategy, which we term domain-adaptive pre-training (DAP), to address the domain shift problem associated with linking unseen entities in a new domain. We present experiments on a new dataset that we construct for this task and show that DAP improves over strong pre-training baselines, including BERT. The data and code are available at https://github.com/lajanugen/zeshel.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1335.pdf",
        "title": "Zero-Shot Entity Linking by Reading Entity Descriptions"
    },
    {
        "paper_id": "P19-1338",
        "conference": "acl",
        "year": "2019",
        "abstract": "Recently, there has been an increasing interest in unsupervised parsers that optimize semantically oriented objectives, typically using reinforcement learning. Unfortunately, the learned trees often do not match actual syntax trees well. Shen et al. (2018) propose a structured attention mechanism for language modeling (PRPN), which induces better syntactic structures but relies on ad hoc heuristics. Also, their model lacks interpretability as it is not grounded in parsing actions. In our work, we propose an imitation learning approach to unsupervised parsing, where we transfer the syntactic knowledge induced by PRPN to a Tree-LSTM model with discrete parsing actions. Its policy is then refined by Gumbel-Softmax training towards a semantically oriented objective. We evaluate our approach on the All Natural Language Inference dataset and show that it achieves a new state of the art in terms of parsing F-score, outperforming our base models, including PRPN.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1338.pdf",
        "title": "An Imitation Learning Approach to Unsupervised Parsing"
    },
    {
        "paper_id": "P19-1341",
        "conference": "acl",
        "year": "2019",
        "abstract": "We present a new method for sentiment lexicon induction that is designed to be applicable to the entire range of typological diversity of the world\u2019s languages. We evaluate our method on Parallel Bible Corpus+ (PBC+), a parallel corpus of 1593 languages. The key idea is to use Byte Pair Encodings (BPEs) as basic units for multilingual embeddings. Through zero-shot transfer from English sentiment, we learn a seed lexicon for each language in the domain of PBC+. Through domain adaptation, we then generalize the domain-specific lexicon to a general one. We show \u2013 across typologically diverse languages in PBC+ \u2013 good quality of seed and general-domain sentiment lexicons by intrinsic and extrinsic and by automatic and human evaluation. We make freely available our code, seed sentiment lexicons for all 1593 languages and induced general-domain sentiment lexicons for 200 languages.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1341.pdf",
        "title": "A Multilingual BPE Embedding Space for Universal Sentiment Lexicon Induction"
    },
    {
        "paper_id": "P19-1342",
        "conference": "acl",
        "year": "2019",
        "abstract": "Tree-LSTMs have been used for tree-based sentiment analysis over Stanford Sentiment Treebank, which allows the sentiment signals over hierarchical phrase structures to be calculated simultaneously. However, traditional tree-LSTMs capture only the bottom-up dependencies between constituents. In this paper, we propose a tree communication model using graph convolutional neural network and graph recurrent neural network, which allows rich information exchange between phrases constituent tree. Experiments show that our model outperforms existing work on bidirectional tree-LSTMs in both accuracy and efficiency, providing more consistent predictions on phrase-level sentiments.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1342.pdf",
        "title": "Tree Communication Models for Sentiment Analysis"
    },
    {
        "paper_id": "P19-1346",
        "conference": "acl",
        "year": "2019",
        "abstract": "We introduce the first large-scale corpus for long form question answering, a task requiring elaborate and in-depth answers to open-ended questions. The dataset comprises 270K threads from the Reddit forum \u201cExplain Like I\u2019m Five\u201d (ELI5) where an online community provides answers to questions which are comprehensible by five year olds. Compared to existing datasets, ELI5 comprises diverse questions requiring multi-sentence answers. We provide a large set of web documents to help answer the question. Automatic and human evaluations show that an abstractive model trained with a multi-task objective outperforms conventional Seq2Seq, language modeling, as well as a strong extractive baseline.However, our best model is still far from human performance since raters prefer gold responses in over 86% of cases, leaving ample opportunity for future improvement.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1346.pdf",
        "title": "ELI5: Long Form Question Answering"
    },
    {
        "paper_id": "P19-1359",
        "conference": "acl",
        "year": "2019",
        "abstract": "It is desirable for dialog systems to have capability to express specific emotions during a conversation, which has a direct, quantifiable impact on improvement of their usability and user satisfaction. After a careful investigation of real-life conversation data, we found that there are at least two ways to express emotions with language. One is to describe emotional states by explicitly using strong emotional words; another is to increase the intensity of the emotional experiences by implicitly combining neutral words in distinct ways. We propose an emotional dialogue system (EmoDS) that can generate the meaningful responses with a coherent structure for a post, and meanwhile express the desired emotion explicitly or implicitly within a unified framework. Experimental results showed EmoDS performed better than the baselines in BLEU, diversity and the quality of emotional expression.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1359.pdf",
        "title": "Generating Responses with a Specific Emotion in Dialog"
    },
    {
        "paper_id": "P19-1367",
        "conference": "acl",
        "year": "2019",
        "abstract": "We study the task of response generation. Conventional methods employ a fixed vocabulary and one-pass decoding, which not only make them prone to safe and general responses but also lack further refining to the first generated raw sequence. To tackle the above two problems, we present a Vocabulary Pyramid Network (VPN) which is able to incorporate multi-pass encoding and decoding with multi-level vocabularies into response generation. Specifically, the dialogue input and output are represented by multi-level vocabularies which are obtained from hierarchical clustering of raw words. Then, multi-pass encoding and decoding are conducted on the multi-level vocabularies. Since VPN is able to leverage rich encoding and decoding information with multi-level vocabularies, it has the potential to generate better responses. Experiments on English Twitter and Chinese Weibo datasets demonstrate that VPN remarkably outperforms strong baselines.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1367.pdf",
        "title": "Vocabulary Pyramid Network: Multi-Pass Encoding and Decoding with Multi-Level Vocabularies for Response Generation"
    },
    {
        "paper_id": "P19-1368",
        "conference": "acl",
        "year": "2019",
        "abstract": "A challenging problem in on-device text classification is to build highly accurate neural models that can fit in small memory footprint and have low latency. To address this challenge, we propose an on-device neural network SGNN++ which dynamically learns compact projection vectors from raw text using structured and context-dependent partition projections. We show that this results in accelerated inference and performance improvements. We conduct extensive evaluation on multiple conversational tasks and languages such as English, Japanese, Spanish and French. Our SGNN++ model significantly outperforms all baselines, improves upon existing on-device neural models and even surpasses RNN, CNN and BiLSTM models on dialog act and intent prediction. Through a series of ablation studies we show the impact of the partitioned projections and structured information leading to 10% improvement. We study the impact of the model size on accuracy and introduce quatization-aware training for SGNN++ to further reduce the model size while preserving the same quality. Finally, we show fast inference on mobile phones.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1368.pdf",
        "title": "On-device Structured and Context Partitioned Projection Networks"
    },
    {
        "paper_id": "P19-1372",
        "conference": "acl",
        "year": "2019",
        "abstract": "Due to its potential applications, open-domain dialogue generation has become popular and achieved remarkable progress in recent years, but sometimes suffers from generic responses. Previous models are generally trained based on 1-to-1 mapping from an input query to its response, which actually ignores the nature of 1-to-n mapping in dialogue that there may exist multiple valid responses corresponding to the same query. In this paper, we propose to utilize the multiple references by considering the correlation of different valid responses and modeling the 1-to-n mapping with a novel two-step generation architecture. The first generation phase extracts the common features of different responses which, combined with distinctive features obtained in the second phase, can generate multiple diverse and appropriate responses. Experimental results show that our proposed model can effectively improve the quality of response and outperform existing neural dialogue models on both automatic and human evaluations.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1372.pdf",
        "title": "Are Training Samples Correlated? Learning to Generate Dialogue Responses with Multiple References"
    },
    {
        "paper_id": "P19-1374",
        "conference": "acl",
        "year": "2019",
        "abstract": "Disentangling conversations mixed together in a single stream of messages is a difficult task, made harder by the lack of large manually annotated datasets. We created a new dataset of 77,563 messages manually annotated with reply-structure graphs that both disentangle conversations and define internal conversation structure. Our data is 16 times larger than all previously released datasets combined, the first to include adjudication of annotation disagreements, and the first to include context. We use our data to re-examine prior work, in particular, finding that 89% of conversations in a widely used dialogue corpus are either missing messages or contain extra messages. Our manually-annotated data presents an opportunity to develop robust data-driven methods for conversation disentanglement, which will help advance dialogue research.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1374.pdf",
        "title": "A Large-Scale Corpus for Conversation Disentanglement"
    },
    {
        "paper_id": "P19-1389",
        "conference": "acl",
        "year": "2019",
        "abstract": "Sentence function is an important linguistic feature referring to a user\u2019s purpose in uttering a specific sentence. The use of sentence function has shown promising results to improve the performance of conversation models. However, there is no large conversation dataset annotated with sentence functions. In this work, we collect a new Short-Text Conversation dataset with manually annotated SEntence FUNctions (STC-Sefun). Classification models are trained on this dataset to (i) recognize the sentence function of new data in a large corpus of short-text conversations; (ii) estimate a proper sentence function of the response given a test query. We later train conversation models conditioned on the sentence functions, including information retrieval-based and neural generative models. Experimental results demonstrate that the use of sentence functions can help improve the quality of the returned responses.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1389.pdf",
        "title": "Fine-Grained Sentence Functions for Short-Text Conversation"
    },
    {
        "paper_id": "P19-1402",
        "conference": "acl",
        "year": "2019",
        "abstract": "Existing approaches for learning word embedding often assume there are sufficient occurrences for each word in the corpus, such that the representation of words can be accurately estimated from their contexts. However, in real-world scenarios, out-of-vocabulary (a.k.a. OOV) words that do not appear in training corpus emerge frequently. How to learn accurate representations of these words to augment a pre-trained embedding by only a few observations is a challenging research problem. In this paper, we formulate the learning of OOV embedding as a few-shot regression problem by fitting a representation function to predict an oracle embedding vector (defined as embedding trained with abundant observations) based on limited contexts. Specifically, we propose a novel hierarchical attention network-based embedding framework to serve as the neural regression function, in which the context information of a word is encoded and aggregated from K observations. Furthermore, we propose to use Model-Agnostic Meta-Learning (MAML) for adapting the learned model to the new corpus fast and robustly. Experiments show that the proposed approach significantly outperforms existing methods in constructing an accurate embedding for OOV words and improves downstream tasks when the embedding is utilized.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1402.pdf",
        "title": "Few-Shot Representation Learning for Out-Of-Vocabulary Words"
    },
    {
        "paper_id": "P19-1403",
        "conference": "acl",
        "year": "2019",
        "abstract": "Language usage can change across periods of time, but document classifiers models are usually trained and tested on corpora spanning multiple years without considering temporal variations. This paper describes two complementary ways to adapt classifiers to shifts across time. First, we show that diachronic word embeddings, which were originally developed to study language change, can also improve document classification, and we show a simple method for constructing this type of embedding. Second, we propose a time-driven neural classification model inspired by methods for domain adaptation. Experiments on six corpora show how these methods can make classifiers more robust over time.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1403.pdf",
        "title": "Neural Temporality Adaptation for Document Classification: Diachronic Word Embeddings and Domain Adaptation Models"
    },
    {
        "paper_id": "P19-1407",
        "conference": "acl",
        "year": "2019",
        "abstract": "We introduce the Scratchpad Mechanism, a novel addition to the sequence-to-sequence (seq2seq) neural network architecture and demonstrate its effectiveness in improving the overall fluency of seq2seq models for natural language generation tasks. By enabling the decoder at each time step to write to all of the encoder output layers, Scratchpad can employ the encoder as a \u201cscratchpad\u201d memory to keep track of what has been generated so far and thereby guide future generation. We evaluate Scratchpad in the context of three well-studied natural language generation tasks \u2014 Machine Translation, Question Generation, and Text Summarization \u2014 and obtain state-of-the-art or comparable performance on standard datasets for each task. Qualitative assessments in the form of human judgements (question generation), attention visualization (MT), and sample output (summarization) provide further evidence of the ability of Scratchpad to generate fluent and expressive output.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1407.pdf",
        "title": "Keeping Notes: Conditional Natural Language Generation with a Scratchpad Encoder"
    },
    {
        "paper_id": "P19-1408",
        "conference": "acl",
        "year": "2019",
        "abstract": "The common practice in coreference resolution is to identify and evaluate the maximum span of mentions. The use of maximum spans tangles coreference evaluation with the challenges of mention boundary detection like prepositional phrase attachment. To address this problem, minimum spans are manually annotated in smaller corpora. However, this additional annotation is costly and therefore, this solution does not scale to large corpora. In this paper, we propose the MINA algorithm for automatically extracting minimum spans to benefit from minimum span evaluation in all corpora. We show that the extracted minimum spans by MINA are consistent with those that are manually annotated by experts. Our experiments show that using minimum spans is in particular important in cross-dataset coreference evaluation, in which detected mention boundaries are noisier due to domain shift. We have integrated MINA into https://github.com/ns-moosavi/coval for reporting standard coreference scores based on both maximum and automatically detected minimum spans.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1408.pdf",
        "title": "Using Automatically Extracted Minimum Spans to Disentangle Coreference Evaluation from Boundary Detection"
    },
    {
        "paper_id": "P19-1409",
        "conference": "acl",
        "year": "2019",
        "abstract": "Recognizing coreferring events and entities across multiple texts is crucial for many NLP applications. Despite the task\u2019s importance, research focus was given mostly to within-document entity coreference, with rather little attention to the other variants. We propose a neural architecture for cross-document coreference resolution. Inspired by Lee et al. (2012), we jointly model entity and event coreference. We represent an event (entity) mention using its lexical span, surrounding context, and relation to entity (event) mentions via predicate-arguments structures. Our model outperforms the previous state-of-the-art event coreference model on ECB+, while providing the first entity coreference results on this corpus. Our analysis confirms that all our representation elements, including the mention span itself, its context, and the relation to other mentions contribute to the model\u2019s success.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1409.pdf",
        "title": "Revisiting Joint Modeling of Cross-document Entity and Event Coreference Resolution"
    },
    {
        "paper_id": "P19-1411",
        "conference": "acl",
        "year": "2019",
        "abstract": "It has been shown that implicit connectives can be exploited to improve the performance of the models for implicit discourse relation recognition (IDRR). An important property of the implicit connectives is that they can be accurately mapped into the discourse relations conveying their functions. In this work, we explore this property in a multi-task learning framework for IDRR in which the relations and the connectives are simultaneously predicted, and the mapping is leveraged to transfer knowledge between the two prediction tasks via the embeddings of relations and connectives. We propose several techniques to enable such knowledge transfer that yield the state-of-the-art performance for IDRR on several settings of the benchmark dataset (i.e., the Penn Discourse Treebank dataset).",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1411.pdf",
        "title": "Employing the Correspondence of Relations and Connectives to Identify Implicit Discourse Relations via Label Embeddings"
    },
    {
        "paper_id": "P19-1412",
        "conference": "acl",
        "year": "2019",
        "abstract": "When a speaker, Mary, asks \u201cDo you know that Florence is packed with visitors?\u201d, we take her to believe that Florence is packed with visitors, but not if she asks \u201cDo you think that Florence is packed with visitors?\u201d. Inferring speaker commitment (aka event factuality) is crucial for information extraction and question answering. Here, we explore the hypothesis that linguistic deficits drive the error patterns of existing speaker commitment models by analyzing the linguistic correlates of model error on a challenging naturalistic dataset. We evaluate two state-of-the-art speaker commitment models on the CommitmentBank, an English dataset of naturally occurring discourses. The CommitmentBank is annotated with speaker commitment towards the content of the complement (\u201cFlorence is packed with visitors\u201d in our example) of clause-embedding verbs (\u201cknow\u201d, \u201cthink\u201d) under four entailment-canceling environments (negation, modal, question, conditional). A breakdown of items by linguistic features reveals asymmetrical error patterns: while the models achieve good performance on some classes (e.g., negation), they fail to generalize to the diverse linguistic constructions (e.g., conditionals) in natural language, highlighting directions for improvement.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1412.pdf",
        "title": "Do You Know That Florence Is Packed with Visitors? Evaluating State-of-the-art Models of Speaker Commitment"
    },
    {
        "paper_id": "P19-1414",
        "conference": "acl",
        "year": "2019",
        "abstract": "In this paper, we propose a method for why-question answering (why-QA) that uses an adversarial learning framework. Existing why-QA methods retrieve \u201canswer passages\u201d that usually consist of several sentences. These multi-sentence passages contain not only the reason sought by a why-question and its connection to the why-question, but also redundant and/or unrelated parts. We use our proposed \u201cAdversarial networks for Generating compact-answer Representation\u201d (AGR) to generate from a passage a vector representation of the non-redundant reason sought by a why-question and exploit the representation for judging whether the passage actually answers the why-question. Through a series of experiments using Japanese why-QA datasets, we show that these representations improve the performance of our why-QA neural model as well as that of a BERT-based why-QA model. We show that they also improve a state-of-the-art distantly supervised open-domain QA (DS-QA) method on publicly available English datasets, even though the target task is not a why-QA.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1414.pdf",
        "title": "Open-Domain Why-Question Answering with Adversarial Learning to Encode Answer Texts"
    },
    {
        "paper_id": "P19-1415",
        "conference": "acl",
        "year": "2019",
        "abstract": "Machine reading comprehension with unanswerable questions is a challenging task. In this work, we propose a data augmentation technique by automatically generating relevant unanswerable questions according to an answerable question paired with its corresponding paragraph that contains the answer. We introduce a pair-to-sequence model for unanswerable question generation, which effectively captures the interactions between the question and the paragraph. We also present a way to construct training data for our question generation models by leveraging the existing reading comprehension dataset. Experimental results show that the pair-to-sequence model performs consistently better compared with the sequence-to-sequence baseline. We further use the automatically generated unanswerable questions as a means of data augmentation on the SQuAD 2.0 dataset, yielding 1.9 absolute F1 improvement with BERT-base model and 1.7 absolute F1 improvement with BERT-large model.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1415.pdf",
        "title": "Learning to Ask Unanswerable Questions for Machine Reading Comprehension"
    },
    {
        "paper_id": "P19-1425",
        "conference": "acl",
        "year": "2019",
        "abstract": "Neural machine translation (NMT) often suffers from the vulnerability to noisy perturbations in the input. We propose an approach to improving the robustness of NMT models, which consists of two parts: (1) attack the translation model with adversarial source examples; (2) defend the translation model with adversarial target inputs to improve its robustness against the adversarial source inputs. For the generation of adversarial inputs, we propose a gradient-based method to craft adversarial examples informed by the translation loss over the clean inputs. Experimental results on Chinese-English and English-German translation tasks demonstrate that our approach achieves significant improvements (2.8 and 1.6 BLEU points) over Transformer on standard clean benchmarks as well as exhibiting higher robustness on noisy data.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1425.pdf",
        "title": "Robust Neural Machine Translation with Doubly Adversarial Inputs"
    },
    {
        "paper_id": "P19-1429",
        "conference": "acl",
        "year": "2019",
        "abstract": "Event detection systems rely on discrimination knowledge to distinguish ambiguous trigger words and generalization knowledge to detect unseen/sparse trigger words. Current neural event detection approaches focus on trigger-centric representations, which work well on distilling discrimination knowledge, but poorly on learning generalization knowledge. To address this problem, this paper proposes a Delta-learning approach to distill discrimination and generalization knowledge by effectively decoupling, incrementally learning and adaptively fusing event representation. Experiments show that our method significantly outperforms previous approaches on unseen/sparse trigger words, and achieves state-of-the-art performance on both ACE2005 and KBP2017 datasets.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1429.pdf",
        "title": "Distilling Discrimination and Generalization Knowledge for Event Detection via Delta-Representation Learning"
    },
    {
        "paper_id": "P19-1435",
        "conference": "acl",
        "year": "2019",
        "abstract": "Natural Language Sentence Matching (NLSM) has gained substantial attention from both academics and the industry, and rich public datasets contribute a lot to this process. However, biased datasets can also hurt the generalization performance of trained models and give untrustworthy evaluation results. For many NLSM datasets, the providers select some pairs of sentences into the datasets, and this sampling procedure can easily bring unintended pattern, i.e., selection bias. One example is the QuoraQP dataset, where some content-independent naive features are unreasonably predictive. Such features are the reflection of the selection bias and termed as the \u201cleakage features.\u201d In this paper, we investigate the problem of selection bias on six NLSM datasets and find that four out of them are significantly biased. We further propose a training and evaluation framework to alleviate the bias. Experimental results on QuoraQP suggest that the proposed framework can improve the generalization ability of trained models, and give more trustworthy evaluation results for real-world adoptions.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1435.pdf",
        "title": "Selection Bias Explorations and Debias Methods for Natural Language Sentence Matching Datasets"
    },
    {
        "paper_id": "P19-1436",
        "conference": "acl",
        "year": "2019",
        "abstract": "Existing open-domain question answering (QA) models are not suitable for real-time usage because they need to process several long documents on-demand for every input query, which is computationally prohibitive. In this paper, we introduce query-agnostic indexable representations of document phrases that can drastically speed up open-domain QA. In particular, our dense-sparse phrase encoding effectively captures syntactic, semantic, and lexical information of the phrases and eliminates the pipeline filtering of context documents. Leveraging strategies for optimizing training and inference time, our model can be trained and deployed even in a single 4-GPU server. Moreover, by representing phrases as pointers to their start and end tokens, our model indexes phrases in the entire English Wikipedia (up to 60 billion phrases) using under 2TB. Our experiments on SQuAD-Open show that our model is on par with or more accurate than previous models with 6000x reduced computational cost, which translates into at least 68x faster end-to-end inference benchmark on CPUs. Code and demo are available at nlp.cs.washington.edu/denspi",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1436.pdf",
        "title": "Real-Time Open-Domain Question Answering with Dense-Sparse Phrase Index"
    },
    {
        "paper_id": "P19-1441",
        "conference": "acl",
        "year": "2019",
        "abstract": "In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for learning representations across multiple natural language understanding (NLU) tasks. MT-DNN not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations to help adapt to new tasks and domains. MT-DNN extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (Devlin et al., 2018). MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7% (2.2% absolute improvement) as of February 25, 2019 on the latest GLUE test set. We also demonstrate using the SNLI and SciTail datasets that the representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations. Our code and pre-trained models will be made publicly available.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1441.pdf",
        "title": "Multi-Task Deep Neural Networks for Natural Language Understanding"
    },
    {
        "paper_id": "P19-1443",
        "conference": "acl",
        "year": "2019",
        "abstract": "We present SParC, a dataset for cross-domainSemanticParsing inContext that consists of 4,298 coherent question sequences (12k+ individual questions annotated with SQL queries). It is obtained from controlled user interactions with 200 complex databases over 138 domains. We provide an in-depth analysis of SParC and show that it introduces new challenges compared to existing datasets. SParC demonstrates complex contextual dependencies, (2) has greater semantic diversity, and (3) requires generalization to unseen domains due to its cross-domain nature and the unseen databases at test time. We experiment with two state-of-the-art text-to-SQL models adapted to the context-dependent, cross-domain setup. The best model obtains an exact match accuracy of 20.2% over all questions and less than10% over all interaction sequences, indicating that the cross-domain setting and the con-textual phenomena of the dataset present significant challenges for future research. The dataset, baselines, and leaderboard are released at https://yale-lily.github.io/sparc.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1443.pdf",
        "title": "SParC: Cross-Domain Semantic Parsing in Context"
    },
    {
        "paper_id": "P19-1446",
        "conference": "acl",
        "year": "2019",
        "abstract": "Evaluating AMR parsing accuracy involves comparing pairs of AMR graphs. The major evaluation metric, SMATCH (Cai and Knight, 2013), searches for one-to-one mappings between the nodes of two AMRs with a greedy hill-climbing algorithm, which leads to search errors. We propose SEMBLEU, a robust metric that extends BLEU (Papineni et al., 2002) to AMRs. It does not suffer from search errors and considers non-local correspondences in addition to local ones. SEMBLEU is fully content-driven and punishes situations where a system\u2019s output does not preserve most information from the input. Preliminary experiments on both sentence and corpus levels show that SEMBLEU has slightly higher consistency with human judgments than SMATCH. Our code is available at http://github.com/ freesunshine0316/sembleu.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1446.pdf",
        "title": "SemBleu: A Robust Metric for AMR Parsing Evaluation"
    },
    {
        "paper_id": "P19-1453",
        "conference": "acl",
        "year": "2019",
        "abstract": "We present a model and methodology for learning paraphrastic sentence embeddings directly from bitext, removing the time-consuming intermediate step of creating para-phrase corpora. Further, we show that the resulting model can be applied to cross lingual tasks where it both outperforms and is orders of magnitude faster than more complex state-of-the-art baselines.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1453.pdf",
        "title": "Simple and Effective Paraphrastic Similarity from Parallel Translations"
    },
    {
        "paper_id": "P19-1457",
        "conference": "acl",
        "year": "2019",
        "abstract": "Neural models have been investigated for sentiment classification over constituent trees. They learn phrase composition automatically by encoding tree structures but do not explicitly model sentiment composition, which requires to encode sentiment class labels. To this end, we investigate two formalisms with deep sentiment representations that capture sentiment subtype expressions by latent variables and Gaussian mixture vectors, respectively. Experiments on Stanford Sentiment Treebank (SST) show the effectiveness of sentiment grammar over vanilla neural encoders. Using ELMo embeddings, our method gives the best results on this benchmark.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1457.pdf",
        "title": "Latent Variable Sentiment Grammar"
    },
    {
        "paper_id": "P19-1458",
        "conference": "acl",
        "year": "2019",
        "abstract": "Text classification approaches have usually required task-specific model architectures and huge labeled datasets. Recently, thanks to the rise of text-based transfer learning techniques, it is possible to pre-train a language model in an unsupervised manner and leverage them to perform effective on downstream tasks. In this work we focus on Japanese and show the potential use of transfer learning techniques in text classification. Specifically, we perform binary and multi-class sentiment classification on the Rakuten product review and Yahoo movie review datasets. We show that transfer learning-based approaches perform better than task-specific models trained on 3 times as much data. Furthermore, these approaches perform just as well for language modeling pre-trained on only 1/30 of the data. We release our pre-trained models and code as open source.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1458.pdf",
        "title": "An Investigation of Transfer Learning-Based Sentiment Analysis in Japanese"
    },
    {
        "paper_id": "P19-1465",
        "conference": "acl",
        "year": "2019",
        "abstract": "In this paper, we present a fast and strong neural approach for general purpose text matching applications. We explore what is sufficient to build a fast and well-performed text matching model and propose to keep three key features available for inter-sequence alignment: original point-wise features, previous aligned features, and contextual features while simplifying all the remaining components. We conduct experiments on four well-studied benchmark datasets across tasks of natural language inference, paraphrase identification and answer selection. The performance of our model is on par with the state-of-the-art on all datasets with much fewer parameters and the inference speed is at least 6 times faster compared with similarly performed ones.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1465.pdf",
        "title": "Simple and Effective Text Matching with Richer Alignment Features"
    },
    {
        "paper_id": "P19-1469",
        "conference": "acl",
        "year": "2019",
        "abstract": "We present a latent variable model for predicting the relationship between a pair of text sequences. Unlike previous auto-encoding\u2013based approaches that consider each sequence separately, our proposed framework utilizes both sequences within a single model by generating a sequence that has a given relationship with a source sequence. We further extend the cross-sentence generating framework to facilitate semi-supervised training. We also define novel semantic constraints that lead the decoder network to generate semantically plausible and diverse sequences. We demonstrate the effectiveness of the proposed model from quantitative and qualitative experiments, while achieving state-of-the-art results on semi-supervised natural language inference and paraphrase identification.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1469.pdf",
        "title": "A Cross-Sentence Latent Variable Model for Semi-Supervised Text Sequence Matching"
    },
    {
        "paper_id": "P19-1470",
        "conference": "acl",
        "year": "2019",
        "abstract": "We present the first comprehensive study on automatic knowledge base construction for two prevalent commonsense knowledge graphs: ATOMIC (Sap et al., 2019) and ConceptNet (Speer et al., 2017). Contrary to many conventional KBs that store knowledge with canonical templates, commonsense KBs only store loosely structured open-text descriptions of knowledge. We posit that an important step toward automatic commonsense completion is the development of generative models of commonsense knowledge, and propose COMmonsEnse Transformers (COMET) that learn to generate rich and diverse commonsense descriptions in natural language. Despite the challenges of commonsense modeling, our investigation reveals promising results when implicit knowledge from deep pre-trained language models is transferred to generate explicit knowledge in commonsense knowledge graphs. Empirical results demonstrate that COMET is able to generate novel knowledge that humans rate as high quality, with up to 77.5% (ATOMIC) and 91.7% (ConceptNet) precision at top 1, which approaches human performance for these resources. Our findings suggest that using generative commonsense models for automatic commonsense KB completion could soon be a plausible alternative to extractive methods.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1470.pdf",
        "title": "COMET: Commonsense Transformers for Automatic Knowledge Graph Construction"
    },
    {
        "paper_id": "P19-1478",
        "conference": "acl",
        "year": "2019",
        "abstract": "The Winograd Schema Challenge (WSC) dataset WSC273 and its inference counterpart WNLI are popular benchmarks for natural language understanding and commonsense reasoning. In this paper, we show that the performance of three language models on WSC273 consistently and robustly improves when fine-tuned on a similar pronoun disambiguation problem dataset (denoted WSCR). We additionally generate a large unsupervised WSC-like dataset. By fine-tuning the BERT language model both on the introduced and on the WSCR dataset, we achieve overall accuracies of 72.5% and 74.7% on WSC273 and WNLI, improving the previous state-of-the-art solutions by 8.8% and 9.6%, respectively. Furthermore, our fine-tuned models are also consistently more accurate on the \u201ccomplex\u201d subsets of WSC273, introduced by Trichelair et al. (2018).",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1478.pdf",
        "title": "A Surprisingly Robust Trick for the Winograd Schema Challenge"
    },
    {
        "paper_id": "P19-1479",
        "conference": "acl",
        "year": "2019",
        "abstract": "Automatic article commenting is helpful in encouraging user engagement on online news platforms. However, the news documents are usually too long for models under traditional encoder-decoder frameworks, which often results in general and irrelevant comments. In this paper, we propose to generate comments with a graph-to-sequence model that models the input news as a topic interaction graph. By organizing the article into graph structure, our model can better understand the internal structure of the article and the connection between topics, which makes it better able to generate coherent and informative comments. We collect and release a large scale news-comment corpus from a popular Chinese online news platform Tencent Kuaibao. Extensive experiment results show that our model can generate much more coherent and informative comments compared with several strong baseline models.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1479.pdf",
        "title": "Coherent Comments Generation for Chinese Articles with a Graph-to-Sequence Model"
    },
    {
        "paper_id": "P19-1481",
        "conference": "acl",
        "year": "2019",
        "abstract": "Automatic question generation (QG) is a challenging problem in natural language understanding. QG systems are typically built assuming access to a large number of training instances where each instance is a question and its corresponding answer. For a new language, such training instances are hard to obtain making the QG problem even more challenging. Using this as our motivation, we study the reuse of an available large QG dataset in a secondary language (e.g. English) to learn a QG model for a primary language (e.g. Hindi) of interest. For the primary language, we assume access to a large amount of monolingual text but only a small QG dataset. We propose a cross-lingual QG model which uses the following training regime: (i) Unsupervised pretraining of language models in both primary and secondary languages and (ii) joint supervised training for QG in both languages. We demonstrate the efficacy of our proposed approach using two different primary languages, Hindi and Chinese. Our proposed framework clearly outperforms a number of baseline models, including a fully-supervised transformer-based model trained on the QG datasets in the primary language. We also create and release a new question answering dataset for Hindi consisting of 6555 sentences.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1481.pdf",
        "title": "Cross-Lingual Training for Automatic Question Generation"
    },
    {
        "paper_id": "P19-1482",
        "conference": "acl",
        "year": "2019",
        "abstract": "Unsupervised text style transfer aims to alter text styles while preserving the content, without aligned data for supervision. Existing seq2seq methods face three challenges: 1) the transfer is weakly interpretable, 2) generated outputs struggle in content preservation, and 3) the trade-off between content and style is intractable. To address these challenges, we propose a hierarchical reinforced sequence operation method, named Point-Then-Operate (PTO), which consists of a high-level agent that proposes operation positions and a low-level agent that alters the sentence. We provide comprehensive training objectives to control the fluency, style, and content of the outputs and a mask-based inference algorithm that allows for multi-step revision based on the single-step trained agents. Experimental results on two text style transfer datasets show that our method significantly outperforms recent methods and effectively addresses the aforementioned challenges.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1482.pdf",
        "title": "A Hierarchical Reinforced Sequence Operation Method for Unsupervised Text Style Transfer"
    },
    {
        "paper_id": "P19-1487",
        "conference": "acl",
        "year": "2019",
        "abstract": "Deep learning models perform poorly on tasks that require commonsense reasoning, which often necessitates some form of world-knowledge or reasoning over information not immediately present in the input. We collect human explanations for commonsense reasoning in the form of natural language sequences and highlighted annotations in a new dataset called Common Sense Explanations (CoS-E). We use CoS-E to train language models to automatically generate explanations that can be used during training and inference in a novel Commonsense Auto-Generated Explanation (CAGE) framework. CAGE improves the state-of-the-art by 10% on the challenging CommonsenseQA task. We further study commonsense reasoning in DNNs using both human and auto-generated explanations including transfer to out-of-domain tasks. Empirical results indicate that we can effectively leverage language models for commonsense reasoning.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1487.pdf",
        "title": "Explain Yourself! Leveraging Language Models for Commonsense Reasoning"
    },
    {
        "paper_id": "P19-1493",
        "conference": "acl",
        "year": "2019",
        "abstract": "In this paper, we show that Multilingual BERT (M-BERT), released by Devlin et al. (2018) as a single language model pre-trained from monolingual corpora in 104 languages, is surprisingly good at zero-shot cross-lingual model transfer, in which task-specific annotations in one language are used to fine-tune the model for evaluation in another language. To understand why, we present a large number of probing experiments, showing that transfer is possible even to languages in different scripts, that transfer works best between typologically similar languages, that monolingual corpora can train models for code-switching, and that the model can find translation pairs. From these results, we can conclude that M-BERT does create multilingual representations, but that these representations exhibit systematic deficiencies affecting certain language pairs.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1493.pdf",
        "title": "How Multilingual is Multilingual BERT?"
    },
    {
        "paper_id": "P19-1495",
        "conference": "acl",
        "year": "2019",
        "abstract": "Complaining is a basic speech act regularly used in human and computer mediated communication to express a negative mismatch between reality and expectations in a particular situation. Automatically identifying complaints in social media is of utmost importance for organizations or brands to improve the customer experience or in developing dialogue systems for handling and responding to complaints. In this paper, we introduce the first systematic analysis of complaints in computational linguistics. We collect a new annotated data set of written complaints expressed on Twitter. We present an extensive linguistic analysis of complaining as a speech act in social media and train strong feature-based and neural models of complaints across nine domains achieving a predictive performance of up to 79 F1 using distant supervision.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1495.pdf",
        "title": "Automatically Identifying Complaints in Social Media"
    },
    {
        "paper_id": "P19-1503",
        "conference": "acl",
        "year": "2019",
        "abstract": "We propose an unsupervised method for sentence summarization using only language modeling. The approach employs two language models, one that is generic (i.e. pretrained), and the other that is specific to the target domain. We show that by using a product-of-experts criteria these are enough for maintaining continuous contextual matching while maintaining output fluency. Experiments on both abstractive and extractive sentence summarization data sets show promising results of our method without being exposed to any paired data.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1503.pdf",
        "title": "Simple Unsupervised Summarization by Contextual Matching"
    },
    {
        "paper_id": "P19-1514",
        "conference": "acl",
        "year": "2019",
        "abstract": "Supplementing product information by extracting attribute values from title is a crucial task in e-Commerce domain. Previous studies treat each attribute only as an entity type and build one set of NER tags (e.g., BIO) for each of them, leading to a scalability issue which unfits to the large sized attribute system in real world e-Commerce. In this work, we propose a novel approach to support value extraction scaling up to thousands of attributes without losing performance: (1) We propose to regard attribute as a query and adopt only one global set of BIO tags for any attributes to reduce the burden of attribute tag or model explosion; (2) We explicitly model the semantic representations for attribute and title, and develop an attention mechanism to capture the interactive semantic relations in-between to enforce our framework to be attribute comprehensive. We conduct extensive experiments in real-life datasets. The results show that our model not only outperforms existing state-of-the-art NER tagging models, but also is robust and generates promising results for up to 8,906 attributes.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1514.pdf",
        "title": "Scaling up Open Tagging from Tens to Thousands: Comprehension Empowered Attribute Value Extraction from Product Title"
    },
    {
        "paper_id": "P19-1516",
        "conference": "acl",
        "year": "2019",
        "abstract": "The mining of adverse drug reaction (ADR) has a crucial role in the pharmacovigilance. The traditional ways of identifying ADR are reliable but time-consuming, non-scalable and offer a very limited amount of ADR relevant information. With the unprecedented growth of information sources in the forms of social media texts (Twitter, Blogs, Reviews etc.), biomedical literature, and Electronic Medical Records (EMR), it has become crucial to extract the most pertinent ADR related information from these free-form texts. In this paper, we propose a neural network inspired multi- task learning framework that can simultaneously extract ADRs from various sources. We adopt a novel adversarial learning-based approach to learn features across multiple ADR information sources. Unlike the other existing techniques, our approach is capable to extracting fine-grained information (such as \u2018Indications\u2019, \u2018Symptoms\u2019, \u2018Finding\u2019, \u2018Disease\u2019, \u2018Drug\u2019) which provide important cues in pharmacovigilance. We evaluate our proposed approach on three publicly available real- world benchmark pharmacovigilance datasets, a Twitter dataset from PSB 2016 Social Me- dia Shared Task, CADEC corpus and Medline ADR corpus. Experiments show that our unified framework achieves state-of-the-art performance on individual tasks associated with the different benchmark datasets. This establishes the fact that our proposed approach is generic, which enables it to achieve high performance on the diverse datasets.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1516.pdf",
        "title": "A Unified Multi-task Adversarial Learning Framework for Pharmacovigilance Mining"
    },
    {
        "paper_id": "P19-1520",
        "conference": "acl",
        "year": "2019",
        "abstract": "Lack of labeled training data is a major bottleneck for neural network based aspect and opinion term extraction on product reviews. To alleviate this problem, we first propose an algorithm to automatically mine extraction rules from existing training examples based on dependency parsing results. The mined rules are then applied to label a large amount of auxiliary data. Finally, we study training procedures to train a neural model which can learn from both the data automatically labeled by the rules and a small amount of data accurately annotated by human. Experimental results show that although the mined rules themselves do not perform well due to their limited flexibility, the combination of human annotated data and rule labeled auxiliary data can improve the neural model and allow it to achieve performance better than or comparable with the current state-of-the-art.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1520.pdf",
        "title": "Neural Aspect and Opinion Term Extraction with Mined Rules as Weak Supervision"
    },
    {
        "paper_id": "P19-1524",
        "conference": "acl",
        "year": "2019",
        "abstract": "Most of the recently proposed neural models for named entity recognition have been purely data-driven, with a strong emphasis on getting rid of the efforts for collecting external resources or designing hand-crafted features. This could increase the chance of overfitting since the models cannot access any supervision signal beyond the small amount of annotated data, limiting their power to generalize beyond the annotated entities. In this work, we show that properly utilizing external gazetteers could benefit segmental neural NER models. We add a simple module on the recently proposed hybrid semi-Markov CRF architecture and observe some promising results.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1524.pdf",
        "title": "Towards Improving Neural Named Entity Recognition with Gazetteers"
    },
    {
        "paper_id": "P19-1526",
        "conference": "acl",
        "year": "2019",
        "abstract": "Most of the unsupervised dependency parsers are based on probabilistic generative models that learn the joint distribution of the given sentence and its parse. Probabilistic generative models usually explicit decompose the desired dependency tree into factorized grammar rules, which lack the global features of the entire sentence. In this paper, we propose a novel probabilistic model called discriminative neural dependency model with valence (D-NDMV) that generates a sentence and its parse from a continuous latent representation, which encodes global contextual information of the generated sentence. We propose two approaches to model the latent representation: the first deterministically summarizes the representation from the sentence and the second probabilistically models the representation conditioned on the sentence. Our approach can be regarded as a new type of autoencoder model to unsupervised dependency parsing that combines the benefits of both generative and discriminative techniques. In particular, our approach breaks the context-free independence assumption in previous generative approaches and therefore becomes more expressive. Our extensive experimental results on seventeen datasets from various sources show that our approach achieves competitive accuracy compared with both generative and discriminative state-of-the-art unsupervised dependency parsers.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1526.pdf",
        "title": "Enhancing Unsupervised Generative Dependency Parser with Contextual Information"
    },
    {
        "paper_id": "P19-1527",
        "conference": "acl",
        "year": "2019",
        "abstract": "We propose two neural network architectures for nested named entity recognition (NER), a setting in which named entities may overlap and also be labeled with more than one label. We encode the nested labels using a linearized scheme. In our first proposed approach, the nested labels are modeled as multilabels corresponding to the Cartesian product of the nested labels in a standard LSTM-CRF architecture. In the second one, the nested NER is viewed as a sequence-to-sequence problem, in which the input sequence consists of the tokens and output sequence of the labels, using hard attention on the word whose label is being predicted. The proposed methods outperform the nested NER state of the art on four corpora: ACE-2004, ACE-2005, GENIA and Czech CNEC. We also enrich our architectures with the recently published contextual embeddings: ELMo, BERT and Flair, reaching further improvements for the four nested entity corpora. In addition, we report flat NER state-of-the-art results for CoNLL-2002 Dutch and Spanish and for CoNLL-2003 English.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1527.pdf",
        "title": "Neural Architectures for Nested NER through Linearization"
    },
    {
        "paper_id": "P19-1531",
        "conference": "acl",
        "year": "2019",
        "abstract": "We use parsing as sequence labeling as a common framework to learn across constituency and dependency syntactic abstractions.To do so, we cast the problem as multitask learning (MTL). First, we show that adding a parsing paradigm as an auxiliary loss consistently improves the performance on the other paradigm. Secondly, we explore an MTL sequence labeling model that parses both representations, at almost no cost in terms of performance and speed. The results across the board show that on average MTL models with auxiliary losses for constituency parsing outperform single-task ones by 1.05 F1 points, and for dependency parsing by 0.62 UAS points.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1531.pdf",
        "title": "Sequence Labeling Parsing by Learning across Representations"
    },
    {
        "paper_id": "P19-1542",
        "conference": "acl",
        "year": "2019",
        "abstract": "Existing personalized dialogue models use human designed persona descriptions to improve dialogue consistency. Collecting such descriptions from existing dialogues is expensive and requires hand-crafted feature designs. In this paper, we propose to extend Model-Agnostic Meta-Learning (MAML) (Finn et al., 2017) to personalized dialogue learning without using any persona descriptions. Our model learns to quickly adapt to new personas by leveraging only a few dialogue samples collected from the same user, which is fundamentally different from conditioning the response on the persona descriptions. Empirical results on Persona-chat dataset (Zhang et al., 2018) indicate that our solution outperforms non-meta-learning baselines using automatic evaluation metrics, and in terms of human-evaluated fluency and consistency.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1542.pdf",
        "title": "Personalizing Dialogue Agents via Meta-Learning"
    },
    {
        "paper_id": "P19-1543",
        "conference": "acl",
        "year": "2019",
        "abstract": "Comprehending multi-turn spoken conversations is an emerging research area, presenting challenges different from reading comprehension of passages due to the interactive nature of information exchange from at least two speakers. Unlike passages, where sentences are often the default semantic modeling unit, in multi-turn conversations, a turn is a topically coherent unit embodied with immediately relevant context, making it a linguistically intuitive segment for computationally modeling verbal interactions. Therefore, in this work, we propose a hierarchical attention neural network architecture, combining turn-level and word-level attention mechanisms, to improve spoken dialogue comprehension performance. Experiments are conducted on a multi-turn conversation dataset, where nurses inquire and discuss symptom information with patients. We empirically show that the proposed approach outperforms standard attention baselines, achieves more efficient learning outcomes, and is more robust to lengthy and out-of-distribution test samples.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1543.pdf",
        "title": "Reading Turn by Turn: Hierarchical Attention Architecture for Spoken Dialogue Comprehension"
    },
    {
        "paper_id": "P19-1557",
        "conference": "acl",
        "year": "2019",
        "abstract": "We report our ongoing work about a new deep architecture working in tandem with a statistical test procedure for jointly training texts and their label descriptions for multi-label and multi-class classification tasks. A statistical hypothesis testing method is used to extract the most informative words for each given class. These words are used as a class description for more label-aware text classification. Intuition is to help the model to concentrate on more informative words rather than more frequent ones. The model leverages the use of label descriptions in addition to the input text to enhance text classification performance. Our method is entirely data-driven, has no dependency on other sources of information than the training data, and is adaptable to different classification problems by providing appropriate training data without major hyper-parameter tuning. We trained and tested our system on several publicly available datasets, where we managed to improve the state-of-the-art on one set with a high margin and to obtain competitive results on all other ones.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1557.pdf",
        "title": "Towards Integration of Statistical Hypothesis Tests into Deep Neural Networks"
    },
    {
        "paper_id": "P19-1564",
        "conference": "acl",
        "year": "2019",
        "abstract": "Developing Video-Grounded Dialogue Systems (VGDS), where a dialogue is conducted based on visual and audio aspects of a given video, is significantly more challenging than traditional image or text-grounded dialogue systems because (1) feature space of videos span across multiple picture frames, making it difficult to obtain semantic information; and (2) a dialogue agent must perceive and process information from different modalities (audio, video, caption, etc.) to obtain a comprehensive understanding. Most existing work is based on RNNs and sequence-to-sequence architectures, which are not very effective for capturing complex long-term dependencies (like in videos). To overcome this, we propose Multimodal Transformer Networks (MTN) to encode videos and incorporate information from different modalities. We also propose query-aware attention through an auto-encoder to extract query-aware features from non-text modalities. We develop a training procedure to simulate token-level decoding to improve the quality of generated responses during inference. We get state of the art performance on Dialogue System Technology Challenge 7 (DSTC7). Our model also generalizes to another multimodal visual-grounded dialogue task, and obtains promising performance.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1564.pdf",
        "title": "Multimodal Transformer Networks for End-to-End Video-Grounded Dialogue Systems"
    },
    {
        "paper_id": "P19-1565",
        "conference": "acl",
        "year": "2019",
        "abstract": "Many real-world open-domain conversation applications have specific goals to achieve during open-ended chats, such as recommendation, psychotherapy, education, etc. We study the problem of imposing conversational goals on open-domain chat agents. In particular, we want a conversational system to chat naturally with human and proactively guide the conversation to a designated target subject. The problem is challenging as no public data is available for learning such a target-guided strategy. We propose a structured approach that introduces coarse-grained keywords to control the intended content of system responses. We then attain smooth conversation transition through turn-level supervised learning, and drive the conversation towards the target with discourse-level constraints. We further derive a keyword-augmented conversation dataset for the study. Quantitative and human evaluations show our system can produce meaningful and effective conversations, significantly improving over other approaches",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1565.pdf",
        "title": "Target-Guided Open-Domain Conversation"
    },
    {
        "paper_id": "P19-1569",
        "conference": "acl",
        "year": "2019",
        "abstract": "Contextual embeddings represent a new generation of semantic representations learned from Neural Language Modelling (NLM) that addresses the issue of meaning conflation hampering traditional word embeddings. In this work, we show that contextual embeddings can be used to achieve unprecedented gains in Word Sense Disambiguation (WSD) tasks. Our approach focuses on creating sense-level embeddings with full-coverage of WordNet, and without recourse to explicit knowledge of sense distributions or task-specific modelling. As a result, a simple Nearest Neighbors (k-NN) method using our representations is able to consistently surpass the performance of previous systems using powerful neural sequencing models. We also analyse the robustness of our approach when ignoring part-of-speech and lemma features, requiring disambiguation against the full sense inventory, and revealing shortcomings to be improved. Finally, we explore applications of our sense embeddings for concept-level analyses of contextual embeddings and their respective NLMs.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1569.pdf",
        "title": "Language Modelling Makes Sense: Propagating Representations through WordNet for Full-Coverage Word Sense Disambiguation"
    },
    {
        "paper_id": "P19-1570",
        "conference": "acl",
        "year": "2019",
        "abstract": "We present an unsupervised method to generate Word2Sense word embeddings that are interpretable \u2014 each dimension of the embedding space corresponds to a fine-grained sense, and the non-negative value of the embedding along the j-th dimension represents the relevance of the j-th sense to the word. The underlying LDA-based generative model can be extended to refine the representation of a polysemous word in a short context, allowing us to use the embedings in contextual tasks. On computational NLP tasks, Word2Sense embeddings compare well with other word embeddings generated by unsupervised methods. Across tasks such as word similarity, entailment, sense induction, and contextual interpretation, Word2Sense is competitive with the state-of-the-art method for that task. Word2Sense embeddings are at least as sparse and fast to compute as prior art.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1570.pdf",
        "title": "Word2Sense: Sparse Interpretable Word Embeddings"
    },
    {
        "paper_id": "P19-1584",
        "conference": "acl",
        "year": "2019",
        "abstract": "De-identification is the task of detecting protected health information (PHI) in medical text. It is a critical step in sanitizing electronic health records (EHR) to be shared for research. Automatic de-identification classifiers can significantly speed up the sanitization process. However, obtaining a large and diverse dataset to train such a classifier that works well across many types of medical text poses a challenge as privacy laws prohibit the sharing of raw medical records. We introduce a method to create privacy-preserving shareable representations of medical text (i.e. they contain no PHI) that does not require expensive manual pseudonymization. These representations can be shared between organizations to create unified datasets for training de-identification models. Our representation allows training a simple LSTM-CRF de-identification model to an F1 score of 97.4%, which is comparable to a strong baseline that exposes private information in its representation. A robust, widely available de-identification classifier based on our representation could potentially enable studies for which de-identification would otherwise be too costly.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1584.pdf",
        "title": "Adversarial Learning of Privacy-Preserving Text Representations for De-Identification of Medical Records"
    },
    {
        "paper_id": "P19-1595",
        "conference": "acl",
        "year": "2019",
        "abstract": "It can be challenging to train multi-task neural networks that outperform or even match their single-task counterparts. To help address this, we propose using knowledge distillation where single-task models teach a multi-task model. We enhance this training with teacher annealing, a novel method that gradually transitions the model from distillation to supervised learning, helping the multi-task model surpass its single-task teachers. We evaluate our approach by multi-task fine-tuning BERT on the GLUE benchmark. Our method consistently improves over standard single-task and multi-task training.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1595.pdf",
        "title": "BAM! Born-Again Multi-Task Networks for Natural Language Understanding"
    },
    {
        "paper_id": "P19-1599",
        "conference": "acl",
        "year": "2019",
        "abstract": "Prior work on controllable text generation usually assumes that the controlled attribute can take on one of a small set of values known a priori. In this work, we propose a novel task, where the syntax of a generated sentence is controlled rather by a sentential exemplar. To evaluate quantitatively with standard metrics, we create a novel dataset with human annotations. We also develop a variational model with a neural module specifically designed for capturing syntactic knowledge and several multitask training objectives to promote disentangled representation learning. Empirically, the proposed model is observed to achieve improvements over baselines and learn to capture desirable characteristics.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1599.pdf",
        "title": "Controllable Paraphrase Generation with a Syntactic Exemplar"
    },
    {
        "paper_id": "P19-1602",
        "conference": "acl",
        "year": "2019",
        "abstract": "Variational auto-encoders (VAEs) are widely used in natural language generation due to the regularization of the latent space. However, generating sentences from the continuous latent space does not explicitly model the syntactic information. In this paper, we propose to generate sentences from disentangled syntactic and semantic spaces. Our proposed method explicitly models syntactic information in the VAE\u2019s latent space by using the linearized tree sequence, leading to better performance of language generation. Additionally, the advantage of sampling in the disentangled syntactic and semantic latent spaces enables us to perform novel applications, such as the unsupervised paraphrase generation and syntax transfer generation. Experimental results show that our proposed model achieves similar or better performance in various tasks, compared with state-of-the-art related work.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1602.pdf",
        "title": "Generating Sentences from Disentangled Syntactic and Semantic Spaces"
    },
    {
        "paper_id": "P19-1603",
        "conference": "acl",
        "year": "2019",
        "abstract": "Automatic story ending generation is an interesting and challenging task in natural language generation. Previous studies are mainly limited to generate coherent, reasonable and diversified story endings, and few works focus on controlling the sentiment of story endings. This paper focuses on generating a story ending which meets the given fine-grained sentiment intensity. There are two major challenges to this task. First is the lack of story corpus which has fine-grained sentiment labels. Second is the difficulty of explicitly controlling sentiment intensity when generating endings. Therefore, we propose a generic and novel framework which consists of a sentiment analyzer and a sentimental generator, respectively addressing the two challenges. The sentiment analyzer adopts a series of methods to acquire sentiment intensities of the story dataset. The sentimental generator introduces the sentiment intensity into decoder via a Gaussian Kernel Layer to control the sentiment of the output. To the best of our knowledge, this is the first endeavor to control the fine-grained sentiment for story ending generation without manually annotating sentiment labels. Experiments show that our proposed framework can generate story endings which are not only more coherent and fluent but also able to meet the given sentiment intensity better.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1603.pdf",
        "title": "Learning to Control the Fine-grained Sentiment for Story Ending Generation"
    },
    {
        "paper_id": "P19-1607",
        "conference": "acl",
        "year": "2019",
        "abstract": "Paraphrase generation can be regarded as monolingual translation. Unlike bilingual machine translation, paraphrase generation rewrites only a limited portion of an input sentence. Hence, previous methods based on machine translation often perform conservatively to fail to make necessary rewrites. To solve this problem, we propose a neural model for paraphrase generation that first identifies words in the source sentence that should be paraphrased. Then, these words are paraphrased by the negative lexically constrained decoding that avoids outputting these words as they are. Experiments on text simplification and formality transfer show that our model improves the quality of paraphrasing by making necessary rewrites to an input sentence.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1607.pdf",
        "title": "Negative Lexically Constrained Decoding for Paraphrase Generation"
    },
    {
        "paper_id": "P19-1623",
        "conference": "acl",
        "year": "2019",
        "abstract": "While neural machine translation (NMT) has achieved remarkable success, NMT systems are prone to make word omission errors. In this work, we propose a contrastive learning approach to reducing word omission errors in NMT. The basic idea is to enable the NMT model to assign a higher probability to a ground-truth translation and a lower probability to an erroneous translation, which is automatically constructed from the ground-truth translation by omitting words. We design different types of negative examples depending on the number of omitted words, word frequency, and part of speech. Experiments on Chinese-to-English, German-to-English, and Russian-to-English translation tasks show that our approach is effective in reducing word omission errors and achieves better translation performance than three baseline methods.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1623.pdf",
        "title": "Reducing Word Omission Errors in Neural Machine Translation: A Contrastive Learning Approach"
    },
    {
        "paper_id": "P19-1628",
        "conference": "acl",
        "year": "2019",
        "abstract": "Single document summarization has enjoyed renewed interest in recent years thanks to the popularity of neural network models and the availability of large-scale datasets. In this paper we develop an unsupervised approach arguing that it is unrealistic to expect large-scale and high-quality training data to be available or created for different types of summaries, domains, or languages. We revisit a popular graph-based ranking algorithm and modify how node (aka sentence) centrality is computed in two ways: (a) we employ BERT, a state-of-the-art neural representation learning model to better capture sentential meaning and (b) we build graphs with directed edges arguing that the contribution of any two nodes to their respective centrality is influenced by their relative position in a document. Experimental results on three news summarization datasets representative of different languages and writing styles show that our approach outperforms strong baselines by a wide margin.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1628.pdf",
        "title": "Sentence Centrality Revisited for Unsupervised Summarization"
    },
    {
        "paper_id": "P19-1629",
        "conference": "acl",
        "year": "2019",
        "abstract": "We introduce a novel semantic parsing task based on Discourse Representation Theory (DRT; Kamp and Reyle 1993). Our model operates over Discourse Representation Tree Structures which we formally define for sentences and documents. We present a general framework for parsing discourse structures of arbitrary length and granularity. We achieve this with a neural model equipped with a supervised hierarchical attention mechanism and a linguistically-motivated copy strategy. Experimental results on sentence- and document-level benchmarks show that our model outperforms competitive baselines by a wide margin.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1629.pdf",
        "title": "Discourse Representation Parsing for Sentences and Documents"
    },
    {
        "paper_id": "P19-1631",
        "conference": "acl",
        "year": "2019",
        "abstract": "Feature attribution methods, proposed recently, help users interpret the predictions of complex models. Our approach integrates feature attributions into the objective function to allow machine learning practitioners to incorporate priors in model building. To demonstrate the effectiveness our technique, we apply it to two tasks: (1) mitigating unintended bias in text classifiers by neutralizing identity terms; (2) improving classifier performance in scarce data setting by forcing model to focus on toxic terms. Our approach adds an L2 distance loss between feature attributions and task-specific prior values to the objective. Our experiments show that i) a classifier trained with our technique reduces undesired model biases without a tradeoff on the original task; ii) incorporating prior helps model performance in scarce data settings.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1631.pdf",
        "title": "Incorporating Priors with Feature Attribution on Text Classification"
    },
    {
        "paper_id": "P19-1635",
        "conference": "acl",
        "year": "2019",
        "abstract": "In this paper, we attempt to answer the question of whether neural network models can learn numeracy, which is the ability to predict the magnitude of a numeral at some specific position in a text description. A large benchmark dataset, called Numeracy-600K, is provided for the novel task. We explore several neural network models including CNN, GRU, BiGRU, CRNN, CNN-capsule, GRU-capsule, and BiGRU-capsule in the experiments. The results show that the BiGRU model gets the best micro-averaged F1 score of 80.16%, and the GRU-capsule model gets the best macro-averaged F1 score of 64.71%. Besides discussing the challenges through comprehensive experiments, we also present an important application scenario, i.e., detecting exaggerated information, for the task.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1635.pdf",
        "title": "Numeracy-600K: Learning Numeracy for Detecting Exaggerated Information in Market Comments"
    },
    {
        "paper_id": "P19-1643",
        "conference": "acl",
        "year": "2019",
        "abstract": "We consider the task of identifying human actions visible in online videos. We focus on the widely spread genre of lifestyle vlogs, which consist of videos of people performing actions while verbally describing them. Our goal is to identify if actions mentioned in the speech description of a video are visually present. We construct a dataset with crowdsourced manual annotations of visible actions, and introduce a multimodal algorithm that leverages information derived from visual and linguistic clues to automatically infer which actions are visible in a video.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1643.pdf",
        "title": "Identifying Visible Actions in Lifestyle Vlogs"
    },
    {
        "paper_id": "P19-1645",
        "conference": "acl",
        "year": "2019",
        "abstract": "We propose a segmental neural language model that combines the generalization power of neural networks with the ability to discover word-like units that are latent in unsegmented character sequences. In contrast to previous segmentation models that treat word segmentation as an isolated task, our model unifies word discovery, learning how words fit together to form sentences, and, by conditioning the model on visual context, how words\u2019 meanings ground in representations of nonlinguistic modalities. Experiments show that the unconditional model learns predictive distributions better than character LSTM models, discovers words competitively with nonparametric Bayesian word segmentation models, and that modeling language conditional on visual context improves performance on both.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1645.pdf",
        "title": "Learning to Discover, Ground and Use Words with Segmental Neural Language Models"
    },
    {
        "paper_id": "P19-1648",
        "conference": "acl",
        "year": "2019",
        "abstract": "This paper presents a new model for visual dialog, Recurrent Dual Attention Network (ReDAN), using multi-step reasoning to answer a series of questions about an image. In each question-answering turn of a dialog, ReDAN infers the answer progressively through multiple reasoning steps. In each step of the reasoning process, the semantic representation of the question is updated based on the image and the previous dialog history, and the recurrently-refined representation is used for further reasoning in the subsequent step. On the VisDial v1.0 dataset, the proposed ReDAN model achieves a new state-of-the-art of 64.47% NDCG score. Visualization on the reasoning process further demonstrates that ReDAN can locate context-relevant visual and textual clues via iterative refinement, which can lead to the correct answer step-by-step.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1648.pdf",
        "title": "Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog"
    },
    {
        "paper_id": "P19-1650",
        "conference": "acl",
        "year": "2019",
        "abstract": "An image caption should fluently present the essential information in a given image, including informative, fine-grained entity mentions and the manner in which these entities interact. However, current captioning models are usually trained to generate captions that only contain common object names, thus falling short on an important \u201cinformativeness\u201d dimension. We present a mechanism for integrating image information together with fine-grained labels (assumed to be generated by some upstream models) into a caption that describes the image in a fluent and informative manner. We introduce a multimodal, multi-encoder model based on Transformer that ingests both image features and multiple sources of entity labels. We demonstrate that we can learn to control the appearance of these entity labels in the output, resulting in captions that are both fluent and informative.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1650.pdf",
        "title": "Informative Image Captioning with External Sources of Information"
    },
    {
        "paper_id": "P19-1653",
        "conference": "acl",
        "year": "2019",
        "abstract": "Previous work on multimodal machine translation has shown that visual information is only needed in very specific cases, for example in the presence of ambiguous words where the textual context is not sufficient. As a consequence, models tend to learn to ignore this information. We propose a translate-and-refine approach to this problem where images are only used by a second stage decoder. This approach is trained jointly to generate a good first draft translation and to improve over this draft by (i) making better use of the target language textual context (both left and right-side contexts) and (ii) making use of visual context. This approach leads to the state of the art results. Additionally, we show that it has the ability to recover from erroneous or missing words in the source language.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1653.pdf",
        "title": "Distilling Translations with Visual Awareness"
    },
    {
        "paper_id": "P19-1657",
        "conference": "acl",
        "year": "2019",
        "abstract": "Chest X-Ray (CXR) images are commonly used for clinical screening and diagnosis. Automatically writing reports for these images can considerably lighten the workload of radiologists for summarizing descriptive findings and conclusive impressions. The complex structures between and within sections of the reports pose a great challenge to the automatic report generation. Specifically, the section Impression is a diagnostic summarization over the section Findings; and the appearance of normality dominates each section over that of abnormality. Existing studies rarely explore and consider this fundamental structure information. In this work, we propose a novel framework which exploits the structure information between and within report sections for generating CXR imaging reports. First, we propose a two-stage strategy that explicitly models the relationship between Findings and Impression. Second, we design a novel co-operative multi-agent system that implicitly captures the imbalanced distribution between abnormality and normality. Experiments on two CXR report datasets show that our method achieves state-of-the-art performance in terms of various evaluation metrics. Our results expose that the proposed approach is able to generate high-quality medical reports through integrating the structure information.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1657.pdf",
        "title": "Show, Describe and Conclude: On Exploiting the Structure Information of Chest X-ray Reports"
    },
    {
        "paper_id": "P19-1658",
        "conference": "acl",
        "year": "2019",
        "abstract": "We introduce the first dataset for human edits of machine-generated visual stories and explore how these collected edits may be used for the visual story post-editing task. The dataset ,VIST-Edit, includes 14,905 human-edited versions of 2,981 machine-generated visual stories. The stories were generated by two state-of-the-art visual storytelling models, each aligned to 5 human-edited versions. We establish baselines for the task, showing how a relatively small set of human edits can be leveraged to boost the performance of large visual storytelling models. We also discuss the weak correlation between automatic evaluation scores and human ratings, motivating the need for new automatic metrics.",
        "url_pdf": "https://www.aclweb.org/anthology/P19-1658.pdf",
        "title": "Visual Story Post-Editing"
    }
]