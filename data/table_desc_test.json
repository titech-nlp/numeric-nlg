[
    {
        "table_id_paper": "D16-1019table_5",
        "description": "Results. Table 5 show the results in the filtered setting. On each dataset we report the metrics on three sets: test-I, test-II, and the whole test set (denoted by test-all). Test-I contains test triples that cannot be directly inferred by performing pure logical inference on the training set, and hence might be intrinsically more difficult for the rules. The remaining test triples (i.e., the directly inferable ones) are included in Test-II. These triples have either been used directly as training instances in KALE-Pre, or encoded explicitly in training ground rules in KALE-Joint, making this set trivial for the rules to some extent. From the results, we can see that in both settings: (i) KALE-Pre and KALE-Joint outperform (or at least perform as well as) the other methods which use triples alone on almost all the test sets, demonstrating the superiority of incorporating logical rules. (ii) On the test-I sets which contain triples beyond the scope of pure logical inference, KALE-Joint performs significantly better than KALE-Pre. On these sets KALE-Joint can still beat all the baselines by a significant margin in most cases, while KALE-Pre can hardly outperform KALE-Trip. It demonstrates the capability of the joint embedding scenario to learn more predictive embeddings, through which we can make better predictions even beyond the scope of pure logical inference. (iii) On the test-II sets which contain directly inferable triples, KALE-Pre can easily beat all the baselines (even KALE-Joint). That means, for triples covered by pure logical inference, it is trivial to improve the performance by directly incorporating them as training instances.",
        "sentences": [
            "Results.",
            "Table 5 show the results in the filtered setting.",
            "On each dataset we report the metrics on three sets: test-I, test-II, and the whole test set (denoted by test-all).",
            "Test-I contains test triples that cannot be directly inferred by performing pure logical inference on the training set, and hence might be intrinsically more difficult for the rules.",
            "The remaining test triples (i.e., the directly inferable ones) are included in Test-II.",
            "These triples have either been used directly as training instances in KALE-Pre, or encoded explicitly in training ground rules in KALE-Joint, making this set trivial for the rules to some extent.",
            "From the results, we can see that in both settings: (i) KALE-Pre and KALE-Joint outperform (or at least perform as well as) the other methods which use triples alone on almost all the test sets, demonstrating the superiority of incorporating logical rules.",
            "(ii) On the test-I sets which contain triples beyond the scope of pure logical inference, KALE-Joint performs significantly better than KALE-Pre.",
            "On these sets KALE-Joint can still beat all the baselines by a significant margin in most cases, while KALE-Pre can hardly outperform KALE-Trip.",
            "It demonstrates the capability of the joint embedding scenario to learn more predictive embeddings, through which we can make better predictions even beyond the scope of pure logical inference.",
            "(iii) On the test-II sets which contain directly inferable triples, KALE-Pre can easily beat all the baselines (even KALE-Joint).",
            "That means, for triples covered by pure logical inference, it is trivial to improve the performance by directly incorporating them as training instances."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            2,
            2,
            1,
            1,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "Test-I",
                "Test-II",
                "Test-ALL"
            ],
            [
                "Test-I"
            ],
            [
                "Test-II"
            ],
            [
                "Test-I",
                "Test-II",
                "Test-ALL",
                "KALE-Pre",
                "KALE-Joint"
            ],
            [
                "KALE-Pre",
                "KALE-Joint"
            ],
            [
                "KALE-Joint",
                "KALE-Pre"
            ],
            [
                "KALE-Joint",
                "KALE-Pre",
                "KALE-Trip"
            ],
            null,
            [
                "Test-II",
                "KALE-Pre",
                "KALE-Joint"
            ],
            null
        ],
        "n_sentence": 12.0,
        "table_id": "table_5",
        "paper_id": "D16-1019",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1031table_3",
        "description": "In Figure 3, we present the validation perplexity to compare the abilities of the three models to learn the compression languages. The ASC+FSC1(red) employs the same dataset for unlabelled and labelled training, while the ASC+FSC2(black) employs the full unlabelled dataset. Here, the joint ASC+FSC1 model obtains better perplexities than the single discriminative FSC model, but there is not much difference between ASC+FSC1 and ASC+FSC2 when the size of the labelled dataset grows. From the perspective of language modelling, the generative ASC model indeed helps the discriminative model learn to generate good summary sentences. Table 3 displays the validation perplexities of the benchmark models, where the joint ASC+FSC1 model trained on the full labelled and unlabelled datasets performs the best on modelling compression languages.",
        "sentences": [
            "In Figure 3, we present the validation perplexity to compare the abilities of the three models to learn the compression languages.",
            "The ASC+FSC1(red) employs the same dataset for unlabelled and labelled training, while the ASC+FSC2(black) employs the full unlabelled dataset.",
            "Here, the joint ASC+FSC1 model obtains better perplexities than the single discriminative FSC model, but there is not much difference between ASC+FSC1 and ASC+FSC2 when the size of the labelled dataset grows.",
            "From the perspective of language modelling, the generative ASC model indeed helps the discriminative model learn to generate good summary sentences.",
            "Table 3 displays the validation perplexities of the benchmark models, where the joint ASC+FSC1 model trained on the full labelled and unlabelled datasets performs the best on modelling compression languages."
        ],
        "class_sentence": [
            0,
            0,
            0,
            0,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            [
                "Auto-encoding (ASC+FSC1)"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_3",
        "paper_id": "D16-1031",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1043table_4",
        "description": "5.1 Maximum coverage comparison. Table 4 shows the results on the maximally covered datasets. This means we cannot directly compare between data sources, because they have different coverage, but we can look at absolute performance and compare network architectures. The first row reports results for the text-based linguistic representations that were obtained from Wikipedia (repeated across columns for convenience). For each of the three architectures, we evaluate on SimLex (SL) and MEN, using either the mean (Mean) or elementwise maximum (Max) method for aggregating image representations into visual ones (see Section 2). For each data source, we report results for the visual representations, as well as for the multi-modal representations that fuse the visual and textual ones together. Performance across architectures is remarkably stable: we have had to report results up to three decimal points to show the difference in performance in some cases. For each of the network architectures, we see a marked improvement of multi-modal representations over uni-modal linguistic representations. In many cases, we also see visual representations outperforming linguistic ones, especially on SimLex. This is interesting, because e.g. Google and Bing have full coverage over the datasets, so their visual representations include highly abstract words, which does not appear to have an adverse impact on the method\u2019s performance. For the ESP Game dataset (on which performance is quite low) and ImageNet, we observe an increase in performance as we move to the right in the table. Interestingly, VGGNet on ImageNet scores very highly, which seems to indicate that VGGNet is somehow more \u201cspecialized\u201d on ImageNet than the others. The difference between mean and max aggregation is relatively small, although the former seems to work better for SimLex while the latter does slightly better for MEN.",
        "sentences": [
            "5.1 Maximum coverage comparison.",
            "Table 4 shows the results on the maximally covered datasets.",
            "This means we cannot directly compare between data sources, because they have different coverage, but we can look at absolute performance and compare network architectures.",
            "The first row reports results for the text-based linguistic representations that were obtained from Wikipedia (repeated across columns for convenience).",
            "For each of the three architectures, we evaluate on SimLex (SL) and MEN, using either the mean (Mean) or elementwise maximum (Max) method for aggregating image representations into visual ones (see Section 2).",
            "For each data source, we report results for the visual representations, as well as for the multi-modal representations that fuse the visual and textual ones together.",
            "Performance across architectures is remarkably stable: we have had to report results up to three decimal points to show the difference in performance in some cases.",
            "For each of the network architectures, we see a marked improvement of multi-modal representations over uni-modal linguistic representations.",
            "In many cases, we also see visual representations outperforming linguistic ones, especially on SimLex.",
            "This is interesting, because e.g. Google and Bing have full coverage over the datasets, so their visual representations include highly abstract words, which does not appear to have an adverse impact on the method\u2019s performance.",
            "For the ESP Game dataset (on which performance is quite low) and ImageNet, we observe an increase in performance as we move to the right in the table.",
            "Interestingly, VGGNet on ImageNet scores very highly, which seems to indicate that VGGNet is somehow more \u201cspecialized\u201d on ImageNet than the others.",
            "The difference between mean and max aggregation is relatively small, although the former seems to work better for SimLex while the latter does slightly better for MEN."
        ],
        "class_sentence": [
            2,
            1,
            2,
            2,
            2,
            2,
            2,
            1,
            1,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            [
                "SL",
                "MEN",
                "Mean",
                "Max"
            ],
            null,
            null,
            null,
            [
                "Visual",
                "MM",
                "SL"
            ],
            [
                "Google",
                "Bing"
            ],
            [
                "ESPGame",
                "ImageNet"
            ],
            [
                "VGGNet",
                "ImageNet"
            ],
            [
                "Max",
                "Mean",
                "MEN",
                "SL"
            ]
        ],
        "n_sentence": 13.0,
        "table_id": "table_4",
        "paper_id": "D16-1043",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1044table_4",
        "description": "4.4 Comparison to State-of-the-Art. Table 4 compares our approach with the state-of-the-art on VQA test set. Our best single model uses MCB pooling with two attention maps. Additionally, we augment our training data with images and QA pairs from the Visual Genome dataset. We also concatenate the learned word embedding with pretrained GloVe vectors (Pennington et al. 2014). Each model in our ensemble of 7 models uses MCB with attention. Some of the models were trained with data from Visual Genome, and some were trained with two attention maps. This ensemble is 1.8 points above the next best approach on the VQA open-ended task and 0.8 points above the next best approach on the multiple-choice task (on Testdev). Even without ensembles, our \u201cMCB + Genome + Att. + GloVe\u201d model still outperforms the next best result by 0.5 points, with an accuracy of 65.4% versus 64.9% on the open-ended task (on Test-dev).",
        "sentences": [
            "4.4 Comparison to State-of-the-Art.",
            "Table 4 compares our approach with the state-of-the-art on VQA test set.",
            "Our best single model uses MCB pooling with two attention maps.",
            "Additionally, we augment our training data with images and QA pairs from the Visual Genome dataset.",
            "We also concatenate the learned word embedding with pretrained GloVe vectors (Pennington et al. 2014).",
            "Each model in our ensemble of 7 models uses MCB with attention.",
            "Some of the models were trained with data from Visual Genome, and some were trained with two attention maps.",
            "This ensemble is 1.8 points above the next best approach on the VQA open-ended task and 0.8 points above the next best approach on the multiple-choice task (on Test-dev).",
            "Even without ensembles, our \u201cMCB + Genome + Att. + GloVe\u201d model still outperforms the next best result by 0.5 points, with an accuracy of 65.4% versus 64.9% on the open-ended task (on Test-dev)."
        ],
        "class_sentence": [
            2,
            1,
            2,
            2,
            2,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null,
            [
                "Ensemble of 7 Att. models"
            ],
            null,
            [
                "Ensemble of 7 Att. models",
                "Test-dev"
            ],
            [
                "MCB + Att. + GloVe + Genome",
                "Naver Labs (challenge 2nd)",
                "Test-dev"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_4",
        "paper_id": "D16-1044",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1086table_1",
        "description": "Results. From the whole corpus of 300 sentences, PropsDE extracted 487 tuples, yielding on average 1.6 per sentence with 2.9 arguments. 60% of them were labeled as correct. Table 1 shows that most extractions are made from Wikipedia articles, whereas the highest precision can be observed for newswire text. According to our expectations, web pages are most challenging, presumably due to noisier language. These differences between the genres can also be seen in the precision-yield curve (Figure 2).",
        "sentences": [
            "Results.",
            "From the whole corpus of 300 sentences, PropsDE extracted 487 tuples, yielding on average 1.6 per sentence with 2.9 arguments.",
            "60% of them were labeled as correct.",
            "Table 1 shows that most extractions are made from Wikipedia articles, whereas the highest precision can be observed for newswire text.",
            "According to our expectations, web pages are most challenging, presumably due to noisier language.",
            "These differences between the genres can also be seen in the precision-yield curve (Figure 2)."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            1,
            0
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "Wiki",
                "News"
            ],
            [
                "Web"
            ],
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_1",
        "paper_id": "D16-1086",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1132table_3",
        "description": "The results in Table 3 show that our MCNN-based method achieved better average precision than the single-column CNN baseline except the method that uses only the BASE column set for the cataphoric case. The results also demonstrate that each column set consistently contributes to improving the average precision for both the anaphoric and cataphoric cases. However, Table 3 shows that the average precision for the cataphoric set remains low. As one future direction for further improvement, we need to explore clues for identifying cataphoric relations more accurately.",
        "sentences": [
            "The results in Table 3 show that our MCNN-based method achieved better average precision than the single-column CNN baseline except the method that uses only the BASE column set for the cataphoric case.",
            "The results also demonstrate that each column set consistently contributes to improving the average precision for both the anaphoric and cataphoric cases.",
            "However, Table 3 shows that the average precision for the cataphoric set remains low.",
            "As one future direction for further improvement, we need to explore clues for identifying cataphoric relations more accurately."
        ],
        "class_sentence": [
            1,
            1,
            1,
            0
        ],
        "header_mention": [
            [
                "MCNN (Proposed)",
                "single-column CNN (w/ position vec.)",
                "MCNN (BASE)",
                "Avg.P"
            ],
            [
                "Avg.P",
                "Anaphoric",
                "Cataphoric"
            ],
            [
                "Cataphoric",
                "Avg.P"
            ],
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "D16-1132",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1136table_3",
        "description": "Quantitative evaluation. Table 3 shows our results compared with prior work. We reimplement Gouws and S\u00f8gaard (2015) using Panlex and Wiktionary dictionaries. The result with Panlex is substantially worse than with Wiktionary. This confirms our hypothesis in \u00a72. That is the context might be corrupted if we just randomly replace the training data with the translation from noisy dictionary such as Panlex. Our model when randomly picking the translation is similar to Gouws and S\u00f8gaard (2015), using the Panlex dictionary. The biggest difference is that they replace the training data (both context and middle word) while we fix the context and only replace the middle word. For a high coverage yet noisy dictionary such as Panlex, our approach gives better average score. Comparing our two most basic models (EM selection and random selection), it is clear that the model using EM to select the translation outperforms random selection by a significant margin. Our joint model, as described in equation (3) which predicts both target word and the translation, further improves the performance, especially for nl-en. We use equation (5) to combine both context embeddings V and word embeddings U for all three language pairs. This modification during training substantially improves the performance. More importantly, all our improvements are consistent for all three language pairs and both evaluation metrics, showing the robustness of our models. Our combined model out-performed previous approaches by a large margin. Vulic and Moens (2015) used bilingual comparable data, but this might be hard to obtain for some language pairs. Their performance on nl-en is poor because their comparable data between en and nl is small. Besides, they also use POS tagger and lemmatizer to filter only Noun and reduce the morphology complexity during training. These tools might not be available for many languages. For a fairer comparison to their work, we also use the same Treetagger (Schmid, 1995) to lemmatize the output of our combined model before evaluation. Table 3 (+lemmatization) shows some improvements but minor. It demonstrates that our model is already good at disambiguating morphology. For example, the top 2 translations for es word lenguas in en are languages and language which correctly prefer the plural translation.",
        "sentences": [
            "Quantitative evaluation.",
            "Table 3 shows our results compared with prior work.",
            "We reimplement Gouws and S\u00f8gaard (2015) using Panlex and Wiktionary dictionaries.",
            "The result with Panlex is substantially worse than with Wiktionary.",
            "This confirms our hypothesis in \u00a72.",
            "That is the context might be corrupted if we just randomly replace the training data with the translation from noisy dictionary such as Panlex.",
            "Our model when randomly picking the translation is similar to Gouws and S\u00f8gaard (2015), using the Panlex dictionary.",
            "The biggest difference is that they replace the training data (both context and middle word) while we fix the context and only replace the middle word.",
            "For a high coverage yet noisy dictionary such as Panlex, our approach gives better average score.",
            "Comparing our two most basic models (EM selection and random selection), it is clear that the model using EM to select the translation outperforms random selection by a significant margin.",
            "Our joint model, as described in equation (3) which predicts both target word and the translation, further improves the performance, especially for nl-en.",
            "We use equation (5) to combine both context embeddings V and word embeddings U for all three language pairs.",
            "This modification during training substantially improves the performance.",
            "More importantly, all our improvements are consistent for all three language pairs and both evaluation metrics, showing the robustness of our models.",
            "Our combined model out-performed previous approaches by a large margin.",
            "Vulic and Moens (2015) used bilingual comparable data, but this might be hard to obtain for some language pairs.",
            "Their performance on nl-en is poor because their comparable data between en and nl is small.",
            "Besides, they also use POS tagger and lemmatizer to filter only Noun and reduce the morphology complexity during training.",
            "These tools might not be available for many languages.",
            "For a fairer comparison to their work, we also use the same Treetagger (Schmid, 1995) to lemmatize the output of our combined model before evaluation.",
            "Table 3 (+lemmatization) shows some improvements but minor.",
            "It demonstrates that our model is already good at disambiguating morphology.",
            "For example, the top 2 translations for es word lenguas in en are languages and language which correctly prefer the plural translation."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            2,
            2,
            1,
            2,
            1,
            1,
            1,
            2,
            1,
            1,
            1,
            2,
            1,
            2,
            2,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Model"
            ],
            [
                "Gouws and S\u00f8gaard (2015) + Panlex",
                "Gouws and S\u00f8gaard (2015) + Wikt"
            ],
            [
                "Gouws and S\u00f8gaard (2015) + Panlex",
                "Gouws and S\u00f8gaard (2015) + Wikt"
            ],
            null,
            null,
            [
                "Our model (random selection)",
                "Gouws and S\u00f8gaard (2015) + Panlex"
            ],
            [
                "Our model (random selection)",
                "Gouws and S\u00f8gaard (2015) + Panlex"
            ],
            [
                "Average"
            ],
            [
                "Our model (random selection)",
                "Our model (EM selection)"
            ],
            [
                "Our model (EM selection) + Joint model",
                "nl-en"
            ],
            [
                "Our model (EM selection) + combine embeddings (\u03b4 = 0.01)"
            ],
            [
                "Our model (EM selection) + combine embeddings (\u03b4 = 0.01)",
                "rec1",
                "rec5"
            ],
            [
                "Our model (EM selection) + combine embeddings (\u03b4 = 0.01)",
                "es-en",
                "it-en",
                "nl-en",
                "rec1",
                "rec5"
            ],
            [
                "Our model (EM selection) + combine embeddings (\u03b4 = 0.01)"
            ],
            [
                "Vulic and Moens (2015)"
            ],
            [
                "Vulic and Moens (2015)",
                "nl-en"
            ],
            [
                "Vulic and Moens (2015)"
            ],
            null,
            null,
            [
                "Our model (EM selection) + lemmatization"
            ],
            [
                "Our model (random selection)",
                "Our model (EM selection)"
            ],
            [
                "es-en"
            ]
        ],
        "n_sentence": 23.0,
        "table_id": "table_3",
        "paper_id": "D16-1136",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1168table_1",
        "description": "Table 1 reports METEOR, we notice that removing the semantic coherence scores in -SEM hurts the performance compared to FULL, this confirms our claim that semantic compatibility is crucial for building coherent stories. On the other hand, -SYN performs similarly to FULL. Closer inspection of the -SYN system\u2019s output reveals a greater diversity in thematic elements as a result of the relaxed syntactic compatibility constraints. Hence it is more likely to have greater overlap with any of the reference rewrites, resulting in higher METEOR scores.",
        "sentences": [
            "Table 1 reports METEOR, we notice that removing the semantic coherence scores in -SEM hurts the performance compared to FULL, this confirms our claim that semantic compatibility is crucial for building coherent stories.",
            "On the other hand, -SYN performs similarly to FULL.",
            "Closer inspection of the -SYN system\u2019s output reveals a greater diversity in thematic elements as a result of the relaxed syntactic compatibility constraints.",
            "Hence it is more likely to have greater overlap with any of the reference rewrites, resulting in higher METEOR scores."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "-SEM",
                "FULL"
            ],
            [
                "FULL",
                "-SYN"
            ],
            [
                "-SYN"
            ],
            [
                "-SYN"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_1",
        "paper_id": "D16-1168",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1188table_2",
        "description": "In Table 2 we report the results of our experiments on the aforementioned datasets, and we distinguish our proposed regularizers LSI, GoW, word2vec with underlining. Our results are inline and confirm that of (Yogatama and Smith, 2014a) showing the advantages of using structured regularizers in the text categorization task. The group based regularizers perform systematically better than the baseline ones.",
        "sentences": [
            "In Table 2 we report the results of our experiments on the aforementioned datasets, and we distinguish our proposed regularizers LSI, GoW, word2vec with underlining.",
            "Our results are inline and confirm that of (Yogatama and Smith, 2014a) showing the advantages of using structured regularizers in the text categorization task.",
            "The group based regularizers perform systematically better than the baseline ones."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "dataset",
                "LSI",
                "GoW",
                "word2vec"
            ],
            null,
            [
                "group lasso"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_2",
        "paper_id": "D16-1188",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1216table_1",
        "description": "Table 1 first presents our reproduced classification accuracy test results (two labels: positive or negative politeness) for the bag-of-words and linguistic features based models of Danescu-Niculescu-Mizil et al. (2013) (for our dataset splits) as well as the performance of our CNN model. As seen, without using any manually defined, theory-inspired linguistic features, the simple CNN model performs better than the feature-based methods. Next, we also show how the linguistic features baseline improves on adding our novelly discovered features (plus correcting some exising features), revealed via the analysis in Sec. 6. Thus, this reduces the gap in performance between the linguistic features baseline and the CNN, and in turn provides a quantitative reasoning for the success of the CNN model. More details in Sec. 6.",
        "sentences": [
            "Table 1 presents our reproduced classification accuracy test results (two labels: positive or negative politeness) for the bag-of-words and linguistic features based models of Danescu-Niculescu-Mizil et al. (2013) (for our dataset splits) as well as the performance of our CNN model.",
            "As seen, without using any manually defined, theory-inspired linguistic features, the simple CNN model performs better than the feature-based methods.",
            "Next, we also show how the linguistic features baseline improves on adding our novelly discovered features (plus correcting some exising features), revealed via the analysis in Sec. 6.",
            "Thus, this reduces the gap in performance between the linguistic features baseline and the CNN, and in turn provides a quantitative reasoning for the success of the CNN model.",
            "More details in Sec. 6."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            0
        ],
        "header_mention": [
            [
                "Wiki",
                "SE",
                "Bag-of-Words",
                "Linguistic Features",
                "With Discovered Features",
                "CNN"
            ],
            [
                "CNN"
            ],
            null,
            null,
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_1",
        "paper_id": "D16-1216",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1220table_3",
        "description": "Finally, Table 3 shows the effect of the different feature sets on VUAMC used by Klebanov et al. (2014). We use the same 12-fold data split as Klebanov et al. (2014), and also in this case we perform a grid-search to optimize the meta-parameter C of the logistic regression classifier. The best value of C identified for each genre and feature set is shown in the column labeled C. On this data, N features alone are significantly outperformed by B (p < 0.01). On the other hand, for the genres \u201cacademic\u201d and \u201cfiction\u201d, combining N and B features improves classification performance over B, and the difference is always statistically significant. Besides, the addition of N always leads to more balanced models, by compensating for the relatively lower precision of B. Due to the lack of a separate test set, as in the original setup by Klebanov et al. (2014), and to the high dimensionality of B\u2019s lexicalized features, we cannot rule out over-fitting as an explanation for the relatively good performance of B on this benchmark. It should also be noted that the results reported in (Klebanov et al., 2014) are not the same, due to the mentioned differences in the implementation of the features and possibly other differences in the experimental setup (e.g., data filtering, pre-processing and meta-parameter optimization). In particular, our implementation of the B features performs better than reported by Klebanov et al. (2014) on all four genres, namely: 0.52 vs. 0.51 for \u201cnews\u201d, 0.51 vs. 0.28 for \u201cacademic\u201d, 0.39 vs. 0.28 for \u201cconversation\u201d and 0.42 vs. 0.33 for \u201cfiction\u201d.",
        "sentences": [
            "Finally, Table 3 shows the effect of the different feature sets on VUAMC used by Klebanov et al. (2014).",
            "We use the same 12-fold data split as Klebanov et al. (2014), and also in this case we perform a grid-search to optimize the meta-parameter C of the logistic regression classifier.",
            "The best value of C identified for each genre and feature set is shown in the column labeled C.",
            "On this data, N features alone are significantly outperformed by B (p < 0.01).",
            "On the other hand, for the genres \u201cacademic\u201d and \u201cfiction\u201d, combining N and B features improves classification performance over B, and the difference is always statistically significant.",
            "Besides, the addition of N always leads to more balanced models, by compensating for the relatively lower precision of B.",
            "Due to the lack of a separate test set, as in the original setup by Klebanov et al. (2014), and to the high dimensionality of B\u2019s lexicalized features, we cannot rule out over-fitting as an explanation for the relatively good performance of B on this benchmark.",
            "It should also be noted that the results reported in (Klebanov et al., 2014) are not the same, due to the mentioned differences in the implementation of the features and possibly other differences in the experimental setup (e.g., data filtering, pre-processing and meta-parameter optimization).",
            "In particular, our implementation of the B features performs better than reported by Klebanov et al. (2014) on all four genres, namely: 0.52 vs. 0.51 for \u201cnews\u201d, 0.51 vs. 0.28 for \u201cacademic\u201d, 0.39 vs. 0.28 for \u201cconversation\u201d and 0.42 vs. 0.33 for \u201cfiction\u201d."
        ],
        "class_sentence": [
            1,
            2,
            2,
            1,
            1,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "C",
                "Genre",
                "Features"
            ],
            [
                "N",
                "B"
            ],
            [
                "Academic",
                "Fiction",
                "B \u222a N",
                "B"
            ],
            [
                "N",
                "P"
            ],
            null,
            null,
            null
        ],
        "n_sentence": 9.0,
        "table_id": "table_3",
        "paper_id": "D16-1220",
        "valid": 1
    },
    {
        "table_id_paper": "D16-1262table_5",
        "description": "Lastly, to show that our parser is both more accurate and efficient than other decoding methods, we decode our full model using best-first search, reranking, and beam search. Table 5 shows the F1 scores with and without the backoff model, the portion of the sentences that each decoder is able to parse, and the time spent decoding relative to the A* parser. In the best-first search comparison, we do not include the informative A* heuristic, and the parser completes very few parses before reaching computational limits\u00d1showing the importance of heuristics in large search spaces. In the reranking comparison, we obtain n-best lists from the backoff model and rerank each result with the full model. In the beam search comparison, we use the approach from Clark et al. (2015) which greedily finds the top-n parses for each span in a bottom-up manner. Results indicate that both approximate methods are less accurate and slower than A*.",
        "sentences": [
            "Lastly, to show that our parser is both more accurate and efficient than other decoding methods, we decode our full model using best-first search, reranking, and beam search.",
            "Table 5 shows the F1 scores with and without the backoff model, the portion of the sentences that each decoder is able to parse, and the time spent decoding relative to the A* parser.",
            "In the best-first search comparison, we do not include the informative A* heuristic, and the parser completes very few parses before reaching computational limits\u00d1showing the importance of heuristics in large search spaces.",
            "In the reranking comparison, we obtain n-best lists from the backoff model and rerank each result with the full model.",
            "In the beam search comparison, we use the approach from Clark et al. (2015) which greedily finds the top-n parses for each span in a bottom-up manner.",
            "Results indicate that both approximate methods are less accurate and slower than A*."
        ],
        "class_sentence": [
            2,
            1,
            2,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "Dev F1",
                "- backoff",
                "backoff",
                "Relative Time"
            ],
            [
                "Best-first"
            ],
            [
                "10-best reranking",
                "100-best reranking"
            ],
            [
                "2-best beam search",
                "4-best beam search",
                "8-best beam search"
            ],
            [
                "Best-first",
                "10-best reranking",
                "100-best reranking",
                "2-best beam search",
                "4-best beam search",
                "8-best beam search",
                "Global A*"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_5",
        "paper_id": "D16-1262",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1241table_3",
        "description": "We evaluate the performance of our models using 10-fold cross validation. The evaluation results shown in Table 3 are based on weighted ROC AUC of the best models. Among all the feature learning methods for Facebook status updates, User-D-DBOW performed the best. It significantly outperformed all the baseline systems that only rely on supervised training (p < 0.01 based on t-tests). It also significantly outperformed all the traditional feature learning methods such as LDA and SVD (p < 0.01 based on t-tests). Moreover, in terms of whether to treat all the posts by the same user as one big document or separate documents, LDA prefers one post one document (models with a \"post\" prefix) while all the document vector-based methods prefer one user one document (models with a User prefix). Moreover, to use post-level LDA to derive the SPE of a user, the document-based aggregation method (PostLDA_Doc) performed better than the wordbased method (PostLDA_Word).",
        "sentences": [
            "We evaluate the performance of our models using 10-fold cross validation.",
            "The evaluation results shown in Table 3 are based on weighted ROC AUC of the best models.",
            "Among all the feature learning methods for Facebook status updates, User-D-DBOW performed the best.",
            "It significantly outperformed all the baseline systems that only rely on supervised training (p < 0.01 based on t-tests).",
            "It also significantly outperformed all the traditional feature learning methods such as LDA and SVD (p < 0.01 based on t-tests).",
            "Moreover, in terms of whether to treat all the posts by the same user as one big document or separate documents, LDA prefers one post one document (models with a \"post\" prefix) while all the document vector-based methods prefer one user one document (models with a User prefix).",
            "Moreover, to use post-level LDA to derive the SPE of a user, the document-based aggregation method (PostLDA_Doc) performed better than the wordbased method (PostLDA_Word)."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "User-D-DBOW",
                "Tobacco",
                "Alcohol",
                "Drug"
            ],
            [
                "Unigram",
                "LIWC"
            ],
            [
                "SVD",
                "UserLDA",
                "PostLDA_Word",
                "PostLDA_Doc"
            ],
            [
                "PostLDA_Doc"
            ],
            [
                "PostLDA_Doc",
                "PostLDA_Word"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_3",
        "paper_id": "D17-1241",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1245table_7",
        "description": "Table 7 reports on the results obtained by the best configuration, i.e. LR + All features, per each category.",
        "sentences": [
            "Table 7 reports on the results obtained by the best configuration, i.e. LR + All features, per each category."
        ],
        "class_sentence": [
            1
        ],
        "header_mention": [
            [
                "avg/total",
                "F1",
                "#argument per category"
            ]
        ],
        "n_sentence": 1.0,
        "table_id": "table_7",
        "paper_id": "D17-1245",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1254table_4",
        "description": "Table 4 shows our results. Consistent with previous experiments, we empirically found that the encoder trained using the fixed word embedding performed better on this task, hence only results using this method are reported. As can be seen, we obtain the same median rank as in Kiros et al.(2015), indicating that our encoder is as competitive as the skip-thought vectors (Kiros et al., 2015). The performance gain between our encoder and the combine-skip model of Kiros et al.(2015) on the R@1 score is significant, which shows that the CNN encoder has more discriminative power on retrieving the most correct item than the skip-thought vector.",
        "sentences": [
            "Table 4 shows our results.",
            "Consistent with previous experiments, we empirically found that the encoder trained using the fixed word embedding performed better on this task, hence only results using this method are reported.",
            "As can be seen, we obtain the same median rank as in Kiros et al.(2015), indicating that our encoder is as competitive as the skip-thought vectors (Kiros et al., 2015).",
            "The performance gain between our encoder and the combine-skip model of Kiros et al.(2015) on the R@1 score is significant, which shows that the CNN encoder has more discriminative power on retrieving the most correct item than the skip-thought vector."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "Our Results"
            ],
            null,
            [
                "uni-skip",
                "bi-skip",
                "combine-skip",
                "Med r",
                "hierarchical model+emb.",
                "composite model+emb.",
                "combine+emb."
            ],
            [
                "combine+emb.",
                "combine-skip",
                "R@1"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_4",
        "paper_id": "D17-1254",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1267table_4",
        "description": "Table 4 shows the results. The phonetic features alone are sufficient to detect just over half of the cognate sets. Each successive variant substantially improves the recall at a cost of slightly lower precision. The full feature set yields a 27% relative increase in the number of found sets over the phonetic-only variant, with only a 5% drop in cluster purity.",
        "sentences": [
            "Table 4 shows the results.",
            "The phonetic features alone are sufficient to detect just over half of the cognate sets.",
            "Each successive variant substantially improves the recall at a cost of slightly lower precision.",
            "The full feature set yields a 27% relative increase in the number of found sets over the phonetic-only variant, with only a 5% drop in cluster purity."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Phonetic only",
                "Purity"
            ],
            [
                "+ Definitions",
                "+ WordNet",
                "+ Word Vectors",
                "Purity"
            ],
            [
                "Phonetic only",
                "Found Sets",
                "Purity"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_4",
        "paper_id": "D17-1267",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1304table_1",
        "description": "Table 1 shows the translation performances on test sets measured in BLEU score. The AttNMT significantly outperforms PBSMT by 2.74 BLEU points on average, indicating that it is a strong baseline NMT system. The baseline Sennrichdeponly improves the performance over the AttNMT by 0.58 BLEU points on average. This indicates that the proposed source dependency constraint is beneficial for improving the performance of NMT. Moreover, SDRNMT-1 gains improvements of 0.92 and 0.34 BLEU points on average than the AttNMT and Sennrich-deponly. These show that the proposed SDR can more effectively capture source dependency information than vector concatenation. Especially, the proposed SDRNMT2 outperforms the AttNMT and Sennrich-deponly on average by 1.64 and 1.03 BLEU points. These verify that the proposed double-context method is effective for word prediction.",
        "sentences": [
            "Table 1 shows the translation performances on test sets measured in BLEU score.",
            "The AttNMT significantly outperforms PBSMT by 2.74 BLEU points on average, indicating that it is a strong baseline NMT system.",
            "The baseline Sennrichdeponly improves the performance over the AttNMT by 0.58 BLEU points on average.",
            "This indicates that the proposed source dependency constraint is beneficial for improving the performance of NMT.",
            "Moreover, SDRNMT-1 gains improvements of 0.92 and 0.34 BLEU points on average than the AttNMT and Sennrich-deponly.",
            "These show that the proposed SDR can more effectively capture source dependency information than vector concatenation.",
            "Especially, the proposed SDRNMT2 outperforms the AttNMT and Sennrich-deponly on average by 1.64 and 1.03 BLEU points.",
            "These verify that the proposed double-context method is effective for word prediction."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "AttNMT",
                "PBSMT",
                "AVG"
            ],
            [
                "Sennrich-deponly",
                "AttNMT",
                "AVG"
            ],
            [
                "Sennrich-deponly",
                "AttNMT"
            ],
            [
                "SDRNMT-1",
                "AttNMT",
                "Sennrich-deponly",
                "AVG"
            ],
            [
                "SDRNMT-1",
                "AttNMT",
                "Sennrich-deponly"
            ],
            [
                "SDRNMT-2",
                "AttNMT",
                "Sennrich-deponly",
                "AVG"
            ],
            [
                "SDRNMT-2",
                "AttNMT",
                "Sennrich-deponly"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_1",
        "paper_id": "D17-1304",
        "valid": 1
    },
    {
        "table_id_paper": "D17-1309table_4",
        "description": "The results in Table 4 show that spending our memory budget on small bigram embeddings is more effective than on larger character embeddings, in terms of both accuracy and model size. Our model featuring bigrams runs at 110KB of text per second, or 39k tokens/second.",
        "sentences": [
            "The results in Table 4 show that spending our memory budget on small bigram embeddings is more effective than on larger character embeddings, in terms of both accuracy and model size.",
            "Our model featuring bigrams runs at 110KB of text per second, or 39k tokens/second."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "Small FF 64 dim bigrams",
                "Accuracy",
                "Size"
            ],
            [
                "Small FF 64 dim bigrams"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_4",
        "paper_id": "D17-1309",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1033table_5",
        "description": "Table 5 shows the results of monolingual word similarity computation on four datasets. From the table, we find that:. (1) Our models perform better than BiLex on both Chinese word similarity datasets. It signifies incorporating sememe information helps learn better monolingual embeddings;. (2) CLSP-WR model does not enhance English word similarity results but CLSPSE model does. It is because CLSP-WR model only post-processes Chinese word embeddings and keeps English word embeddings unchanged while CLSP-SE model undertakes bilingual alignment and sememe information incorporation together, which makes English word embeddings improve with Chinese word embeddings.",
        "sentences": [
            "Table 5 shows the results of monolingual word similarity computation on four datasets.",
            "From the table, we find that:.",
            "(1) Our models perform better than BiLex on both Chinese word similarity datasets.",
            "It signifies incorporating sememe information helps learn better monolingual embeddings;.",
            "(2) CLSP-WR model does not enhance English word similarity results but CLSPSE model does.",
            "It is because CLSP-WR model only post-processes Chinese word embeddings and keeps English word embeddings unchanged while CLSP-SE model undertakes bilingual alignment and sememe information incorporation together, which makes English word embeddings improve with Chinese word embeddings."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "BiLex",
                "CLSP-WR",
                "CLSP-SE",
                "Chinese (source)",
                "WS-240",
                "WS-297"
            ],
            null,
            [
                "CLSP-WR",
                "CLSP-SE",
                "English (target)",
                "WS-353",
                "SL-999"
            ],
            [
                "CLSP-WR",
                "CLSP-SE",
                "Chinese (source)",
                "English (target)"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_5",
        "paper_id": "D18-1033",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1049table_4",
        "description": "Table 4 shows that our model also outperforms Transformer by 0.89 BLEU points on French-English translation task.",
        "sentences": [
            "Table 4 shows that our model also outperforms Transformer by 0.89 BLEU points on French-English translation task."
        ],
        "class_sentence": [
            1
        ],
        "header_mention": [
            [
                "this work",
                "Transformer",
                "Test"
            ]
        ],
        "n_sentence": 1.0,
        "table_id": "table_4",
        "paper_id": "D18-1049",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1054table_2",
        "description": "3.4 Results. Table 2 shows the performance of various models on NarrativeQA. It can be noted that our model with sample size 5 (choosing 5 relevant sentences) outperforms the best ROUGE-L score available so far by 12.62% compared to Tay et al. (2018). The low performance of Baseline 1 shows that the hybrid approach (ConZNet) for generating words from a fixed vocabulary as well as copying words from the document is better suited than span prediction models (Seq2Seq, ASR, BiDAF, MRU). To validate the importance of finding relevant sentences in contrast to using an entire document for answer generation, we experimented with sample sizes beyond 5. The performance of our model gradually dropped from sample size 7 onwards. This result shows evidence that only a few relevant sentences are sufficient to answer a question. We also experimented with various sample sizes to see the effect of intra sentence relations for answer generation. The performance of the model improved dramatically with sample sizes 3 and 5 compared to the sample size of 1. These results show that the importance of selecting multiple relevant sentences for generating an answer. In addition, the low performance of Baseline 2 indicates that just selecting multiple sentences is not enough, they should also be related to each other. This result points out that the self-attention mechanism in the Context zoom layer is an important component to identify related relevant sentences.",
        "sentences": [
            "3.4 Results.",
            "Table 2 shows the performance of various models on NarrativeQA.",
            "It can be noted that our model with sample size 5 (choosing 5 relevant sentences) outperforms the best ROUGE-L score available so far by 12.62% compared to Tay et al. (2018).",
            "The low performance of Baseline 1 shows that the hybrid approach (ConZNet) for generating words from a fixed vocabulary as well as copying words from the document is better suited than span prediction models (Seq2Seq, ASR, BiDAF, MRU).",
            "To validate the importance of finding relevant sentences in contrast to using an entire document for answer generation, we experimented with sample sizes beyond 5.",
            "The performance of our model gradually dropped from sample size 7 onwards.",
            "This result shows evidence that only a few relevant sentences are sufficient to answer a question.",
            "We also experimented with various sample sizes to see the effect of intra sentence relations for answer generation.",
            "The performance of the model improved dramatically with sample sizes 3 and 5 compared to the sample size of 1.",
            "These results show that the importance of selecting multiple relevant sentences for generating an answer.",
            "In addition, the low performance of Baseline 2 indicates that just selecting multiple sentences is not enough, they should also be related to each other.",
            "This result points out that the self-attention mechanism in the Context zoom layer is an important component to identify related relevant sentences."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2,
            1,
            2,
            2,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Seq2Seq",
                "ASR",
                "BiDAF",
                "MRU (Tay et al. 2018)",
                "Baseline 1 (SS=5)",
                "Baseline 2 (SS=5)",
                "ConZNet (SS=1)",
                "ConZNet (SS=3)",
                "ConZNet (SS=5)",
                "ConZNet (SS=7)"
            ],
            [
                "ConZNet (SS=5)",
                "MRU (Tay et al. 2018)",
                "ROUGE-L"
            ],
            [
                "Baseline 1 (SS=5)",
                "Seq2Seq",
                "ASR",
                "BiDAF",
                "MRU (Tay et al. 2018)"
            ],
            [
                "ConZNet (SS=7)"
            ],
            [
                "ConZNet (SS=7)"
            ],
            null,
            [
                "ConZNet (SS=1)",
                "ConZNet (SS=3)",
                "ConZNet (SS=5)",
                "ConZNet (SS=7)"
            ],
            [
                "ConZNet (SS=1)",
                "ConZNet (SS=3)",
                "ConZNet (SS=5)"
            ],
            null,
            [
                "Baseline 2 (SS=5)"
            ],
            null
        ],
        "n_sentence": 12.0,
        "table_id": "table_2",
        "paper_id": "D18-1054",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1058table_3",
        "description": "4.5 Comparison to other systems. We compared the results of our models to other published results. Table 3 displays those results. Rink and Harabagiu (2012) is the pattern-based model with naive Bayes. Mikolov et al. (2013b), Levy and Goldberg (2014), and Iacobacci et al. (2015) are the vector offset models. Zhila et al. (2013) is the model composed of various features. Turney (2013) extracts the statistical features of two word pairs from a word-context co-occurrence matrix and trains the classifier with additional semantic relational data to assign a relational similarity for two word pairs. NLRA+VecOff achieved a competitive performance to the state-of-the-art method of Turney (2013). Note that our method learns unsupervisedly and does not exploit additional resources, and the method of Turney (2013) cannot obtain the distributed representation of word pairs. A work similar to ours, Bollegala et al. (2015), represented lexico-syntactic patterns as the vector offset of co-occurring word pairs and updated the vector offsets of word pairs such that word pairs that co-occur in similar patterns have similar offsets. They evaluated their model on all 79 semantic relations of the dataset and achieved 0.449 accuracy. In their setting, NLRA+VecOff achieved 0.47 accuracy, outperforming their model.",
        "sentences": [
            "4.5 Comparison to other systems.",
            "We compared the results of our models to other published results.",
            "Table 3 displays those results.",
            "Rink and Harabagiu (2012) is the pattern-based model with naive Bayes.",
            "Mikolov et al. (2013b), Levy and Goldberg (2014), and Iacobacci et al. (2015) are the vector offset models.",
            "Zhila et al. (2013) is the model composed of various features.",
            "Turney (2013) extracts the statistical features of two word pairs from a word-context co-occurrence matrix and trains the classifier with additional semantic relational data to assign a relational similarity for two word pairs.",
            "NLRA+VecOff achieved a competitive performance to the state-of-the-art method of Turney (2013).",
            "Note that our method learns unsupervisedly and does not exploit additional resources, and the method of Turney (2013) cannot obtain the distributed representation of word pairs.",
            "A work similar to ours, Bollegala et al. (2015), represented lexico-syntactic patterns as the vector offset of co-occurring word pairs and updated the vector offsets of word pairs such that word pairs that co-occur in similar patterns have similar offsets.",
            "They evaluated their model on all 79 semantic relations of the dataset and achieved 0.449 accuracy.",
            "In their setting, NLRA+VecOff achieved 0.47 accuracy, outperforming their model."
        ],
        "class_sentence": [
            "2",
            "2",
            "1",
            "2",
            "2",
            "2",
            "2",
            "1",
            "2",
            "0",
            "2",
            "1"
        ],
        "header_mention": [
            null,
            [
                "Rink and Harabagiu (2012)",
                "Mikolov et al. (2013b)",
                "Levy and Goldberg (2014)",
                "Zhila et al. (2013)",
                "Iacobacci et al. (2015)",
                "Turney (2013)",
                "VecOff",
                "LRA",
                "NLRA"
            ],
            null,
            [
                "Rink and Harabagiu (2012)"
            ],
            [
                "Mikolov et al. (2013b)",
                "Levy and Goldberg (2014)",
                "Iacobacci et al. (2015)"
            ],
            [
                "Zhila et al. (2013)"
            ],
            [
                "Turney (2013)"
            ],
            [
                "NLRA+VecOff",
                "Turney (2013)"
            ],
            [
                "NLRA+VecOff",
                "Turney (2013)"
            ],
            null,
            null,
            [
                "NLRA+VecOff"
            ]
        ],
        "n_sentence": 12.0,
        "table_id": "table_3",
        "paper_id": "D18-1058",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1082table_1",
        "description": "Automatic Evaluation:. Table 1 shows the automatic evaluation results. We see that both SPMN and DPMN obtain huge improvements over the two baselines in terms of PPL-V and PPL-T. Also, we observe that SPMN1000 outperforms SPMN500 in all the four automatic metrics. Note that each post has the same prototypes provided by SPMN. This is reasonable that the relevant response is more likely to be covered by storing more prototypes in SPMN. As for the DPMN, we can see that DPMN achieves the best performance with only 100 prototypes in terms of PPL-T, compared with the other 4 methods. This suggests that using a retrieval mechanism to incorporate the relevant responses brings more useful information for better response generation. Note that S2SA outperforms the others in terms of distinct-1 and distinct-2. Further human evaluation indicates that many responses generated by S2SA are irrelevant and meaningless, which could inevitably increase the distinct scores.",
        "sentences": [
            "Automatic Evaluation:.",
            "Table 1 shows the automatic evaluation results.",
            "We see that both SPMN and DPMN obtain huge improvements over the two baselines in terms of PPL-V and PPL-T.",
            "Also, we observe that SPMN1000 outperforms SPMN500 in all the four automatic metrics.",
            "Note that each post has the same prototypes provided by SPMN.",
            "This is reasonable that the relevant response is more likely to be covered by storing more prototypes in SPMN.",
            "As for the DPMN, we can see that DPMN achieves the best performance with only 100 prototypes in terms of PPL-T, compared with the other 4 methods.",
            "This suggests that using a retrieval mechanism to incorporate the relevant responses brings more useful information for better response generation.",
            "Note that S2SA outperforms the others in terms of distinct-1 and distinct-2.",
            "Further human evaluation indicates that many responses generated by S2SA are irrelevant and meaningless, which could inevitably increase the distinct scores."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2,
            2,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "SPMN500",
                "SPMN1000",
                "DPMN100",
                "S2SA",
                "TAS2S",
                "PPL-V",
                "PPL-T"
            ],
            [
                "SPMN1000",
                "SPMN500",
                "PPL-V",
                "PPL-T",
                "distinct-1",
                "distinct-2"
            ],
            [
                "SPMN500",
                "SPMN1000"
            ],
            [
                "SPMN500",
                "SPMN1000"
            ],
            [
                "DPMN100",
                "S2SA",
                "TAS2S",
                "SPMN500",
                "SPMN1000",
                "PPL-T"
            ],
            [
                "DPMN100"
            ],
            [
                "S2SA",
                "TAS2S",
                "SPMN500",
                "SPMN1000",
                "DPMN100",
                "distinct-1",
                "distinct-2"
            ],
            [
                "S2SA"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_1",
        "paper_id": "D18-1082",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1160table_4",
        "description": "These results are summarized in Table 4 and Table 5. While fastText embeddings lead to reduced performance with our model, our approach still yields an improvement over the Gaussian baseline with the new observed embeddings space.",
        "sentences": [
            "These results are summarized in Table 4 and Table 5.",
            "While fastText embeddings lead to reduced performance with our model, our approach still yields an improvement over the Gaussian baseline with the new observed embeddings space."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Ours (4 layers)",
                "Ours (8 layers)",
                "Ours (16 layers)",
                "Gaussian HMM"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_4",
        "paper_id": "D18-1160",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1166table_2",
        "description": "4.3 Baseline Results. We report the performance of the baseline models in Table 2 which indicates the ratio of correct answers against the total questions in the test. For the textual cloze, the comparison between text-only and multimodal Impatient Reader models shows that the additional visual modality helps the model to understand the question better and to provide more accurate answers. While for the cloze style questions, the Impatient Reader outperforms the Hasty student, for the visual coherence and visual ordering style questions Hasty student gives way better results. This demonstrates that better neural models are needed to be able to effectively deal with this kind of questions. Some qualitative examples are provided in the supplementary material.",
        "sentences": [
            "4.3 Baseline Results.",
            "We report the performance of the baseline models in Table 2 which indicates the ratio of correct answers against the total questions in the test.",
            "For the textual cloze, the comparison between text-only and multimodal Impatient Reader models shows that the additional visual modality helps the model to understand the question better and to provide more accurate answers.",
            "While for the cloze style questions, the Impatient Reader outperforms the Hasty student, for the visual coherence and visual ordering style questions Hasty student gives way better results.",
            "This demonstrates that better neural models are needed to be able to effectively deal with this kind of questions.",
            "Some qualitative examples are provided in the supplementary material."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "Hasty Student",
                "Impatient Reader (Text only)",
                "Impatient Reader (Multimodal)"
            ],
            [
                "Textual Cloze",
                "Impatient Reader (Text only)",
                "Impatient Reader (Multimodal)"
            ],
            [
                "Hasty Student",
                "Impatient Reader (Text only)",
                "Impatient Reader (Multimodal)",
                "Visual Cloze",
                "Textual Cloze",
                "Visual Coherence",
                "Visual Ordering"
            ],
            [
                "Impatient Reader (Text only)",
                "Impatient Reader (Multimodal)"
            ],
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_2",
        "paper_id": "D18-1166",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1176table_1",
        "description": "4.2 Results. Table 1 shows the results. We report accuracy scores averaged over five runs with different random seeds, together with their standard deviation, for the SNLI and MultiNLI datasets. We include two versions of the naive baseline: one with a 512dimensional BiLSTM encoder; and a bigger one with 2048 dimensions. Both naive baseline models outperform the single encoders that have only GloVe or FastText embeddings. This shows how including more than one embeddings can help performance. Next, we observe that the DME embeddings outperform the naive concatenation baselines, while having fewer parameters. Differences between the three DME variants are small and not significant, although we do note that we found the highest maximum performance for the contextualized version, which adds very few additional parameters. It is important to note that the imposition of weighting thus is not detrimental to performance, which means that DME and CDME provide additional interpretability without sacrificing performance. Finally, we obtain results for using the six different embedding types (marked *), and show that adding in more embeddings increases performance further. To our knowledge, these numbers constitute the state of the art within the model class of single sentence encoders on these tasks.",
        "sentences": [
            "4.2 Results.",
            "Table 1 shows the results.",
            "We report accuracy scores averaged over five runs with different random seeds, together with their standard deviation, for the SNLI and MultiNLI datasets.",
            "We include two versions of the naive baseline: one with a 512dimensional BiLSTM encoder; and a bigger one with 2048 dimensions.",
            "Both naive baseline models outperform the single encoders that have only GloVe or FastText embeddings.",
            "This shows how including more than one embeddings can help performance.",
            "Next, we observe that the DME embeddings outperform the naive concatenation baselines, while having fewer parameters.",
            "Differences between the three DME variants are small and not significant, although we do note that we found the highest maximum performance for the contextualized version, which adds very few additional parameters.",
            "It is important to note that the imposition of weighting thus is not detrimental to performance, which means that DME and CDME provide additional interpretability without sacrificing performance.",
            "Finally, we obtain results for using the six different embedding types (marked *), and show that adding in more embeddings increases performance further.",
            "To our knowledge, these numbers constitute the state of the art within the model class of single sentence encoders on these tasks."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            1,
            2,
            1,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "SNLI",
                "MNLI"
            ],
            [
                "Naive baseline (9.8M)",
                "Naive baseline (61.3M)"
            ],
            [
                "Naive baseline (9.8M)",
                "Naive baseline (61.3M)",
                "GloVe BiLSTM-Max (8.6M)",
                "FastText BiLSTM-Max (8.6M)"
            ],
            null,
            [
                "DME (8.6M)",
                "Naive baseline (9.8M)",
                "Naive baseline (61.3M)"
            ],
            [
                "Unweighted DME (8.6M)",
                "DME (8.6M)",
                "CDME (8.6M)"
            ],
            [
                "DME (8.6M)",
                "CDME (8.6M)"
            ],
            [
                "DME* (9.0M)",
                "CDME* (9.0M)"
            ],
            null
        ],
        "n_sentence": 11.0,
        "table_id": "table_1",
        "paper_id": "D18-1176",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1188table_2",
        "description": "From Table 2, we can see that STAMP performs better than existing systems when trained on the full WikiSQL training dataset, achieving state-ofthe-art execution accuracy and logical form accuracy on WikiSQL. We further conduct experiments to demonstrate the effectiveness of our question generation driven approach. We run the entire pipeline (STAMP+QG) with different percentages of training data. The second column \u201cTraining Data\u201d in Table 2 and the x-axis in Figure 3 represent the proportion of WikiSQL training data we use for training the QG model and semantic parser. That is to say, STAMP +QG with 30% means that we sample 30% WikiSQL training data to train the QG model, and then combine QG generated data and exactly the same 30% WikiSQL training data we sampled before to train the semantic parser. In this experiment, we sample five SQL queries for each table in the training data, resulting in 43.5K SQL queries. Applying the QG model on these SQL queries, we get 92.8K SQLquestion pairs. From Figure 3, we see that accuracy increases as the amount of supervised training data expands. Results show that QG empowers the STAMP model to achieve the same accuracy on WikiSQL dataset with 30% of the training data. Applying QG to the STAMP model under the full setting brings further improvements, resulting in new state-of-the-art accuracies.",
        "sentences": [
            "From Table 2, we can see that STAMP performs better than existing systems when trained on the full WikiSQL training dataset, achieving state-ofthe-art execution accuracy and logical form accuracy on WikiSQL.",
            "We further conduct experiments to demonstrate the effectiveness of our question generation driven approach.",
            "We run the entire pipeline (STAMP+QG) with different percentages of training data.",
            "The second column \u201cTraining Data\u201d in Table 2 and the x-axis in Figure 3 represent the proportion of WikiSQL training data we use for training the QG model and semantic parser.",
            "That is to say, STAMP +QG with 30% means that we sample 30% WikiSQL training data to train the QG model, and then combine QG generated data and exactly the same 30% WikiSQL training data we sampled before to train the semantic parser.",
            "In this experiment, we sample five SQL queries for each table in the training data, resulting in 43.5K SQL queries.",
            "Applying the QG model on these SQL queries, we get 92.8K SQLquestion pairs.",
            "From Figure 3, we see that accuracy increases as the amount of supervised training data expands.",
            "Results show that QG empowers the STAMP model to achieve the same accuracy on WikiSQL dataset with 30% of the training data.",
            "Applying QG to the STAMP model under the full setting brings further improvements, resulting in new state-of-the-art accuracies."
        ],
        "class_sentence": [
            1,
            2,
            2,
            1,
            2,
            2,
            2,
            0,
            1,
            1
        ],
        "header_mention": [
            [
                "STAMP",
                "100%",
                "Acclf",
                "Accex"
            ],
            null,
            [
                "STAMP + QG",
                "30%",
                "100%"
            ],
            [
                "Training Data"
            ],
            [
                "STAMP + QG",
                "30%"
            ],
            null,
            [
                "STAMP + QG"
            ],
            null,
            [
                "STAMP + QG",
                "30%"
            ],
            [
                "STAMP + QG",
                "100%",
                "Attentional Seq2Seq",
                "Aug.PntNet (Zhong et al. 2017)",
                "Aug.PntNet (re-implemented by us)",
                "Seq2SQL (Zhong et al. 2017)",
                "SQLNet (Xu et al. 2017)"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_2",
        "paper_id": "D18-1188",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1219table_5",
        "description": "Table 5 lists the results for bridging anaphora resolution based on different word representation resources. We notice that there is not much difference between GloVe GigaWiki14 and GloVe Giga. We find that using embeddings PP achieves an accuracy of 33.03% on the ISNotes corpus, which outperforms the results based on GloVe GigaWiki14 and GloVe Giga by a large margin. Using embeddings bridging further improves the result by 1.8%. Although the improvement is not significant, we suspect that the representations for words without the suffix \u201c PP\u201d in embeddings bridging are more accurate because they are trained on many more instances in the vanilla GloVe vectors (GloVe GigaWiki14).",
        "sentences": [
            "Table 5 lists the results for bridging anaphora resolution based on different word representation resources.",
            "We notice that there is not much difference between GloVe GigaWiki14 and GloVe Giga.",
            "We find that using embeddings PP achieves an accuracy of 33.03% on the ISNotes corpus, which outperforms the results based on GloVe GigaWiki14 and GloVe Giga by a large margin.",
            "Using embeddings bridging further improves the result by 1.8%.",
            "Although the improvement is not significant, we suspect that the representations for words without the suffix \u201c PP\u201d in embeddings bridging are more accurate because they are trained on many more instances in the vanilla GloVe vectors (GloVe GigaWiki14)."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "GloVe GigaWiki14",
                "GloVe Giga",
                "embeddings PP",
                "embeddings bridging"
            ],
            [
                "GloVe GigaWiki14",
                "GloVe Giga"
            ],
            [
                "embeddings PP",
                "GloVe GigaWiki14",
                "GloVe Giga",
                "acc"
            ],
            [
                "embeddings bridging",
                "embeddings PP",
                "acc"
            ],
            [
                "GloVe GigaWiki14"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_5",
        "paper_id": "D18-1219",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1262table_5",
        "description": "Effect of deep encoder. Table 5 reports F1 scores of our Syn-GCN model, Marcheggiani and Titov (2017), He et al. (2018) and Cai et al. (2018) on English test set in both syntax-agnostic and syntax-aware settings. The comparison shows that our framework is more effective for incorporating syntactic information by giving more performance improvement through introducing syntax over syntax-agnostic SRL than previous state-ofthe-art systems did.",
        "sentences": [
            "Effect of deep encoder.",
            "Table 5 reports F1 scores of our Syn-GCN model, Marcheggiani and Titov (2017), He et al. (2018) and Cai et al. (2018) on English test set in both syntax-agnostic and syntax-aware settings.",
            "The comparison shows that our framework is more effective for incorporating syntactic information by giving more performance improvement through introducing syntax over syntax-agnostic SRL than previous state-ofthe-art systems did."
        ],
        "class_sentence": [
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "M&T (2017)",
                "He et al. (2018)",
                "Cai et al. (2018)",
                "Our model",
                "syntax-agnostic",
                "syntax-aware"
            ],
            [
                "M&T (2017)",
                "He et al. (2018)",
                "Cai et al. (2018)",
                "Our model"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_5",
        "paper_id": "D18-1262",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1273table_6",
        "description": "Results. Table 6 shows the information of 300 sentences and the annotation results on them. The average recall in the table illustrates that three students have a higher recognition rate for errors from D-asr than that from D-ocr, which, to some extent, indicates that P-style errors are easier to be detected than V-style ones. Besides, we observe that three volunteers fail to identify around 36.9% errors on average, which may indicate that our generated sentences include some challenging errors which are likely to be made by human. Such errors are valuable for CSC since they are potential real cases in people\u00e2\u20ac\u2122s writing or typing.",
        "sentences": [
            "Results.",
            "Table 6 shows the information of 300 sentences and the annotation results on them.",
            "The average recall in the table illustrates that three students have a higher recognition rate for errors from D-asr than that from D-ocr, which, to some extent, indicates that P-style errors are easier to be detected than V-style ones.",
            "Besides, we observe that three volunteers fail to identify around 36.9% errors on average, which may indicate that our generated sentences include some challenging errors which are likely to be made by human.",
            "Such errors are valuable for CSC since they are potential real cases in people\u00e2\u20ac\u2122s writing or typing."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "E-asr",
                "E-ocr",
                "R"
            ],
            [
                "E-asr",
                "E-ocr",
                "R"
            ],
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_6",
        "paper_id": "D18-1273",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1276table_2",
        "description": "For any two words appearing in an exhaustive segmentation, we keep an edge only if both the words overlap within a distance of k characters. We experiment with k = 5, 10, 15 and 20. Hence, for K = 20, a word will form edges with all the words that fall within 20 characters to left and 20 characters to right. The average length of an input sequence in DCS10K is 40.79 characters. We do not modify our inference procedure in system 8 other than to take care of the possibility that a clique need not always be returned. Table 2 shows the results for different values of k. Interestingly, the results show a monotonic increase with the increase in context window, and the results with the entire context are still better than those with k = 20, even though only marginally. It is interesting to note that, keeping the entire context does not adversely affect the predictions as none of the pruned models outperforms System 8.",
        "sentences": [
            "For any two words appearing in an exhaustive segmentation, we keep an edge only if both the words overlap within a distance of k characters.",
            "We experiment with k = 5, 10, 15 and 20.",
            "Hence, for K = 20, a word will form edges with all the words that fall within 20 characters to left and 20 characters to right.",
            "The average length of an input sequence in DCS10K is 40.79 characters.",
            "We do not modify our inference procedure in system 8 other than to take care of the possibility that a clique need not always be returned.",
            "Table 2 shows the results for different values of k.",
            "Interestingly, the results show a monotonic increase with the increase in context window, and the results with the entire context are still better than those with k = 20, even though only marginally.",
            "It is interesting to note that, keeping the entire context does not adversely affect the predictions as none of the pruned models outperforms System 8."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "k"
            ],
            [
                "k",
                "5",
                "10",
                "15",
                "20"
            ],
            [
                "k",
                "20"
            ],
            null,
            null,
            [
                "k",
                "5",
                "10",
                "15",
                "20"
            ],
            [
                "k",
                "5",
                "10",
                "15",
                "20"
            ],
            null
        ],
        "n_sentence": 8.0,
        "table_id": "table_2",
        "paper_id": "D18-1276",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1281table_2",
        "description": "Table 2 shows the results with and without regressor. The regressor significantly improved the accuracy with high IoU thresholds, which demonstrates that the regressor improved the localization accuracy. In addition, the accuracy did not decrease as a result of sharing the hidden layer or reducing the number of units in the hidden layer. This suggests that the regressor lies in a very low-dimensional manifold because the regressor for one concept can be shared by many concepts (e.g., the person regressor can be used for man, woman, girl, boy, etc.). The number of parameters was significantly reduced by these tricks, to even fewer than in the linear transformation. The accuracy slightly decreased with a threshold of 0.5, because the regressor was not learned properly for the categories that did not frequently appear in the training data.",
        "sentences": [
            "Table 2 shows the results with and without regressor.",
            "The regressor significantly improved the accuracy with high IoU thresholds, which demonstrates that the regressor improved the localization accuracy.",
            "In addition, the accuracy did not decrease as a result of sharing the hidden layer or reducing the number of units in the hidden layer.",
            "This suggests that the regressor lies in a very low-dimensional manifold because the regressor for one concept can be shared by many concepts (e.g., the person regressor can be used for man, woman, girl, boy, etc.).",
            "The number of parameters was significantly reduced by these tricks, to even fewer than in the linear transformation.",
            "The accuracy slightly decreased with a threshold of 0.5, because the regressor was not learned properly for the categories that did not frequently appear in the training data."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "w/o regression",
                "300-16(-4096)",
                "300-64(-4096)",
                "300-256(-4096)",
                "300-1024(-4096)",
                "300(-256-4096)",
                "300-4096"
            ],
            [
                "300-16(-4096)",
                "300-64(-4096)",
                "300-256(-4096)",
                "300-1024(-4096)",
                "300(-256-4096)",
                "300-4096",
                "IoU",
                "0.6",
                "0.7",
                "0.8",
                "0.9"
            ],
            [
                "300(-256-4096)",
                "300-4096"
            ],
            [
                "300-16(-4096)",
                "300-64(-4096)",
                "300-256(-4096)",
                "300-1024(-4096)",
                "300(-256-4096)",
                "300-4096"
            ],
            [
                "Params",
                "300-4096",
                "300(-256-4096)"
            ],
            [
                "w/o regression",
                "300-16(-4096)",
                "300-64(-4096)",
                "300-256(-4096)",
                "300-1024(-4096)",
                "300(-256-4096)",
                "300-4096",
                "IoU",
                "0.5"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_2",
        "paper_id": "D18-1281",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1303table_1",
        "description": "5 Results. See Table 1 for single-label results on the selected harassment categories, where CNN-RNN was the best performing model compared to several nonneural and neural baselines.",
        "sentences": [
            "5 Results.",
            "See Table 1 for single-label results on the selected harassment categories, where CNN-RNN was the best performing model compared to several nonneural and neural baselines."
        ],
        "class_sentence": [
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "CNN-RNN",
                "Commenting",
                "Ogling",
                "Groping"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_1",
        "paper_id": "D18-1303",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1349table_6",
        "description": "Table 6 and 7 detail the results of classification for each label in terms of performance scores (precision, recall and F1) and confusion matrix, respectively (for our HSLN-RNN model trained on the PubMed 20k dataset). These show that the classifier is very good at predicting the labels Methods, Results and Conclusions, whereas the greatest difficulty the classifier has is in distinguishing Background sections from Objectives sections. One fifth of Background sentences are incorrectly classified as Objectives, while around one forth of Objectives sentences are wrongly assigned to the label of Background. We conjecture this difficulty mainly comes from the fact that the difference between Background and Objectives sentences in terms of writing style is less obvious compared with the other sections of the abstract. Moreover, our model has some difficulty in telling Methods sentences apart from Results sentences.",
        "sentences": [
            "Table 6 and 7 detail the results of classification for each label in terms of performance scores (precision, recall and F1) and confusion matrix, respectively (for our HSLN-RNN model trained on the PubMed 20k dataset).",
            "These show that the classifier is very good at predicting the labels Methods, Results and Conclusions, whereas the greatest difficulty the classifier has is in distinguishing Background sections from Objectives sections.",
            "One fifth of Background sentences are incorrectly classified as Objectives, while around one forth of Objectives sentences are wrongly assigned to the label of Background.",
            "We conjecture this difficulty mainly comes from the fact that the difference between Background and Objectives sentences in terms of writing style is less obvious compared with the other sections of the abstract.",
            "Moreover, our model has some difficulty in telling Methods sentences apart from Results sentences."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            [
                "Background",
                "Objectives",
                "Methods",
                "Results",
                "Conclusions",
                "P",
                "R",
                "F1"
            ],
            [
                "Methods",
                "Results",
                "Conclusions",
                "Background",
                "Objectives"
            ],
            [
                "Background",
                "Objectives"
            ],
            [
                "Background",
                "Objectives"
            ],
            [
                "Methods",
                "Results"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_6",
        "paper_id": "D18-1349",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1351table_4",
        "description": "4.2 Topic Coherence Comparison In Section 4.1, we find that TMN can significantly outperform comparison models on short text classification. In this section, we study whether jointly learning topic models and classification can be helpful in producing coherent and meaningful topics. We use the CV metric (R\u00c2\u00a8oder et al., 2015) computed by Palmetto toolkit12 to evaluate the topic coherence, which has been shown to give the closest scores to human evaluation compared to other widely-used topic coherence metrics like NPMI (Bouma, 2009). Table 4 shows the comparison results of LDA, BTM, NTM, and TMN on the three English datasets. Note that we do not report CV scores for Chinese Weibo dataset as the Palmetto toolkit cannot process Chinese topics. As can be seen, TMN yields higher CV scores by large margins than all others in comparison. This indicates that jointly exploring classification would be effective in producing coherent topics. The reason is that the supervision from classification labels can guide unsupervised topic models in discovering meaningful and interpretable topics. We also observe that NTM produces better results than LDA and BTM, which implies the effectiveness of inducing topic models by neural networks.",
        "sentences": [
            "4.2 Topic Coherence Comparison In Section 4.1, we find that TMN can significantly outperform comparison models on short text classification.",
            "In this section, we study whether jointly learning topic models and classification can be helpful in producing coherent and meaningful topics.",
            "We use the CV metric (R\u00c2\u00a8oder et al., 2015) computed by Palmetto toolkit12 to evaluate the topic coherence, which has been shown to give the closest scores to human evaluation compared to other widely-used topic coherence metrics like NPMI (Bouma, 2009).",
            "Table 4 shows the comparison results of LDA, BTM, NTM, and TMN on the three English datasets.",
            "Note that we do not report CV scores for Chinese Weibo dataset as the Palmetto toolkit cannot process Chinese topics.",
            "As can be seen, TMN yields higher CV scores by large margins than all others in comparison.",
            "This indicates that jointly exploring classification would be effective in producing coherent topics.",
            "The reason is that the supervision from classification labels can guide unsupervised topic models in discovering meaningful and interpretable topics.",
            "We also observe that NTM produces better results than LDA and BTM, which implies the effectiveness of inducing topic models by neural networks."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            2,
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "LDA",
                "BTM",
                "NTM",
                "TMN",
                "Snippets",
                "TagMyNews",
                "Twitter"
            ],
            null,
            [
                "TMN"
            ],
            [
                "TMN"
            ],
            null,
            [
                "NTM",
                "LDA",
                "BTM"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_4",
        "paper_id": "D18-1351",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1362table_3",
        "description": "5.2 Ablation Study. We perform an ablation study where we remove reward shaping (\u2212RS) and action dropout (\u2212AD) from Ours(ConvE) and compare their MRRs to the whole model on the dev sets. As shown in Table 3, on most datasets, removing each component results in a significant performance drop. The exception is WN18RR, where removing the ConvE reward shaping module improves the performance. Removing reward shaping on NELL995 does not change the results significantly. In general, removing action dropout has a greater impact, suggesting that thorough exploration of the path space is important across datasets.",
        "sentences": [
            "5.2 Ablation Study.",
            "We perform an ablation study where we remove reward shaping (\u2212RS) and action dropout (\u2212AD) from Ours(ConvE) and compare their MRRs to the whole model on the dev sets.",
            "As shown in Table 3, on most datasets, removing each component results in a significant performance drop.",
            "The exception is WN18RR, where removing the ConvE reward shaping module improves the performance.",
            "Removing reward shaping on NELL995 does not change the results significantly.",
            "In general, removing action dropout has a greater impact, suggesting that thorough exploration of the path space is important across datasets."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Ours(ConvE)",
                "-RS",
                "-AD"
            ],
            [
                "-RS",
                "-AD"
            ],
            [
                "WN18RR",
                "-RS",
                "Ours(ConvE)"
            ],
            [
                "NELL995",
                "-RS",
                "Ours(ConvE)"
            ],
            [
                "-AD"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_3",
        "paper_id": "D18-1362",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1363table_3",
        "description": "The input to a multi-source model is the concatenation of all sources and corresponding tags. During training, we randomly sample (with repetition) one or three additional forms from the paradigm of each example. At test time, we sample the additional forms from the given partial paradigm; without repetition first, but repeating if not enough inflected forms are available. For autoencoding examples in the training data, we simply concatenate two or four copies of the source and the autoencoding tag. We randomly select five languages for this experiment. Table 3 shows that, for SET3, four sources (column header\"4\") are generally better than two sources (\"2\"), which in turn are better than one source (\"1\"); thus, as expected, making additional sources available in training improves results. We attribute one exception (German accuracy is .4391 for \"1\" and .4179 for \"2\") to the noisiness of the problem training sets in terms of number of paradigms are relatively small, even for SET3.",
        "sentences": [
            "The input to a multi-source model is the concatenation of all sources and corresponding tags.",
            "During training, we randomly sample (with repetition) one or three additional forms from the paradigm of each example.",
            "At test time, we sample the additional forms from the given partial paradigm; without repetition first, but repeating if not enough inflected forms are available.",
            "For autoencoding examples in the training data, we simply concatenate two or four copies of the source and the autoencoding tag.",
            "We randomly select five languages for this experiment.",
            "Table 3 shows that, for SET3, four sources (column header\"4\") are generally better than two sources (\"2\"), which in turn are better than one source (\"1\"); thus, as expected, making additional sources available in training improves results.",
            "We attribute one exception (German accuracy is .4391 for \"1\" and .4179 for \"2\") to the noisiness of the problem training sets in terms of number of paradigms are relatively small, even for SET3."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null,
            [
                "SET3",
                "1",
                "2",
                "4"
            ],
            [
                "german"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_3",
        "paper_id": "D18-1363",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1368table_2",
        "description": "Table 4 shows cross-genre experimental results of our neural network models on the training set of MASC+Wiki by treating each genre as one crossvalidation fold. As we expected, both the macroaverage F1-score and class-wise F1 scores are lower compared with the results in Table 2 where in-genre data were used for model training as well. But the performance drop on the paragraph-level models is little, which clearly outperform the previous system (Friedrich et al., 2016) and the baseline model by a large margin. As shown in Table 5, benefited from modeling wider contexts and common SE label patterns, our full paragraphlevel model improves performance across almost all the genres. The high performance in the crossgenre setting demonstrates the robustness of our paragraph-level model across genres. Table 2 reports the cross-validation classification results. Consistently, our clause-level baseline model already outperforms the previous best model. By exploiting paragraph-wide contexts, the basic paragraph-level model obtains consistent performance improvements across all the classes compared with the baseline clause-level prediction model, especially for the classes GENERIC and GENERALIZING, where the improvements are significant. After using the CRF layer to fine-tune the predicted SE label sequence, slight performance improvements were observed on the four small classes. Overall, the full paragraphlevel neural network model achieves the best macro-average F1-score of 77.8% in predicting SE types, which not only outperforms all previous approaches but also reaches human-like performance on some classes.",
        "sentences": [
            "Table 4 shows cross-genre experimental results of our neural network models on the training set of MASC+Wiki by treating each genre as one crossvalidation fold.",
            "As we expected, both the macroaverage F1-score and class-wise F1 scores are lower compared with the results in Table 2 where in-genre data were used for model training as well.",
            "But the performance drop on the paragraph-level models is little, which clearly outperform the previous system (Friedrich et al., 2016) and the baseline model by a large margin.",
            "As shown in Table 5, benefited from modeling wider contexts and common SE label patterns, our full paragraphlevel model improves performance across almost all the genres.",
            "The high performance in the crossgenre setting demonstrates the robustness of our paragraph-level model across genres.",
            "Table 2 reports the cross-validation classification results.",
            "Consistently, our clause-level baseline model already outperforms the previous best model.",
            "By exploiting paragraph-wide contexts, the basic paragraph-level model obtains consistent performance improvements across all the classes compared with the baseline clause-level prediction model, especially for the classes GENERIC and GENERALIZING, where the improvements are significant.",
            "After using the CRF layer to fine-tune the predicted SE label sequence, slight performance improvements were observed on the four small classes.",
            "Overall, the full paragraphlevel neural network model achieves the best macro-average F1-score of 77.8% in predicting SE types, which not only outperforms all previous approaches but also reaches human-like performance on some classes."
        ],
        "class_sentence": [
            0,
            2,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null,
            null,
            [
                "Clause-level Bi-LSTM",
                "CRF (Friedrich et al. 2016)"
            ],
            [
                "Clause-level Bi-LSTM",
                "Paragraph-level Model",
                "F1 GENI",
                "F1 GENA"
            ],
            [
                "Paragraph-level Model",
                "Paragraph-level Model+CRF",
                "F1 REP",
                "F1 GENI",
                "F1 GENA",
                "F1 IMP"
            ],
            [
                "Paragraph-level Model+CRF",
                "Macro",
                "Humans"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_2",
        "paper_id": "D18-1368",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1369table_2",
        "description": "7.5 Results. Table 2 shows the clustering accuracy of our method and the baseline methods in real world datasets. We average the results with five runs for each model. The highest value for each metric is indicated with boldface. From the results, we establish that our model outperforms the baseline methods in both the NYT narrative reconstruction task and the Wikipedia thread reconstruction task. For the NYT, to see the accuracy of our model in more detail, we compute and show the F-scores for the top ten most frequent labels and the micro and macro averages in table 3. To compute the F-score between the true labels and the recovered cluster labels, we select the cluster with the highest F-score as the corresponding cluster. From the results, we establish that our model performs better than the baseline model, HDHP.",
        "sentences": [
            "7.5 Results.",
            "Table 2 shows the clustering accuracy of our method and the baseline methods in real world datasets.",
            "We average the results with five runs for each model.",
            "The highest value for each metric is indicated with boldface.",
            "From the results, we establish that our model outperforms the baseline methods in both the NYT narrative reconstruction task and the Wikipedia thread reconstruction task.",
            "For the NYT, to see the accuracy of our model in more detail, we compute and show the F-scores for the top ten most frequent labels and the micro and macro averages in table 3.",
            "To compute the F-score between the true labels and the recovered cluster labels, we select the cluster with the highest F-score as the corresponding cluster.",
            "From the results, we establish that our model performs better than the baseline model, HDHP."
        ],
        "class_sentence": [
            2,
            1,
            2,
            2,
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            [
                "HD-GMHP (100D)",
                "NYT",
                "Wiki"
            ],
            [
                "HD-GMHP (100D)",
                "NYT"
            ],
            null,
            [
                "HD-GMHP (100D)"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_2",
        "paper_id": "D18-1369",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1381table_2",
        "description": "4.2 Experimental Results. Table 1 and Table 2 report the results of our experiments. The results on TRAIN-ALL are higher than TRAIN for SemEval16 in lieu of the larger dataset. Firstly, we observe that our proposed AGLR outperforms all neural baselines on 3-way classification. The overall performance of AGLR achieves state-of-the-art performance. On average, AGLR outperforms Lexicon RNN and AT-BiLSTM by 1% \u00e2\u02c6\u2019 3% in terms of F1 score. We also observe that AGLR always improves AT-BiLSTM which ascertains the effectiveness of learning auxiliary lexicon embeddings. The key idea here is that the auxiliary lexicon embeddings provide a different view of the sentence which supports the network in making predictions. We also observe that Lexicon RNN does not handle 3-way classification well. Even though it has achieved good performance on binary classification, the performance on 3-way classification is lackluster (the performance of AGLR outperforms Lexicon RNN by up to 8% on SemEval16 TRAIN). This could also be attributed to the MSE based loss function. Conversely, by learning an auxiliary embedding (instead of a scalar score), our model becomes more flexible at the final layer and can be adapted to using a k softmax function. Finally, we observe that BiLSTM and AT-BiLSTM outperform Lexicon RNN on average with Lexicon RNN being slightly better on binary classification.",
        "sentences": [
            "4.2 Experimental Results.",
            "Table 1 and Table 2 report the results of our experiments.",
            "The results on TRAIN-ALL are higher than TRAIN for SemEval16 in lieu of the larger dataset.",
            "Firstly, we observe that our proposed AGLR outperforms all neural baselines on 3-way classification.",
            "The overall performance of AGLR achieves state-of-the-art performance.",
            "On average, AGLR outperforms Lexicon RNN and AT-BiLSTM by 1% \u00e2\u02c6\u2019 3% in terms of F1 score.",
            "We also observe that AGLR always improves AT-BiLSTM which ascertains the effectiveness of learning auxiliary lexicon embeddings.",
            "The key idea here is that the auxiliary lexicon embeddings provide a different view of the sentence which supports the network in making predictions.",
            "We also observe that Lexicon RNN does not handle 3-way classification well.",
            "Even though it has achieved good performance on binary classification, the performance on 3-way classification is lackluster (the performance of AGLR outperforms Lexicon RNN by up to 8% on SemEval16 TRAIN).",
            "This could also be attributed to the MSE based loss function.",
            "Conversely, by learning an auxiliary embedding (instead of a scalar score), our model becomes more flexible at the final layer and can be adapted to using a k softmax function.",
            "Finally, we observe that BiLSTM and AT-BiLSTM outperform Lexicon RNN on average with Lexicon RNN being slightly better on binary classification."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            2,
            1,
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Sem2016 (TRAIN)",
                "Sem2016 (TRAIN-ALL)"
            ],
            [
                "AGLR",
                "3-way"
            ],
            [
                "AGLR"
            ],
            [
                "AGLR",
                "AT-BiLSTM",
                "Lexicon RNN",
                "AVG",
                "F1"
            ],
            [
                "AGLR",
                "AT-BiLSTM"
            ],
            null,
            [
                "Lexicon RNN",
                "3-way"
            ],
            [
                "Lexicon RNN",
                "Binary",
                "AGLR",
                "Sem2016 (TRAIN)"
            ],
            null,
            [
                "AGLR"
            ],
            [
                "BiLSTM",
                "AT-BiLSTM",
                "Lexicon RNN",
                "Binary",
                "AVG"
            ]
        ],
        "n_sentence": 13.0,
        "table_id": "table_2",
        "paper_id": "D18-1381",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1396table_4",
        "description": "Table 4 shows the BLEU score on the En-Jp test set. It can be observed that regardless of decoding direction (i.e., from left-to-right or from right-toleft) and with or without teacher forcing, the accuracy of the right half is always higher than that in the left half. This observation on Japanese is opposite to English, German and Chinese in Section3.1and3.2, and motivates us to investigate the differences between these languages.",
        "sentences": [
            "Table 4 shows the BLEU score on the En-Jp test set.",
            "It can be observed that regardless of decoding direction (i.e., from left-to-right or from right-toleft) and with or without teacher forcing, the accuracy of the right half is always higher than that in the left half.",
            "This observation on Japanese is opposite to English, German and Chinese in Section3.1and3.2, and motivates us to investigate the differences between these languages."
        ],
        "class_sentence": [
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "right"
            ],
            null
        ],
        "n_sentence": 3.0,
        "table_id": "table_4",
        "paper_id": "D18-1396",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1400table_3",
        "description": "We then evaluate all models on the IKEA dataset. Table 3 shows the results. Our VAG-NMT has a higher value in BLEU and a comparable value in METEOR compared to the Text-only NMT baseline. Our VAG-NMT outperforms LIUMCVC's multimodal system by a large margin, which shows that the LIUMCVC's multimodal's good performance on Multi30K does not generalize to this real-world product dataset. The good performance may come from the visual attention mechanism that learns to focus on text segments that are related to the images. Such attention therefore teaches the decoder to apply the visual context to translate those words. This learned attention is especially useful for datasets with long sentences that have irrelevant text information with respect to the image.",
        "sentences": [
            "We then evaluate all models on the IKEA dataset.",
            "Table 3 shows the results.",
            "Our VAG-NMT has a higher value in BLEU and a comparable value in METEOR compared to the Text-only NMT baseline.",
            "Our VAG-NMT outperforms LIUMCVC's multimodal system by a large margin, which shows that the LIUMCVC's multimodal's good performance on Multi30K does not generalize to this real-world product dataset.",
            "The good performance may come from the visual attention mechanism that learns to focus on text segments that are related to the images.",
            "Such attention therefore teaches the decoder to apply the visual context to translate those words.",
            "This learned attention is especially useful for datasets with long sentences that have irrelevant text information with respect to the image."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "VAG-NMT",
                "Text-Only NMT",
                "BLEU",
                "METEOR"
            ],
            [
                "VAG-NMT",
                "LIUMCVC-Multi"
            ],
            null,
            null,
            null
        ],
        "n_sentence": 7.0,
        "table_id": "table_3",
        "paper_id": "D18-1400",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1401table_2",
        "description": "Table 2 summarizes the experimental results of all the approaches above, and we can find that:. (1) All LSTM-based approaches are superior to SVM, indicating the effectiveness of neural network for this task. (2) The proposed approaches, with novel QA contextual representation, outperform the other baseline approaches. (3) When only employing QA bidirectional matching mechanism, Bidirectional-Match QA, which takes the sentence segmentation strategy, consistently outperforms Bidirectional-Match (without sentence segmentation) in all domains. It confirms our hypothesis that sentence segmentation helps to extract the sentiment matching information between the question and answer. (4) When comparing to QA unidirectional matching mechanism, Bidirectional-Match QA, which employs QA bidirectional matching mechanism, performs better than AtoQMatch and QtoA-Match. It confirms our hypothesis that both the question and answer information contribute to sentiment polarity of the QA text pair. (5) Impressively, the proposed approach HMN significantly outperforms all the other approaches in all domains (p-value<0.05 via t-test). It verifies the advantages of both QA bidirectional matching mechanism and selfmatching attention mechanism for this task.",
        "sentences": [
            "Table 2 summarizes the experimental results of all the approaches above, and we can find that:.",
            "(1) All LSTM-based approaches are superior to SVM, indicating the effectiveness of neural network for this task.",
            "(2) The proposed approaches, with novel QA contextual representation, outperform the other baseline approaches.",
            "(3) When only employing QA bidirectional matching mechanism, Bidirectional-Match QA, which takes the sentence segmentation strategy, consistently outperforms Bidirectional-Match (without sentence segmentation) in all domains.",
            "It confirms our hypothesis that sentence segmentation helps to extract the sentiment matching information between the question and answer.",
            "(4) When comparing to QA unidirectional matching mechanism, Bidirectional-Match QA, which employs QA bidirectional matching mechanism, performs better than AtoQMatch and QtoA-Match.",
            "It confirms our hypothesis that both the question and answer information contribute to sentiment polarity of the QA text pair.",
            "(5) Impressively, the proposed approach HMN significantly outperforms all the other approaches in all domains (p-value<0.05 via t-test).",
            "It verifies the advantages of both QA bidirectional matching mechanism and selfmatching attention mechanism for this task."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "SVM",
                "LSTM",
                "Bi-LSTM"
            ],
            [
                "HMN"
            ],
            [
                "Bidirectional-Match",
                "Bidirectional-Match QA"
            ],
            null,
            [
                "AtoQ-Match",
                "QtoA-Match",
                "Bidirectional-Match QA"
            ],
            null,
            [
                "HMN"
            ],
            [
                "HMN"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_2",
        "paper_id": "D18-1401",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1408table_2",
        "description": "Experiment results of our model and four baselines are shown in Table 2. Micro and macro accuracies are two composite indicators for evaluating transfer performance of tasks whose metric is classification accuracy. Macro accuracy is the proportion of true results in the population of instances from all tasks. Micro accuracy is the arithmetic mean of dev accuracies for each task. PSAN achieves the state-of-the-art performance with considerably fewer parameters, outperforming a RNN-based model, a CNN-based model, a fully attention-based model and a model that utilize syntactic information. Especially when compared with previous best model BiLSTM-Max, PSAN can outperform their model with only 5% of their parameter numbers, demonstrating the effectiveness of our model at extracting semantically important information from a sentence.",
        "sentences": [
            "Experiment results of our model and four baselines are shown in Table 2.",
            "Micro and macro accuracies are two composite indicators for evaluating transfer performance of tasks whose metric is classification accuracy.",
            "Macro accuracy is the proportion of true results in the population of instances from all tasks.",
            "Micro accuracy is the arithmetic mean of dev accuracies for each task.",
            "PSAN achieves the state-of-the-art performance with considerably fewer parameters, outperforming a RNN-based model, a CNN-based model, a fully attention-based model and a model that utilize syntactic information.",
            "Especially when compared with previous best model BiLSTM-Max, PSAN can outperform their model with only 5% of their parameter numbers, demonstrating the effectiveness of our model at extracting semantically important information from a sentence."
        ],
        "class_sentence": [
            1,
            2,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            [
                "Model"
            ],
            [
                "Micro",
                "Macro"
            ],
            [
                "Macro"
            ],
            [
                "Micro"
            ],
            [
                "PSAN"
            ],
            [
                "PSAN",
                "BiLSTM-Max"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_2",
        "paper_id": "D18-1408",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1408table_3",
        "description": "In Table 3, we compare our model with baseline sentence encoders in each transfer task. PSAN can consistently outperform the baselines in almost every task considered.",
        "sentences": [
            "In Table 3, we compare our model with baseline sentence encoders in each transfer task.",
            "PSAN can consistently outperform the baselines in almost every task considered."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "PSAN"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_3",
        "paper_id": "D18-1408",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1417table_2",
        "description": "4.4 Independent Learning. The results of separate training for slot filling and intent detection are reported in Table 1 and Table 2 respectively. On the independent slot filling task, we fixed the intent information as the ground truth labels in the dataset. But on the independent intent detection task, there is no interaction with slot labels. Table 2 compares the performance of our proposed model to previously reported results on intent detection task. Our model gives good performance in terms of classification error rate, but not as good as Attention Encoder-Decoder (with aligned inputs) method (Liu and Lane, 2016a). As their published state-of-the-art result described in (Liu and Lane, 2016a), their attention-based model is based on word-level embeddings. While in our model, we introduce character-level embeddings to improve the performance of joint learning. But independent learning for intent classification aims at capturing the global information of an utterance, not caring much about the details of specific word. The character-level embeddings introduced in our model bring very little hurt to independent learning of intent detection, as a trade-off in performance between both criterion.",
        "sentences": [
            "4.4 Independent Learning.",
            "The results of separate training for slot filling and intent detection are reported in Table 1 and Table 2 respectively.",
            "On the independent slot filling task, we fixed the intent information as the ground truth labels in the dataset.",
            "But on the independent intent detection task, there is no interaction with slot labels.",
            "Table 2 compares the performance of our proposed model to previously reported results on intent detection task.",
            "Our model gives good performance in terms of classification error rate, but not as good as Attention Encoder-Decoder (with aligned inputs) method (Liu and Lane, 2016a).",
            "As their published state-of-the-art result described in (Liu and Lane, 2016a), their attention-based model is based on word-level embeddings.",
            "While in our model, we introduce character-level embeddings to improve the performance of joint learning.",
            "But independent learning for intent classification aims at capturing the global information of an utterance, not caring much about the details of specific word.",
            "The character-level embeddings introduced in our model bring very little hurt to independent learning of intent detection, as a trade-off in performance between both criterion."
        ],
        "class_sentence": [
            2,
            1,
            2,
            0,
            1,
            1,
            2,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            [
                "Our Model"
            ],
            [
                "Our Model",
                "Attention Enc-Dec (Liu and Lane 2016a)",
                "Error(%)"
            ],
            [
                "Attention Enc-Dec (Liu and Lane 2016a)"
            ],
            [
                "Our Model"
            ],
            null,
            [
                "Our Model"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_2",
        "paper_id": "D18-1417",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1417table_3",
        "description": "Table 3 compares our joint model with reported results from previous works. We can see that our model achieves state-of-the-art results and outperforms previous best result by 0.54% in terms of F1-score on slot filling, and by 0.34% in terms of error rate on intent detection. This improvement is statistically significant. Besides, the joint learning achieves better results than separate learning. It can be interpreted that the two tasks are highly correlated and boost the performance each other. The slot filling task enables the model to learn more meaningful representations which give more supervisory signals for the learning of shared parameters. Similarly, intent is also useful to determine the slot label.",
        "sentences": [
            "Table 3 compares our joint model with reported results from previous works.",
            "We can see that our model achieves state-of-the-art results and outperforms previous best result by 0.54% in terms of F1-score on slot filling, and by 0.34% in terms of error rate on intent detection.",
            "This improvement is statistically significant.",
            "Besides, the joint learning achieves better results than separate learning.",
            "It can be interpreted that the two tasks are highly correlated and boost the performance each other.",
            "The slot filling task enables the model to learn more meaningful representations which give more supervisory signals for the learning of shared parameters.",
            "Similarly, intent is also useful to determine the slot label."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            [
                "Our Model"
            ],
            [
                "Our Model",
                "F1",
                "Error(%)"
            ],
            [
                "Our Model"
            ],
            null,
            null,
            null,
            null
        ],
        "n_sentence": 7.0,
        "table_id": "table_3",
        "paper_id": "D18-1417",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1421table_3",
        "description": "Table 3 shows the performances on the Twitter corpus. Our models again outperform the baselines in terms of all the evaluation measures. Note that RbM-IRL performs better than RbM-SL in this case. The reason might be that the evaluator of RbM-SL might not be effectively trained with the relatively small dataset, while RbM-IRL can leverage its advantage in learning of the evaluator with less data.",
        "sentences": [
            "Table 3 shows the performances on the Twitter corpus.",
            "Our models again outperform the baselines in terms of all the evaluation measures.",
            "Note that RbM-IRL performs better than RbM-SL in this case.",
            "The reason might be that the evaluator of RbM-SL might not be effectively trained with the relatively small dataset, while RbM-IRL can leverage its advantage in learning of the evaluator with less data."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "RbM-SL (ours)",
                "RbM-IRL (ours)"
            ],
            [
                "RbM-IRL (ours)",
                "RbM-SL (ours)"
            ],
            [
                "RbM-SL (ours)",
                "RbM-IRL (ours)"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "D18-1421",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1448table_2",
        "description": "Table 2 shows our results for user satisfaction test. User ratings of pictorial summaries are 12.4% higher than text summaries. It shows that users prefer this way of presenting information. It also confirms our motivation for MSMO.",
        "sentences": [
            "Table 2 shows our results for user satisfaction test.",
            "User ratings of pictorial summaries are 12.4% higher than text summaries.",
            "It shows that users prefer this way of presenting information.",
            "It also confirms our motivation for MSMO."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Text",
                "Pictorial"
            ],
            [
                "Pictorial"
            ],
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "D18-1448",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1486table_3",
        "description": "Multi-Task Performance. We simultaneously train our model McapsNet on six tasks in Table 1 and compare it with singletask scenario (Table 3). We can see that our multitask architecture clearly improves the performance over the single task models, which demonstrates the benefits of our multi-task architecture. As Table 3 shows, MCapsNet also outperforms the state-of-the-art multi-task learning models by at least 1.1%. This shows the advantages of our task routing algorithm, which can cluster the features for each task, instead of freely sharing the features among tasks.",
        "sentences": [
            "Multi-Task Performance.",
            "We simultaneously train our model McapsNet on six tasks in Table 1 and compare it with singletask scenario (Table 3).",
            "We can see that our multitask architecture clearly improves the performance over the single task models, which demonstrates the benefits of our multi-task architecture.",
            "As Table 3 shows, MCapsNet also outperforms the state-of-the-art multi-task learning models by at least 1.1%.",
            "This shows the advantages of our task routing algorithm, which can cluster the features for each task, instead of freely sharing the features among tasks."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "MCapsNet"
            ],
            [
                "MCapsNet"
            ],
            [
                "MCapsNet",
                "MT-GRNN",
                "MT-RNN",
                "MT-DNN",
                "MT-CNN"
            ],
            [
                "MCapsNet"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_3",
        "paper_id": "D18-1486",
        "valid": 1
    },
    {
        "table_id_paper": "D18-1533table_1",
        "description": "Table 1 reports the results of the approaches we described in Section 4. Out of the Kneser-Ney n-gram models, we found that the FST-C-9GRAMKN and the version modelling word boundaries (FST-Cwb-9GRAM-KN) to perform best on the synthetic parallel corpus and newspapers, respectively. Cwb was not applied to the synthetic parallel corpus as we did not model word splitting. The hybrid models (FST-RNNLM) outperformed all FST-only approaches. Thus, we found the wordsplitting hybrid model (FST-RNNLM-Cwb) to be the best performing model overall (Table 1).",
        "sentences": [
            "Table 1 reports the results of the approaches we described in Section 4.",
            "Out of the Kneser-Ney n-gram models, we found that the FST-C-9GRAMKN and the version modelling word boundaries (FST-Cwb-9GRAM-KN) to perform best on the synthetic parallel corpus and newspapers, respectively.",
            "Cwb was not applied to the synthetic parallel corpus as we did not model word splitting.",
            "The hybrid models (FST-RNNLM) outperformed all FST-only approaches.",
            "Thus, we found the wordsplitting hybrid model (FST-RNNLM-Cwb) to be the best performing model overall (Table 1)."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "FST-(C/Cwb)-9GRAM-KN",
                "Newspaper 1",
                "Newspaper 2",
                "Corpus"
            ],
            null,
            [
                "FST-RNNLM-(C/Cwb)"
            ],
            [
                "FST-RNNLM-(C/Cwb)"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_1",
        "paper_id": "D18-1533",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1037table_4",
        "description": "Table 4 gives another comparison using AUC with all p-values less than 5e-02 from t-test evaluation. The results indicate that the larger AUC, the better performance. A simple ensemble model of two networks (AUC: 0.371) has a similar result as a single model (NetAtt, AUC: 0.368). The main purpose of adding an additional student network is to introduce con\u00ef\u00ac\u201aicts and to build the collaborative curriculum learning scheme. With the CCL strategies, our models improve performances by removing 'hard' (noisy) entity pair bags during.",
        "sentences": [
            "Table 4 gives another comparison using AUC with all p-values less than 5e-02 from t-test evaluation.",
            "The results indicate that the larger AUC, the better performance.",
            "A simple ensemble model of two networks (AUC: 0.371) has a similar result as a single model (NetAtt, AUC: 0.368).",
            "The main purpose of adding an additional student network is to introduce con\u00ef\u00ac\u201aicts and to build the collaborative curriculum learning scheme.",
            "With the CCL strategies, our models improve performances by removing 'hard' (noisy) entity pair bags during."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "+SelfAtt"
            ],
            null,
            [
                "CCL-CT"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_4",
        "paper_id": "D19-1037",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1068table_3",
        "description": "Exploring the Syntactic Order Event Detector. We compare our syntactic order event detector (CL Trans GCN) with several event detectors, including 1) CL Trans MLP, which employs a feed-forward network as event detector; 2) CL Trans CNN, which uses CNNs as the event detector; 3) CL Trans Hbrid, which use a hybrid network (Feng et al., 2016) for event detection. We also compared our model with several variants including 4) CL Trans Self., which replaces the GCNs with a self-attention network, and 5) CL Trans GCN Self, which combines GCNs with a self-attention network. We train these models on the same translated English data. Table 3 shows the results. From the results, CL Trans MLP, CL Trans CNN, and CL Trans Hbrid behave poorly, as expected. The reason might be that these models usually employ order-sensitive structures (e.g., CNNs) for ED, which would suffer the word order inconsistency problem when trained on the translated data. CL Trans Self. yields relatively good performance. The reason might be that self-attention network could provide each word with a feature vector based on all the words of a sentence, which is also irrespective of the words\u00e2\u20ac\u2122 positions in a sentence. This could address the word order difference to some extent. Our syntactic order event detector yields the best performance.",
        "sentences": [
            "Exploring the Syntactic Order Event Detector.",
            "We compare our syntactic order event detector (CL Trans GCN) with several event detectors, including 1) CL Trans MLP, which employs a feed-forward network as event detector; 2) CL Trans CNN, which uses CNNs as the event detector; 3) CL Trans Hbrid, which use a hybrid network (Feng et al., 2016) for event detection.",
            "We also compared our model with several variants including 4) CL Trans Self., which replaces the GCNs with a self-attention network, and 5) CL Trans GCN Self, which combines GCNs with a self-attention network.",
            "We train these models on the same translated English data.",
            "Table 3 shows the results.",
            "From the results, CL Trans MLP, CL Trans CNN, and CL Trans Hbrid behave poorly, as expected.",
            "The reason might be that these models usually employ order-sensitive structures (e.g., CNNs) for ED, which would suffer the word order inconsistency problem when trained on the translated data.",
            "CL Trans Self. yields relatively good performance.",
            "The reason might be that self-attention network could provide each word with a feature vector based on all the words of a sentence, which is also irrespective of the words\u00e2\u20ac\u2122 positions in a sentence.",
            "This could address the word order difference to some extent.",
            "Our syntactic order event detector yields the best performance."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            1,
            1,
            2,
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null,
            [
                "CL_Trans_MLP",
                "CL_Trans_CNN",
                "CL_Trans_Hbrid"
            ],
            null,
            [
                "CL_Trans_Self."
            ],
            null,
            null,
            [
                "CL_Trans_GCN (ours)"
            ]
        ],
        "n_sentence": 11.0,
        "table_id": "table_3",
        "paper_id": "D19-1068",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1070table_8",
        "description": "For PROPARA, Table 5 shows that the model does not significantly outperform the SOTA models in state change detection (Cat-1). However, for those correctly detected events, the transformer model outperforms the previous models for detecting the exact step of state change (Cat-2), primarily based on verb semantics. We do a finer-grained study in Table 8 by breaking down the performance for the three state changes: creation (C), movement (M), and destruction (D), separately. Across the three state changes, the model suffers a loss of performance in the movement cases. This is owing to the fact that the movement cases require a deeper compositional and implicit event tracking. Also, a majority of errors leading to false negatives are due to the the formation of new sub-entities which are then mentioned with other names. For example, when talking about weak acid in \"he water becomes a weak acid. the water dissolves limestone\"\u009d the weak acid is also considered to move to the limestone.",
        "sentences": [
            "For PROPARA, Table 5 shows that the model does not significantly outperform the SOTA models in state change detection (Cat-1).",
            "However, for those correctly detected events, the transformer model outperforms the previous models for detecting the exact step of state change (Cat-2), primarily based on verb semantics.",
            "We do a finer-grained study in Table 8 by breaking down the performance for the three state changes: creation (C), movement (M), and destruction (D), separately.",
            "Across the three state changes, the model suffers a loss of performance in the movement cases.",
            "This is owing to the fact that the movement cases require a deeper compositional and implicit event tracking.",
            "Also, a majority of errors leading to false negatives are due to the the formation of new sub-entities which are then mentioned with other names.",
            "For example, when talking about weak acid in \"he water becomes a weak acid. the water dissolves limestone\"\u009d the weak acid is also considered to move to the limestone."
        ],
        "class_sentence": [
            0,
            0,
            1,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "C",
                "M",
                "D"
            ],
            [
                "M"
            ],
            [
                "M"
            ],
            null,
            null
        ],
        "n_sentence": 7.0,
        "table_id": "table_8",
        "paper_id": "D19-1070",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1154table_1",
        "description": "Table 1 presents the results on the Multi30K testing set. The VSE baselines in the first five rows are trained with English and German descriptions independently. In contrast, PIVOT (Gella et al., 2017) and the proposed model are capable of handling multilingual input queries with single model. For a fair comparison with PIVOT, we also report the result of swapping Faster-RCNN with VGG as the visual feature encoder in our model. As can be seen, the proposed models successfully obtain state-of-the-art results, outperforming other baselines by a significant margin. GermanImage matching benefit more from joint training with English-Image pairs. The models with pretrained multilingual embeddings and contextualized embeddings achieve better performance in comparison to randomly initialized word embeddings, especially for German. One explanation is that the degradation from German singletons is alleviated by the multi-task training with English and the pre-trained embeddings. While the model with BERT performs better in English, FastText is preferred for German-Image matching.",
        "sentences": [
            "Table 1 presents the results on the Multi30K testing set.",
            "The VSE baselines in the first five rows are trained with English and German descriptions independently.",
            "In contrast, PIVOT (Gella et al., 2017) and the proposed model are capable of handling multilingual input queries with single model.",
            "For a fair comparison with PIVOT, we also report the result of swapping Faster-RCNN with VGG as the visual feature encoder in our model.",
            "As can be seen, the proposed models successfully obtain state-of-the-art results, outperforming other baselines by a significant margin.",
            "GermanImage matching benefit more from joint training with English-Image pairs.",
            "The models with pretrained multilingual embeddings and contextualized embeddings achieve better performance in comparison to randomly initialized word embeddings, especially for German.",
            "One explanation is that the degradation from German singletons is alleviated by the multi-task training with English and the pre-trained embeddings.",
            "While the model with BERT performs better in English, FastText is preferred for German-Image matching."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1,
            1,
            2,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "VSE\u2020* (Kiros et al., 2014)",
                "VSE++* (Faghri et al., 2018)"
            ],
            [
                "Pivot\u2020 (Gella et al., 2017)",
                "Ours\u2020 (Random, VGG19)",
                "Ours (Random, No diversity)",
                "Ours (Random)",
                "Ours (w/ FastText)",
                "Ours (w/ BERT)"
            ],
            [
                "Pivot\u2020 (Gella et al., 2017)"
            ],
            [
                "Ours\u2020 (Random, VGG19)",
                "Ours (Random, No diversity)",
                "Ours (Random)",
                "Ours (w/ FastText)",
                "Ours (w/ BERT)"
            ],
            null,
            null,
            null,
            [
                "Ours (w/ FastText)",
                "Ours (w/ BERT)",
                "English to Image",
                "Image to English",
                "German to Image",
                "Image to German"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_1",
        "paper_id": "D19-1154",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1166table_8",
        "description": "Table 8 summarizes the results: the multitask model slightly outperforms a dedicated simplification model on English simplification, showing the benefits of the additional training data from other tasks. By contrast, on the resource-rich MT task, the standalone translation system performs better. This can be explained by the fact that the standalone system is only responsible for text translation, while the multi-task model is exposed to samples of more diverse complexity levels during training which damage its ability to preserve complexity.",
        "sentences": [
            "Table 8 summarizes the results: the multitask model slightly outperforms a dedicated simplification model on English simplification, showing the benefits of the additional training data from other tasks.",
            "By contrast, on the resource-rich MT task, the standalone translation system performs better.",
            "This can be explained by the fact that the standalone system is only responsible for text translation, while the multi-task model is exposed to samples of more diverse complexity levels during training which damage its ability to preserve complexity."
        ],
        "class_sentence": [
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "Machine Translation",
                "English Simplification"
            ],
            [
                "Machine Translation",
                "Translate"
            ],
            null
        ],
        "n_sentence": 3.0,
        "table_id": "table_8",
        "paper_id": "D19-1166",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1174table_2",
        "description": "5.3 Results. Table 2 shows results produced using traditional ML methods (SVM, RF, and LR) across four different feature sets (word n-grams, character ngrams, averaged ELMo vectors, and composite features). We use Label Powerset for these methods, since the direct (non-transformative) formulation cannot be used with them. Among these combinations, logistic regression with averaged ELMo embeddings as features performs the best.",
        "sentences": [
            "5.3 Results.",
            "Table 2 shows results produced using traditional ML methods (SVM, RF, and LR) across four different feature sets (word n-grams, character ngrams, averaged ELMo vectors, and composite features).",
            "We use Label Powerset for these methods, since the direct (non-transformative) formulation cannot be used with them.",
            "Among these combinations, logistic regression with averaged ELMo embeddings as features performs the best."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "SVM",
                "RF",
                "LR",
                "Word n-grams",
                "Averaged ELMo vectors",
                "Composite features"
            ],
            null,
            [
                "LR",
                "Averaged ELMo vectors"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "D19-1174",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1177table_3",
        "description": "Identification of helpful reviews: As both the training and test sets are imbalanced, we adopt the Area under Receiver Operating Characteristic (AUROC) as the metric to evaluate the performance of all the approaches on helpful review identification. In line with Table 3, MTNL (Fan et al., 2018) achieves the best performance up-to-date on this classification task among the baseline approaches as it achieves the best performance on 12 of 14 categories in Amazon and Yelp datasets. R2HP surpasses MTNL on both datasets and obtains state-of-the-art (microaveraged) results of 67.5% AUROC (Amazon) and 75.1% AUROC (Yelp) with absolute improvements of 4.9% AUROC and 4.7% AUROC, respectively.",
        "sentences": [
            "Identification of helpful reviews: As both the training and test sets are imbalanced, we adopt the Area under Receiver Operating Characteristic (AUROC) as the metric to evaluate the performance of all the approaches on helpful review identification.",
            "In line with Table 3, MTNL (Fan et al., 2018) achieves the best performance up-to-date on this classification task among the baseline approaches as it achieves the best performance on 12 of 14 categories in Amazon and Yelp datasets.",
            "R2HP surpasses MTNL on both datasets and obtains state-of-the-art (microaveraged) results of 67.5% AUROC (Amazon) and 75.1% AUROC (Yelp) with absolute improvements of 4.9% AUROC and 4.7% AUROC, respectively."
        ],
        "class_sentence": [
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "MTNL"
            ],
            [
                "R2HP",
                "Area under Receiver Operating Characteristic (AUROC)"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_3",
        "paper_id": "D19-1177",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1184table_3",
        "description": "MGT is applied on top of the dual encoder baseline (Lowe et al., 2015) and Deep Attention Matching networks (Zhou et al., 2018). The results shown in Table 3 show the performance of MGT using two different underlying architectures, as well as previous work. Across both base architectures, MGT outperforms ensembling. The primary difference between these two methods is that MGT explicitly ensures that several granularities of representation are learned. As such, these results reaffirm the hypothesis that learning multiple granularities of representation leads to more diverse models, and more general representations of dialog.",
        "sentences": [
            "MGT is applied on top of the dual encoder baseline (Lowe et al., 2015) and Deep Attention Matching networks (Zhou et al., 2018).",
            "The results shown in Table 3 show the performance of MGT using two different underlying architectures, as well as previous work.",
            "Across both base architectures, MGT outperforms ensembling.",
            "The primary difference between these two methods is that MGT explicitly ensures that several granularities of representation are learned.",
            "As such, these results reaffirm the hypothesis that learning multiple granularities of representation leads to more diverse models, and more general representations of dialog."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "Multi-Granularity (5)",
                "Dual Encoder Experiments",
                "Deep Attention Matching Experiments"
            ],
            [
                "Multi-Granularity (5)"
            ],
            [
                "Multi-Granularity (5)",
                "Ensemble (5)"
            ],
            [
                "Multi-Granularity (5)",
                "Ensemble (5)"
            ],
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_3",
        "paper_id": "D19-1184",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1315table_6",
        "description": "It can be observed from Table 6 that the cascade model achieves a lower score than MTL + SD. However, the cascade model (gold) improves the ROUGE-L score. This result indicates that the classification error propagates to the generators when the cascade model is applied. Our proposed HCL has an advantage in that it does not suffer from an error of the classifier, and thus this method achieves the best score. Furthermore, we compare our proposed HCL model with other settings. A soft-parameter sharing method (Guo et al., 2018) penalizes the difference between parameters in pairs of decoders. The non-hierarchical consistency loss is almost the same as the hierarchical consistency loss (Eqn. 10). We replace a ramp function in Eqn. 10 with an absolute value function. Neither the soft-parameter sharing method nor the non-hierarchical consistency loss has the ability to consider the hierarchy among tasks. It can be observed from Table 6 that both methods achieved a lower score than the hierarchical consistency loss. This is because our hierarchical consistency loss enables the model to penalize a multi-task model with hierarchy among the tasks. Although the HCL with normalized-attention weights adopts the hierarchical consistency loss model indicated in Eqn. 10, however, we substitute the normalized attention weights ?ds ij for non-normalized attention weights eds ij . HCL with normalized attention weights is not as effective as HCL with non-normalized attention weights. Our HCL is based on the assumption that if one specific input word is important for both the shorter and longer text generation tasks, the attention weights of the words for the shorter text generation task would be smaller than the attention weights for the longer text generation task. However, the distributions of the normalized attention weights converge to a few words for the shorter text generation task; thus the normalized attention weights does not satisfy the assumption. As a result, our proposed HCL with non-normalized attention weights can accurately compute this inconsistency, contrary to the HCL with normalized attention weights. Comparison of encoder information sharing methods. To determine the dependence of HCL on MTL, we conduct two experiments. HCL applies the hierarchical consistency loss without MTL and SD, while SD + HCL applies the scheduling sampling and hierarchical consistency loss; however, multi-task learning is not applied. Table 6 presents the results of two methods for the job advertisement dataset.",
        "sentences": [
            "It can be observed from Table 6 that the cascade model achieves a lower score than MTL + SD.",
            "However, the cascade model (gold) improves the ROUGE-L score.",
            "This result indicates that the classification error propagates to the generators when the cascade model is applied.",
            "Our proposed HCL has an advantage in that it does not suffer from an error of the classifier, and thus this method achieves the best score.",
            "Furthermore, we compare our proposed HCL model with other settings.",
            "A soft-parameter sharing method (Guo et al., 2018) penalizes the difference between parameters in pairs of decoders.",
            "The non-hierarchical consistency loss is almost the same as the hierarchical consistency loss (Eqn. 10).",
            "We replace a ramp function in Eqn. 10 with an absolute value function.",
            "Neither the soft-parameter sharing method nor the non-hierarchical consistency loss has the ability to consider the hierarchy among tasks.",
            "It can be observed from Table 6 that both methods achieved a lower score than the hierarchical consistency loss.",
            "This is because our hierarchical consistency loss enables the model to penalize a multi-task model with hierarchy among the tasks.",
            "Although the HCL with normalized-attention weights adopts the hierarchical consistency loss model indicated in Eqn. 10, however, we substitute the normalized attention weights ?ds ij for non-normalized attention weights eds ij .",
            "HCL with normalized attention weights is not as effective as HCL with non-normalized attention weights.",
            "Our HCL is based on the assumption that if one specific input word is important for both the shorter and longer text generation tasks, the attention weights of the words for the shorter text generation task would be smaller than the attention weights for the longer text generation task.",
            "However, the distributions of the normalized attention weights converge to a few words for the shorter text generation task; thus the normalized attention weights does not satisfy the assumption.",
            "As a result, our proposed HCL with non-normalized attention weights can accurately compute this inconsistency, contrary to the HCL with normalized attention weights.",
            "Comparison of encoder information sharing methods.",
            "To determine the dependence of HCL on MTL, we conduct two experiments.",
            "HCL applies the hierarchical consistency loss without MTL and SD, while SD + HCL applies the scheduling sampling and hierarchical consistency loss; however, multi-task learning is not applied.",
            "Table 6 presents the results of two methods for the job advertisement dataset."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            2,
            2,
            1,
            1,
            1,
            1,
            2,
            2,
            1,
            2,
            2,
            1,
            2,
            2,
            2,
            1
        ],
        "header_mention": [
            [
                "Comparison of Decoder Information Sharing Method MTL + SD + Cascade Model",
                "Proposed (MTL + SD + HCL)"
            ],
            [
                "Comparison of Decoder Information Sharing Method MTL + SD + Cascade Model (Gold)"
            ],
            null,
            [
                "Proposed (MTL + SD + HCL)"
            ],
            null,
            null,
            null,
            null,
            null,
            [
                "Comparison of Decoder Information Sharing Method MTL + SD + Soft-Parameter Sharing",
                "Comparison of Decoder Information Sharing Method MTL + SD + Non-Hierarchical Consistency Loss"
            ],
            null,
            null,
            [
                "Comparison of Decoder Information Sharing Method MTL + SD + HCL with Normalized Attention Weights",
                "Proposed (MTL + SD + HCL)"
            ],
            null,
            null,
            [
                "Proposed (MTL + SD + HCL)",
                "Comparison of Decoder Information Sharing Method MTL + SD + HCL with Normalized Attention Weights"
            ],
            null,
            null,
            [
                "Proposed (MTL + SD + HCL)"
            ],
            null
        ],
        "n_sentence": 20.0,
        "table_id": "table_6",
        "paper_id": "D19-1315",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1381table_4",
        "description": "Table 4 shows the performance comparison of the models on nested, overlapping and flat event detection. Our model yields higher F1-scores than TEES which can be attributed to its ability to maintain multiple beams and to detect events from all these beams during search.",
        "sentences": [
            "Table 4 shows the performance comparison of the models on nested, overlapping and flat event detection.",
            "Our model yields higher F1-scores than TEES which can be attributed to its ability to maintain multiple beams and to detect events from all these beams during search."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "Nested",
                "Overlapping",
                "Flat"
            ],
            [
                "SBNN k = 8",
                "TEES",
                "Overall F1 (%)"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_4",
        "paper_id": "D19-1381",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1399table_4",
        "description": "OntoNotes Chinese Table 4 shows the performance comparison on the Chinese datasets. We compare our models against the state-of-the-art NER model on this dataset, Lattice LSTM (Zhang and Yang, 2018). Our implementation of the strong BiLSTM-CRF baseline achieves comparable performance against the Lattice LSTM. Similar to the English dataset, our model with L = 0 significantly improves the performance compared to the BiLSTM-CRF (L = 0) model. Our DGLSTM-CRF model achieves the best performance with L = 2 and is consistently better (p < 0.02) than the strong BiLSTM-CRF baselines. As we can see from the table, the improvements of the DGLSTM-CRF model mainly come from recall (p < 0.001) compared to the BiLSTM model, especially in the scenario with word embeddings only. Empirically, we also found that those correctly retrieved entities of the DGLSTM-CRF (compared against the baseline) mostly correlate with the following dependency relations: \u201cnn\u201d, \u201cnsubj\u201d, \u201cnummod\u201d. However, DGLSTM-CRF achieves lower precisions against BiLSTM-CRF, which indicates that the DGLSTM-CRF model makes more false-positive predictions. The reason could be the relatively lower ratio of ST(%) as shown in Table 2, which means some of the entities do not form subtrees under the complete dependency trees. In such a scenario, the model may not correctly identify the boundary of the entities, which results in lower precision.",
        "sentences": [
            "OntoNotes Chinese Table 4 shows the performance comparison on the Chinese datasets.",
            "We compare our models against the state-of-the-art NER model on this dataset, Lattice LSTM (Zhang and Yang, 2018).",
            "Our implementation of the strong BiLSTM-CRF baseline achieves comparable performance against the Lattice LSTM.",
            "Similar to the English dataset, our model with L = 0 significantly improves the performance compared to the BiLSTM-CRF (L = 0) model.",
            "Our DGLSTM-CRF model achieves the best performance with L = 2 and is consistently better (p < 0.02) than the strong BiLSTM-CRF baselines. As we can see from the table, the improvements of the DGLSTM-CRF model mainly come from recall (p < 0.001) compared to the BiLSTM model, especially in the scenario with word embeddings only.",
            "Empirically, we also found that those correctly retrieved entities of the DGLSTM-CRF (compared against the baseline) mostly correlate with the following dependency relations: \u201cnn\u201d, \u201cnsubj\u201d, \u201cnummod\u201d.",
            "However, DGLSTM-CRF achieves lower precisions against BiLSTM-CRF, which indicates that the DGLSTM-CRF model makes more false-positive predictions.",
            "The reason could be the relatively lower ratio of ST(%) as shown in Table 2, which means some of the entities do not form subtrees under the complete dependency trees.",
            "In such a scenario, the model may not correctly identify the boundary of the entities, which results in lower precision."
        ],
        "class_sentence": [
            1,
            2,
            2,
            1,
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "Lattice LSTM (Z&Y, 2018)"
            ],
            [
                "Lattice LSTM (Z&Y, 2018)",
                "BiLSTM-CRF (L = 0)",
                "BiLSTM-CRF (L = 1)",
                "BiLSTM-CRF (L = 2)",
                "BiLSTM-CRF (L = 3)",
                "BiLSTM-CRF (L = 0) + ELMo",
                "BiLSTM-CRF (L = 1) + ELMo",
                "BiLSTM-CRF(L = 2) + ELMo",
                "BiLSTM-CRF (L = 3) + ELMo"
            ],
            [
                "BiLSTM-CRF (L = 0)",
                "DGLSTM-CRF (L = 0)"
            ],
            [
                "DGLSTM-CRF (L = 2)",
                "BiLSTM-CRF (L = 0)",
                "BiLSTM-CRF (L = 1)",
                "BiLSTM-CRF (L = 2)",
                "BiLSTM-CRF (L = 3)",
                "BiLSTM-CRF (L = 0) + ELMo",
                "BiLSTM-CRF (L = 1) + ELMo",
                "BiLSTM-CRF(L = 2) + ELMo",
                "BiLSTM-CRF (L = 3) + ELMo"
            ],
            [
                "DGLSTM-CRF (L = 0)",
                "DGLSTM-CRF (L = 1)",
                "DGLSTM-CRF (L = 2)",
                "DGLSTM-CRF (L = 3)",
                "DGLSTM-CRF (L = 0) + ELMo",
                "DGLSTM-CRF (L = 1) + ELMo",
                "DGLSTM-CRF (L = 2) + ELMo",
                "DGLSTM-CRF (L = 3) + ELMo"
            ],
            [
                "BiLSTM-CRF (L = 0)",
                "BiLSTM-CRF (L = 1)",
                "BiLSTM-CRF (L = 2)",
                "BiLSTM-CRF (L = 3)",
                "BiLSTM-CRF (L = 0) + ELMo",
                "BiLSTM-CRF (L = 1) + ELMo",
                "BiLSTM-CRF(L = 2) + ELMo",
                "BiLSTM-CRF (L = 3) + ELMo",
                "DGLSTM-CRF (L = 0)"
            ],
            null,
            null
        ],
        "n_sentence": 9.0,
        "table_id": "table_4",
        "paper_id": "D19-1399",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1403table_2",
        "description": "5.3 Experiment Results Overall Performance Experiment results on ARSC are presented in Table 2. The proposed Induction Networks achieves a 85.63% accuracy, outperforming the existing state-of-the-art model, ROBUSTTC-FSL, by a notable 3% improvement. We due the improvement to the fact that ROBUSTTC-FSL builds a general metric method by integrating several metrics at the sample level, which faces the difficulty of getting rid of the noise among different expressions in the same class. In addition to that, the task-clustering-based method used by ROBUSTTC-FSL must be found on the relevance matrix, which is inefficient when applied to real-world scenarios where the tasks change rapidly. Our Induction Networks, however, is trained in the meta-learning framework with more flexible generalization and its induction ability can hence be accumulated through different tasks. Ablation Study To analyze the effect of varying different components of the Induction Module and Relation Module, we further report the ablation experiments on the ARSC dataset as shown in Table 4. We can see that the best performance is achieved when we used 3 iterations, corresponding to the best result reported in Table 2 (more rounds of iterations did not further improve the performance), and the table shows the effectiveness of the routing component. We also changed the Induction Module with sum and self-attention and changed Relation Module with cosine distance. Changes in the performances validate the benefit of both the Relation Module and Induction Module. The Attention+Relation models the induction ability by self-attention mechanism, but the ability is limited by the learnt attention parameters. Conversely, the proposed dynamic routing induction method captures class-level information by automatically adjusting the coupling coefficients according to inputted support sets, which is more suitable for the few-shot learning task.",
        "sentences": [
            "5.3 Experiment Results Overall Performance Experiment results on ARSC are presented in Table 2.",
            "The proposed Induction Networks achieves a 85.63% accuracy, outperforming the existing state-of-the-art model, ROBUSTTC-FSL, by a notable 3% improvement.",
            "We due the improvement to the fact that ROBUSTTC-FSL builds a general metric method by integrating several metrics at the sample level, which faces the difficulty of getting rid of the noise among different expressions in the same class.",
            "In addition to that, the task-clustering-based method used by ROBUSTTC-FSL must be found on the relevance matrix, which is inefficient when applied to real-world scenarios where the tasks change rapidly.",
            "Our Induction Networks, however, is trained in the meta-learning framework with more flexible generalization and its induction ability can hence be accumulated through different tasks.",
            "Ablation Study To analyze the effect of varying different components of the Induction Module and Relation Module, we further report the ablation experiments on the ARSC dataset as shown in Table 4.",
            "We can see that the best performance is achieved when we used 3 iterations, corresponding to the best result reported in Table 2 (more rounds of iterations did not further improve the performance), and the table shows the effectiveness of the routing component.",
            "We also changed the Induction Module with sum and self-attention and changed Relation Module with cosine distance.",
            "Changes in the performances validate the benefit of both the Relation Module and Induction Module.",
            "The Attention+Relation models the induction ability by self-attention mechanism, but the ability is limited by the learnt attention parameters.",
            "Conversely, the proposed dynamic routing induction method captures class-level information by automatically adjusting the coupling coefficients according to inputted support sets, which is more suitable for the few-shot learning task."
        ],
        "class_sentence": [
            2,
            1,
            2,
            2,
            2,
            0,
            0,
            0,
            0,
            0,
            0
        ],
        "header_mention": [
            null,
            [
                "Induction Networks (ours)",
                "Mean Acc",
                "ROBUSTTC-FSL (Yu et al., 2018)",
                "Relation Network (Sung et al., 2018)",
                "SNAIL (Mishra et al., 2018)"
            ],
            [
                "ROBUSTTC-FSL (Yu et al., 2018)",
                "Relation Network (Sung et al., 2018)",
                "SNAIL (Mishra et al., 2018)"
            ],
            [
                "ROBUSTTC-FSL (Yu et al., 2018)",
                "Relation Network (Sung et al., 2018)",
                "SNAIL (Mishra et al., 2018)"
            ],
            [
                "Induction Networks (ours)"
            ],
            null,
            null,
            null,
            null,
            null,
            null
        ],
        "n_sentence": 11.0,
        "table_id": "table_2",
        "paper_id": "D19-1403",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1450table_2",
        "description": "Comparison with unsupervised GANs . As we have mentioned before, the preliminary mappings trained by the method of Lample et al. (2018) perform well for some similar language pairs, such as Spanish to English and French to English. After refinement, their unsupervised GANs models can reach the same level as supervised models for these similar language pairs. However, the biggest drawback of standard GANs is that they exhibit poor performance for distant language pairs. The results from Table 2 clearly confirm this. For example, without refinement, the mapping trained by the unsupervised GAN method can only correctly predict 12% of the words from Turkish to English. Given that the quality of preliminary mappings can seriously affect the effect of refinement, the low-quality preliminary mappings for distant language pairs severely limits the improvements brought by post-refinement. Notice that the method of Lample et al. (2018) scores a null result for English to Finnish on both BLI-1 and BLI2, indicating that totally unsupervised adversarial training can yields rather unpredictable results.",
        "sentences": [
            "Comparison with unsupervised GANs .",
            "As we have mentioned before, the preliminary mappings trained by the method of Lample et al. (2018) perform well for some similar language pairs, such as Spanish to English and French to English.",
            "After refinement, their unsupervised GANs models can reach the same level as supervised models for these similar language pairs.",
            "However, the biggest drawback of standard GANs is that they exhibit poor performance for distant language pairs.",
            "The results from Table 2 clearly confirm this.",
            "For example, without refinement, the mapping trained by the unsupervised GAN method can only correctly predict 12% of the words from Turkish to English.",
            "Given that the quality of preliminary mappings can seriously affect the effect of refinement, the low-quality preliminary mappings for distant language pairs severely limits the improvements brought by post-refinement.",
            "Notice that the method of Lample et al. (2018) scores a null result for English to Finnish on both BLI-1 and BLI2, indicating that totally unsupervised adversarial training can yields rather unpredictable results."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            null,
            [
                "Without refinement",
                "MUSE",
                "tr-en"
            ],
            [
                "Without refinement",
                "With refinement"
            ],
            null
        ],
        "n_sentence": 8.0,
        "table_id": "table_2",
        "paper_id": "D19-1450",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1491table_1",
        "description": "SEQ uses 300-dimensional GloVe embeddings as word representations (Pennington et al., 2014). The model is trained to predict the binary complexity of words as annotated in the dataset of Yimam et al. (2017). Training is performed over 20 iterations on randomly shuffled sentences from all genres included within the dataset. To test this novel architecture on CWI, we apply it to the CWI 2018 shared task test data (Yimam et al., 2018) and compare the results to the current state-of-theart (SOTA) CAMB system. Table 1 shows that the SEQ model outperforms the current SOTA system on all three text genres for binary CWI (statistically significant using McNemar's test, p=0.0016, chi-square=9.95).",
        "sentences": [
            "SEQ uses 300-dimensional GloVe embeddings as word representations (Pennington et al., 2014).",
            "The model is trained to predict the binary complexity of words as annotated in the dataset of Yimam et al. (2017).",
            "Training is performed over 20 iterations on randomly shuffled sentences from all genres included within the dataset.",
            "To test this novel architecture on CWI, we apply it to the CWI 2018 shared task test data (Yimam et al., 2018) and compare the results to the current state-of-theart (SOTA) CAMB system.",
            "Table 1 shows that the SEQ model outperforms the current SOTA system on all three text genres for binary CWI (statistically significant using McNemar's test, p=0.0016, chi-square=9.95)."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            [
                "SEQ",
                "NEWS",
                "WIKINEWS",
                "WIKIPEDIA"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_1",
        "paper_id": "D19-1491",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1497table_3",
        "description": "Main Results. Table 3 presents the performance of different methods on citation count prediction. We can make the following observations. First, the four traditional baselines (LR, KNN, SVR and GBRT) perform worse than the two deep learning baselines (W&D, MILAM). These four baselines only utilize the wide features with traditional machine learning models. Second, MILAM performs consistently better than W&D, since it has designed a more elaborate architecture to model the review text. Finally, our model outperforms all the baselines with a substantial margin, especially for the ICLR dataset. Our model is able to integrate the wide features and learn the comprehensive representation for peer review text, which is the key of the performance the two improvement over baselines. Overall, datasets show the similar findings. In what follows, we will report the results on ICLR dataset due to space limit.",
        "sentences": [
            "Main Results.",
            "Table 3 presents the performance of different methods on citation count prediction.",
            "We can make the following observations.",
            "First, the four traditional baselines (LR, KNN, SVR and GBRT) perform worse than the two deep learning baselines (W&D, MILAM).",
            "These four baselines only utilize the wide features with traditional machine learning models.",
            "Second, MILAM performs consistently better than W&D, since it has designed a more elaborate architecture to model the review text.",
            "Finally, our model outperforms all the baselines with a substantial margin, especially for the ICLR dataset.",
            "Our model is able to integrate the wide features and learn the comprehensive representation for peer review text, which is the key of the performance the two improvement over baselines.",
            "Overall, datasets show the similar findings.",
            "In what follows, we will report the results on ICLR dataset due to space limit."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2,
            1,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "LR",
                "KNN",
                "SVR",
                "GBRT",
                "MILAM"
            ],
            [
                "LR",
                "KNN",
                "SVR",
                "GBRT"
            ],
            [
                "MILAM"
            ],
            [
                "Our model",
                "ICLR"
            ],
            [
                "Our model"
            ],
            null,
            null
        ],
        "n_sentence": 10.0,
        "table_id": "table_3",
        "paper_id": "D19-1497",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1507table_1",
        "description": "6.3 Decoding Results . We performed comprehensive experiments to analyze the performance of query auto-completion. Table 1 shows the generation result of MPC, the character baseline, and our model variants. For BPE models, we varied the maximum retrace step to 0 (without retrace algorithm), 1, 2, and \u00e2\u02c6\u017e (no limitation on retracing step size). For SR models, we compare decoding results without any techniques, with marginalization only, with retrace algorithm only, and with both. MPC is a very fast and remarkably strong baseline. It is worse than language models in the overall score (MRR, PMRR, and MRL), but better for previously seen queries. However, it is unable to predict unseen queries. Even with efficient data structures, MPC requires huge memory to keep statistics of all previous queries. As a practical view, combining frequency-based traditional method and neural language model approach can boost the accuracy and meet trade-off between the performance and computational costs. MRRs and PMRRs of our best methods are close to that of the character model with less than 0.02 point drop. Notably, the SR model has better generalization ability in that their PMRR for unseen queries is higher than that of the character model. In a real scenario, it is more critical because unseen queries come in increasingly as time goes by.",
        "sentences": [
            "6.3 Decoding Results .",
            "We performed comprehensive experiments to analyze the performance of query auto-completion.",
            "Table 1 shows the generation result of MPC, the character baseline, and our model variants.",
            "For BPE models, we varied the maximum retrace step to 0 (without retrace algorithm), 1, 2, and \u00e2\u02c6\u017e (no limitation on retracing step size).",
            "For SR models, we compare decoding results without any techniques, with marginalization only, with retrace algorithm only, and with both.",
            "MPC is a very fast and remarkably strong baseline.",
            "It is worse than language models in the overall score (MRR, PMRR, and MRL), but better for previously seen queries.",
            "However, it is unable to predict unseen queries.",
            "Even with efficient data structures, MPC requires huge memory to keep statistics of all previous queries.",
            "As a practical view, combining frequency-based traditional method and neural language model approach can boost the accuracy and meet trade-off between the performance and computational costs.",
            "MRRs and PMRRs of our best methods are close to that of the character model with less than 0.02 point drop.",
            "Notably, the SR model has better generalization ability in that their PMRR for unseen queries is higher than that of the character model.",
            "In a real scenario, it is more critical because unseen queries come in increasingly as time goes by."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            2,
            2,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "MPC",
                "Char",
                "BPE",
                "SR"
            ],
            [
                "BPE",
                "BPE+R1",
                "BPE+R2",
                "BPE+R\u221e"
            ],
            [
                "SR",
                "SR+M",
                "SR+R\u221e",
                "SR+R\u221e+M"
            ],
            [
                "MPC"
            ],
            [
                "MPC",
                "MRR Unseen",
                "PMRR Unseen",
                "MRL Unseen",
                "MRR Seen",
                "PMRR Seen",
                "MRL Seen"
            ],
            [
                "MPC",
                "MRR Unseen",
                "PMRR Unseen",
                "MRL Unseen"
            ],
            [
                "MPC"
            ],
            null,
            [
                "MRR Seen",
                "MRR Unseen",
                "MRR All",
                "PMRR Seen",
                "PMRR Unseen",
                "PMRR All",
                "SR",
                "BPE"
            ],
            [
                "SR",
                "PMRR Unseen",
                "Char"
            ],
            [
                "MRR Unseen",
                "PMRR Unseen",
                "MRL Unseen"
            ]
        ],
        "n_sentence": 13.0,
        "table_id": "table_1",
        "paper_id": "D19-1507",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1512table_4",
        "description": "4.4 Evaluation Results . Table 4 reports evaluation results in terms of both automatic metrics and human annotations. On most automatic metrics, DeepCom outperforms baseline methods, and the improvement is statistically significant (t-test with p-value < 0.01). The improvement on BLEU-1 and W-BLEU-1 is much bigger than that on other metrics. This is because BLEU-1 only measures the proportion of matched unigrams out of the total number of unigrams in the generated comments. In human evaluation, although the absolute numbers are different from those reported in (Qin et al., 2018) due to the difference between human judgements, the overall trend is consistent. In human evaluation, the values of Fleiss' kappa over all models are more than 0:6, indicating substantial agreement among the annotators. Although built in a complicated structure, GANN does not bring much improvement over other baseline methods, which demonstrates that only using news titles is not enough in comment generation. IR-TC and Att-TC represent the best retrieval model and the best generation model among the baselines on both datasets, implying that news bodies, even used in a simple way, can provide useful information to comment generation.",
        "sentences": [
            "4.4 Evaluation Results .",
            "Table 4 reports evaluation results in terms of both automatic metrics and human annotations.",
            "On most automatic metrics, DeepCom outperforms baseline methods, and the improvement is statistically significant (t-test with p-value < 0.01).",
            "The improvement on BLEU-1 and W-BLEU-1 is much bigger than that on other metrics.",
            "This is because BLEU-1 only measures the proportion of matched unigrams out of the total number of unigrams in the generated comments.",
            "In human evaluation, although the absolute numbers are different from those reported in (Qin et al., 2018) due to the difference between human judgements, the overall trend is consistent.",
            "In human evaluation, the values of Fleiss' kappa over all models are more than 0:6, indicating substantial agreement among the annotators.",
            "Although built in a complicated structure, GANN does not bring much improvement over other baseline methods, which demonstrates that only using news titles is not enough in comment generation.",
            "IR-TC and Att-TC represent the best retrieval model and the best generation model among the baselines on both datasets, implying that news bodies, even used in a simple way, can provide useful information to comment generation."
        ],
        "class_sentence": [
            0,
            1,
            2,
            1,
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "DeepCom"
            ],
            [
                "DeepCom",
                "BLEU-1",
                "W-BLEU-1"
            ],
            null,
            [
                "DeepCom",
                "Human"
            ],
            [
                "Models",
                "Kappa"
            ],
            [
                "GANN"
            ],
            [
                "IR-TC",
                "Att-TC",
                "Dataset"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_4",
        "paper_id": "D19-1512",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1581table_4",
        "description": "Contrary to our expectations, supervised models (ACP) outperformed semi-supervised models (ACP+AL+CA+CO). This suggests that the training set of 0.6 million events is sufficiently large for training the models. For comparison, we trained the models with a subset (6,000 events) of the ACP dataset. As the results shown in Table 4 demonstrate, our method is effective when labeled data are small.",
        "sentences": [
            "Contrary to our expectations, supervised models (ACP) outperformed semi-supervised models (ACP+AL+CA+CO).",
            "This suggests that the training set of 0.6 million events is sufficiently large for training the models.",
            "For comparison, we trained the models with a subset (6,000 events) of the ACP dataset.",
            "As the results shown in Table 4 demonstrate, our method is effective when labeled data are small."
        ],
        "class_sentence": [
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            [
                "ACP (6K)",
                " +AL+CA+CO"
            ],
            null,
            [
                "ACP (6K)"
            ],
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_4",
        "paper_id": "D19-1581",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1633table_4",
        "description": "Table 4 shows that increasing the number of decoding iterations (T) appears to mainly improve the performance on longer sequences. Having said that, the performance differences across length buckets are not very large, and it seems that even 4 mask-predict iterations are enough to produce decent translations for long sequences (40 ? N).",
        "sentences": [
            "Table 4 shows that increasing the number of decoding iterations (T) appears to mainly improve the performance on longer sequences.",
            "Having said that, the performance differences across length buckets are not very large, and it seems that even 4 mask-predict iterations are enough to produce decent translations for long sequences (40 ? N)."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "T = N"
            ],
            null
        ],
        "n_sentence": 2.0,
        "table_id": "table_4",
        "paper_id": "D19-1633",
        "valid": 1
    },
    {
        "table_id_paper": "D19-1674table_2",
        "description": "Table 2 shows the human evaluation results, where we compute the average score by first computing the average score of the three evaluators for each of the top-K outputs, then selecting the maximum of the K scores as the score of the method. The average shown in the table is the average of such maximum scores. The proposed method failed to generate any anagram for 3 instances. We therefore set the score of the proposed method to 1.0 for them. We can see that the proposed method yielded significantly higher human evaluation scores. Table 3 shows example anagrams generated by the proposed method and the baseline method. While the baseline method seems to employ some heuristics for deciding the word order, the combinations of selected words tend to form meaningless sentences. On the other hand, the proposed method tends to generate readable sentences. The anagrams shown in the table are listed in decreasing order of sentence probability. We can find that sentences with high probability are not always highly rated by the human evaluations. However, the normalized discounted cumulative gain (NDCG) score for the proposed method was 0.86. It suggests that using sentence probability works well for finding natural anagrams.",
        "sentences": [
            "Table 2 shows the human evaluation results, where we compute the average score by first computing the average score of the three evaluators for each of the top-K outputs, then selecting the maximum of the K scores as the score of the method. The average shown in the table is the average of such maximum scores.",
            "The proposed method failed to generate any anagram for 3 instances.",
            "We therefore set the score of the proposed method to 1.0 for them.",
            "We can see that the proposed method yielded significantly higher human evaluation scores.",
            "Table 3 shows example anagrams generated by the proposed method and the baseline method.",
            "While the baseline method seems to employ some heuristics for deciding the word order, the combinations of selected words tend to form meaningless sentences.",
            "On the other hand, the proposed method tends to generate readable sentences.",
            "The anagrams shown in the table are listed in decreasing order of sentence probability.",
            "We can find that sentences with high probability are not always highly rated by the human evaluations.",
            "However, the normalized discounted cumulative gain (NDCG) score for the proposed method was 0.86.",
            "It suggests that using sentence probability works well for finding natural anagrams."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0
        ],
        "header_mention": [
            null,
            [
                "Proposed"
            ],
            [
                "Score",
                "Proposed"
            ],
            [
                "Proposed",
                "Average"
            ],
            null,
            null,
            null,
            null,
            null,
            null,
            null
        ],
        "n_sentence": 11.0,
        "table_id": "table_2",
        "paper_id": "D19-1674",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1165table_9",
        "description": "As baselines, we use three models: (i) MEb, (ii) Ba MaxEnt using BOW representation; LSTMp, which is now trained on the concatenated set of sentences from MRDA and CON training sets; and (iii) MEe, a MaxEnt using sentence embeddings extracted from the B-LSTMp, i.e., the sentence embeddings are used as feature vectors. We experiment with the CRF variants in Table 1. The CRFs are trained on the CON training set using the sentence embeddings that are extracted by applying the B-LSTMp model, as was done with MEe. Table 9 shows our results. We notice that CRFs generally outperform MEs in accuracy. This indicates that there are conversational dependencies between the sentences in a conversation. When we compare between CRF variants, we notice that the model that does not consider any link across comments perform the worst; see CRF (LC-NO). A simple linear chain connection between sentences in their temporal order does not improve much (CRF (LC-LC)), which indicates that the widely used linear chain CRF (Lafferty et al., 2001) is not the most appropriate model for capturing conversational dependencies in these conversations. The CRF (LC-LC1) is one of the best performing models and perform significantly better than B-LSTMp. This model considers linear chain connections between sentences inside comments and only to the first comment. Note that both QC3 and TA are forum sites, where participants in a conversation interact mostly with the person who posts the first comment asking for some information. This is interesting that our model can capture this aspect. Another interesting observation is that when we change the above model to consider relations with every sentence in the first comment (CRF (LC-FC1)), this degrades the performance. This could be due to the fact that the information seeking person first explains her situation, and then asks for the information. Others tend to respond to the requested information rather than to her situation. The CRF (FC-FC) also yields as good results as CRF (LC-LC1).",
        "sentences": [
            "As baselines, we use three models: (i) MEb, (ii) Ba MaxEnt using BOW representation; LSTMp, which is now trained on the concatenated set of sentences from MRDA and CON training sets; and (iii) MEe, a MaxEnt using sentence embeddings extracted from the B-LSTMp, i.e., the sentence embeddings are used as feature vectors.",
            "We experiment with the CRF variants in Table 1.",
            "The CRFs are trained on the CON training set using the sentence embeddings that are extracted by applying the B-LSTMp model, as was done with MEe.",
            "Table 9 shows our results.",
            "We notice that CRFs generally outperform MEs in accuracy.",
            "This indicates that there are conversational dependencies between the sentences in a conversation.",
            "When we compare between CRF variants, we notice that the model that does not consider any link across comments perform the worst; see CRF (LC-NO).",
            "A simple linear chain connection between sentences in their temporal order does not improve much (CRF (LC-LC)), which indicates that the widely used linear chain CRF (Lafferty et al., 2001) is not the most appropriate model for capturing conversational dependencies in these conversations.",
            "The CRF (LC-LC1) is one of the best performing models and perform significantly better than B-LSTMp.",
            "This model considers linear chain connections between sentences inside comments and only to the first comment.",
            "Note that both QC3 and TA are forum sites, where participants in a conversation interact mostly with the person who posts the first comment asking for some information.",
            "This is interesting that our model can capture this aspect.",
            "Another interesting observation is that when we change the above model to consider relations with every sentence in the first comment (CRF (LC-FC1)), this degrades the performance.",
            "This could be due to the fact that the information seeking person first explains her situation, and then asks for the information.",
            "Others tend to respond to the requested information rather than to her situation.",
            "The CRF (FC-FC) also yields as good results as CRF (LC-LC1)."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            2,
            2,
            1,
            1,
            1,
            2,
            2,
            2,
            2,
            2,
            2,
            2
        ],
        "header_mention": [
            [
                "MEb",
                "B-LSTMp",
                "MEe"
            ],
            [
                "CRF (LC-NO)",
                "CRF (LC-LC)",
                "CRF (LC-LC1)",
                "CRF (LC-FC1)",
                "CRF (FC-FC)"
            ],
            [
                "CRF (LC-NO)",
                "CRF (LC-LC)",
                "CRF (LC-LC1)",
                "CRF (LC-FC1)",
                "CRF (FC-FC)"
            ],
            null,
            [
                "CRF (LC-NO)",
                "CRF (LC-LC)",
                "CRF (LC-LC1)",
                "CRF (LC-FC1)",
                "CRF (FC-FC)",
                "MEb",
                "MEe"
            ],
            null,
            [
                "CRF (LC-NO)"
            ],
            [
                "CRF (LC-LC)"
            ],
            [
                "CRF (LC-LC1)",
                "B-LSTMp"
            ],
            [
                "CRF (LC-LC1)"
            ],
            [
                "QC3",
                "TA"
            ],
            [
                "QC3",
                "TA"
            ],
            null,
            null,
            null,
            null
        ],
        "n_sentence": 16.0,
        "table_id": "table_9",
        "paper_id": "P16-1165",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1181table_4",
        "description": "Table 4 shows the results on the test sets. For WSJ, where sentence segmentation is almost trivial, we see only minor drops in LAS between GOLD and the systems that use predicted sentence boundaries. Among the systems that use predicted boundaries, no differences are significant. For WSJ\u2217 and Switchboard the picture is much different. Compared to GOLD, all systems show considerable drops in accuracy which asserts that errors from the sentence boundary detection task propagate to the parser and worsen the parser accuracy. On Switchboard the parsers yield significantly better results than MARMOT. The best result is obtained after reparsing and this is also significantly better than any other system. Although there is a slight drop in accuracy between NOSYNTAX and JOINT, this difference is not significant. The results on WSJ\u2217 show that not only does syntax help to improve sentence segmentation, it does so to a degree that parsing results deteriorate when simpler sentence boundary detectors are used. Here, both JOINT and JOINT-REPARSED obtain significantly better parsing accuracies than the systems that do not have access to syntax during sentence boundary prediction. Although JOINT-REPARSED performs a bit worse, the difference compared to JOINT is not significant.",
        "sentences": [
            "Table 4 shows the results on the test sets.",
            "For WSJ, where sentence segmentation is almost trivial, we see only minor drops in LAS between GOLD and the systems that use predicted sentence boundaries.",
            "Among the systems that use predicted boundaries, no differences are significant.",
            "For WSJ\u2217 and Switchboard the picture is much different.",
            "Compared to GOLD, all systems show considerable drops in accuracy which asserts that errors from the sentence boundary detection task propagate to the parser and worsen the parser accuracy.",
            "On Switchboard the parsers yield significantly better results than MARMOT.",
            "The best result is obtained after reparsing and this is also significantly better than any other system.",
            "Although there is a slight drop in accuracy between NOSYNTAX and JOINT, this difference is not significant.",
            "The results on WSJ\u2217 show that not only does syntax help to improve sentence segmentation, it does so to a degree that parsing results deteriorate when simpler sentence boundary detectors are used.",
            "Here, both JOINT and JOINT-REPARSED obtain significantly better parsing accuracies than the systems that do not have access to syntax during sentence boundary prediction.",
            "Although JOINT-REPARSED performs a bit worse, the difference compared to JOINT is not significant."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            1,
            1,
            1,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "WSJ",
                "GOLD"
            ],
            null,
            [
                "WSJ*",
                "Switchboard"
            ],
            [
                "GOLD"
            ],
            [
                "Switchboard",
                "GOLD",
                "MARMOT"
            ],
            [
                "Switchboard",
                "JOINT-REPARSED"
            ],
            [
                "Switchboard",
                "JOINT-REPARSED",
                "NOSYNTAX",
                "JOINT"
            ],
            [
                "WSJ*"
            ],
            [
                "WSJ*",
                "JOINT-REPARSED",
                "JOINT"
            ],
            [
                "WSJ*",
                "JOINT-REPARSED",
                "JOINT"
            ]
        ],
        "n_sentence": 11.0,
        "table_id": "table_4",
        "paper_id": "P16-1181",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1189table_4",
        "description": "Table 4 presents the results on the Accurat test sets for LEXACC and STACC using their respective optimal similarity thresholds. On the 21 test sets, the two systems were tied on two occasions, with STACC obtaining better results in 89.5% of the remaining cases. On the noisiest datasets, STACC was consistently and markedly better across language pairs.",
        "sentences": [
            "Table 4 presents the results on the Accurat test sets for LEXACC and STACC using their respective optimal similarity thresholds.",
            "On the 21 test sets, the two systems were tied on two occasions, with STACC obtaining better results in 89.5% of the remaining cases.",
            "On the noisiest datasets, STACC was consistently and markedly better across language pairs."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "LEXACC",
                "STACC"
            ],
            [
                "STACC"
            ],
            [
                "STACC"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_4",
        "paper_id": "P16-1189",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1201table_4",
        "description": "Table 4 shows the experimental results, from which we can see that the three-layer ANN model is surprisingly effective for event detection, which even yields competitive results compared with Nguyen\u2019s CNN and Chen\u2019s DMCNN. We believe the reason is that, compared with CNN and DMCNN, ANN focuses on capturing lexical features which have been proved much more important than sentence features for the ED task by (Chen et al., 2015).,. Moreover, our basic model achieves much higher precision than state-of-the-art approaches (79.5% vs. 75.6%). We also investigate the performance of the basic ED model without pre-trained word embeddings (denoted by ANN-Random). The result shows that randomly initialized word embeddings decrease the F1 score by 7.3 (61.5 vs. 68.8).",
        "sentences": [
            "Table 4 shows the experimental results, from which we can see that the three-layer ANN model is surprisingly effective for event detection, which even yields competitive results compared with Nguyen\u2019s CNN and Chen\u2019s DMCNN.",
            "We believe the reason is that, compared with CNN and DMCNN, ANN focuses on capturing lexical features which have been proved much more important than sentence features for the ED task by (Chen et al., 2015).,.",
            "Moreover, our basic model achieves much higher precision than state-of-the-art approaches (79.5% vs. 75.6%).",
            "We also investigate the performance of the basic ED model without pre-trained word embeddings (denoted by ANN-Random).",
            "The result shows that randomly initialized word embeddings decrease the F1 score by 7.3 (61.5 vs. 68.8)."
        ],
        "class_sentence": [
            1,
            2,
            1,
            2,
            1
        ],
        "header_mention": [
            [
                "ANN (ours)",
                "Nguyen\u2019s CNN (2015)",
                "Chen\u2019s DMCNN (2015)"
            ],
            [
                "ANN (ours)",
                "Nguyen\u2019s CNN (2015)",
                "Chen\u2019s DMCNN (2015)"
            ],
            [
                "ANN (ours)",
                "Nguyen\u2019s CNN (2015)"
            ],
            [
                "ANN-Random (ours)"
            ],
            [
                "ANN-Random (ours)",
                "ANN (ours)"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_4",
        "paper_id": "P16-1201",
        "valid": 1
    },
    {
        "table_id_paper": "P16-1201table_5",
        "description": "Table 5 shows the results of manual evaluations. Through the comparison of ANN and SF, we can see that the application of H1 caused a loss of 5.5 point. It happens mainly because the performance of SF is very sensitive to the wrongly mapped frames. That is, if a frame is mismapped, then all sentences under it would be mislabeled as events. Thus, even a single mismapped frame could significantly hurt the performance. This result also proves that H1 is inappropriate to be used as a hard constraint. As H2 is only an extension of H1, RF performs similarly with SF. Moreover, SL obtains a gain of 2.0% improvement compared with ANN, which demonstrates that the \u201dsame LU\u201d hypothesis is very useful. Finally, with all the hypotheses, the PSL-based approach achieves the best performance, which demonstrates that our hypotheses are useful and it is an effective way to jointly utilize them as soft constraints through PSL for event detection in FN.",
        "sentences": [
            "Table 5 shows the results of manual evaluations.",
            "Through the comparison of ANN and SF, we can see that the application of H1 caused a loss of 5.5 point.",
            "It happens mainly because the performance of SF is very sensitive to the wrongly mapped frames.",
            "That is, if a frame is mismapped, then all sentences under it would be mislabeled as events.",
            "Thus, even a single mismapped frame could significantly hurt the performance.",
            "This result also proves that H1 is inappropriate to be used as a hard constraint.",
            "As H2 is only an extension of H1, RF performs similarly with SF.",
            "Moreover, SL obtains a gain of 2.0% improvement compared with ANN, which demonstrates that the \u201dsame LU\u201d hypothesis is very useful.",
            "Finally, with all the hypotheses, the PSL-based approach achieves the best performance, which demonstrates that our hypotheses are useful and it is an effective way to jointly utilize them as soft constraints through PSL for event detection in FN."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            2,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "ANN",
                "SF"
            ],
            [
                "SF"
            ],
            null,
            null,
            null,
            null,
            [
                "SL",
                "ANN"
            ],
            [
                "PSL-based Approach"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_5",
        "paper_id": "P16-1201",
        "valid": 1
    },
    {
        "table_id_paper": "P16-2011table_2",
        "description": "Table 2 shows the overall performance of all methods on the ACE2005 English corpus. We can see that our approach significantly outperforms all previous methods. The better performance of HNN can be further explained by the following reasons: (1) Compared with feature based methods, such as MaxEnt, Cross-Event, Cross-Entity, and Joint Model, neural network based methods (including CNN, Bi-LSTM, HNN) performs better because they can make better use of word semantic information and avoid the errors propagated from NLP tools which may hinder the performance for event detection. (2) Moreover, Bi-LSTM can capture both preceding and following sequence information, which is much richer than dependency path. For example, in S2, the semantic of \u201ccourt\u201d can be delivered to release by a forward sequence in our approach. It is an important clue which can help to predict \u201crelease\u201d as a trigger for \u201cReleaseParole\u201d. For explicit feature based methods, they can not establish a relation between \u201ccourt\u201d and \u201crelease\u201d, because they belong to different clauses, and there is no direct dependency path between them. While in our approach, the semantics of \u201ccourt\u201d can be delivered to release by a forward sequence. (3) Cross-entity system achieves higher recall because it uses not only sentence-level information but also document-level information. It utilizes event concordance to predict a local trigger\u2019s event type based on cross-sentence inference. For example, an \u201cattack\u201d event is more likely to occur with \u201ckilled\u201d or \u201cdie\u201d event rather than \u201cmarry\u201d event. However, this method heavily relies on lexical and syntactic features, thus the precision is lower than neural network based methods. (4) RNN and LSTM perform slightly worse than Bi-LSTM. An obvious reason is that RNN and LSTM only consider the preceding sequence information of the trigger, which may miss some important following clues. Considering S1 again, when extracting the trigger \u201creleases\u201d, both models will miss the following sequence \u201c20 million euros to Iraq\u201d. This may seriously hinder the performance of RNN and LSTM for event detection.",
        "sentences": [
            "Table 2 shows the overall performance of all methods on the ACE2005 English corpus.",
            "We can see that our approach significantly outperforms all previous methods.",
            "The better performance of HNN can be further explained by the following reasons: (1) Compared with feature based methods, such as MaxEnt, Cross-Event, Cross-Entity, and Joint Model, neural network based methods (including CNN, Bi-LSTM, HNN) performs better because they can make better use of word semantic information and avoid the errors propagated from NLP tools which may hinder the performance for event detection.",
            "(2) Moreover, Bi-LSTM can capture both preceding and following sequence information, which is much richer than dependency path.",
            "For example, in S2, the semantic of \u201ccourt\u201d can be delivered to release by a forward sequence in our approach.",
            "It is an important clue which can help to predict \u201crelease\u201d as a trigger for \u201cReleaseParole\u201d.",
            "For explicit feature based methods, they can not establish a relation between \u201ccourt\u201d and \u201crelease\u201d, because they belong to different clauses, and there is no direct dependency path between them.",
            "While in our approach, the semantics of \u201ccourt\u201d can be delivered to release by a forward sequence.",
            "(3) Cross-entity system achieves higher recall because it uses not only sentence-level information but also document-level information.",
            "It utilizes event concordance to predict a local trigger\u2019s event type based on cross-sentence inference.",
            "For example, an \u201cattack\u201d event is more likely to occur with \u201ckilled\u201d or \u201cdie\u201d event rather than \u201cmarry\u201d event.",
            "However, this method heavily relies on lexical and syntactic features, thus the precision is lower than neural network based methods.",
            "(4) RNN and LSTM perform slightly worse than Bi-LSTM.",
            "An obvious reason is that RNN and LSTM only consider the preceding sequence information of the trigger, which may miss some important following clues.",
            "Considering S1 again, when extracting the trigger \u201creleases\u201d, both models will miss the following sequence \u201c20 million euros to Iraq\u201d.",
            "This may seriously hinder the performance of RNN and LSTM for event detection."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "HNN"
            ],
            [
                "HNN",
                "MaxEnt",
                "Cross-Event",
                "Cross-Entity",
                "Joint Model",
                "CNN",
                "Bi-LSTM"
            ],
            null,
            null,
            null,
            null,
            null,
            [
                "Cross-Entity"
            ],
            [
                "Cross-Entity"
            ],
            [
                "Cross-Entity"
            ],
            [
                "Cross-Entity"
            ],
            [
                "RNN",
                "LSTM",
                "Bi-LSTM"
            ],
            [
                "RNN",
                "LSTM"
            ],
            null,
            [
                "RNN",
                "LSTM"
            ]
        ],
        "n_sentence": 16.0,
        "table_id": "table_2",
        "paper_id": "P16-2011",
        "valid": 1
    },
    {
        "table_id_paper": "P16-2011table_4",
        "description": "Table 4 presents the performance of our method on the Spanish ERE corpus. The results show that HNN approach performed better than LSTM and Bi-LSTM. It indicates that our proposed model could achieve the best performance in multiple languages than other neural network methods. We did not compare our system with other systems (Tanev et al., 2009), because they reported the results on a non-standard data set .",
        "sentences": [
            "Table 4 presents the performance of our method on the Spanish ERE corpus.",
            "The results show that HNN approach performed better than LSTM and Bi-LSTM.",
            "It indicates that our proposed model could achieve the best performance in multiple languages than other neural network methods.",
            "We did not compare our system with other systems (Tanev et al., 2009), because they reported the results on a non-standard data set ."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "HNN",
                "LSTM",
                "Bi-LSTM"
            ],
            [
                "HNN"
            ],
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_4",
        "paper_id": "P16-2011",
        "valid": 1
    },
    {
        "table_id_paper": "P17-1034table_5",
        "description": "To illustrate the effectiveness of our model, we conduct experiments on the public datasets, and make comparison with the most effective traditional linguistic features, e.g., bigrams, and the three practicable traditional behavioral features (RL, RD, MCS (Mukherjee et al., 2013b)) referred in Section 3.2. The results are shown in Table 5. For our model, we set the dimension of embeddings to 100, the number of CNN filters to 100, \u00ce\u00b8 to 0.1, Z to 2. The hyper-parameters are tuned by grid search on the development dataset. The product and reviewer embeddings are randomly intialized from a uniform distribution (Socher et al., 2013). The word embeddings are initialized with 100-dimensions vectors pre-trained by the CBOW model (Word2Vec) (Mikolov et al., 2013). As Table 5 showed, our model observably performs better in detecting review spam for the cold-start task in both hotel and restaurant domains.",
        "sentences": [
            "To illustrate the effectiveness of our model, we conduct experiments on the public datasets, and make comparison with the most effective traditional linguistic features, e.g., bigrams, and the three practicable traditional behavioral features (RL, RD, MCS (Mukherjee et al., 2013b)) referred in Section 3.2.",
            "The results are shown in Table 5.",
            "For our model, we set the dimension of embeddings to 100, the number of CNN filters to 100, \u00ce\u00b8 to 0.1, Z to 2.",
            "The hyper-parameters are tuned by grid search on the development dataset.",
            "The product and reviewer embeddings are randomly intialized from a uniform distribution (Socher et al., 2013).",
            "The word embeddings are initialized with 100-dimensions vectors pre-trained by the CBOW model (Word2Vec) (Mikolov et al., 2013).",
            "As Table 5 showed, our model observably performs better in detecting review spam for the cold-start task in both hotel and restaurant domains."
        ],
        "class_sentence": [
            2,
            1,
            2,
            2,
            2,
            2,
            1
        ],
        "header_mention": [
            [
                "Ours RE",
                "Ours RE+RRE+PRE"
            ],
            null,
            [
                "Ours RE",
                "Ours RE+RRE+PRE"
            ],
            [
                "Ours RE",
                "Ours RE+RRE+PRE"
            ],
            null,
            [
                "Ours RE",
                "Ours RE+RRE+PRE"
            ],
            [
                "Ours RE",
                "Ours RE+RRE+PRE"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_5",
        "paper_id": "P17-1034",
        "valid": 1
    },
    {
        "table_id_paper": "P17-1054table_2",
        "description": "Table 2 provides the performances of the six baseline models, as well as our proposed models (i.e., RNN and CopyRNN). For each method, the table lists its F-measure at top 5 and top 10 predictions on the five datasets. The best scores are highlighted in bold and the underlines indicate the second best performances. The results show that the four unsupervised models (Tf-idf, TextRank, SingleRank and ExpandRank) have a robust performance across different datasets. The ExpandRank fails to return any result on the KP20k dataset, due to its high time complexity. The measures on NUS and SemEval here are higher than the ones reported in (Hasan and Ng, 2010) and (Kim et al., 2010), probably because we utilized the paper abstract instead of the full text for training, which may filter out some noisy information. The performance of the two supervised models (i.e., Maui and KEA) were unstable on some datasets, but Maui achieved the best performances on three datasets among all the baseline models. As for our proposed keyphrase prediction approaches, the RNN model with the attention mechanism did not perform as well as we expected. It might be because the RNN model is only concerned with finding the hidden semantics behind the text, which may tend to generate keyphrases or words that are too general and may not necessarily refer to the source text. In addition, we observe that 2.5% (70,891/2,780,316) of keyphrases in our dataset contain out-of-vocabulary words, which the RNN model is not able to recall, since the RNN model can only generate results with the 50,000 words in vocabulary. This indicates that a pure generative model may not fit the extraction task, and we need to further link back to the language usage within the source text. The CopyRNN model, by considering more contextual information, significantly outperforms not only the RNN model but also all baselines, exceeding the best baselines by more than 20% on average.",
        "sentences": [
            "Table 2 provides the performances of the six baseline models, as well as our proposed models (i.e., RNN and CopyRNN).",
            "For each method, the table lists its F-measure at top 5 and top 10 predictions on the five datasets.",
            "The best scores are highlighted in bold and the underlines indicate the second best performances.",
            "The results show that the four unsupervised models (Tf-idf, TextRank, SingleRank and ExpandRank) have a robust performance across different datasets.",
            "The ExpandRank fails to return any result on the KP20k dataset, due to its high time complexity.",
            "The measures on NUS and SemEval here are higher than the ones reported in (Hasan and Ng, 2010) and (Kim et al., 2010), probably because we utilized the paper abstract instead of the full text for training, which may filter out some noisy information.",
            "The performance of the two supervised models (i.e., Maui and KEA) were unstable on some datasets, but Maui achieved the best performances on three datasets among all the baseline models.",
            "As for our proposed keyphrase prediction approaches, the RNN model with the attention mechanism did not perform as well as we expected.",
            "It might be because the RNN model is only concerned with finding the hidden semantics behind the text, which may tend to generate keyphrases or words that are too general and may not necessarily refer to the source text.",
            "In addition, we observe that 2.5% (70,891/2,780,316) of keyphrases in our dataset contain out-of-vocabulary words, which the RNN model is not able to recall, since the RNN model can only generate results with the 50,000 words in vocabulary.",
            "This indicates that a pure generative model may not fit the extraction task, and we need to further link back to the language usage within the source text.",
            "The CopyRNN model, by considering more contextual information, significantly outperforms not only the RNN model but also all baselines, exceeding the best baselines by more than 20% on average."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            2,
            2,
            1,
            1,
            2,
            2,
            2,
            1
        ],
        "header_mention": [
            [
                "RNN",
                "CopyRNN"
            ],
            [
                "F1@5",
                "F1@10",
                "Inspec",
                "Krapivin",
                "NUS",
                "SemEval",
                "KP20k"
            ],
            null,
            [
                "Tf-Idf",
                "TextRank",
                "SingleRank",
                "ExpandRank"
            ],
            [
                "ExpandRank",
                "KP20k"
            ],
            [
                "NUS",
                "SemEval"
            ],
            [
                "Maui",
                "KEA"
            ],
            [
                "RNN"
            ],
            [
                "RNN"
            ],
            [
                "RNN"
            ],
            [
                "RNN"
            ],
            [
                "CopyRNN",
                "RNN"
            ]
        ],
        "n_sentence": 12.0,
        "table_id": "table_2",
        "paper_id": "P17-1054",
        "valid": 1
    },
    {
        "table_id_paper": "P17-1176table_4",
        "description": "Table 4 shows BLEU scores on the Europarl corpus of our proposed methods. For sentence-level approaches, the sent-beam method outperforms the sent-greedy method by +0.59 BLEU points over Spanish-French translation and +2.51 BLEU points over German-French translation on the test set. For word-level experiments, we observe that the word-sampling method performs much better than the other two methods: +1.94 BLEU points on Spanish-French translation and +1.88 BLEU points on German-French translation over the word-greedy method; +2.65 BLEU points on Spanish-French translation and +2.84 BLEU points on German-French translation over the word-beam method.",
        "sentences": [
            "Table 4 shows BLEU scores on the Europarl corpus of our proposed methods.",
            "For sentence-level approaches, the sent-beam method outperforms the sent-greedy method by +0.59 BLEU points over Spanish-French translation and +2.51 BLEU points over German-French translation on the test set.",
            "For word-level experiments, we observe that the word-sampling method performs much better than the other two methods: +1.94 BLEU points on Spanish-French translation and +1.88 BLEU points on German-French translation over the word-greedy method; +2.65 BLEU points on Spanish-French translation and +2.84 BLEU points on German-French translation over the word-beam method."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "sent-beam",
                "sent-greedy",
                "Es\u2192 Fr",
                "test"
            ],
            [
                "word-sampling",
                "Es\u2192 Fr",
                "De\u2192 Fr",
                "word-greedy",
                "word-beam"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_4",
        "paper_id": "P17-1176",
        "valid": 1
    },
    {
        "table_id_paper": "P17-1183table_1",
        "description": "On the low resource setting (CELEX), our hard attention model significantly outperforms both the recent neural models of Kann and Schutze (2016a) (MED) and Rastogi et al. (2016) (NWFST) and the morphologically aware latent variable model of Dreyer et al. (2008) (LAT), as detailed in Table 1. In addition, it significantly outperforms our implementation of the soft attention model (Soft). It is also, to our knowledge, the first model that surpassed in overall accuracy the latent variable model on this dataset. We attribute our advantage over the soft attention models to the ability of the hard attention control mechanism to harness the monotonic alignments found in the data. The advantage over the FST models may be explained by our conditioning on the entire output history which is not available in those models.",
        "sentences": [
            "On the low resource setting (CELEX), our hard attention model significantly outperforms both the recent neural models of Kann and Schutze (2016a) (MED) and Rastogi et al. (2016) (NWFST) and the morphologically aware latent variable model of Dreyer et al. (2008) (LAT), as detailed in Table 1.",
            "In addition, it significantly outperforms our implementation of the soft attention model (Soft).",
            "It is also, to our knowledge, the first model that surpassed in overall accuracy the latent variable model on this dataset.",
            "We attribute our advantage over the soft attention models to the ability of the hard attention control mechanism to harness the monotonic alignments found in the data.",
            "The advantage over the FST models may be explained by our conditioning on the entire output history which is not available in those models."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            2
        ],
        "header_mention": [
            [
                "Hard",
                "MED (Kann and Schutze 2016a)",
                "NWFST (Rastogi et al. 2016)",
                "LAT (Dreyer et al. 2008)"
            ],
            [
                "Hard",
                "Soft"
            ],
            [
                "Hard"
            ],
            [
                "Soft",
                "Hard"
            ],
            [
                "NWFST (Rastogi et al. 2016)"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_1",
        "paper_id": "P17-1183",
        "valid": 1
    },
    {
        "table_id_paper": "P17-1195table_1",
        "description": "The reasoning module (i.e., the formula rewriting and the deduction with CAS and ATP) of the system has been extensively tested on a large collection of manually formalized pre-university math problems that includes more than 1,500 problems. It solves 70% of the them in the time limit of 10 minutes per problem. Table 1 shows the rate of successfully solved problems in the manually formalized version of the benchmark problems used in the current paper.",
        "sentences": [
            "The reasoning module (i.e., the formula rewriting and the deduction with CAS and ATP) of the system has been extensively tested on a large collection of manually formalized pre-university math problems that includes more than 1,500 problems.",
            "It solves 70% of the them in the time limit of 10 minutes per problem.",
            "Table 1 shows the rate of successfully solved problems in the manually formalized version of the benchmark problems used in the current paper."
        ],
        "class_sentence": [
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Success %",
                "Avg. Time"
            ],
            [
                "Success %"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_1",
        "paper_id": "P17-1195",
        "valid": 1
    },
    {
        "table_id_paper": "P17-2030table_1",
        "description": "Table 1 shows the result of unlabeled dependency accuracy (UAS). On the error-free test set (0%), the baseline (EF pipeline) outperforms other EREF models; the accuracy is lower when the parser is trained on noisier data. The difference among the models becomes small when the test set has 10% error injection rate. As the rate increases further, the trend of parser accuracy reverses. When the test set has 15% or higher noise, the E20 is the most accurate parser.",
        "sentences": [
            "Table 1 shows the result of unlabeled dependency accuracy (UAS).",
            "On the error-free test set (0%), the baseline (EF pipeline) outperforms other EREF models; the accuracy is lower when the parser is trained on noisier data.",
            "The difference among the models becomes small when the test set has 10% error injection rate.",
            "As the rate increases further, the trend of parser accuracy reverses.",
            "When the test set has 15% or higher noise, the E20 is the most accurate parser."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "0%",
                "Baseline",
                "E05",
                "E10",
                "E15",
                "E20"
            ],
            [
                "10%"
            ],
            null,
            [
                "15%",
                "E20"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_1",
        "paper_id": "P17-2030",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1008table_3",
        "description": "Table 3 summarizes training performance and model statistics. The Transformer Base model is the fastest model in terms of training speed. RNMT+ is slower to train than the Transformer\nBig model on a per-GPU basis. However, sinc the RNMT+ model is quite stable, we were able to offset the lower per-GPU throughput with higher\nconcurrency by increasing the number of model replicas, and hence the overall time to convergence was not slowed down much. We also computed the number of floating point operations (FLOPs) in the model\u0081fs forward path as well as the number of total parameters for all architectures (cf. Table 3). RNMT+ requires fewer FLOPs than the Transformer Big model, even though both models have a comparable number of parameters.",
        "sentences": [
            "Table 3 summarizes training performance and model statistics.",
            "The Transformer Base model is the fastest model in terms of training speed.",
            "RNMT+ is slower to train than the Transformer\nBig model on a per-GPU basis.",
            "However, sinc the RNMT+ model is quite stable, we were able to offset the lower per-GPU throughput with higher\nconcurrency by increasing the number of model replicas, and hence the overall time to convergence was not slowed down much.",
            "We also computed the number of floating point operations (FLOPs) in the model\u0081fs forward path as well as the number of total parameters for all architectures (cf. Table 3).",
            "RNMT+ requires fewer FLOPs than the Transformer Big model, even though both models have a comparable number of parameters."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Trans. Base"
            ],
            [
                "RNMT+",
                "Trans. Base"
            ],
            [
                "RNMT+"
            ],
            [
                "FLOPs",
                "ConvS2S",
                "Trans. Base",
                "Trans. Big",
                "RNMT+"
            ],
            [
                "RNMT+",
                "Trans. Big"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_3",
        "paper_id": "P18-1008",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1021table_5",
        "description": "Table 5 shows that our model with separate decoder and attention over keyphrases produce significantly more informative and relevant arguments than seq2seq trained without evidence.8 . However, we also observe that human judges prefer the retrieved arguments over generation-based models, illustrating the gap between system arguments and human edited text.",
        "sentences": [
            "Table 5 shows that our model with separate decoder and attention over keyphrases produce significantly more informative and relevant arguments than seq2seq trained without evidence.8 .",
            "However, we also observe that human judges prefer the retrieved arguments over generation-based models, illustrating the gap between system arguments and human edited text."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "OUR MODEL",
                "SEQ2SEQ"
            ],
            [
                "RETRIEVAL",
                "SEQ2SEQ",
                "OUR MODEL"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_5",
        "paper_id": "P18-1021",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1038table_1",
        "description": "Our parser achieves an accuracy of 90.35 for EDS and 89.51 for DMRS in terms of ELEMENTARY DEPENDENCY MATCH (EDM) which outperforms the best existing grammar-free model (Buys and Blunsom, 2017) by a significant margin (see Table 1). This marked result affirms the value of modeling the syntacto-semantic composition process for semantic parsing. On sentences that can be parsed by ERG-guided parsers, e.g. PET2 or ACE3 , significant accuracy gaps between ERG-guided parsers and data-driven parsers are repeatedly reported (see Table 1).",
        "sentences": [
            "Our parser achieves an accuracy of 90.35 for EDS and 89.51 for DMRS in terms of ELEMENTARY DEPENDENCY MATCH (EDM) which outperforms the best existing grammar-free model (Buys and Blunsom, 2017) by a significant margin (see Table 1).",
            "This marked result affirms the value of modeling the syntacto-semantic composition process for semantic parsing.",
            "On sentences that can be parsed by ERG-guided parsers, e.g. PET2 or ACE3 , significant accuracy gaps between ERG-guided parsers and data-driven parsers are repeatedly reported (see Table 1)."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "SHRG-based",
                "EDS",
                "DMRS"
            ],
            null,
            [
                "ERG-based",
                "Data-driven"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_1",
        "paper_id": "P18-1038",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1048table_2",
        "description": "Table 2 shows the performance of multi-class classification. SELF achieves nearly the same F-score as Feng et al (2016)'s Hybrid, and outperforms the others. More importantly, SELF is the only one which obtains a performance higher than 70% for both precision and recall. Besides, by analyzing the experimental results, we have identified the following regularities:. ? Similar to the pattern classifiers that are based on hand-designed features, the CNN models enable higher precision to be obtained. However the recall is lower. ? The RNN models contribute to achieving a higher recall. However the precision is lower. ? Expansion of the training data set helps to increase the precision. Let us turn to the structurally more complicated models, SELF and Hybrid. SELF inherits the merits of the RNN models, classifying the events with higher recall. Besides, by the utilization of GAN, SELF has evolved from the traditional learning strategies, being capable of learning from GAN and getting rid of the mistakenly generated spurious features. So that it outperforms other RNNs, with improvements of no less than 4.5% precision and 1.7% recall.",
        "sentences": [
            "Table 2 shows the performance of multi-class classification.",
            "SELF achieves nearly the same F-score as Feng et al (2016)'s Hybrid, and outperforms the others.",
            "More importantly, SELF is the only one which obtains a performance higher than 70% for both precision and recall.",
            "Besides, by analyzing the experimental results, we have identified the following regularities:.",
            "? Similar to the pattern classifiers that are based on hand-designed features, the CNN models enable higher precision to be obtained.",
            "However the recall is lower.",
            "? The RNN models contribute to achieving a higher recall.",
            "However the precision is lower.",
            "? Expansion of the training data set helps to increase the precision.",
            "Let us turn to the structurally more complicated models, SELF and Hybrid.",
            "SELF inherits the merits of the RNN models, classifying the events with higher recall.",
            "Besides, by the utilization of GAN, SELF has evolved from the traditional learning strategies, being capable of learning from GAN and getting rid of the mistakenly generated spurious features.",
            "So that it outperforms other RNNs, with improvements of no less than 4.5% precision and 1.7% recall."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            1,
            1,
            1,
            1,
            2,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "SELF: Bi-LSTM+GAN",
                "Hybrid: Bi-LSTM+CNN",
                "P (%)",
                "R (%)",
                "F (%)"
            ],
            [
                "SELF: Bi-LSTM+GAN",
                "P (%)",
                "R (%)"
            ],
            null,
            [
                "CNN",
                "DM-CNN",
                "P (%)"
            ],
            [
                "CNN",
                "DM-CNN",
                "R (%)"
            ],
            [
                "FB-RNN (GRU)",
                "Bi-RNN (GRU)",
                "R (%)"
            ],
            [
                "FB-RNN (GRU)",
                "Bi-RNN (GRU)",
                "P (%)"
            ],
            [
                "P (%)"
            ],
            [
                "Hybrid: Bi-LSTM+CNN",
                "SELF: Bi-LSTM+GAN"
            ],
            [
                "SELF: Bi-LSTM+GAN",
                "FB-RNN (GRU)",
                "Bi-RNN (GRU)",
                "R (%)"
            ],
            [
                "SELF: Bi-LSTM+GAN"
            ],
            [
                "FB-RNN (GRU)",
                "SELF: Bi-LSTM+GAN",
                "R (%)"
            ]
        ],
        "n_sentence": 13.0,
        "table_id": "table_2",
        "paper_id": "P18-1048",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1050table_6",
        "description": "Table 6 shows the comparisons of our results with the performance of several previous models, which were all trained with 1,500k event chains extracted from the NYT portion of the Gigaword corpus (Graff and Cieri 2003). Each event chain consists of a sequence of verbs sharing an actor within a news article. Except Chambers and Jurafsky (2008), other recent models utilized more and more sophisticated neural language models. Granroth-Wilding and Clark (2016) proposed a two layer neural network model that learns embeddings of event predicates and their arguments for predicting the next event. Pichotta and Mooney (2016) introduced a LSTM-based language model for event prediction. Wang et al.(2017) used dynamic memory as attention in LSTM for prediction. It is encouraging that by using event knowledge extracted from automatically identified narratives, we achieved the best event prediction performance, which is 2.2% higher than the best neural network model.",
        "sentences": [
            "Table 6 shows the comparisons of our results with the performance of several previous models, which were all trained with 1,500k event chains extracted from the NYT portion of the Gigaword corpus (Graff and Cieri 2003).",
            "Each event chain consists of a sequence of verbs sharing an actor within a news article.",
            "Except Chambers and Jurafsky (2008), other recent models utilized more and more sophisticated neural language models.",
            "Granroth-Wilding and Clark (2016) proposed a two layer neural network model that learns embeddings of event predicates and their arguments for predicting the next event.",
            "Pichotta and Mooney (2016) introduced a LSTM-based language model for event prediction.",
            "Wang et al.(2017) used dynamic memory as attention in LSTM for prediction.",
            "It is encouraging that by using event knowledge extracted from automatically identified narratives, we achieved the best event prediction performance, which is 2.2% higher than the best neural network model."
        ],
        "class_sentence": [
            1,
            2,
            2,
            2,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "(Granroth-Wilding and Clark 2016)",
                "(Pichotta and Mooney 2016)",
                "(Wang et al. 2017)"
            ],
            [
                "(Granroth-Wilding and Clark 2016)"
            ],
            [
                "(Pichotta and Mooney 2016)"
            ],
            [
                "(Wang et al. 2017)"
            ],
            [
                "(Wang et al. 2017)",
                "Our Results"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_6",
        "paper_id": "P18-1050",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1076table_5",
        "description": "Table 5 shows that for the NE dataset, the two strategies performequally well on the Dev set, whereas the Subj/Obj strategy works slightly better on the Test set. For Common Nouns, Subj/Obj is better.",
        "sentences": [
            "Table 5 shows that for the NE dataset, the two strategies performequally well on the Dev set, whereas the Subj/Obj strategy works slightly better on the Test set.",
            "For Common Nouns, Subj/Obj is better."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "NE",
                "Subj/Obj",
                "Obj/Obj",
                "Dev",
                "Test"
            ],
            [
                "CN",
                "Subj/Obj"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_5",
        "paper_id": "P18-1076",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1101table_4",
        "description": "Table 4 shows that both methods work well and DI-VST achieved better homogeneity than DI-VAE.",
        "sentences": [
            "Table 4 shows that both methods work well and DI-VST achieved better homogeneity than DI-VAE."
        ],
        "class_sentence": [
            1
        ],
        "header_mention": [
            [
                "DI-VST",
                "DI-VAE"
            ]
        ],
        "n_sentence": 1.0,
        "table_id": "table_4",
        "paper_id": "P18-1101",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1102table_4",
        "description": "Table 4 shows the human evaluation results. We can observe that: (1) SC-Seq2SeqNIWF s=1 generates the most informative responses and interesting (labeled as \u0081g+2\u0081h) and the least general responses than all the baseline models. Meanwhile, SC-Seq2SeqNIWF s=0 generates the most general responses (labeled as \u0081g+1\u0081h); (2) MARM generates the most bad responses (labeled as \u0081g+0\u0081h), which indicates the drawbacks of the unknown latent responding mechanisms; (3) The kappa values of our models are all larger than 0.4, considered as \u0081gmoderate agreement\u0081h regarding quality of responses. The largest kappa value is achieved by SC-Seq2SeqNIWF s=0, which seems reasonable since it is easy to reach an agreement on general responses. Sign tests demonstrate the improvements of SC-Seq2SeqNIWF s=1 to the baseline models are statistically significant (p-value < 0.01). All the human judgement results again demonstrate the effectiveness of our controlled generation mechanism.",
        "sentences": [
            "Table 4 shows the human evaluation results.",
            "We can observe that: (1) SC-Seq2SeqNIWF s=1 generates the most informative responses and interesting (labeled as \u0081g+2\u0081h) and the least general responses than all the baseline models.",
            "Meanwhile, SC-Seq2SeqNIWF s=0 generates the most general responses (labeled as \u0081g+1\u0081h); (2) MARM generates the most bad responses (labeled as \u0081g+0\u0081h), which indicates the drawbacks of the unknown latent responding mechanisms; (3) The kappa values of our models are all larger than 0.4, considered as \u0081gmoderate agreement\u0081h regarding quality of responses.",
            "The largest kappa value is achieved by SC-Seq2SeqNIWF s=0, which seems reasonable since it is easy to reach an agreement on general responses.",
            "Sign tests demonstrate the improvements of SC-Seq2SeqNIWF s=1 to the baseline models are statistically significant (p-value < 0.01).",
            "All the human judgement results again demonstrate the effectiveness of our controlled generation mechanism."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "SC-Seq2SeqNIWF s=1",
                "+2"
            ],
            [
                "SC-Seq2SeqNIWF s=0",
                "+1",
                "kappa",
                "SC-Seq2SeqNIWF s=1",
                "SC-Seq2SeqNIWF s=0.5"
            ],
            [
                "kappa",
                "SC-Seq2SeqNIWF s=0"
            ],
            null,
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_4",
        "paper_id": "P18-1102",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1145table_2",
        "description": "Table 2 shows the results on ACE2005 and KBPEval2017. From this table, we can see that:. 1) NPNs steadily outperform all baselines significantly. Compared with baselines, NPN(Taskspecific) gains at least 1.6 (2.5%) and 1.5 (3.0%) F1-score improvements on trigger classification task on ACE2005 and KBPEval2017 respectively. 2) By exploiting compositional structures of triggers, our trigger nugget generator can effectively resolve the word-trigger mismatch problem. As shown in Table 2, NPN(Taskspecific) achieved significant F1-score improvements on trigger identification task on both datasets. It is notable that our method achieved a remarkable high recall on both datasets, which indicates that NPNs do detect a number of triggers which previous methods can not identify. 3) By summarizing information from both characters and words, the hybrid representation learning is effective for event detection. Comparing with corresponding characterbased methods3 , word-based methods achieved 2 to 3 F1-score improvements, which indicates that words can provide additional information for event detection. By combining character-level and word-level features, NPNs are able to perform character-based event detection meanwhile take word-level knowledge into consideration too.",
        "sentences": [
            "Table 2 shows the results on ACE2005 and KBPEval2017.",
            "From this table, we can see that:.",
            "1) NPNs steadily outperform all baselines significantly.",
            "Compared with baselines, NPN(Taskspecific) gains at least 1.6 (2.5%) and 1.5 (3.0%) F1-score improvements on trigger classification task on ACE2005 and KBPEval2017 respectively.",
            "2) By exploiting compositional structures of triggers, our trigger nugget generator can effectively resolve the word-trigger mismatch problem.",
            "As shown in Table 2, NPN(Taskspecific) achieved significant F1-score improvements on trigger identification task on both datasets.",
            "It is notable that our method achieved a remarkable high recall on both datasets, which indicates that NPNs do detect a number of triggers which previous methods can not identify.",
            "3) By summarizing information from both characters and words, the hybrid representation learning is effective for event detection.",
            "Comparing with corresponding characterbased methods3 , word-based methods achieved 2 to 3 F1-score improvements, which indicates that words can provide additional information for event detection.",
            "By combining character-level and word-level features, NPNs are able to perform character-based event detection meanwhile take word-level knowledge into consideration too."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            1,
            2,
            2,
            1,
            2
        ],
        "header_mention": [
            [
                "ACE2005",
                "KBPEval2017"
            ],
            null,
            [
                "NPN(Concat)",
                "NPN(General)",
                "NPN(Task-specific)"
            ],
            [
                "NPN(Task-specific)",
                "ACE2005",
                "KBPEval2017",
                "F1"
            ],
            [
                "Trigger Identification",
                "Trigger Classification"
            ],
            [
                "NPN(Task-specific)",
                "Trigger Identification",
                "Trigger Classification"
            ],
            [
                "NPN(Task-specific)"
            ],
            null,
            [
                "NPN(Concat)",
                "NPN(General)",
                "NPN(Task-specific)",
                "C-BiLSTM*",
                "FBRNN(Char)",
                "DMCNN(Char)",
                "F1"
            ],
            [
                "NPN(Concat)",
                "NPN(General)",
                "NPN(Task-specific)"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_2",
        "paper_id": "P18-1145",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1168table_4",
        "description": "Table 4 describes our main results. Our weakly-supervised semantic parser with re-ranking (W.+DISC) obtains 84.0 accuracy and 65.0 consistency on the public test set and 82.5 accuracy and 63.9 on the hidden one, improving accuracy by 14.7 points compared to state-of-theart. The accuracy of the rule-based parser (RULE) is less than 2 points below MAXENT, showing that a semantic parsing approach is very suitable for this task. The supervised parser obtains better performance (especially in consistency), and with re-ranking reaches 76.6 accuracy, showing that generalizing from generated examples is better than memorizing manually-defined patterns. Our weakly-supervised parser significantly improves over SUP., reaching an accuracy of 81.7 before reranking, and 84.0 after re-ranking (on the public test set). Consistency results show an even crisper trend of improvement across the models.",
        "sentences": [
            "Table 4 describes our main results.",
            "Our weakly-supervised semantic parser with re-ranking (W.+DISC) obtains 84.0 accuracy and 65.0 consistency on the public test set and 82.5 accuracy and 63.9 on the hidden one, improving accuracy by 14.7 points compared to state-of-theart.",
            "The accuracy of the rule-based parser (RULE) is less than 2 points below MAXENT, showing that a semantic parsing approach is very suitable for this task.",
            "The supervised parser obtains better performance (especially in consistency), and with re-ranking reaches 76.6 accuracy, showing that generalizing from generated examples is better than memorizing manually-defined patterns.",
            "Our weakly-supervised parser significantly improves over SUP., reaching an accuracy of 81.7 before reranking, and 84.0 after re-ranking (on the public test set).",
            "Consistency results show an even crisper trend of improvement across the models."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "W.+DISC",
                "Test-P",
                "Acc.",
                "Con.",
                "Test-H"
            ],
            [
                "RULE",
                "Acc.",
                "MAXENT"
            ],
            [
                "SUP.+DISC",
                "Test-P",
                "Acc."
            ],
            [
                "WEAKSUP.",
                "SUP.",
                "Test-P",
                "Acc."
            ],
            [
                "Con."
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_4",
        "paper_id": "P18-1168",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1177table_5",
        "description": "Table 5 shows the results of the human evaluation. Bold indicates top scores. We see that the original human questions are preferred over the two NQG ssystems\u0081foutputs, which is understandable given the examples in Figure 3. The humangenerated questions make more sense and correspond better with the provided answers, particularly when they require information in the preceding context. How exactly to capture the preceding context so as to ask better and more diverse questions is an interesting future direction for research. In terms of grammaticality, however, the neural models do quite well, achieving very close to human performance. In addition, we see that our method (CorefNQG) performs statistically significantly better across all metrics in comparison to the baseline model (ContextNQG), which has access to the entire preceding context in the passage.",
        "sentences": [
            "Table 5 shows the results of the human evaluation.",
            "Bold indicates top scores.",
            "We see that the original human questions are preferred over the two NQG ssystems\u0081foutputs, which is understandable given the examples in Figure 3.",
            "The humangenerated questions make more sense and correspond better with the provided answers, particularly when they require information in the preceding context.",
            "How exactly to capture the preceding context so as to ask better and more diverse questions is an interesting future direction for research.",
            "In terms of grammaticality, however, the neural models do quite well, achieving very close to human performance.",
            "In addition, we see that our method (CorefNQG) performs statistically significantly better across all metrics in comparison to the baseline model (ContextNQG), which has access to the entire preceding context in the passage."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Human",
                "ContextNQG",
                "CorefNQG"
            ],
            [
                "Human"
            ],
            null,
            [
                "Grammaticality",
                "CorefNQG",
                "Human"
            ],
            [
                "CorefNQG",
                "ContextNQG"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_5",
        "paper_id": "P18-1177",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1188table_3",
        "description": "The results of our human evaluation study are shown in Table 3. As one might imagine, HUMAN gets ranked 1st most of the time (41%). However, it is closely followed by XNET which ranked 1st 28% of the time. In comparison, POINTERNET and LEAD were mostly ranked at 3rd and 4th places. We also carried out pairwise comparisons between all models in Table 3 for their statistical significance using a one-way ANOVA with post-hoc Tukey HSD tests with (p < 0.01). It showed that XNET is significantly better than LEAD and POINTERNET, and it does not differ significantly from HUMAN. On the other hand, POINTERNET does not differ significantly from LEAD and it differs significantly from both XNET and HUMAN. The human evaluation results corroborates our empirical results in Table 1 and Table 2: XNET is better than LEAD and POINTERNET in producing informative and fluent summaries.",
        "sentences": [
            "The results of our human evaluation study are shown in Table 3.",
            "As one might imagine, HUMAN gets ranked 1st most of the time (41%).",
            "However, it is closely followed by XNET which ranked 1st 28% of the time.",
            "In comparison, POINTERNET and LEAD were mostly ranked at 3rd and 4th places.",
            "We also carried out pairwise comparisons between all models in Table 3 for their statistical significance using a one-way ANOVA with post-hoc Tukey HSD tests with (p < 0.01).",
            "It showed that XNET is significantly better than LEAD and POINTERNET, and it does not differ significantly from HUMAN.",
            "On the other hand, POINTERNET does not differ significantly from LEAD and it differs significantly from both XNET and HUMAN.",
            "The human evaluation results corroborates our empirical results in Table 1 and Table 2: XNET is better than LEAD and POINTERNET in producing informative and fluent summaries."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "HUMAN"
            ],
            [
                "XNET"
            ],
            [
                "LEAD",
                "POINTERNET"
            ],
            null,
            [
                "XNET",
                "LEAD",
                "POINTERNET",
                "HUMAN"
            ],
            [
                "LEAD",
                "POINTERNET",
                "XNET",
                "HUMAN"
            ],
            [
                "XNET",
                "LEAD",
                "POINTERNET"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_3",
        "paper_id": "P18-1188",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1191table_6",
        "description": "Table 6 shows precision and recall for joint span detection and question generation, using exact match for both. This metric is exceedingly hard, but it shows that almost 40% of predictions are exactly correct in both span and question.",
        "sentences": [
            "Table 6 shows precision and recall for joint span detection and question generation, using exact match for both.",
            "This metric is exceedingly hard, but it shows that almost 40% of predictions are exactly correct in both span and question."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "Span + Local",
                "Span + Seq. (tau = 0.5)",
                "P",
                "R"
            ],
            [
                "Span + Seq. (tau = 0.5)",
                "P",
                "R"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_6",
        "paper_id": "P18-1191",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1211table_2",
        "description": "Table 2 shows performance of the method compared with other approaches for coherence prediction. We note that Sequential-CG performs competitively with the state-of-the-art for generative approaches for the task, while needing no other annotation. In comparison, the HMM based approaches use significant annotation and syntactic features. Sequential-CG also outperforms several discriminative approaches for the task.",
        "sentences": [
            "Table 2 shows performance of the method compared with other approaches for coherence prediction.",
            "We note that Sequential-CG performs competitively with the state-of-the-art for generative approaches for the task, while needing no other annotation.",
            "In comparison, the HMM based approaches use significant annotation and syntactic features.",
            "Sequential-CG also outperforms several discriminative approaches for the task."
        ],
        "class_sentence": [
            1,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Sequential CG"
            ],
            [
                "HMM (2012)"
            ],
            [
                "Sequential CG",
                "Entity-Grid (2008)",
                "Graph (2013)",
                "Earthquakes"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "P18-1211",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1230table_3",
        "description": "Furthermore, Table 3 shows the effectiveness of multi-pass operation in the memory module. It shows that multiple passes operation performs better than one pass, though the improvement is not significant. The reason of this phenomenon is that for most target words, one main word sense accounts for the majority of their appearances. Therefore, in most circumstances, one-pass inference can lead to the correct word senses. Case studies in Table 2 show that the proposed multipass inference can help to recognize the infrequent senses like the third sense for word play. In Table 3, with the increasing number of passes, the F1-score increases. However, when the number of passes is larger than 3, the F1-score stops increasing or even decreases due to over-fitting. It shows that appropriate number of passes can boost the performance as well as avoid over-fitting of the model.",
        "sentences": [
            "Furthermore, Table 3 shows the effectiveness of multi-pass operation in the memory module.",
            "It shows that multiple passes operation performs better than one pass, though the improvement is not significant.",
            "The reason of this phenomenon is that for most target words, one main word sense accounts for the majority of their appearances.",
            "Therefore, in most circumstances, one-pass inference can lead to the correct word senses.",
            "Case studies in Table 2 show that the proposed multipass inference can help to recognize the infrequent senses like the third sense for word play.",
            "In Table 3, with the increasing number of passes, the F1-score increases.",
            "However, when the number of passes is larger than 3, the F1-score stops increasing or even decreases due to over-fitting.",
            "It shows that appropriate number of passes can boost the performance as well as avoid over-fitting of the model."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            0,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "Pass",
                "1"
            ],
            [
                "Pass",
                "1"
            ],
            [
                "Pass",
                "1"
            ],
            null,
            [
                "Pass",
                "1",
                "2",
                "3",
                "SE2",
                "SE3",
                "SE13",
                "SE15"
            ],
            [
                "Pass",
                "4",
                "5"
            ],
            [
                "Pass"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_3",
        "paper_id": "P18-1230",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1238table_2",
        "description": "The results are presented in Table 2 and show that, out of 3 annotations, over 90% of the captions receive a majority (2+) of GOOD judgments. This indicates that the Conceptual Captions pipeline, though involving extensive algorithmic processing, produces high-quality image captions.",
        "sentences": [
            "The results are presented in Table 2 and show that, out of 3 annotations, over 90% of the captions receive a majority (2+) of GOOD judgments.",
            "This indicates that the Conceptual Captions pipeline, though involving extensive algorithmic processing, produces high-quality image captions."
        ],
        "class_sentence": [
            1,
            2
        ],
        "header_mention": [
            [
                "Conceptual Captions",
                "GOOD (out of 3)",
                "1+",
                "2+"
            ],
            [
                "Conceptual Captions"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_2",
        "paper_id": "P18-1238",
        "valid": 1
    },
    {
        "table_id_paper": "P18-1239table_1",
        "description": "Table 1 shows the fraction of images that were judged to be good representations of the search word. It also demonstrates that as the concreteness of a word increases, the proportion of good images associated with that word increases as well. We further discuss the role of concreteness in Section 6.1. Overall, 85% of the English images, 72% of French, 66% of Indonesian, and 60% of Uzbek were judged to be good.",
        "sentences": [
            "Table 1 shows the fraction of images that were judged to be good representations of the search word.",
            "It also demonstrates that as the concreteness of a word increases, the proportion of good images associated with that word increases as well.",
            "We further discuss the role of concreteness in Section 6.1.",
            "Overall, 85% of the English images, 72% of French, 66% of Indonesian, and 60% of Uzbek were judged to be good."
        ],
        "class_sentence": [
            1,
            1,
            0,
            1
        ],
        "header_mention": [
            null,
            [
                "Concreteness Ratings"
            ],
            null,
            [
                "Overall",
                "English",
                "French",
                "Indonesian",
                "Uzbek"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_1",
        "paper_id": "P18-1239",
        "valid": 1
    },
    {
        "table_id_paper": "P18-2009table_3",
        "description": "Table 3 indicates that our method, denoted by triplet-sen, clearly outperforms the other tested methods. Surprizingly, skip-thoughts-SICK is inferior to skip-thoughts-CS.",
        "sentences": [
            "Table 3 indicates that our method, denoted by triplet-sen, clearly outperforms the other tested methods.",
            "Surprizingly, skip-thoughts-SICK is inferior to skip-thoughts-CS."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            [
                "triplets-sen",
                "mean-vectors",
                "skip-thoughts-CS",
                "skip-thoughts-SICK"
            ],
            [
                "skip-thoughts-CS",
                "skip-thoughts-SICK"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_3",
        "paper_id": "P18-2009",
        "valid": 1
    },
    {
        "table_id_paper": "P18-2015table_3",
        "description": "Table 3 presents the performance of noise reduction methods. Recall that the K-means-based method achieves a high P@50 for the seed selection method. Our assumption is that each cluster may represent a set in which elements have similar semantic properties. However, we observed that as the number of relations is relatively high and there is no distinct definition between some relations in the distantly labeled data (e.g., the following three relations /location/country/capital, are quite /location/province/capital, and /location/us state/capital, we decided not to perform the K-means-based method for our noise reduction task. The performances of the HITS-based, LSA-based, and NMF-based noise reduction methods are presented in Table 3. We experimentally set the portion of retained data from the distantly labeled data to 90%, given that the performance can be affected if too many sentences are removed from the original data. We also perform experiments with an ensemble method that combines the HITS-based and LSA-based strategies to merge rankings from their outputs, with half of the triples coming from the LSA-based method and the other half from the HITS-based method. Table 3 indicates that our proposed methods improved the performance of all CNN and PCNN models. Our ensemble method achieved the best improvements for three out of four systems, except that the HITS-based method obtained the best score for CNN+ONE.",
        "sentences": [
            "Table 3 presents the performance of noise reduction methods.",
            "Recall that the K-means-based method achieves a high P@50 for the seed selection method.",
            "Our assumption is that each cluster may represent a set in which elements have similar semantic properties.",
            "However, we observed that as the number of relations is relatively high and there is no distinct definition between some relations in the distantly labeled data (e.g., the following three relations /location/country/capital, are quite /location/province/capital, and /location/us state/capital, we decided not to perform the K-means-based method for our noise reduction task.",
            "The performances of the HITS-based, LSA-based, and NMF-based noise reduction methods are presented in Table 3.",
            "We experimentally set the portion of retained data from the distantly labeled data to 90%, given that the performance can be affected if too many sentences are removed from the original data.",
            "We also perform experiments with an ensemble method that combines the HITS-based and LSA-based strategies to merge rankings from their outputs, with half of the triples coming from the LSA-based method and the other half from the HITS-based method.",
            "Table 3 indicates that our proposed methods improved the performance of all CNN and PCNN models.",
            "Our ensemble method achieved the best improvements for three out of four systems, except that the HITS-based method obtained the best score for CNN+ONE."
        ],
        "class_sentence": [
            1,
            0,
            2,
            2,
            1,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            [
                "Original+HITS",
                "Original+LSA",
                "Original+NMF"
            ],
            null,
            [
                "Original+Ensemble"
            ],
            [
                "CNN+ONE",
                "CNN+ATT",
                "PCNN+ONE",
                "PCNN+ATT",
                "Original+HITS",
                "Original+Ensemble"
            ],
            [
                "Original+Ensemble",
                "CNN+ATT",
                "PCNN+ONE",
                "PCNN+ATT"
            ]
        ],
        "n_sentence": 9.0,
        "table_id": "table_3",
        "paper_id": "P18-2015",
        "valid": 1
    },
    {
        "table_id_paper": "P18-2022table_1",
        "description": "Table 1 shows that the alignment framework performs better than either baseline. MSFC yields the highest recall and lowest AER with an absolute improvement of 0%, 19%, and 10% for precision, recall and AER, over the 1-second delay baseline. Modified k-means achieves higher precision with an absolute improvement of 6%, 14%, and 14%\nover baseline.",
        "sentences": [
            "Table 1 shows that the alignment framework performs better than either baseline.",
            "MSFC yields the highest recall and lowest AER with an absolute improvement of 0%, 19%, and 10% for precision, recall and AER, over the 1-second delay baseline.",
            "Modified k-means achieves higher precision with an absolute improvement of 6%, 14%, and 14%\nover baseline."
        ],
        "class_sentence": [
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "Alignment framework",
                "Simultaneous",
                "1-second delay"
            ],
            [
                "Alignment framework",
                "1-second delay",
                "MSFC",
                "Precision",
                "Recall",
                "AER"
            ],
            [
                "Alignment framework",
                "Modified k-means",
                "1-second delay",
                "Precision",
                "Recall",
                "AER"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_1",
        "paper_id": "P18-2022",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1013table_8",
        "description": "Results . Table 8 shows the F1 scores of depccg in the respective settings. Remarkably, we observe huge additive performance improvement. While, in terms of labeled F1, ELMo contributes about 4 points on top of the plain depccg, adding the new training set (converted from dependency trees) improves more than 10 points. Examining the resulting trees, we observe that the huge gain is primarily involved with expressions unique to math.",
        "sentences": [
            "Results .",
            "Table 8 shows the F1 scores of depccg in the respective settings.",
            "Remarkably, we observe huge additive performance improvement.",
            "While, in terms of labeled F1, ELMo contributes about 4 points on top of the plain depccg, adding the new training set (converted from dependency trees) improves more than 10 points.",
            "Examining the resulting trees, we observe that the huge gain is primarily involved with expressions unique to math."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            [
                "UF1",
                "LF1"
            ],
            [
                "+ Proposed"
            ],
            [
                "LF1",
                "+ ELMo",
                "depccg",
                "+ Proposed"
            ],
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_8",
        "paper_id": "P19-1013",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1046table_4",
        "description": "4.3.2 Discussion on Modality Importance. To explore the underlying information of each modality, we carry out an experiment to compare the performance among unimodal, bimodal and trimodal models. For unimodal models, we can infer from Table 4 that language modality is the most predictive for emotion prediction, outperforming acoustic and visual modalities with significant margin. When coupled with acoustic and visual modalities, the trimodal HFFN performs best, whose result is 1% ~ 2% better than the language-HFFN, indicting that acoustic and visual modalities actually play auxiliary roles while language is dominant. However, in our model, when conducting outer product, all three modalities are treated equally, which is probably not the optimal choice. In the future, we aim to develop a fusion technique paying more attention to the language modality, while the other two modalities only serve as accessory sources of information.",
        "sentences": [
            "4.3.2 Discussion on Modality Importance.",
            "To explore the underlying information of each modality, we carry out an experiment to compare the performance among unimodal, bimodal and trimodal models.",
            "For unimodal models, we can infer from Table 4 that language modality is the most predictive for emotion prediction, outperforming acoustic and visual modalities with significant margin.",
            "When coupled with acoustic and visual modalities, the trimodal HFFN performs best, whose result is 1% ~ 2% better than the language-HFFN, indicting that acoustic and visual modalities actually play auxiliary roles while language is dominant.",
            "However, in our model, when conducting outer product, all three modalities are treated equally, which is probably not the optimal choice.",
            "In the future, we aim to develop a fusion technique paying more attention to the language modality, while the other two modalities only serve as accessory sources of information."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "L",
                "A",
                "V"
            ],
            [
                "L+A+V",
                "L"
            ],
            [
                "L+A+V"
            ],
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_4",
        "paper_id": "P19-1046",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1064table_2",
        "description": "Since mention detection is a subtask of coreference resolution, it is worthwhile to study the performance. Table 2 shows the mention detection results on the test set. Similar to coreference linking results, our model achieves higher precision and F1 score, which indicates that our model can significantly reduce false positive mentions while it can still \u00ef\u00ac\u0081nd a reasonable number of mentions.",
        "sentences": [
            "Since mention detection is a subtask of coreference resolution, it is worthwhile to study the performance.",
            "Table 2 shows the mention detection results on the test set.",
            "Similar to coreference linking results, our model achieves higher precision and F1 score, which indicates that our model can significantly reduce false positive mentions while it can still \u00ef\u00ac\u0081nd a reasonable number of mentions."
        ],
        "class_sentence": [
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Our full model",
                "Prec.",
                "F1"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_2",
        "paper_id": "P19-1064",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1117table_2",
        "description": "The main results on the one-to-many translation scenario, including one-to-two, one-to-three and one-to-four translation tasks are reported in Table 2. We present a typical Multi-NMT adopting Johnson et al. (2017) method on Transformer as our Multi-NMT baselines model. Obviously, Multi-NMT Baselines cannot outperform NMT Baselines in all cases, among which four directions are comparable and twelve are worse. With respect to our proposed method, it is clear that our compact method consistently outperforms the baseline systems. Compared with another strong one-to-many translation model Three-Stgy proposed by Wang et al. (2018), our compact method can achieve better results as well. Moreover, our method can perform even better than individually trained systems in most cases (eleven out of sixteen cases). The results demonstrate the effectiveness of our method.",
        "sentences": [
            "The main results on the one-to-many translation scenario, including one-to-two, one-to-three and one-to-four translation tasks are reported in Table 2.",
            "We present a typical Multi-NMT adopting Johnson et al. (2017) method on Transformer as our Multi-NMT baselines model.",
            "Obviously, Multi-NMT Baselines cannot outperform NMT Baselines in all cases, among which four directions are comparable and twelve are worse.",
            "With respect to our proposed method, it is clear that our compact method consistently outperforms the baseline systems.",
            "Compared with another strong one-to-many translation model Three-Stgy proposed by Wang et al. (2018), our compact method can achieve better results as well.",
            "Moreover, our method can perform even better than individually trained systems in most cases (eleven out of sixteen cases).",
            "The results demonstrate the effectiveness of our method."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "One-to-Two",
                "One-to-Three",
                "One-to-Four"
            ],
            [
                "Multi-NMT Baselines Johnson et al.(2017)"
            ],
            [
                "Multi-NMT Baselines Johnson et al.(2017)",
                "NMT Baselines"
            ],
            [
                "Rep+Emb+Attn+Dis"
            ],
            [
                "Three-Stgy Wang et al.(2018)",
                "Rep+Emb+Attn+Dis"
            ],
            [
                "Rep+Emb+Attn+Dis"
            ],
            [
                "Rep+Emb+Attn+Dis"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_2",
        "paper_id": "P19-1117",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1134table_1",
        "description": "5.1 Held-out Evaluation. DISTRE with selective attention achieves a new state-of-the-art AUC value of 0.422. The precision-recall curve in Figure 4 shows that it outperforms RESIDE and PCNN+ATT at higher recall levels, while precision is lower for top predicted relation instances. The results of the PCNN+ATT model indicate that its performance is only better in the very beginning of the curve, but its precision drops early and only achieves an AUC value of 0.341. Similar, RESIDE performs better in the beginning but drops in precision after a recall-level of approximately 0.25. This suggests that our method yields a more balanced overall performance, which we believe is important in many real-world applications. Table 1 also shows detailed precision values measured at different points along the P-R curve. We again can observe that while DISTRE has lower precision for the top 500 predicted relation instances, it shows a state-of-the-art precision of 60.2% for the top 1000 and continues to perform higher for the remaining, much larger part of the predictions.",
        "sentences": [
            "5.1 Held-out Evaluation.",
            "DISTRE with selective attention achieves a new state-of-the-art AUC value of 0.422.",
            "The precision-recall curve in Figure 4 shows that it outperforms RESIDE and PCNN+ATT at higher recall levels, while precision is lower for top predicted relation instances.",
            "The results of the PCNN+ATT model indicate that its performance is only better in the very beginning of the curve, but its precision drops early and only achieves an AUC value of 0.341.",
            "Similar, RESIDE performs better in the beginning but drops in precision after a recall-level of approximately 0.25.",
            "This suggests that our method yields a more balanced overall performance, which we believe is important in many real-world applications.",
            "Table 1 also shows detailed precision values measured at different points along the P-R curve.",
            "We again can observe that while DISTRE has lower precision for the top 500 predicted relation instances, it shows a state-of-the-art precision of 60.2% for the top 1000 and continues to perform higher for the remaining, much larger part of the predictions."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "DISTRE",
                "AUC"
            ],
            [
                "RESIDE",
                "PCNN+ATT"
            ],
            [
                "PCNN+ATT",
                "AUC"
            ],
            [
                "RESIDE"
            ],
            [
                "DISTRE"
            ],
            [
                "P@100",
                "P@200",
                "P@300",
                "P@500",
                "P@1000",
                "P@2000"
            ],
            [
                "DISTRE"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_1",
        "paper_id": "P19-1134",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1170table_2",
        "description": "4.1.1 Detailed Comparison to Basic SVD. We present a more detailed comparison to the SVD method described in (Smith et al., 2017). We focus on methods in their base form, that is without post-processing techniques, i.e. cross-domain similarity local scaling (CSLS) (Lample et al., 2018) or inverted softmax (ISF) (Smithet al., 2017). Note that (Smith et al., 2017) used the scikit-learn 2 implementation of CCA, which uses an iterative estimation of partial least squares. This does not give the same results as the standard CCA procedure. In Table 2 we reproduce the results from (Smith et al., 2017) using the dictionaries and embeddings provided by (Dinu et al., 2014)3 and we compare our method (IBFA) using both the expert dictionaries from (Dinu et al., 2014) and the pseudo-dictionaries as constructed in (Smith et al., 2017). We significantly outperform both SVD and CCA, especially when using the pseudo-dictionaries.",
        "sentences": [
            "4.1.1 Detailed Comparison to Basic SVD.",
            "We present a more detailed comparison to the SVD method described in (Smith et al., 2017).",
            "We focus on methods in their base form, that is without post-processing techniques, i.e. cross-domain similarity local scaling (CSLS) (Lample et al., 2018) or inverted softmax (ISF) (Smithet al., 2017).",
            "Note that (Smith et al., 2017) used the scikit-learn 2 implementation of CCA, which uses an iterative estimation of partial least squares.",
            "This does not give the same results as the standard CCA procedure.",
            "In Table 2 we reproduce the results from (Smith et al., 2017) using the dictionaries and embeddings provided by (Dinu et al., 2014)3 and we compare our method (IBFA) using both the expert dictionaries from (Dinu et al., 2014) and the pseudo-dictionaries as constructed in (Smith et al., 2017).",
            "We significantly outperform both SVD and CCA, especially when using the pseudo-dictionaries."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "SVD"
            ],
            null,
            [
                "CCA"
            ],
            [
                "CCA"
            ],
            [
                "IBFA (Ours)"
            ],
            [
                "IBFA (Ours)",
                "SVD",
                "CCA"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_2",
        "paper_id": "P19-1170",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1193table_4",
        "description": "Adversarial training . Another conclusion is that adversarial training can better benefit the model to enhance the topic-consistency of the generated essay compared to memory mechanism. In detail, Table 4 shows that the consistency score given by humans for ablated versions without adversarial training and memory mechanism decline 0.53 and 0.31, respectively. The reason is that the discriminative signal in training not only evaluates the quality of the generated text, but also models its degree of association with the input topics, thus enhancing the topic-consistency.",
        "sentences": [
            "Adversarial training .",
            "Another conclusion is that adversarial training can better benefit the model to enhance the topic-consistency of the generated essay compared to memory mechanism.",
            "In detail, Table 4 shows that the consistency score given by humans for ablated versions without adversarial training and memory mechanism decline 0.53 and 0.31, respectively.",
            "The reason is that the discriminative signal in training not only evaluates the quality of the generated text, but also models its degree of association with the input topics, thus enhancing the topic-consistency."
        ],
        "class_sentence": [
            2,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "w/o Adversarial",
                "w/o Memory",
                "Consistency"
            ],
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_4",
        "paper_id": "P19-1193",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1230table_2",
        "description": "When lambda is set to 0 and 1, our joint span HPSG parser works as the dependency-only parser and constituent-only parser respectively. Table 2 shows that even in such a work mode, our HPSG parser still outperforms the separate constituent parser in terms of either constituent and dependency parsing performance. As lambda is set to 0.5, our HPSG parser will give constituent and dependency structures at the same time, which are shown better than the work alone mode of either constituent or dependency parsing. Besides, the comparison also shows that the directly predicted dependencies from our model are slightly better than those converted from the predicted constituent parse trees.",
        "sentences": [
            "When lambda is set to 0 and 1, our joint span HPSG parser works as the dependency-only parser and constituent-only parser respectively.",
            "Table 2 shows that even in such a work mode, our HPSG parser still outperforms the separate constituent parser in terms of either constituent and dependency parsing performance.",
            "As lambda is set to 0.5, our HPSG parser will give constituent and dependency structures at the same time, which are shown better than the work alone mode of either constituent or dependency parsing.",
            "Besides, the comparison also shows that the directly predicted dependencies from our model are slightly better than those converted from the predicted constituent parse trees."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "HPSG Parser joint span lambda = 0.0",
                "HPSG Parser joint span lambda = 1.0"
            ],
            [
                "HPSG Parser joint span lambda = 0.0",
                "HPSG Parser joint span lambda = 1.0"
            ],
            [
                "HPSG Parser joint span lambda = 0.5",
                "HPSG Parser joint span lambda = 0.0",
                "HPSG Parser joint span lambda = 1.0"
            ],
            [
                "HPSG Parser converted dependency",
                "converted dependency"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "P19-1230",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1276table_5",
        "description": "Slot Coherence. Table 5 shows the comparison of averaged slot coherence results over all the slots in  the  schemas.   . Note  that  we  do  not  report  the slot coherence for the Clusteringmodel because it does not output the top-N head words in eachslot.  . The averaged slot coherence of ODEE-FER is the highest, which is consistent with the conclusion from Table 4.   . The averaged slot coherence of ODEE-F is comparable to that of Nguyen et al.(2015)(p= 0.3415),  which again demonstrates that  the  contextual  features  are  a  strong  alterna-tive to discrete features. The scores of ODEE-FE(p= 0.06) and ODEE-FER(p= 10^5) are both higher than that of ODEE-F, which proves that the latent event type is critical in ODEE.",
        "sentences": [
            "Slot Coherence.",
            "Table 5 shows the comparison of averaged slot coherence results over all the slots in  the  schemas.   .",
            "Note  that  we  do  not  report  the slot coherence for the Clusteringmodel because it does not output the top-N head words in eachslot.  .",
            "The averaged slot coherence of ODEE-FER is the highest, which is consistent with the conclusion from Table 4.   .",
            "The averaged slot coherence of ODEE-F is comparable to that of Nguyen et al.(2015)(p= 0.3415),  which again demonstrates that  the  contextual  features  are  a  strong  alterna-tive to discrete features.",
            "The scores of ODEE-FE(p= 0.06) and ODEE-FER(p= 10^5) are both higher than that of ODEE-F, which proves that the latent event type is critical in ODEE."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Ave Slot Coherence"
            ],
            null,
            [
                "ODEE-FER"
            ],
            [
                "Ave Slot Coherence",
                "ODEE-F",
                "Nguyen et al. (2015)"
            ],
            [
                "ODEE-FE",
                "ODEE-FER",
                "ODEE-F"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_5",
        "paper_id": "P19-1276",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1290table_1",
        "description": "Main Results . Table 1 shows the performance of various models on EN-DE and EN-FR translation tasks. These test set case sensitive BLEU scores are obtained using SacreBLEU toolkit1 (Post, 2018). The BLEU score difference between our hard-attention based Transformer model and the original soft-attention based Transformer model indicates the effectiveness of selecting a few relevant source tokens for each target token. The performance gap between our method and sequence loss based Transformer (Wu et al., 2018) shows that the improvements are indeed coming from the hard-attention mechanism. Our approach of incorporating hard-attention into decoder\u2019s top selfattention layer to select relevant tokens yielded better results compared to the Localness SelfAttention (Yang et al., 2018) approach of incorporating localness bias only to lower self-attention layers. It can be noted that our model achieved 29.29 and 42.26 BLEU points on EN-DE and ENFR tasks respectively \u2013 surpassing the previously published models.",
        "sentences": [
            "Main Results .",
            "Table 1 shows the performance of various models on EN-DE and EN-FR translation tasks.",
            "These test set case sensitive BLEU scores are obtained using SacreBLEU toolkit1 (Post, 2018).",
            "The BLEU score difference between our hard-attention based Transformer model and the original soft-attention based Transformer model indicates the effectiveness of selecting a few relevant source tokens for each target token.",
            "The performance gap between our method and sequence loss based Transformer (Wu et al., 2018) shows that the improvements are indeed coming from the hard-attention mechanism.",
            "Our approach of incorporating hard-attention into decoder\u2019s top selfattention layer to select relevant tokens yielded better results compared to the Localness SelfAttention (Yang et al., 2018) approach of incorporating localness bias only to lower self-attention layers.",
            "It can be noted that our model achieved 29.29 and 42.26 BLEU points on EN-DE and ENFR tasks respectively \u2013 surpassing the previously published models."
        ],
        "class_sentence": [
            2,
            1,
            2,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "EN-DE",
                "EN-FR"
            ],
            null,
            [
                "this work Transformer big + hard-attention"
            ],
            [
                "this work Transformer big + hard-attention",
                "Wu et al. (2018) Transformer big + sequence-loss"
            ],
            [
                "this work Transformer big + hard-attention",
                "Yang et al. (2018) Transformer big + localness"
            ],
            [
                "this work Transformer big + hard-attention",
                "EN-DE",
                "EN-FR"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_1",
        "paper_id": "P19-1290",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1305table_5",
        "description": "Layers for Attention Relay. Transformer architecture used in our experiment is with six layers on both encoder and decoder. Attention relay can take place on each layer. Since each layer has eight heads for attention computation, we average the weights of all eight heads in the same layer. We study the attention relay effects on all six layers. The results in Table 5 show that relaying attention on the last layer achieves the best performance.",
        "sentences": [
            "Layers for Attention Relay.",
            "Transformer architecture used in our experiment is with six layers on both encoder and decoder.",
            "Attention relay can take place on each layer.",
            "Since each layer has eight heads for attention computation, we average the weights of all eight heads in the same layer.",
            "We study the attention relay effects on all six layers.",
            "The results in Table 5 show that relaying attention on the last layer achieves the best performance."
        ],
        "class_sentence": [
            2,
            2,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Layer"
            ],
            [
                "Layer"
            ],
            [
                "Layer"
            ],
            [
                "Layer"
            ],
            [
                "6",
                "Layer"
            ]
        ],
        "n_sentence": 6.0,
        "table_id": "table_5",
        "paper_id": "P19-1305",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1309table_3",
        "description": "Table 3 reports the results on the test set for both the Europarl and the UN model in comparison to previous work.9 . Our proposed system outperforms all previous methods by a large margin, obtaining improvements of 10-15 F1 points and showing very consistent performance across different languages, including distant ones.",
        "sentences": [
            "Table 3 reports the results on the test set for both the Europarl and the UN model in comparison to previous work.9 .",
            "Our proposed system outperforms all previous methods by a large margin, obtaining improvements of 10-15 F1 points and showing very consistent performance across different languages, including distant ones."
        ],
        "class_sentence": [
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "Proposed method (Europarl)",
                "Proposed method (UN)"
            ]
        ],
        "n_sentence": 2.0,
        "table_id": "table_3",
        "paper_id": "P19-1309",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1316table_1",
        "description": "We compare the performance of the baseline models (Cordeiro et al., 2016) and Poincare embeddings as a single signal on the reduced version of the three gold standard datasets: RD-R (79 instances), RD++-R (146 instances), FD-R (780 instances) in order to closely examine the influence of Poincare embeddings. Table 1 shows the performance for all the baselines in terms of Spearman's rank correlation. We observe that W2V-CBOW model produces the best performance across all the three datasets and W2V-SG achieves the second-best performance. As noted in the table, the Poincare embeddings on their own perform worse than all the other baselines. Further, since our final model is based on an interpolation between Poincare embeddings and W2VCBOW, we also attempted interpolation between other four baseline models, but the best results were always close to the better of the two models, and are not reported here.",
        "sentences": [
            "We compare the performance of the baseline models (Cordeiro et al., 2016) and Poincare embeddings as a single signal on the reduced version of the three gold standard datasets: RD-R (79 instances), RD++-R (146 instances), FD-R (780 instances) in order to closely examine the influence of Poincare embeddings.",
            "Table 1 shows the performance for all the baselines in terms of Spearman's rank correlation.",
            "We observe that W2V-CBOW model produces the best performance across all the three datasets and W2V-SG achieves the second-best performance.",
            "As noted in the table, the Poincare embeddings on their own perform worse than all the other baselines.",
            "Further, since our final model is based on an interpolation between Poincare embeddings and W2VCBOW, we also attempted interpolation between other four baseline models, but the best results were always close to the better of the two models, and are not reported here."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "Poincare",
                "RD-R",
                "RD++-R",
                "FD-R"
            ],
            null,
            [
                "W2V-CBOW",
                "W2V-SG"
            ],
            [
                "Poincare"
            ],
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_1",
        "paper_id": "P19-1316",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1332table_8",
        "description": "MTL+Copy, DNPG and Adapted DNPG as well as the reference. The evaluators are asked to rank the candidates from 1 (best) to 4 (worst) by their readability, accuracy and surface dissimilarity to the input sentence. The detailed evaluation guide can be found in Appendix B. Table 8 shows the mean rank and inter-annotator agreement (Cohen's kappa) of each model. Adapted DNPG again significantly outperforms MTL+Copy by a large margin (p-value < 0.01). The performance of the original DNPG and MTL+Copy has no significant difference (p-value = 0.18). All of the interannotator agreement is regarded as fair or above.",
        "sentences": [
            "MTL+Copy, DNPG and Adapted DNPG as well as the reference.",
            "The evaluators are asked to rank the candidates from 1 (best) to 4 (worst) by their readability, accuracy and surface dissimilarity to the input sentence.",
            "The detailed evaluation guide can be found in Appendix B.",
            "Table 8 shows the mean rank and inter-annotator agreement (Cohen's kappa) of each model.",
            "Adapted DNPG again significantly outperforms MTL+Copy by a large margin (p-value < 0.01).",
            "The performance of the original DNPG and MTL+Copy has no significant difference (p-value = 0.18).",
            "All of the interannotator agreement is regarded as fair or above."
        ],
        "class_sentence": [
            1,
            2,
            0,
            1,
            1,
            1,
            1
        ],
        "header_mention": [
            [
                "MTL+Copy",
                "Na\u00efve DNPG",
                "Adapted DNPG"
            ],
            null,
            null,
            [
                "Mean Rank",
                "Agreement"
            ],
            [
                "Adapted DNPG",
                "MTL+Copy"
            ],
            [
                "Na\u00efve DNPG",
                "MTL+Copy"
            ],
            [
                "Agreement"
            ]
        ],
        "n_sentence": 7.0,
        "table_id": "table_8",
        "paper_id": "P19-1332",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1370table_2",
        "description": "Answer to Q2: as a case study of co-teaching with two networks in different capabilities, we initialize model A and model B with DAM and SMN respectively, and select teaching with dynamic margins for Douban and teaching with dynamic data curriculum for ECD (i.e., the best strategies for the two data sets when co-teaching is initialized with one network). Table 2 shows comparison between models before/after co-teaching. We find that co-teaching is still effective when starting from two networks, as both SMN and DAM get improved on the two data sets. Despite the improvement, it is still better to learn the two networks one by one, as co-teaching with two networks cannot bring more improvement than coteaching with one network, and the performance.",
        "sentences": [
            "Answer to Q2: as a case study of co-teaching with two networks in different capabilities, we initialize model A and model B with DAM and SMN respectively, and select teaching with dynamic margins for Douban and teaching with dynamic data curriculum for ECD (i.e., the best strategies for the two data sets when co-teaching is initialized with one network).",
            "Table 2 shows comparison between models before/after co-teaching.",
            "We find that co-teaching is still effective when starting from two networks, as both SMN and DAM get improved on the two data sets.",
            "Despite the improvement, it is still better to learn the two networks one by one, as co-teaching with two networks cannot bring more improvement than coteaching with one network, and the performance."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2
        ],
        "header_mention": [
            [
                "DAM-Co-teaching",
                "SMN-Co-teaching"
            ],
            null,
            [
                "DAM-Co-teaching",
                "SMN-Co-teaching"
            ],
            null
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "P19-1370",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1402table_1",
        "description": "Experimental Results . Table 1 lists the performance of HiCE and baselines with different numbers of context sentences. In particular, our method (HiCE+Morph+MAML)6 achieves the best performance among all the other baseline methods under most settings. Compared with the current state-of-the-art method, `a la carte, the relative improvements (i.e., the performance difference divided by the baseline performance) of HiCE are 4.0%, 5.4% and 9.3% in terms of 2,4,6shot learning, respectively. We also compare our results with that of the oracle embedding, which is the embeddings trained from DT , and used as ground-truth to train HiCE. This results can be regarded as an upper bound. As is shown, when the number of context sentences (K) is relatively large (i.e., K = 6), the performance of HiCE is on a par with the upper bound (Oracle Embedding) and the relative performance difference is merely 2.7%. This indicates the significance of using an advanced aggregation model.",
        "sentences": [
            "Experimental Results .",
            "Table 1 lists the performance of HiCE and baselines with different numbers of context sentences.",
            "In particular, our method (HiCE+Morph+MAML)6 achieves the best performance among all the other baseline methods under most settings.",
            "Compared with the current state-of-the-art method, `a la carte, the relative improvements (i.e., the performance difference divided by the baseline performance) of HiCE are 4.0%, 5.4% and 9.3% in terms of 2,4,6shot learning, respectively.",
            "We also compare our results with that of the oracle embedding, which is the embeddings trained from DT , and used as ground-truth to train HiCE.",
            "This results can be regarded as an upper bound.",
            "As is shown, when the number of context sentences (K) is relatively large (i.e., K = 6), the performance of HiCE is on a par with the upper bound (Oracle Embedding) and the relative performance difference is merely 2.7%.",
            "This indicates the significance of using an advanced aggregation model."
        ],
        "class_sentence": [
            2,
            2,
            1,
            1,
            2,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            [
                "HiCE + Morph + MAML"
            ],
            [
                "HiCE + Morph + MAML"
            ],
            [
                "Oracle Embedding  "
            ],
            null,
            [
                "HiCE w/o Morph ",
                "HiCE + Morph ",
                "HiCE + Morph + Fine-tune",
                "HiCE + Morph + MAML",
                "Oracle Embedding  "
            ],
            null
        ],
        "n_sentence": 8.0,
        "table_id": "table_1",
        "paper_id": "P19-1402",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1410table_1",
        "description": "Results. Table 1 shows our segmentation results. As mentioned in Section 4, we implemented three key improvements on the top of (Li et al., 2018). Using encoder hidden states as decoder inputs and adopting dot product as the attention score function together gives 0.40%-7.29% relative improvement  in F1 over  the  first  four  baselines. Using ELMo,  our  segmenter  outperforms  all  the  baselines  in  all  three  measures. We  achieve  2.3%-11.9%, 2.4%-11.3% and 2.3%-12.3% relative improvements  inF1,  Recall  and  Precision,  respectively.   Jointly  training  with  the  parser  improves this further (95.55 F1). It is worthwhile to mention that our segmenter\u2019s performance of 95.55 F1 is very close to the human agreement of 98.3 F1. ELMo, as a transfer learning method, provides notable improvements. A similar observation was reported in (Wang et al., 2018). Surprisingly, the results with BERT were not as good. We suspect this is due to BERT\u2019s special tokenization.",
        "sentences": [
            "Results.",
            "Table 1 shows our segmentation results.",
            "As mentioned in Section 4, we implemented three key improvements on the top of (Li et al., 2018).",
            "Using encoder hidden states as decoder inputs and adopting dot product as the attention score function together gives 0.40%-7.29% relative improvement  in F1 over  the  first  four  baselines.",
            "Using ELMo,  our  segmenter  outperforms  all  the  baselines  in  all  three  measures.",
            "We  achieve  2.3%-11.9%, 2.4%-11.3% and 2.3%-12.3% relative improvements  inF1,  Recall  and  Precision,  respectively.   Jointly  training  with  the  parser  improves this further (95.55 F1).",
            "It is worthwhile to mention that our segmenter\u2019s performance of 95.55 F1 is very close to the human agreement of 98.3 F1.",
            "ELMo, as a transfer learning method, provides notable improvements.",
            "A similar observation was reported in (Wang et al., 2018).",
            "Surprisingly, the results with BERT were not as good.",
            "We suspect this is due to BERT\u2019s special tokenization."
        ],
        "class_sentence": [
            0,
            1,
            2,
            2,
            1,
            1,
            1,
            2,
            2,
            1,
            2
        ],
        "header_mention": [
            null,
            null,
            null,
            null,
            [
                "Our Segmenter"
            ],
            [
                "Our Segmenter",
                "Precision",
                "Recall",
                "F1"
            ],
            [
                "Our Segmenter",
                "Human Agreement",
                "F1"
            ],
            [
                "Pointer Net (ELMo)"
            ],
            null,
            [
                "Pointer Net (BERT)"
            ],
            [
                "Pointer Net (BERT)"
            ]
        ],
        "n_sentence": 11.0,
        "table_id": "table_1",
        "paper_id": "P19-1410",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1410table_2",
        "description": "Thanks to the pointer network as the backbone of our model, we are able to train our segmenter and parser jointly by sharing the same encoder. The last row of Table 2 shows the results when we train the model jointly, and feed the parser with gold EDU segmentation during inference. The performance is improved further with joint training, achieving 97.44, 91.34, 81.70 F1 score, in Span, Nuclearity and Relation, respectively. The results accord with our assumption that discourse segmentation and parsing may benefit from each other. Our parser surpasses human agreement in span and nuclearity. We are also approaching human agreement in the most difficult task of relation labeling. We show a confusion matrix for the relation labels in Figure 5. We see that our model gets confused between relations that are semantically similar (e.g., CAUSE vs. EXPLANATION, COMPARISON vs. CONTRAST, and TEMPORAL vs. JOINT).",
        "sentences": [
            "Thanks to the pointer network as the backbone of our model, we are able to train our segmenter and parser jointly by sharing the same encoder.",
            "The last row of Table 2 shows the results when we train the model jointly, and feed the parser with gold EDU segmentation during inference.",
            "The performance is improved further with joint training, achieving 97.44, 91.34, 81.70 F1 score, in Span, Nuclearity and Relation, respectively.",
            "The results accord with our assumption that discourse segmentation and parsing may benefit from each other.",
            "Our parser surpasses human agreement in span and nuclearity.",
            "We are also approaching human agreement in the most difficult task of relation labeling.",
            "We show a confusion matrix for the relation labels in Figure 5.",
            "We see that our model gets confused between relations that are semantically similar (e.g., CAUSE vs. EXPLANATION, COMPARISON vs. CONTRAST, and TEMPORAL vs. JOINT)."
        ],
        "class_sentence": [
            2,
            1,
            1,
            2,
            1,
            2,
            0,
            0
        ],
        "header_mention": [
            null,
            [
                "+ Joint training"
            ],
            [
                "+ Joint training",
                "Span",
                "Nuclearity",
                "Relation"
            ],
            null,
            [
                "Our Parser",
                "Human Agreement",
                "Span",
                "Nuclearity"
            ],
            null,
            null,
            null
        ],
        "n_sentence": 8.0,
        "table_id": "table_2",
        "paper_id": "P19-1410",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1427table_4",
        "description": "The results are shown in Table 4. From the table, we see that using SIMILE performs the best when using BLEU and SIM as evaluation metrics for  all  four  languages. It  is  interesting  that  using SIMILE in the cost leads to larger BLEU improvements than using BLEU alone,  the reasons for which we examine further in the following sections. It is important to emphasize that increasing BLEU was not the goal of our proposed method, human evaluations  were  our  target,  but  this  is  a welcome surprise.  Similarly, using BLEU as the cost function leads to large gains in SIM, though these gains are not as large as when using SIMILE in training.",
        "sentences": [
            "The results are shown in Table 4.",
            "From the table, we see that using SIMILE performs the best when using BLEU and SIM as evaluation metrics for  all  four  languages.",
            "It  is  interesting  that  using SIMILE in the cost leads to larger BLEU improvements than using BLEU alone,  the reasons for which we examine further in the following sections.",
            "It is important to emphasize that increasing BLEU was not the goal of our proposed method, human evaluations  were  our  target,  but  this  is  a welcome surprise.",
            " Similarly, using BLEU as the cost function leads to large gains in SIM, though these gains are not as large as when using SIMILE in training."
        ],
        "class_sentence": [
            1,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "SIMILE",
                "BLEU",
                "SIM"
            ],
            [
                "SIMILE",
                "BLEU"
            ],
            [
                "BLEU"
            ],
            [
                "SIM",
                "SIMILE"
            ]
        ],
        "n_sentence": 5.0,
        "table_id": "table_4",
        "paper_id": "P19-1427",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1444table_4",
        "description": "Table 4 presents the ablation study results. It is clear that our base model significantly outperforms SyntaxSQLNet, SyntaxSQLNet(augment) and SyntaxSQLNet(BERT). Performing schema linking (\u2018+SL\u2019) brings about 8.5% and 6.4% absolute improvement on IRNet and IRNet(BERT). Predicting columns in the WHERE clause is known to be challenging (Yavuz et al., 2018). The F1 score on the WHERE clause increases by 12.5% when IRNet performs schema linking. The significant improvement demonstrates the effectiveness of schema linking in addressing the lexical problem. Using the memory augmented pointer network (\u2018+MEM\u2019) further improves the performance of IRNet and IRNet(BERT). We observe that the vanilla pointer network is prone to selecting same columns during synthesis. The number of examples suffering from this problem decreases by 70%, when using the memory augmented pointer network. At last, adopting the coarse-to-fine framework (\u2018+CF\u2019) can further boost performance.",
        "sentences": [
            "Table 4 presents the ablation study results.",
            "It is clear that our base model significantly outperforms SyntaxSQLNet, SyntaxSQLNet(augment) and SyntaxSQLNet(BERT).",
            "Performing schema linking (\u2018+SL\u2019) brings about 8.5% and 6.4% absolute improvement on IRNet and IRNet(BERT).",
            "Predicting columns in the WHERE clause is known to be challenging (Yavuz et al., 2018).",
            "The F1 score on the WHERE clause increases by 12.5% when IRNet performs schema linking.",
            "The significant improvement demonstrates the effectiveness of schema linking in addressing the lexical problem.",
            "Using the memory augmented pointer network (\u2018+MEM\u2019) further improves the performance of IRNet and IRNet(BERT).",
            "We observe that the vanilla pointer network is prone to selecting same columns during synthesis.",
            "The number of examples suffering from this problem decreases by 70%, when using the memory augmented pointer network.",
            "At last, adopting the coarse-to-fine framework (\u2018+CF\u2019) can further boost performance."
        ],
        "class_sentence": [
            1,
            2,
            1,
            2,
            2,
            2,
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "+SL",
                "IRNet",
                "IRNet(BERT)"
            ],
            null,
            null,
            null,
            [
                "+SL + MEM",
                "IRNet",
                "IRNet(BERT)"
            ],
            null,
            null,
            [
                "+SL + MEM + CF"
            ]
        ],
        "n_sentence": 10.0,
        "table_id": "table_4",
        "paper_id": "P19-1444",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1458table_5",
        "description": "7.2 Low-Shot Learning . Low-shot learning refers to the practice of feeding a model with a small amount of training data, contrary to the normal practice of using a large amount of data. We chose the Yahoo dataset for this experiment due to its small size. Experimental results in Table 5 show that, with only 1 3 of the total dataset, ULMFiT and BERT perform better than task-specific models, while BCN+ELMo shows a comparable result. Clearly, this shows that the models have learned significantly during the transfer learning process.",
        "sentences": [
            "7.2 Low-Shot Learning .",
            "Low-shot learning refers to the practice of feeding a model with a small amount of training data, contrary to the normal practice of using a large amount of data.",
            "We chose the Yahoo dataset for this experiment due to its small size.",
            "Experimental results in Table 5 show that, with only 1 3 of the total dataset, ULMFiT and BERT perform better than task-specific models, while BCN+ELMo shows a comparable result.",
            "Clearly, this shows that the models have learned significantly during the transfer learning process."
        ],
        "class_sentence": [
            2,
            2,
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "Yahoo Binary"
            ],
            [
                "ULMFiT",
                "BERTBASE",
                "BCN+ELMo"
            ],
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_5",
        "paper_id": "P19-1458",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1475table_2",
        "description": "Results on Recast MNLI and JOCI. Table 2 shows results on the recast MNLI and JOCI datasets. We find that for the two synthetic MNLI datasets, margin-loss performs similarly to cross entropy log-loss. Shifting to the JOCI datasets, with less extreme (contradiction / entailed) hypotheses, especially in the adversarial JOCI2 variant, marginloss outperforms log-loss.",
        "sentences": [
            "Results on Recast MNLI and JOCI.",
            "Table 2 shows results on the recast MNLI and JOCI datasets.",
            "We find that for the two synthetic MNLI datasets, margin-loss performs similarly to cross entropy log-loss.",
            "Shifting to the JOCI datasets, with less extreme (contradiction / entailed) hypotheses, especially in the adversarial JOCI2 variant, marginloss outperforms log-loss."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "MNLI 1",
                "JOCI 1",
                "MNLI 2",
                "JOCI 2"
            ],
            [
                "MNLI 1",
                "MNLI 2",
                "Margin loss",
                "Log loss"
            ],
            [
                "JOCI 1",
                "JOCI 2",
                "Margin loss",
                "Log loss"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "P19-1475",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1475table_3",
        "description": "Results on COPA. Table 3 shows our results on COPA. Compared with previous state-of-theart knowledge-driven baseline methods, a BERT model trained with a log-loss achieves better performance. When training the BERT model with a margin-loss instead of a log-loss, our method gets the new state-of-the-art result on the established COPA splits, with an accuracy of 75.4%.3 Analyses Table 4 shows some examples from the MNLI1, JOCI1 and COPA datasets, with scores 3 We exclude a blog-posted GPT result, which comes.",
        "sentences": [
            "Results on COPA.",
            "Table 3 shows our results on COPA.",
            "Compared with previous state-of-theart knowledge-driven baseline methods, a BERT model trained with a log-loss achieves better performance.",
            "When training the BERT model with a margin-loss instead of a log-loss, our method gets the new state-of-the-art result on the established COPA splits, with an accuracy of 75.4%."
        ],
        "class_sentence": [
            2,
            1,
            1,
            1
        ],
        "header_mention": [
            null,
            null,
            [
                "PMI (Jabeen et al., 2014)",
                "PMI EX (Gordon et al., 2011)",
                "CS (Luo et al., 2016)",
                "CS MWP (Sasaki et al., 2017)",
                "BERTlog (ours)"
            ],
            [
                "BERTmargin (ours)",
                "BERTlog (ours)",
                "Acc (%)"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_3",
        "paper_id": "P19-1475",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1487table_2",
        "description": "Table 2 shows results that compare a BERT baseline that uses only the CQA inputs and the same architecture but trained using inputs that contain explanations from CoS-E during training. The BERT baseline model reaches 64% accuracy and adding open-ended human explanations (CoS-E-open-ended) alongside the questions during training results in a 2% boost in accuracy. By generating explanations as described in Section 4.1, we can give the commonsense question answering model access to an explanation that is not conditioned on the ground truth. These explanations (CAGE-reasoning) can be provided during both training and validation and increases the accuracy to 72%.",
        "sentences": [
            "Table 2 shows results that compare a BERT baseline that uses only the CQA inputs and the same architecture but trained using inputs that contain explanations from CoS-E during training.",
            "The BERT baseline model reaches 64% accuracy and adding open-ended human explanations (CoS-E-open-ended) alongside the questions during training results in a 2% boost in accuracy.",
            "By generating explanations as described in Section 4.1, we can give the commonsense question answering model access to an explanation that is not conditioned on the ground truth.",
            "These explanations (CAGE-reasoning) can be provided during both training and validation and increases the accuracy to 72%."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "BERT (baseline)",
                "Accuracy (%)",
                "CoS-E-open-ended"
            ],
            null,
            [
                "CAGE-reasoning"
            ]
        ],
        "n_sentence": 4.0,
        "table_id": "table_2",
        "paper_id": "P19-1487",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1530table_2",
        "description": "Table 2 summarizes the performances of our system and previous ones. These results demonstrate that our system significantly outperforms the previous methods in nonlocal dependency identification. Although the main reason for this is because of the performance of the K&K parser, the important point is that our proposed approximation method enables us to use the K&K parser for the nonlocal dependency identification task. The previous methods that introduce additional operations cannot adopt such parser directly. On the other hand, although post-processing approach can use any parser in pre-processing, our approach outperforms the post-processing approach, even if the pre-processing parser is assumed to always generate gold PTB trees.",
        "sentences": [
            "Table 2 summarizes the performances of our system and previous ones.",
            "These results demonstrate that our system significantly outperforms the previous methods in nonlocal dependency identification.",
            "Although the main reason for this is because of the performance of the K&K parser, the important point is that our proposed approximation method enables us to use the K&K parser for the nonlocal dependency identification task.",
            "The previous methods that introduce additional operations cannot adopt such parser directly.",
            "On the other hand, although post-processing approach can use any parser in pre-processing, our approach outperforms the post-processing approach, even if the pre-processing parser is assumed to always generate gold PTB trees."
        ],
        "class_sentence": [
            1,
            1,
            2,
            2,
            1
        ],
        "header_mention": [
            null,
            [
                "ours"
            ],
            [
                "ours"
            ],
            null,
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_2",
        "paper_id": "P19-1530",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1565table_4",
        "description": "Table 4 shows the results of 500 simulations for each of the comparison systems. Our system with Kernel transition obtains the highest success rate, significantly improving over other approaches. The success rate of the base Retrieval agent is lower than 10%, which proves that a chitchat agent without a target-guided strategy can hardly accomplish our task. The Retrieval-Stgy agent has a relatively high success rate, while taking more turns (6.56) to accomplish this. This is partially due to the lack of coarse-grained keyword modeling and transition. We further note that, in the Kernel system, around 81% of predicted keywords eventually occur in the produced utterances, indicating that the predicted keywords have a great impact on the retrieval module.",
        "sentences": [
            "Table 4 shows the results of 500 simulations for each of the comparison systems.",
            "Our system with Kernel transition obtains the highest success rate, significantly improving over other approaches.",
            "The success rate of the base Retrieval agent is lower than 10%, which proves that a chitchat agent without a target-guided strategy can hardly accomplish our task.",
            "The Retrieval-Stgy agent has a relatively high success rate, while taking more turns (6.56) to accomplish this.",
            "This is partially due to the lack of coarse-grained keyword modeling and transition.",
            "We further note that, in the Kernel system, around 81% of predicted keywords eventually occur in the produced utterances, indicating that the predicted keywords have a great impact on the retrieval module."
        ],
        "class_sentence": [
            1,
            1,
            1,
            1,
            2,
            2
        ],
        "header_mention": [
            null,
            [
                "Ours-Kernel",
                "Succ. (%)"
            ],
            [
                "Retrieval",
                "Succ. (%)"
            ],
            [
                "Retrieval-Stgy",
                "#Turns"
            ],
            null,
            null
        ],
        "n_sentence": 6.0,
        "table_id": "table_4",
        "paper_id": "P19-1565",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1570table_7",
        "description": "Results . Table 7 shows that sense embeddings using context information perform better than all the existing models, except MSSG models (Neelakantan et al., 2015). Also, computing the embeddings of a word using the contextual information improves results by aprox. 0.025, compared to the case when words embeddings are used directly.",
        "sentences": [
            "Results .",
            "Table 7 shows that sense embeddings using context information perform better than all the existing models, except MSSG models (Neelakantan et al., 2015).",
            "Also, computing the embeddings of a word using the contextual information improves results by aprox. 0.025, compared to the case when words embeddings are used directly."
        ],
        "class_sentence": [
            2,
            1,
            1
        ],
        "header_mention": [
            null,
            [
                "WordCtx2Sense (a)",
                "WordCtx2Sense (b)",
                "MSSG.300D.30K",
                "MSSG.300D.6K"
            ],
            [
                "WordCtx2Sense (a)",
                "WordCtx2Sense (b)"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_7",
        "paper_id": "P19-1570",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1581table_2",
        "description": "We also compare the quality of our best dictionary (EU+UFAL+orth) to previous work by running bilingual lexicon induction using the test lexicons of Braune et al. (2018) containing frequent and rare medical words respectively. Accuracies of 1-best and 5-best translations in Table 2 show comparable word translation quality to previous work, although we do not employ any task specific steps in contrast to Braune et al. (2018). Note that our dictionary does not contain some of the rare words of the test lexicons which we ignore during evaluation.",
        "sentences": [
            "We also compare the quality of our best dictionary (EU+UFAL+orth) to previous work by running bilingual lexicon induction using the test lexicons of Braune et al. (2018) containing frequent and rare medical words respectively.",
            "Accuracies of 1-best and 5-best translations in Table 2 show comparable word translation quality to previous work, although we do not employ any task specific steps in contrast to Braune et al. (2018).",
            "Note that our dictionary does not contain some of the rare words of the test lexicons which we ignore during evaluation."
        ],
        "class_sentence": [
            2,
            1,
            2
        ],
        "header_mention": [
            [
                "EU+UFAL+orth",
                "(Braune et al., 2018)",
                "freq",
                "rare"
            ],
            [
                "Acc1",
                "Acc5",
                "(Braune et al., 2018)"
            ],
            null
        ],
        "n_sentence": 3.0,
        "table_id": "table_2",
        "paper_id": "P19-1581",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1603table_1",
        "description": "Table 1 shows the automatic evaluation results of three sentiment analyzers. We find that: (1) The rule-based method RB performs the best. This accords with the fact that story endings in the ROC-Stories corpus are simple and have relatively obvious emotional words. (2) DA can not improve the performance of sentiment analysis in our task compared to RM. We hypothesize that is because the domains of labeled SST corpus and ROCStories corpus differ too much that affects the performance of domain adaptation.",
        "sentences": [
            "Table 1 shows the automatic evaluation results of three sentiment analyzers.",
            "We find that: (1) The rule-based method RB performs the best.",
            "This accords with the fact that story endings in the ROC-Stories corpus are simple and have relatively obvious emotional words.",
            "(2) DA can not improve the performance of sentiment analysis in our task compared to RM.",
            "We hypothesize that is because the domains of labeled SST corpus and ROCStories corpus differ too much that affects the performance of domain adaptation."
        ],
        "class_sentence": [
            1,
            1,
            2,
            1,
            2
        ],
        "header_mention": [
            [
                "Model"
            ],
            [
                "Rule-Based (RB)"
            ],
            null,
            [
                "Domain Adversarial (DA)",
                "Regression Model (RM)"
            ],
            null
        ],
        "n_sentence": 5.0,
        "table_id": "table_1",
        "paper_id": "P19-1603",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1613table_4",
        "description": "Finally, Table 4 shows the F1 score on the test set for distractor setting and full wiki setting on the leaderboard. These include unpublished models that are concurrent to our work. DECOMPRC achieves the best result out of models that report both distractor and full wiki setting.",
        "sentences": [
            "Finally, Table 4 shows the F1 score on the test set for distractor setting and full wiki setting on the leaderboard.",
            "These include unpublished models that are concurrent to our work.",
            "DECOMPRC achieves the best result out of models that report both distractor and full wiki setting."
        ],
        "class_sentence": [
            1,
            2,
            1
        ],
        "header_mention": [
            [
                "Dist F1",
                "Open F1"
            ],
            null,
            [
                "DECOMPRC",
                "Dist F1",
                "Open F1"
            ]
        ],
        "n_sentence": 3.0,
        "table_id": "table_4",
        "paper_id": "P19-1613",
        "valid": 1
    },
    {
        "table_id_paper": "P19-1617table_1",
        "description": "4.2 Main Results . We first present a comparison between baseline models and our DFGN2. Table 1 shows the performance of different models in the private test set of HotpotQA. From the table we can see that our model achieves the second best result on the leaderboard now3 (on March 1st). Besides, the answer performance and the joint performance of our model are competitive against state-of-the-art unpublished models. We also include the result of our model with a revised entity graph whose entities are recognized by a BERT NER model (Devlin et al., 2018). We fine-tune the pre-trained BERT model on the dataset of the CoNLL\u00e2\u20ac\u212203 NER shared task (Sang and De Meulder, 2003) and use it to extract named entities from the input paragraphs. The results show that our model achieves a 1.5% gain in the joint F1-score with the entity graph built from a better entity recognizer.",
        "sentences": [
            "4.2 Main Results .",
            "We first present a comparison between baseline models and our DFGN2.",
            "Table 1 shows the performance of different models in the private test set of HotpotQA.",
            "From the table we can see that our model achieves the second best result on the leaderboard now3 (on March 1st).",
            "Besides, the answer performance and the joint performance of our model are competitive against state-of-the-art unpublished models.",
            "We also include the result of our model with a revised entity graph whose entities are recognized by a BERT NER model (Devlin et al., 2018).",
            "We fine-tune the pre-trained BERT model on the dataset of the CoNLL\u00e2\u20ac\u212203 NER shared task (Sang and De Meulder, 2003) and use it to extract named entities from the input paragraphs.",
            "The results show that our model achieves a 1.5% gain in the joint F1-score with the entity graph built from a better entity recognizer."
        ],
        "class_sentence": [
            0,
            2,
            1,
            1,
            1,
            1,
            2,
            1
        ],
        "header_mention": [
            null,
            null,
            null,
            [
                "DFGN(Ours)"
            ],
            [
                "DFGN(Ours)"
            ],
            [
                "DFGN(Ours)\u2020"
            ],
            [
                "DFGN(Ours)\u2020"
            ],
            [
                "DFGN(Ours)\u2020",
                "DFGN(Ours)"
            ]
        ],
        "n_sentence": 8.0,
        "table_id": "table_1",
        "paper_id": "P19-1617",
        "valid": 1
    }
]